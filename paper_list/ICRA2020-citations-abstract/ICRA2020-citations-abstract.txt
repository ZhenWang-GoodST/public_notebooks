TY  - CONF
TI  - ICRA 2020 Table of Contents
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - i
EP  - lvi
PY  - 2020
DO  - 10.1109/ICRA40945.2020.9196768
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Presents the table of contents/splash page of the proceedings record.
ER  - 

TY  - CONF
TI  - Metrically-Scaled Monocular SLAM using Learned Scale Factors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 43
EP  - 50
AU  - W. N. Greene
AU  - N. Roy
PY  - 2020
KW  - cameras
KW  - graph theory
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - SLAM (robots)
KW  - geometric SLAM factor graph
KW  - SLAM systems
KW  - relative geometry
KW  - learned depth estimation approaches
KW  - learned depth predictions
KW  - image space
KW  - network architecture
KW  - coarse images
KW  - GPU acceleration
KW  - learned metric data
KW  - unary scale factors
KW  - hardware accelerators
KW  - observable epipolar geometry
KW  - monocular SLAM
KW  - learned scale factors
KW  - monocular simultaneous localization and mapping
KW  - hardware acceleration
KW  - neural network
KW  - Simultaneous localization and mapping
KW  - Feature extraction
KW  - Cameras
KW  - Loss measurement
KW  - Neural networks
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9196900
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.
ER  - 

TY  - CONF
TI  - Inertial-Only Optimization for Visual-Inertial Initialization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 51
EP  - 57
AU  - C. Campos
AU  - J. M. M. Montiel
AU  - J. D. Tard√≥s
PY  - 2020
KW  - feature extraction
KW  - least squares approximations
KW  - maximum likelihood estimation
KW  - optimisation
KW  - SLAM (robots)
KW  - EuRoC dataset show
KW  - time visual-inertial initialization
KW  - optimal estimation problem
KW  - maximum-a-posteriori estimation
KW  - algebraic equations
KW  - ad-hoc cost functions
KW  - ORB-SLAM visual-inertial boosting
KW  - inertial-only optimization
KW  - IMU measurement uncertainty
KW  - MAP estimation
KW  - least squares
KW  - Estimation
KW  - Trajectory
KW  - Simultaneous localization and mapping
KW  - Gravity
KW  - Visualization
KW  - Optimization
KW  - Accelerometers
DO  - 10.1109/ICRA40945.2020.9197334
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.
ER  - 

TY  - CONF
TI  - Hierarchical Quadtree Feature Optical Flow Tracking Based Sparse Pose-Graph Visual-Inertial SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 58
EP  - 64
AU  - H. Xie
AU  - W. Chen
AU  - J. Wang
AU  - H. Wang
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - image sequences
KW  - optimisation
KW  - pose estimation
KW  - quadtrees
KW  - pose-graph optimization time cost
KW  - localization accuracy
KW  - sparse pose-graph visual-inertial SLAM algorithms
KW  - hierarchical quadtree feature optical flow tracking algorithm
KW  - SPVIS
KW  - high-precision pose estimation
KW  - computational complexity
KW  - VIO-VI-SLAM system
KW  - GPU
KW  - Optical flow
KW  - Optimization
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Tracking
KW  - Feature extraction
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197278
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate, robust and real-time localization under constrained-resources is a critical problem to be solved. In this paper, we present a new sparse pose-graph visual-inertial SLAM (SPVIS). Unlike the existing methods that are costly to deal with a large number of redundant features and 3D map points, which are inefficient for improving positioning accuracy, we focus on the concise visual cues for high-precision pose estimating. We propose a novel hierarchical quadtree based optical flow tracking algorithm, it achieves high accuracy and robustness within very few concise features, which is only about one fifth features of the state-of-the-art visual-inertial SLAM algorithms. Benefiting from the efficient optical flow tracking, our sparse pose-graph optimization time cost achieves bounded complexity. By selecting and optimizing the informative features in sliding window and local VIO, the computational complexity is bounded, it achieves low time cost in long-term operation. We compare with the state-of-the-art VIO/VI-SLAM systems on the challenging public datasets by the embedded platform without GPUs, the results effectively verify that the proposed method has better real-time performance and localization accuracy.
ER  - 

TY  - CONF
TI  - Keypoint Description by Descriptor Fusion Using Autoencoders
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 65
EP  - 71
AU  - Z. Dai
AU  - X. Huang
AU  - W. Chen
AU  - C. Chen
AU  - L. He
AU  - S. Wen
AU  - H. Zhang
PY  - 2020
KW  - convolutional neural nets
KW  - image fusion
KW  - image matching
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - SLAM (robots)
KW  - keypoint description
KW  - keypoint matching
KW  - computer vision
KW  - visual simultaneous localization and mapping
KW  - SLAM
KW  - matching operation
KW  - descriptor fusion model
KW  - robust keypoint descriptor
KW  - CNN-based descriptors
KW  - DFM architecture
KW  - CNN models
KW  - mean mAP
KW  - HardNet
KW  - DenseNet169
KW  - convolutional neural networks
KW  - Fuses
KW  - Lighting
KW  - Robustness
KW  - Computer vision
KW  - Simultaneous localization and mapping
KW  - Image coding
DO  - 10.1109/ICRA40945.2020.9197205
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.
ER  - 

TY  - CONF
TI  - Towards Noise Resilient SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 72
EP  - 79
AU  - A. Thyagharajan
AU  - O. J. Omer
AU  - D. Mandal
AU  - S. Subramoney
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image sensors
KW  - optimisation
KW  - photometry
KW  - pose estimation
KW  - SLAM (robots)
KW  - stereo image processing
KW  - RGB-D input
KW  - TUM datasets
KW  - EuRoC datasets
KW  - stereo image pairs
KW  - adaptive algorithm
KW  - error vector
KW  - outlier rejection
KW  - computational efficiency
KW  - map-point consensus
KW  - adaptive virtual camera
KW  - noise resilient SLAM
KW  - ORB-SLAM2
KW  - sparse-indirect SLAM systems
KW  - virtual camera location
KW  - axial depth error
KW  - pose optimization
KW  - consensus information
KW  - axial noise
KW  - lateral noise
KW  - depth noise components
KW  - axial components
KW  - lateral components
KW  - noise sources
KW  - scale information
KW  - SLAM frameworks
KW  - depth sensors
KW  - photometric invariance properties
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Measurement
KW  - Feature extraction
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196745
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Sparse-indirect SLAM systems have been dominantly popular due to their computational efficiency and photometric invariance properties. Depth sensors are critical to SLAM frameworks for providing scale information to the 3D world, yet known to be plagued by a wide variety of noise sources, possessing lateral and axial components. In this work, we demonstrate the detrimental impact of these depth noise components on the performance of the state-of-the-art sparse-indirect SLAM system (ORB-SLAM2). We propose (i) Map-Point Consensus based Outlier Rejection (MC-OR) to counter lateral noise, and (ii) Adaptive Virtual Camera (AVC) to combat axial noise accurately. MC-OR utilizes consensus information between multiple sightings of the same landmark to disambiguate noisy depth and filter it out before pose optimization. In AVC, we introduce an error vector as an accurate representation of the axial depth error. We additionally propose an adaptive algorithm to find the virtual camera location for projecting the error used in the objective function of the pose optimization. Our techniques work equally well for stereo image pairs and RGB-D input directly used by sparse-indirect SLAM systems. Our methods were tested on the TUM (RGB-D) and EuRoC (stereo) datasets and we show that they outperform existing state-of-the-art ORB-SLAM2 by 2-3x, especially in sequences critically affected by depth noise.
ER  - 

TY  - CONF
TI  - LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 80
EP  - 86
AU  - K. Ebadi
AU  - Y. Chang
AU  - M. Palieri
AU  - A. Stephens
AU  - A. Hatteland
AU  - E. Heiden
AU  - A. Thakur
AU  - N. Funabiki
AU  - B. Morrell
AU  - S. Wood
AU  - L. Carlone
AU  - A. -a. Agha-mohammadi
PY  - 2020
KW  - distance measurement
KW  - geophysical image processing
KW  - mobile robots
KW  - multi-robot systems
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - terrain mapping
KW  - tunnels
KW  - long corridors
KW  - salient features
KW  - spurious loop closures
KW  - repetitive appearance
KW  - stark contrast
KW  - highly-accurate 3D maps
KW  - underground extraterrestrial worlds
KW  - lidar-based multirobot SLAM system
KW  - DARPA subterranean challenge
KW  - subterranean operation
KW  - accurate lidar-based front-end
KW  - perceptually-degraded subterranean environments
KW  - complex subterranean environments
KW  - off-nominal conditions
KW  - uneven terrains
KW  - slippery terrains
KW  - large-scale autonomous mapping-positioning
KW  - simultaneous localization and mapping
KW  - unknown subterranean environment
KW  - large-scale subterranean environment
KW  - complex subterranean environment
KW  - inaccurate wheel odometry
KW  - disaster response
KW  - flexible back-end
KW  - robust back-end
KW  - tunnel circuit
KW  - Simultaneous localization and mapping
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Base stations
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197082
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.
ER  - 

TY  - CONF
TI  - BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 87
EP  - 93
AU  - A. Harakeh
AU  - M. Smart
AU  - S. L. Waslander
PY  - 2020
KW  - Bayes methods
KW  - Gaussian processes
KW  - neural nets
KW  - object detection
KW  - NonMaximum suppression components
KW  - common object detection datasets
KW  - BayesOD
KW  - minimum Gaussian uncertainty error metric
KW  - minimum Categorical uncertainty error metric
KW  - Bayesian approach
KW  - deep object detectors
KW  - deep neural networks
KW  - uncertainty measures
KW  - output predictions
KW  - detectors nonmaximum suppression stage
KW  - anchor-based object detection
KW  - uncertainty estimation approach
KW  - standard object detector inference
KW  - Uncertainty
KW  - Detectors
KW  - Neural networks
KW  - Bayes methods
KW  - Estimation
KW  - Object detection
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196544
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors nonmaximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77%-13.13% on the minimum Gaussian uncertainty error metric and a reduction of 1.63%-5.23% on the minimum Categorical uncertainty error metric. Code will be released at https://github.com/asharakeh/bayes-od-rc.
ER  - 

TY  - CONF
TI  - Learning Object Placements For Relational Instructions by Hallucinating Scene Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 94
EP  - 100
AU  - O. Mees
AU  - A. Emek
AU  - J. Vertens
AU  - W. Burgard
PY  - 2020
KW  - control engineering computing
KW  - convolutional neural nets
KW  - human-robot interaction
KW  - image classification
KW  - image representation
KW  - learning (artificial intelligence)
KW  - probability
KW  - robot vision
KW  - object placement learning
KW  - relational instructions
KW  - spatial relation
KW  - convolutional neural network
KW  - pixelwise object placement probability estimation
KW  - hallucinated high-level scene representation classification
KW  - pixelwise relational probabilities
KW  - human-robot experiments
KW  - single input image
KW  - learning signal
KW  - 3D models
KW  - Robots
KW  - Training
KW  - Three-dimensional displays
KW  - Natural languages
KW  - Graphical models
KW  - Distribution functions
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9197472
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots coexisting with humans in their environment and performing services for them need the ability to interact with them. One particular requirement for such robots is that they are able to understand spatial relations and can place objects in accordance with the spatial relations expressed by their user. In this work, we present a convolutional neural network for estimating pixelwise object placement probabilities for a set of spatial relations from a single input image. During training, our network receives the learning signal by classifying hallucinated high-level scene representations as an auxiliary task. Unlike previous approaches, our method does not require ground truth data for the pixelwise relational probabilities or 3D models of the objects, which significantly expands the applicability in practical applications. Our results obtained using real-world data and human-robot experiments demonstrate the effectiveness of our method in reasoning about the best way to place objects to reproduce a spatial relation. Videos of our experiments can be found at https://youtu.be/zaZkHTWFMKM.
ER  - 

TY  - CONF
TI  - FADNet: A Fast and Accurate Network for Disparity Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 101
EP  - 107
AU  - Q. Wang
AU  - S. Shi
AU  - S. Zheng
AU  - K. Zhao
AU  - X. Chu
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image matching
KW  - stereo image processing
KW  - deep neural networks
KW  - computer vision
KW  - disparity estimation problem
KW  - stereo matching
KW  - traditional hand-crafted feature based methods
KW  - designed DNNs
KW  - computation resources
KW  - 3D convolution based networks
KW  - real-time applications
KW  - computation-efficient networks
KW  - expression capability
KW  - large-scale datasets
KW  - multiscale predictions
KW  - FADNet
KW  - multiscale weight scheduling training technique
KW  - Estimation
KW  - Convolution
KW  - Correlation
KW  - Three-dimensional displays
KW  - Training
KW  - Feature extraction
KW  - Computer architecture
DO  - 10.1109/ICRA40945.2020.9197031
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https://github.com/HKBU-HPML/FADNet.
ER  - 

TY  - CONF
TI  - Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 108
EP  - 114
AU  - S. Kuutti
AU  - S. Fallah
AU  - R. Bowden
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - adversarial agent training
KW  - learned control policies
KW  - autonomous driving
KW  - deep neural network-driven
KW  - adversarial reinforcement learning agent
KW  - autonomous vehicle problem
KW  - automated black box testing
KW  - safety-critical applications
KW  - control policy
KW  - deep neural networks
KW  - autonomous vehicles
KW  - robot navigation
KW  - robotic arm manipulation
KW  - control problems
KW  - deep learning
KW  - deep control policies
KW  - Testing
KW  - Autonomous vehicles
KW  - Training
KW  - Learning (artificial intelligence)
KW  - Machine learning
KW  - Robots
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197351
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning has become an increasingly common technique for various control problems, such as robotic arm manipulation, robot navigation, and autonomous vehicles. However, the downside of using deep neural networks to learn control policies is their opaque nature and the difficulties of validating their safety. As the networks used to obtain state-of-the-art results become increasingly deep and complex, the rules they have learned and how they operate become more challenging to understand. This presents an issue, since in safety-critical applications the safety of the control policy must be ensured to a high confidence level. In this paper, we propose an automated black box testing framework based on adversarial reinforcement learning. The technique uses an adversarial agent, whose goal is to degrade the performance of the target model under test. We test the approach on an autonomous vehicle problem, by training an adversarial reinforcement learning agent, which aims to cause a deep neural network-driven autonomous vehicle to collide. Two neural networks trained for autonomous driving are compared, and the results from the testing are used to compare the robustness of their learned control policies. We show that the proposed framework is able to find weaknesses in both control policies that were not evident during online testing and therefore, demonstrate a significant benefit over manual testing methods.
ER  - 

TY  - CONF
TI  - TRASS: Time Reversal as Self-Supervision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 115
EP  - 121
AU  - S. Nair
AU  - M. Babaeizadeh
AU  - C. Finn
AU  - S. Levine
AU  - V. Kumar
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - optimal control
KW  - predictive control
KW  - robot programming
KW  - robot vision
KW  - self-supervision technique
KW  - high level supervision
KW  - time-reversal model
KW  - self-supervised model
KW  - goal states
KW  - complex manipulation tasks
KW  - tetris-style block pairs
KW  - visual model predictive control
KW  - robot learning
KW  - RGB camera input
KW  - TRASS
KW  - Time Reversal as Self-Supervision
KW  - Task analysis
KW  - Trajectory
KW  - Robots
KW  - Transmission line measurements
KW  - Visualization
KW  - Predictive models
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9196862
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning based approaches have shown promise in producing robust policies, but require heavy supervision and large number of environment interactions, especially from visual inputs. We propose a novel self-supervision technique that uses time-reversal to provide high level supervision to reach goals. In particular, we introduce the time-reversal model (TRM), a self-supervised model which explores outward from a set of goal states and learns to predict these trajectories in reverse. This provides a high level plan towards goals, allowing us to learn complex manipulation tasks with no demonstrations or exploration at test time. We test our method on the domain of assembly, specifically the mating of tetris-style block pairs. Using our method operating atop visual model predictive control, we are able to assemble tetris blocks on a KuKa IIWA-7 using only uncalibrated RGB camera input, and generalize to unseen block pairs. Project's-page: https://sites.google.com/view/time-reversal.
ER  - 

TY  - CONF
TI  - Advanced BIT* (ABIT*): Sampling-Based Planning with Advanced Graph-Search Techniques
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 130
EP  - 136
AU  - M. P. Strub
AU  - J. D. Gammell
PY  - 2020
KW  - approximation theory
KW  - graph theory
KW  - path planning
KW  - robots
KW  - sampling methods
KW  - search problems
KW  - sampling-based approximation
KW  - Advanced BIT*
KW  - almost-surely asymptotically optimal sampling-based planners
KW  - ABIT*
KW  - unified planning paradigm
KW  - path planning problem
KW  - advanced graph-search techniques
KW  - robotics
KW  - truncated anytime graph-based searches
KW  - RRT*
KW  - single-query
KW  - Planning
KW  - Approximation algorithms
KW  - Search problems
KW  - Path planning
KW  - Robots
KW  - Acceleration
KW  - Conferences
DO  - 10.1109/ICRA40945.2020.9196580
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning is an active area of research essential for many applications in robotics. Popular techniques include graph-based searches and sampling-based planners. These approaches are powerful but have limitations.This paper continues work to combine their strengths and mitigate their limitations using a unified planning paradigm. It does this by viewing the path planning problem as the two subproblems of search and approximation and using advanced graph-search techniques on a sampling-based approximation.This perspective leads to Advanced BIT*. ABIT* combines truncated anytime graph-based searches, such as ATD*, with anytime almost-surely asymptotically optimal sampling-based planners, such as RRT*. This allows it to quickly find initial solutions and then converge towards the optimum in an anytime manner. ABIT* outperforms existing single-query, sampling- based planners on the tested problems in ‚Ñù4 and ‚Ñù8, and was demonstrated on real-world problems with NASA/JPL-Caltech.
ER  - 

TY  - CONF
TI  - Voxel-based General Voronoi Diagram for Complex Data with Application on Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 137
EP  - 143
AU  - S. Dorn
AU  - N. Wolpert
AU  - E. Sch√∂mer
PY  - 2020
KW  - assembly planning
KW  - CAD
KW  - computational geometry
KW  - data structures
KW  - graphics processing units
KW  - path planning
KW  - production engineering computing
KW  - rendering (computer graphics)
KW  - path planning
KW  - motion planning
KW  - assembly sequence planning
KW  - real-world CAD-scenarios
KW  - disassembly path
KW  - GVD
KW  - Voronoi voxel history
KW  - disassembly paths
KW  - general Voronoi diagram graph
KW  - hash table-based data structure
KW  - error-bounded wavefront propagation
KW  - error-bounded GPU render approach
KW  - voxel-based general Voronoi diagram
KW  - roadmap
KW  - representative vehicle data set
KW  - Octrees
KW  - Planning
KW  - Three-dimensional displays
KW  - Approximation algorithms
KW  - Task analysis
KW  - Runtime
DO  - 10.1109/ICRA40945.2020.9196775
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - One major challenge in Assembly Sequence Planning (ASP) for complex real-world CAD-scenarios is to find appropriate disassembly paths for all assembled parts. Such a path places demands on its length and clearance. In the past, it became apparent that planning the disassembly path based on the (approximate) General Voronoi Diagram (GVD) is a good approach to achieve these requirements. But for complex real-world data, every known solution for computing the GVD is either too slow or very memory consuming, even if only approximating the GVD.We present a new approach for computing the approximate GVD and demonstrate its practicability using a representative vehicle data set. We can calculate an approximation of the GVD within minutes and meet the accuracy requirement of some few millimeters for the subsequent path planning. This is achieved by voxelizing the surface with a common error-bounded GPU render approach. We then use an error-bounded wavefront propagation technique and combine it with a novel hash table-based data structure, the so-called Voronoi Voxel History (VVH). On top of the GVD, we present a novel approach for the creation of a General Voronoi Diagram Graph (GVDG) that leads to an extensive roadmap. For the later motion planning task this roadmap can be used to suggest appropriate disassembly paths.
ER  - 

TY  - CONF
TI  - Dynamic Movement Primitives for moving goals with temporal scaling adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 144
EP  - 150
AU  - L. Koutras
AU  - Z. Doulgeri
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - human robot collaboration
KW  - motion profiles
KW  - learned kinematic pattern
KW  - KUKA LWR4+ robot
KW  - DMP
KW  - dynamic movement primitives framework
KW  - adaptive temporal scaling
KW  - static goal
KW  - temporal scaling adaptation
KW  - moving goal
KW  - Trajectory
KW  - Robots
KW  - Dynamics
KW  - Encoding
KW  - Collaboration
KW  - Adaptation models
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196765
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose an augmentation to the Dynamic Movement Primitives (DMP) framework which allows the system to generalize to moving goals without the use of any known or approximation model for estimating the goal's motion. We aim to maintain the demonstrated velocity levels during the execution to the moving goal, generating motion profiles appropriate for human robot collaboration. The proposed method employs a modified version of a DMP, learned by a demonstration to a static goal, with adaptive temporal scaling in order to achieve reaching of the moving goal with the learned kinematic pattern. Only the current position and velocity of the goal are required. The goal's reaching error and its derivative is proved to converge to zero via contraction analysis. The theoretical results are verified by simulations and experiments on a KUKA LWR4+ robot.
ER  - 

TY  - CONF
TI  - Navigating Discrete Difference Equation Governed WMR by Virtual Linear Leader Guided HMPC
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 151
EP  - 157
AU  - C. Huang
AU  - X. Chen
AU  - E. Tang
AU  - M. He
AU  - L. Bu
AU  - S. Qin
AU  - Y. Zeng
PY  - 2020
KW  - difference equations
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - path planning
KW  - predictive control
KW  - reachability analysis
KW  - set theory
KW  - navigating discrete difference equation governed WMR
KW  - virtual linear leader guided HMPC
KW  - model predictive control
KW  - classical wheeled mobile robot navigation problem
KW  - hierarchical MPC
KW  - state-of-the-art MPC
KW  - WMR navigation
KW  - nonexistence
KW  - nontrivial linear system
KW  - under-approximate reachable set
KW  - VLL-MPC
KW  - HMPC structure
KW  - virtual linear system
KW  - under-approximate path
KW  - RRT*
KW  - Navigation
KW  - Linear systems
KW  - Stability analysis
KW  - Planning
KW  - Robots
KW  - Mathematical model
KW  - Nonlinear dynamical systems
DO  - 10.1109/ICRA40945.2020.9197375
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we revisit model predictive control (MPC) for the classical wheeled mobile robot (WMR) navigation problem. We prove that the reachable set based hierarchical MPC (HMPC), a state-of-the-art MPC, cannot handle WMR navigation in theory due to the non-existence of non-trivial linear system with an under-approximate reachable set of WMR. Nevertheless, we propose a virtual linear leader guided MPC (VLL-MPC) to enable HMPC structure. Different from current HMPCs, we use a virtual linear system with an under-approximate path set rather than the traditional trace set to guide the WMR. We provide a valid construction of the virtual linear leader. We prove the stability of VLL-MPC, and discuss its complexity. In the experiment, we demonstrate the advantage of VLL-MPC empirically by comparing it with NMPC, LMPC and anytime RRT* in several scenarios.
ER  - 

TY  - CONF
TI  - Aggregation and localization of simple robots in curved environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 165
EP  - 171
AU  - R. A. Moan
AU  - V. M. Baez
AU  - A. T. Becker
AU  - J. M. O‚ÄôKane
PY  - 2020
KW  - friction
KW  - microrobots
KW  - mobile robots
KW  - path planning
KW  - curved environments
KW  - extremely simple robots
KW  - biomedical applications
KW  - tiny robots moves
KW  - shared external stimulus
KW  - low-friction models
KW  - environment boundaries
KW  - Robot sensing systems
KW  - Collision avoidance
KW  - Computational modeling
KW  - Biological system modeling
KW  - Propulsion
DO  - 10.1109/ICRA40945.2020.9197198
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper is about the closely-related problems of localization and aggregation for extremely simple robots, for which the only available action is to move in a given direction as far as the geometry of the environment allows. Such problems may arise, for example, in biomedical applications, wherein a large group of tiny robots moves in response to a shared external stimulus. Specifically, we extend the prior work on these kinds of problems presenting two algorithms for localization in environments with curved (rather than polygonal) boundaries and under low-friction models of interaction with the environment boundaries. We present both simulations and physical demonstrations to validate the approach.
ER  - 

TY  - CONF
TI  - Stable Control in Climbing and Descending Flight under Upper Walls using Ceiling Effect Model based on Aerodynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 172
EP  - 178
AU  - T. Nishio
AU  - M. Zhao
AU  - F. Shi
AU  - T. Anzai
AU  - K. Kawaharazuka
AU  - K. Okada
AU  - M. Inaba
PY  - 2020
KW  - aerodynamics
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - motion control
KW  - rotors (mechanical)
KW  - stability
KW  - wakes
KW  - ceiling effect model
KW  - flight control stability
KW  - multirotor unmanned aerial vehicles
KW  - rotor thrust
KW  - vertical flight tests
KW  - in unsteady state model based controller
KW  - aerodynamics based thrust model
KW  - vertical climbing
KW  - vertical descending
KW  - wake interaction
KW  - momentum theory
KW  - Rotors
KW  - Data models
KW  - Adaptation models
KW  - Aerodynamics
KW  - Steady-state
KW  - Atmospheric modeling
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197137
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Stable flight control under ceilings is difficult for multirotor Unmanned Aerial Vehicles (UAVs). The wake interaction between rotors and upper walls, called the "ceiling effect", causes an increase of rotor thrust. As a result of the thrust increase, multi-rotors are drawn upward abruptly and collide with ceilings. In previous work, several thrust models of the ceiling effect have been proposed for stable flight under ceilings, assuming that the airflow around rotors is in steady states. However, the airflow around rotors in vertical flight is not in steady states and each thrust model in previous work is skillfully determined based on large amounts of precise experimental data. In this paper, we introduce an aerodynamics-based thrust model and a stable control method under ceilings. This model is derived from the momentum theory and the relationship between vertical climbing/descending rates of rotors and an induced velocity. To confirm our proposed model, we collect thrust data at various vertical rates in flight. In addition, we use only onboard sensors to estimate selfstate for structural inspections. Consequently, we reveal that the proposed model is consistent with the experimental results. Based on an aerodynamic model, we need not collect large amounts of precise experimental data to realize stable flight. Furthermore, the vertical flight tests under ceilings demonstrate that our in-unsteady-state-model-based controller outperforms the conventional steady-state ones.
ER  - 

TY  - CONF
TI  - Motion Primitives-based Path Planning for Fast and Agile Exploration using Aerial Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 179
EP  - 185
AU  - M. Dharmadhikari
AU  - T. Dang
AU  - L. Solanka
AU  - J. Loje
AU  - H. Nguyen
AU  - N. Khedekar
AU  - K. Alexis
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - motion primitives-based path planning
KW  - agile exploration
KW  - aerial robots
KW  - path planning strategy
KW  - microaerial vehicles
KW  - volumetric representation
KW  - collision-tolerant flying robot
KW  - velocity 2.0 m/s
KW  - size 0.8 m
KW  - Vehicle dynamics
KW  - Collision avoidance
KW  - Path planning
KW  - Unmanned aerial vehicles
KW  - Educational robots
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196964
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel path planning strategy for fast and agile exploration using aerial robots. Tailored to the combined need for large-scale exploration of challenging and confined environments, despite the limited endurance of micro aerial vehicles, the proposed planner employs motion primitives to identify admissible paths that search the configuration space, while exploiting the dynamic flight properties of small aerial robots. Utilizing a computationally efficient volumetric representation of the environment, the planner provides fast collision-free and future-safe paths that maximize the expected exploration gain and ensure continuous fast navigation through the unknown environment. The new method is field-verified in a set of deployments relating to subterranean exploration and specifically, in both modern and abandoned underground mines in Northern Nevada utilizing a 0.55m-wide collision-tolerant flying robot exploring with a speed of up to 2m/s and navigating sections with width as small as 0.8m.
ER  - 

TY  - CONF
TI  - Unsupervised Anomaly Detection for Self-flying Delivery Drones
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 186
EP  - 192
AU  - V. Sindhwani
AU  - H. Sidahmed
AU  - K. Choromanski
AU  - B. Jones
PY  - 2020
KW  - aerodynamics
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - regression analysis
KW  - unsupervised anomaly detection
KW  - hybrid aerial vehicles
KW  - machine learning models
KW  - flight profiles
KW  - flight log measurements
KW  - sensor readings
KW  - predictive flight dynamics models
KW  - aircraft aerodynamics
KW  - self-flying delivery drones
KW  - Aerodynamics
KW  - Smoothing methods
KW  - Robustness
KW  - Training
KW  - Anomaly detection
KW  - Optimization
KW  - Aircraft
DO  - 10.1109/ICRA40945.2020.9197074
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel anomaly detection framework for a fleet of hybrid aerial vehicles executing high-speed package pickup and delivery missions. The detection is based on machine learning models of normal flight profiles, trained on millions of flight log measurements of control inputs and sensor readings. We develop a new scalable algorithm for robust regression which can simultaneously fit predictive flight dynamics models while identifying and discarding abnormal flight missions from the training set. The resulting unsupervised estimator has a very high breakdown point and can withstand massive contamination of training data to uncover what normal flight patterns look like, without requiring any form of prior knowledge of aircraft aerodynamics or manual labeling of anomalies upfront. Across many different anomaly types, spanning simple 3sigma statistical thresholds to turbulence and other equipment anomalies, our models achieve high detection rates across the board. Our method consistently outperforms alternative robust detection methods on synthetic benchmark problems. To the best of our knowledge, dynamics modeling of hybrid delivery drones for anomaly detection at the scale of 100 million measurements from 5000 real flight missions in variable flight conditions is unprecedented.
ER  - 

TY  - CONF
TI  - Keyfilter-Aware Real-Time UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 193
EP  - 199
AU  - Y. Li
AU  - C. Fu
AU  - Z. Huang
AU  - Y. Zhang
AU  - J. Pan
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image filtering
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - SLAM (robots)
KW  - keyframe-based simultaneous localization and mapping
KW  - keyfilter restriction
KW  - visual tracking
KW  - background distraction
KW  - filter corruption
KW  - boundary effect
KW  - unmanned aerial vehicle
KW  - correlation filter-based tracking
KW  - keyfilter-aware real-time UAV object tracking
KW  - Unmanned aerial vehicles
KW  - Correlation
KW  - Visualization
KW  - Object tracking
KW  - Frequency-domain analysis
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196943
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.
ER  - 

TY  - CONF
TI  - Aerial Regrasping: Pivoting with Transformable Multilink Aerial Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 200
EP  - 207
AU  - F. Shi
AU  - M. Zhao
AU  - M. Murooka
AU  - K. Okada
AU  - M. Inaba
PY  - 2020
KW  - aerospace robotics
KW  - dexterous manipulators
KW  - grippers
KW  - motion control
KW  - remotely operated vehicles
KW  - stability
KW  - aerial regrasping
KW  - aerial manipulator
KW  - dexterous manipulation
KW  - transformable multilink aerial robot
KW  - transformable multilink drone
KW  - grasping stability
KW  - thrust force
KW  - continous grasping force
KW  - admittance controller
KW  - impedance controller
KW  - contact aware regrasping
KW  - Task analysis
KW  - Force
KW  - Grasping
KW  - Unmanned aerial vehicles
KW  - Rotors
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196576
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Regrasping is one of the most common and important manipulation skills used in our daily life. However, aerial regrasping has not been seriously investigated yet, since most of the aerial manipulator lacks dexterous manipulation abilities except for the basic pick-and-place. In this paper, we focus on pivoting a long box, which is one of the most classical problems among regrasping researches, using a transformable multilink aerial robot. First, we improve our previous controller by compensating for the external wrench. Second, we optimize the joints configuration of our transformable multilink drone for stable grasping form under the constraints of thrust force and joints effort. Third, we sequentially optimize the grasping force in the pivoting process. The optimization goal is to generate continous grasping force whilst maximizing the friction force in case of the downwash, which would influence the grasped object and is difficult to model. Fourth, we develop the impedance controller in joint space and admittance controller in task space. As far as we know, it is the first research to achieve extrinsic contact-aware regrasping task on aerial robots.
ER  - 

TY  - CONF
TI  - Grounding Language to Landmarks in Arbitrary Outdoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 208
EP  - 215
AU  - M. Berg
AU  - D. Bayazit
AU  - R. Mathew
AU  - A. Rotter-Aboyoun
AU  - E. Pavlick
AU  - S. Tellex
PY  - 2020
KW  - human-robot interaction
KW  - mobile robots
KW  - natural language processing
KW  - path planning
KW  - robot vision
KW  - natural language phrases
KW  - arbitrary outdoor environments
KW  - urban environments
KW  - natural language commands
KW  - robot control
KW  - semantic similarities
KW  - Natural languages
KW  - Semantics
KW  - Robots
KW  - Grounding
KW  - Training
KW  - Planning
KW  - Databases
DO  - 10.1109/ICRA40945.2020.9197068
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots operating in outdoor, urban environments need the ability to follow complex natural language commands which refer to never-before-seen landmarks. Existing approaches to this problem are limited because they require training a language model for the landmarks of a particular environment before a robot can understand commands referring to those landmarks. To generalize to new environments outside of the training set, we present a framework that parses references to landmarks, then assesses semantic similarities between the referring expression and landmarks in a predefined semantic map of the world, and ultimately translates natural language commands to motion plans for a drone. This framework allows the robot to ground natural language phrases to landmarks in a map when both the referring expressions to landmarks and the landmarks themselves have not been seen during training. We test our framework with a 14-person user evaluation demonstrating an end-to-end accuracy of 76.19% in an unseen environment. Subjective measures show that users find our system to have high performance and low workload. These results demonstrate our approach enables untrained users to control a robot in large unseen outdoor environments with unconstrained natural language.
ER  - 

TY  - CONF
TI  - Deep Merging: Vehicle Merging Controller Based on Deep Reinforcement Learning with Embedding Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 216
EP  - 221
AU  - I. Nishitani
AU  - H. Yang
AU  - R. Guo
AU  - S. Keshavamurthy
AU  - K. Oguchi
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - traffic conditions
KW  - traffic flow
KW  - Deep Merging
KW  - vehicle Merging controller
KW  - embedding network
KW  - highway merging sections
KW  - lane change
KW  - vehicle controller
KW  - merging efficiency
KW  - merging section
KW  - target vehicle speed
KW  - controlled vehicle speed
KW  - deep reinforcement learning network architecture
KW  - learning efficiency
KW  - merging behavior
KW  - Merging
KW  - Machine learning
KW  - Feature extraction
KW  - Acceleration
KW  - Road transportation
KW  - Vehicle dynamics
KW  - Network architecture
DO  - 10.1109/ICRA40945.2020.9197559
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Vehicles at highway merging sections must make lane changes to join the highway. This lane change can generate congestion. To reduce congestion, vehicles should merge so as not to affect traffic flow as much as possible. In our study, we propose a vehicle controller called Deep Merging that uses deep reinforcement learning to improve the merging efficiency of vehicles while considering the impact on traffic flow. The system uses the images of a merging section as input to output the target vehicle speed. Moreover, an embedding network for estimating the controlled vehicle speed is introduced to the deep reinforcement learning network architecture to improve the learning efficiency. In order to show the effectiveness of the proposed method, the merging behavior and traffic conditions in several situations are verified by experiments using a traffic simulator. Through these experiments, it is confirmed that the proposed method enables controlled vehicles to effectively merge without adversely affecting to the traffic flow.
ER  - 

TY  - CONF
TI  - Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 222
EP  - 228
AU  - S. Chadwick
AU  - P. Newman
PY  - 2020
KW  - data analysis
KW  - object detection
KW  - radar imaging
KW  - road vehicles
KW  - supervised learning
KW  - traffic engineering computing
KW  - noisy labels
KW  - noise-aware training techniques
KW  - training data
KW  - weakly supervised vehicle detection
KW  - radar labels
KW  - object detector
KW  - image-based vehicle detection
KW  - Training
KW  - Noise measurement
KW  - Training data
KW  - Radar imaging
KW  - Labeling
KW  - Lenses
DO  - 10.1109/ICRA40945.2020.9196855
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It has been demonstrated that the performance of an object detector degrades when it is used outside the domain of the data used to train it. However, obtaining training data for a new domain can be time consuming and expensive. In this work we demonstrate how a radar can be used to generate plentiful (but noisy) training data for image-based vehicle detection. We then show that the performance of a detector trained using the noisy labels can be considerably improved through a combination of noise-aware training techniques and relabelling of the training data using a second viewpoint. In our experiments, using our proposed process improves average precision by more than 17 percentage points when training from scratch and 10 percentage points when fine-tuning a pre-trained model.
ER  - 

TY  - CONF
TI  - Robust Lane Detection with Binary Integer Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 229
EP  - 235
AU  - K. Brandes
AU  - A. Wang
AU  - R. Shah
PY  - 2020
KW  - driver information systems
KW  - integer programming
KW  - object detection
KW  - road safety
KW  - road vehicles
KW  - vehicle dynamics
KW  - false positive cone detections
KW  - binary integer optimization
KW  - competition rules
KW  - average cone spacings
KW  - minimum track width
KW  - robust lane detection
KW  - Formula Student Driverless
KW  - FSD
KW  - student teams
KW  - autonomous racecar
KW  - main dynamic event
KW  - unknown track
KW  - Automobiles
KW  - Optimization
KW  - Robustness
KW  - Meters
KW  - Roads
KW  - Real-time systems
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197098
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Formula Student Driverless (FSD) is a competition where student teams compete to build an autonomous racecar. The main dynamic event in FSD is trackdrive, where the racecar traverses an unknown track with lanes demarcated by cones. One major challenge of the event is to determine the boundaries of the lane from cones perceived online despite false positive cone detections and sharp turns. We present a binary integer optimization to address this problem by leveraging a priori knowledge from competition rules on parameters such as average cone spacings and minimum track width. In this paper, we describe our approach, and analyze its latency, accuracy, and robustness to false positive cone detections. This approach is used on-board to solve the lane detection problem during the competition in real-time.
ER  - 

TY  - CONF
TI  - A Synchronization Approach for Achieving Cooperative Adaptive Cruise Control Based Non-Stop Intersection Passing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 236
EP  - 242
AU  - Z. Liu
AU  - H. Wei
AU  - H. Hu
AU  - C. Suo
AU  - H. Wang
AU  - H. Li
AU  - Y. -H. Liu
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - distributed control
KW  - Lyapunov methods
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - road traffic control
KW  - road vehicles
KW  - stability
KW  - velocity control
KW  - synchronization approach
KW  - adaptive cruise control based nonstop intersection passing
KW  - intelligent vehicles
KW  - cruise control performance
KW  - traffic congestion
KW  - increasing traffic flow capacity
KW  - CACC problem
KW  - synchronization control
KW  - spatial-temporal synchronization mechanism
KW  - vehicle platoon control
KW  - robust CACC
KW  - cross-coupling based space synchronization mechanism
KW  - distributed control algorithm
KW  - single-lane CACC
KW  - vehicle-to-vehicle communications
KW  - autonomous vehicles
KW  - desired platoon trajectory
KW  - expected inter-vehicle distance
KW  - enter-time scheduling mechanism
KW  - high-level intersection control strategy
KW  - Lyapunov-based time-domain stability analysis approach
KW  - traditional string stability based approach
KW  - CACC system
KW  - Synchronization
KW  - Stability analysis
KW  - Cruise control
KW  - Robustness
KW  - Autonomous vehicles
KW  - Motion control
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9196991
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Cooperative adaptive cruise control (CACC) of intelligent vehicles contributes to improving cruise control performance, reducing traffic congestion, saving energy and increasing traffic flow capacity. In this paper, we resolve the CACC problem from the viewpoint of synchronization control, our main idea is to introduce the spatial-temporal synchronization mechanism into vehicle platoon control to achieve the robust CACC and to further realize the non-stop intersection control. Firstly, by introducing the cross-coupling based space synchronization mechanism, a distributed control algorithm is presented to achieve the single-lane CACC in the presence of vehicle-to-vehicle (V2V) communications, which enables autonomous vehicles to track the desired platoon trajectory while synchronizing their longitudinal velocities to keeping the expected inter-vehicle distance. Secondly, by designing the enter-time scheduling mechanism (temporal synchronization), a high-level intersection control strategy is proposed to command vehicles to form a virtual platoon to pass through the intersection without stopping. Thirdly, a Lyapunov-based time-domain stability analysis approach is presented. Compared with the traditional string stability based approach, the proposed approach guarantees the global asymptotical convergence of the proposed CACC system. Experiments in the small-scale simulated system demonstrate the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - Urban Driving with Conditional Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 251
EP  - 257
AU  - J. Hawke
AU  - R. Shen
AU  - C. Gurau
AU  - S. Sharma
AU  - D. Reda
AU  - N. Nikolov
AU  - P. Mazur
AU  - S. Micklethwaite
AU  - N. Griffiths
AU  - A. Shah
AU  - A. Kndall
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - decision making
KW  - driver information systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - road traffic
KW  - road vehicles
KW  - real-world urban autonomous driving
KW  - human driving demonstrations
KW  - user-defined route
KW  - single camera view
KW  - heavily cropped frames
KW  - lateral control
KW  - longitudinal control
KW  - real-world complexities
KW  - end-to-end conditional imitation learning approach
KW  - urban routes
KW  - simple traffic
KW  - autonomous vehicle
KW  - European urban streets
KW  - urban driving
KW  - hand-crafting generalised decision-making rules
KW  - Cameras
KW  - Sensor fusion
KW  - Autonomous vehicles
KW  - Roads
KW  - Computational modeling
KW  - Aerospace electronics
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197408
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.
ER  - 

TY  - CONF
TI  - Vehicle Localization Based on Visual Lane Marking and Topological Map Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 258
EP  - 264
AU  - R. Asghar
AU  - M. Garz√≥n
AU  - J. Lussereau
AU  - C. Laugier
PY  - 2020
KW  - image filtering
KW  - image fusion
KW  - image matching
KW  - iterative methods
KW  - Kalman filters
KW  - nonlinear filters
KW  - road vehicles
KW  - traffic engineering computing
KW  - visual lane marking
KW  - topological map matching
KW  - autonomous vehicle navigation
KW  - driver assistance systems
KW  - online vehicle localization
KW  - distinct map matching algorithms
KW  - visual lane tracker
KW  - map matching algorithm
KW  - grid map
KW  - iterative closest point based lane level map matching
KW  - Roads
KW  - Iterative closest point algorithm
KW  - Global Positioning System
KW  - Dead reckoning
KW  - Sensors
KW  - Visualization
KW  - Cameras
KW  - Map Relative Localization
KW  - Topological Map Matching
KW  - Lane Level Matching
KW  - Autonomous Vehicles
DO  - 10.1109/ICRA40945.2020.9197543
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate and reliable localization is crucial to autonomous vehicle navigation and driver assistance systems. This paper presents a novel approach for online vehicle localization in a digital map. Two distinct map matching algorithms are proposed: i) Iterative Closest Point (ICP) based lane level map matching is performed with visual lane tracker and grid map ii) decision-rule based approach is used to perform topological map matching. Results of both the map matching algorithms are fused together with GPS and dead reckoning using Extended Kalman Filter to estimate vehicle's pose relative to the map. The proposed approach has been validated on real life conditions on an equipped vehicle. Detailed analysis of the experimental results show improved localization using the two aforementioned map matching algorithms.
ER  - 

TY  - CONF
TI  - RISE: A Novel Indoor Visual Place Recogniser
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 265
EP  - 271
AU  - C. S√°nchez-Belenguer
AU  - E. Wolfart
AU  - V. Sequeira
PY  - 2020
KW  - cameras
KW  - image retrieval
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - indoor visual place recogniser
KW  - deep learning
KW  - image retrieval
KW  - image similarity metric
KW  - 3D laser sensor
KW  - calibrated spherical camera
KW  - deep neural network
KW  - geo-referenced images
KW  - data collection stage
KW  - 3D laser measurements
KW  - spherical panoramas
KW  - indoor areas
KW  - observed pixels
KW  - image mapping
KW  - query image
KW  - RISE
KW  - indoor visual place recognition problem
DO  - 10.1109/ICRA40945.2020.9196871
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a new technique to solve the Indoor Visual Place Recognition problem from the Deep Learning perspective. It consists on an image retrieval approach supported by a novel image similarity metric. Our work uses a 3D laser sensor mounted on a backpack with a calibrated spherical camera i) to generate the data for training the deep neural network and ii) to build a database of geo-referenced images for an environment. The data collection stage is fully automatic and requires no user intervention for labelling. Thanks to the 3D laser measurements and the spherical panoramas, we can efficiently survey large indoor areas in a very short time. The underlying 3D data associated to the map allows us to define the similarity between two training images as the geometric overlap between the observed pixels. We exploit this similarity metric to effectively train a CNN that maps images into compact embeddings. The goal of the training is to ensure that the L2 distance between the embeddings associated to two images is small when they are observing the same place and large when they are observing different places. After the training, similarities between a query image and the geo-referenced images in the database are efficiently retrieved by performing a nearest neighbour search in the embeddings space.
ER  - 

TY  - CONF
TI  - Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 272
EP  - 278
AU  - J. Quenzel
AU  - R. A. Rosu
AU  - T. L√§be
AU  - C. Stachniss
AU  - S. Behnke
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - gradient methods
KW  - image matching
KW  - image registration
KW  - image sensors
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - visual SLAM systems
KW  - photometric consistency
KW  - gradient-based dissimilarity
KW  - camera pose estimation
KW  - map building
KW  - central ingredients
KW  - autonomous robots
KW  - photometric error
KW  - gradient orientation
KW  - magnitude-dependent scaling term
KW  - stereo estimation
KW  - visual odometry systems
KW  - direct image registration tasks
KW  - robust estimates
KW  - scene depth
KW  - camera trajectory
KW  - mapping capabilities
KW  - mobile robots
KW  - sensor data registration
KW  - Measurement
KW  - Robustness
KW  - Cameras
KW  - Estimation
KW  - Visual odometry
KW  - Simultaneous localization and mapping
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197483
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicate that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.
ER  - 

TY  - CONF
TI  - ICS: Incremental Constrained Smoothing for State Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 279
EP  - 285
AU  - P. Sodhi
AU  - S. Choudhury
AU  - J. G. Mangelson
AU  - M. Kaess
PY  - 2020
KW  - matrix decomposition
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - state estimation
KW  - ICS
KW  - primal-dual method
KW  - matrix factorizations
KW  - primal-dual methods
KW  - incremental factorization
KW  - matrix structure
KW  - incremental unconstrained optimization
KW  - robot state estimate
KW  - smoothing-based estimation methods
KW  - state estimation
KW  - incremental constrained smoothing
KW  - Optimization
KW  - Smoothing methods
KW  - Time measurement
KW  - Integrated circuits
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9196649
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A robot operating in the world constantly receives information about its environment in the form of new measurements at every time step. Smoothing-based estimation methods seek to optimize for the most likely robot state estimate using all measurements up till the current time step. Existing methods solve for this smoothing objective efficiently by framing the problem as that of incremental unconstrained optimization. However, in many cases observed measurements and knowledge of the environment is better modeled as hard constraints derived from real-world physics or dynamics. A key challenge is that the new optimality conditions introduced by the hard constraints break the matrix structure needed for incremental factorization in these incremental optimization methods. Our key insight is that if we leverage primal-dual methods, we can recover a matrix structure amenable to incremental factorization. We propose a framework ICS that combines a primal-dual method like the Augmented Lagrangian with an incremental Gauss Newton approach that reuses previously computed matrix factorizations. We evaluate ICS on a set of simulated and real-world problems involving equality constraints like object contact and inequality constraints like collision avoidance.
ER  - 

TY  - CONF
TI  - Drone-aided Localization in LoRa IoT Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 286
EP  - 292
AU  - V. Delafontaine
AU  - F. Schiano
AU  - G. Cocco
AU  - A. Rusu
AU  - D. Floreano
PY  - 2020
KW  - autonomous aerial vehicles
KW  - Internet of Things
KW  - wide area networks
KW  - realistic simulated scenario
KW  - fully autonomous localization system
KW  - ten-fold improvement
KW  - localization precision
KW  - fixed network
KW  - UAV
KW  - localization accuracy
KW  - LoRa IoT networks
KW  - node localization
KW  - widespread IoT communication technologies
KW  - Long Range Wide Area Network
KW  - long communication distances
KW  - drone-aided localization system
KW  - communication system
KW  - search algorithm
KW  - Internet of Things
KW  - 3D mobility
KW  - Logic gates
KW  - Drones
KW  - Estimation
KW  - Receivers
KW  - Servers
KW  - Internet of Things
KW  - Global navigation satellite system
DO  - 10.1109/ICRA40945.2020.9196869
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Besides being part of the Internet of Things (IoT), drones can play a relevant role in it as enablers. The 3D mobility of UAVs can be exploited to improve node localization in IoT networks for, e.g., search and rescue or goods localization and tracking. One of the widespread IoT communication technologies is Long Range Wide Area Network (LoRaWAN), which allows achieving long communication distances with low power. In this work, we present a drone-aided localization system for LoRa networks in which a UAV is used to improve the estimation of a node's location initially provided by the network. We characterize the relevant parameters of the communication system and use them to develop and test a search algorithm in a realistic simulated scenario. We then move to the full implementation of a real system in which a drone is seamlessly integrated into Swisscom's LoRa network. The drone coordinates with the network with a two-way exchange of information which results in an accurate and fully autonomous localization system. The results obtained in our field tests show a ten-fold improvement in localization precision with respect to the estimation provided by the fixed network. Up to our knowledge, this is the first time a UAV is successfully integrated in a LoRa network to improve its localization accuracy.
ER  - 

TY  - CONF
TI  - A fast and practical method of indoor localization for resource-constrained devices with limited sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 293
EP  - 299
AU  - J. Wietrzykowski
AU  - P. Skrzypczy≈Ñski
PY  - 2020
KW  - collision avoidance
KW  - indoor radio
KW  - mobile robots
KW  - navigation
KW  - probability
KW  - fast method
KW  - indoor localization
KW  - resource-constrained devices
KW  - limited sensing capabilities
KW  - wearable devices
KW  - sparse WiFi
KW  - image-based measurements
KW  - dense signal maps
KW  - conditional random fields
KW  - agent positions
KW  - known floor plan
KW  - sparse absolute position estimates
KW  - motion sequences
KW  - low-quality measurements
KW  - handheld devices
KW  - mobile devices
KW  - sensory data
KW  - Wireless fidelity
KW  - Position measurement
KW  - Computational modeling
KW  - Dead reckoning
KW  - Robot sensing systems
KW  - Buildings
DO  - 10.1109/ICRA40945.2020.9197215
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We describe and experimentally demonstrate a practical method for indoor localization using measurements obtained from resource-constrained devices with limited sensing capabilities. We focus on handheld/mobile devices but the method can be useful for a variety of wearable devices. Our system works with sparse WiFi or image-based measurements, avoiding laborious site surveying for dense signal maps and runs in real-time. It uses Conditional Random Fields to infer the most probable sequence of agent positions from a known floor plan, dead reckoning and sparse absolute position estimates. Our solution leverages known topology of the environment by pre-computing allowed motion sequences of an agent, which are then used to constraint the motion inferred from the sensory data. The system is evaluated in a typical office building, demonstrating good accuracy and robustness to sparse, low-quality measurements.
ER  - 

TY  - CONF
TI  - Long-Term Robot Navigation in Indoor Environments Estimating Patterns in Traversability Changes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 300
EP  - 306
AU  - L. Nardi
AU  - C. Stachniss
PY  - 2020
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - term robot navigation
KW  - traversability changes
KW  - mobile robots
KW  - hospitals
KW  - informed decisions
KW  - probabilistic graphical model
KW  - currently unobserved locations
KW  - indoor environments
KW  - Robots
KW  - Navigation
KW  - Predictive models
KW  - Correlation
KW  - Probabilistic logic
KW  - Planning
KW  - Indoor environments
DO  - 10.1109/ICRA40945.2020.9197078
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Nowadays, mobile robots are deployed in many indoor environments such as offices or hospitals. These environments are subject to changes in the traversability that often happen following patterns. In this paper, we investigate the problem of navigating in such environments over extended periods of time by capturing and exploiting these patterns to make informed decisions for navigation. Our approach uses a probabilistic graphical model to incrementally estimate a model of the traversability changes from the robot's observations and to make predictions at currently unobserved locations. In the belief space defined by the predictions, we plan paths that trade off the risk to encounter obstacles and the information gain of visiting unknown locations. We implemented our approach and tested it in different indoor environments. The experiments suggest that, in the long run, our approach leads robots to navigate along shorter paths compared to following a greedy shortest path policy.
ER  - 

TY  - CONF
TI  - Sample-and-computation-efficient Probabilistic Model Predictive Control with Random Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 307
EP  - 313
AU  - C. -Y. Kuo
AU  - Y. Cui
AU  - T. Matsubara
PY  - 2020
KW  - approximation theory
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - predictive control
KW  - random features
KW  - Gaussian processes
KW  - reinforcement learning methods
KW  - model predictive control
KW  - MPC
KW  - computational cost
KW  - linear Gaussian model
KW  - approximated GP dynamics
KW  - state prediction
KW  - simulated robot control tasks
KW  - sample-and-computation-efficient nature
KW  - model-based RL method
KW  - analytic moment-matching scheme
KW  - Training
KW  - Predictive models
KW  - Kernel
KW  - Probabilistic logic
KW  - Computational modeling
KW  - Computational efficiency
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197449
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gaussian processes (GPs) based Reinforcement Learning (RL) methods with Model Predictive Control (MPC) have demonstrated their excellent sample efficiency. However, since the computational cost of GPs largely depends on the training sample size, learning an accurate dynamics using GPs result in low control frequency in MPC. To alleviate this trade-off and achieve a sample-and-computation-efficient nature, we propose a novel model-based RL method with MPC. Our approach employs a linear Gaussian model with randomized features using the Fastfood as an approximated GP dynamics. Then, we derive an analytic moment-matching scheme in state prediction with the model and uncertain inputs. As a result, the computational cost of the MPC in our RL method does not depend on the training sample size and can improve the control frequency over previous methods. Through experiments with simulated and real robot control tasks, the sample efficiency, as well as the computation efficiency of our model-based RL method, are demonstrated.
ER  - 

TY  - CONF
TI  - Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 314
EP  - 320
AU  - J. A. Delgado-Guerrero
AU  - A. Colom√©
AU  - C. Torras
PY  - 2020
KW  - Gaussian processes
KW  - learning systems
KW  - manipulator dynamics
KW  - motion control
KW  - search problems
KW  - sample-efficient robot motion learning
KW  - Gaussian process latent variable models
KW  - robotic manipulators
KW  - household environments
KW  - kinesthetic teaching
KW  - parametric function
KW  - movement primitive
KW  - mutual-information-weighted Gaussian process latent variable model
KW  - trial-and-error
KW  - trajectory production
KW  - task dynamics
KW  - MP parameter latent space
KW  - robot motion task
KW  - search space
KW  - surrogate model
KW  - PS algorithms
KW  - policy search reinforcement learning
KW  - Gaussian processes
KW  - Optimization
KW  - Trajectory
KW  - Task analysis
KW  - Robots
KW  - Mutual information
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196658
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function - usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and/or a model for the dynamics of the task. However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.
ER  - 

TY  - CONF
TI  - Iterative Learning based feedforward control for Transition of a Biplane-Quadrotor Tailsitter UAS
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 321
EP  - 327
AU  - N. Raj
AU  - A. Simha
AU  - M. Kothari
AU  - Abhishek
AU  - R. N. Banavar
PY  - 2020
KW  - aerodynamics
KW  - aircraft control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - error compensation
KW  - feedforward
KW  - helicopters
KW  - iterative learning control
KW  - learning systems
KW  - neurocontrollers
KW  - pitch control (position)
KW  - polynomials
KW  - propellers
KW  - robust control
KW  - wind tunnels
KW  - iterative learning based feedforward control
KW  - biplane-quadrotor tailsitter UAS
KW  - time on-board algorithm
KW  - forward transition maneuver
KW  - repeated flight trials
KW  - pitch angle
KW  - propeller thrust
KW  - polynomials
KW  - simplified aerodynamics
KW  - optimal coefficients
KW  - terminal conditions
KW  - air speed
KW  - modeling error compensation
KW  - geometric attitude controller
KW  - flight modes
KW  - feedforward law
KW  - high-fidelity thrust model
KW  - orientation angle
KW  - neural network model
KW  - experimental flight trials
KW  - learning algorithm
KW  - maneuver control
KW  - wind tunnel data
KW  - feedforward thrust
KW  - robustness
KW  - UAS
KW  - Aerodynamics
KW  - Propellers
KW  - Wind tunnels
KW  - Atmospheric modeling
KW  - Feedforward systems
KW  - Data models
KW  - Trajectory
KW  - VTOL UAS
KW  - transition maneuver
KW  - iterative learning
DO  - 10.1109/ICRA40945.2020.9196671
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper provides a real time on-board algorithm for a biplane-quadrotor to iteratively learn a forward transition maneuver via repeated flight trials. The maneuver is controlled by regulating the pitch angle and propeller thrust according to feedforward control laws that are parameterized by polynomials. Based on a nominal model with simplified aerodynamics, the optimal coefficients of the polynomials are chosen through simulation such that the maneuver is completed with specified terminal conditions on altitude and air speed. In order to compensate for modeling errors, repeated flight trials are performed by updating the feedforward control parameters according to an iterative learning algorithm until the maneuver is perfected. A geometric attitude controller, valid for all flight modes is employed in order to track the pitch angle according to the feedforward law. Further, a high-fidelity thrust model of the propeller for varying advance-ratio and orientation angle is obtained from wind tunnel data which is captured using a neural network model. This facilitates accurate application of feedforward thrust for varying flow conditions during transition. Experimental flight trials are performed to demonstrate the robustness and rapid convergence of the proposed learning algorithm.
ER  - 

TY  - CONF
TI  - Reinforcement Learning for Adaptive Illumination with X-rays
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 328
EP  - 334
AU  - J. -R. Betterton
AU  - D. Ratner
AU  - S. Webb
AU  - M. Kochenderfer
PY  - 2020
KW  - convolutional neural nets
KW  - image resolution
KW  - image sampling
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - X-ray imaging
KW  - adaptive illumination
KW  - reinforcement learning
KW  - image sampling
KW  - image surface
KW  - convolutional neural network
KW  - rastering method
KW  - X-rays
KW  - Apertures
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Time measurement
KW  - Imaging
KW  - Image reconstruction
KW  - X-rays
DO  - 10.1109/ICRA40945.2020.9196614
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a learning algorithm for automating image sampling in scientific applications. We consider settings where images are sampled by controlling a probe beam's scanning trajectory over the image surface. We explore alternatives to obtaining images by the standard rastering method. We formulate the scanner control problem as a reinforcement learning (RL) problem and train a policy to adaptively sample only the highest value regions of the image, choosing the acquisition time and resolution for each sample position based on an observation of previous readings. We use convolutional neural network (CNN) policies to control the scanner as a way to generalize our approach to larger samples. We show simulation results for a simple policy on both synthetic data and real world data from an archaeological application.
ER  - 

TY  - CONF
TI  - Efficient Updates for Data Association with Mixtures of Gaussian Processes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 335
EP  - 341
AU  - K. M. Brian Lee
AU  - W. Martens
AU  - J. Khatkar
AU  - R. Fitch
AU  - R. Mettu
PY  - 2020
KW  - covariance matrices
KW  - Gaussian processes
KW  - mixture models
KW  - probability
KW  - robot vision
KW  - GP mixtures
KW  - data association
KW  - Gaussian processes
KW  - probabilistic approach
KW  - important estimation
KW  - classification tasks
KW  - robotics applications
KW  - GP-based methods
KW  - sparse methods
KW  - covariance matrix
KW  - orthogonal approach
KW  - GP inference
KW  - online update scheme
KW  - sparse GPs
KW  - memoisation approach
KW  - robotic vision applications
KW  - Covariance matrices
KW  - Robots
KW  - Gaussian processes
KW  - Mixture models
KW  - Sparse representation
KW  - Inference algorithms
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196734
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gaussian processes (GPs) enable a probabilistic approach to important estimation and classification tasks that arise in robotics applications. Meanwhile, most GP-based methods are often prohibitively slow, thereby posing a substantial barrier to practical applications. Existing "sparse" methods to speed up GPs seek to either make the model more sparse, or find ways to more efficiently manage a large covariance matrix. In this paper, we present an orthogonal approach that memoises (i.e. reuses) previous computations in GP inference. We demonstrate that a substantial speedup can be achieved by incorporating memoisation into applications in which GPs must be updated frequently. Moreover, we derive a novel online update scheme for sparse GPs that can be used in conjunction with our memoisation approach for a synergistic improvement in performance. Across three robotic vision applications, we demonstrate between 40-100% speed-up over the standard method for inference in GP mixtures.
ER  - 

TY  - CONF
TI  - Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 350
EP  - 356
AU  - H. Peng
AU  - X. Yang
AU  - Y. -H. Su
AU  - B. Hannaford
PY  - 2020
KW  - biomechanics
KW  - end effectors
KW  - medical robotics
KW  - position control
KW  - surgery
KW  - telerobotics
KW  - time data driven precision Estimator
KW  - RAVEN-II surgical robot end effector position
KW  - surgical robots
KW  - cable-driven nature
KW  - kinematics calcu-lation
KW  - reported end effector position
KW  - position inaccuracy
KW  - data-driven pipeline
KW  - robot end effector position precision estimation
KW  - improved end effector position error
KW  - RMS
KW  - entire robot workspace
KW  - End effectors
KW  - Cameras
KW  - Medical robotics
KW  - Robot sensing systems
KW  - Image edge detection
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196915
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calcu-lation [1]. Since the reported end effector position of surgical robots like RAVEN-II [2] is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker. The open source code, data sets, videos, and user guide can be found at //github.com/HaonanPeng/RAVEN Neural Network Estimator.
ER  - 

TY  - CONF
TI  - Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 371
EP  - 377
AU  - Y. Qin
AU  - S. A. Pedram
AU  - S. Feyzabadi
AU  - M. Allan
AU  - A. J. McLeod
AU  - J. W. Burdick
AU  - M. Azizian
PY  - 2020
KW  - biomedical ultrasonics
KW  - finite state machines
KW  - learning (artificial intelligence)
KW  - medical computing
KW  - medical image processing
KW  - medical robotics
KW  - surgery
KW  - Skill Assessment Working Set
KW  - robotic intra-operative ultrasound imaging
KW  - da Vinci¬Æ Xi surgical system
KW  - superior frame-wise state estimation accuracy
KW  - temporal segmentation
KW  - deep learning
KW  - data sources
KW  - robot-assisted surgeries
KW  - finite-state machines
KW  - surgical task
KW  - temporal perception
KW  - current surgical scene
KW  - real-time estimation
KW  - task progresses
KW  - state estimation models
KW  - surgical state estimation models
KW  - State estimation
KW  - Data models
KW  - Task analysis
KW  - Feature extraction
KW  - Hidden Markov models
KW  - Robots
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196560
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci¬Æ Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset.
ER  - 

TY  - CONF
TI  - Controlling Assistive Robots with Learned Latent Actions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 378
EP  - 384
AU  - D. P. Losey
AU  - K. Srinivasan
AU  - A. Mandlekar
AU  - A. Garg
AU  - D. Sadigh
PY  - 2020
KW  - control system synthesis
KW  - handicapped aids
KW  - learning systems
KW  - manipulators
KW  - telerobotics
KW  - learned latent actions
KW  - assistive robotic arms
KW  - high-dimensional robot behavior
KW  - user-friendly latent actions
KW  - low-dimensional embeddings
KW  - robotic arm
KW  - assistive eating
KW  - cooking tasks
KW  - assistive robot control
KW  - physical disabilities
KW  - teleoperation
KW  - handheld joystick
KW  - shared autonomy baselines
KW  - Robot kinematics
KW  - Task analysis
KW  - Manipulators
KW  - Aerospace electronics
KW  - Robot sensing systems
KW  - Glass
KW  - Index Terms‚ÄîPhysically assistive devices
KW  - cognitive humanrobot interaction
KW  - human-centered robotics
DO  - 10.1109/ICRA40945.2020.9197197
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful also makes them challenging to teleoperate: the robot has more degrees-of-freedom than the human can directly coordinate with a handheld joystick. Our insight is that we can make assistive robots easier for humans to control by leveraging latent actions. Latent actions provide a low-dimensional embedding of high-dimensional robot behavior: for example, one latent dimension might guide the assistive arm along a pouring motion. In this paper, we design a teleoperation algorithm for assistive robots that learns latent actions from task demonstrations. We formulate the controllability, consistency, and scaling properties that user-friendly latent actions should have, and evaluate how different low-dimensional embeddings capture these properties. Finally, we conduct two user studies on a robotic arm to compare our latent action approach to both state-of-the-art shared autonomy baselines and a teleoperation strategy currently used by assistive arms. Participants completed assistive eating and cooking tasks more efficiently when leveraging our latent actions, and also subjectively reported that latent actions made the task easier to perform. The video accompanying this paper can be found at: https://youtu.be/wjnhrzugBj4.
ER  - 

TY  - CONF
TI  - On the efficient control of series-parallel compliant articulated robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 385
EP  - 391
AU  - V. D. Amara
AU  - J. Malzahn
AU  - Z. Ren
AU  - W. Roozing
AU  - N. Tsagarakis
PY  - 2020
KW  - actuators
KW  - legged locomotion
KW  - optimisation
KW  - position control
KW  - robot dynamics
KW  - torque
KW  - torque control
KW  - series-parallel compliant articulated leg prototype
KW  - highly-efficient parallel actuation branches
KW  - torque allocation
KW  - transmission ratio
KW  - actuator hardware specifications
KW  - periodic squat motions
KW  - motion efficiency
KW  - parallel actuators
KW  - quadratic criteria
KW  - optimization based controller
KW  - redundant robots
KW  - torque distribution
KW  - series-parallel compliant articulated robots
KW  - Torque
KW  - Actuators
KW  - Joints
KW  - Tendons
KW  - Legged locomotion
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196786
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Torque distribution in redundant robots that combine the potential of asymmetric series-parallel actuated branches and multi-articulation pose a non-trivial challenge. To address the problem, this work proposes a novel optimization based controller that can accommodate various quadratic criteria to perform the torque distribution among dissimilar series and parallel actuators in order to maximize the motion efficiency. Three candidate criteria are composed and their performances are compared during periodic squat motions with a 3 degree of freedom series-parallel compliant articulated leg prototype. It is first shown that by minimizing a criterion that takes into account the actuator hardware specifications such as torque constant and transmission ratio, the gravity-driven phases can be lengthened. Thereby, this particular criterion results in slightly better performance than when adopting a strategy that maximizes the torque allocation to the higher efficiency actuators. Furthermore, valuable insights such as that the efficacy of maximum utilization of the highly-efficient parallel actuation branches decreases progressively at high frequencies were observed.
ER  - 

TY  - CONF
TI  - Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 392
EP  - 398
AU  - D. Wisth
AU  - M. Camurri
AU  - M. Fallon
PY  - 2020
KW  - control nonlinearities
KW  - distance measurement
KW  - graph theory
KW  - image fusion
KW  - inertial navigation
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot vision
KW  - state estimation
KW  - preintegrated velocity bias estimation
KW  - contact nonlinearities
KW  - legged robot odometry
KW  - factor graph formulation
KW  - quadruped robot
KW  - slippery terrain
KW  - deformable terrain
KW  - preintegrated velocity factor
KW  - leg flexibility
KW  - leg odometry drift
KW  - IMU factors
KW  - ANYmal robot
KW  - proprioceptive state estimator
KW  - Legged locomotion
KW  - Robot sensing systems
KW  - Velocity measurement
KW  - Estimation
KW  - Kinematics
KW  - Foot
DO  - 10.1109/ICRA40945.2020.9197214
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a novel factor graph formulation to estimate the pose and velocity of a quadruped robot on slippery and deformable terrain. The factor graph introduces a preintegrated velocity factor that incorporates velocity inputs from leg odometry and also estimates related biases. From our experimentation we have seen that it is difficult to model uncertainties at the contact point such as slip or deforming terrain, as well as leg flexibility. To accommodate for these effects and to minimize leg odometry drift, we extend the robot's state vector with a bias term for this preintegrated velocity factor. The bias term can be accurately estimated thanks to the tight fusion of the preintegrated velocity factor with stereo vision and IMU factors, without which it would be unobservable. The system has been validated on several scenarios that involve dynamic motions of the ANYmal robot on loose rocks, slopes and muddy ground. We demonstrate a 26% improvement of relative pose error compared to our previous work and 52% compared to a state-of-the-art proprioceptive state estimator.
ER  - 

TY  - CONF
TI  - Optimized Foothold Planning and Posture Searching for Energy-Efficient Quadruped Locomotion over Challenging Terrains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 399
EP  - 405
AU  - L. Chen
AU  - S. Ye
AU  - C. Sun
AU  - A. Zhang
AU  - G. Deng
AU  - T. Liao
PY  - 2020
KW  - energy conservation
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - search problems
KW  - torque control
KW  - posture searching
KW  - energy-efficient quadruped locomotion
KW  - energy-efficient locomotion
KW  - legged robot
KW  - quadrupedal robot
KW  - nominal stance parameters
KW  - leg torque distribution
KW  - foothold planner
KW  - standing legs
KW  - energy-saving stance posture
KW  - stairs climbing
KW  - stairs climbing
KW  - center of gravity trajectory planner
KW  - COG
KW  - foothold planning optimization
KW  - Legged locomotion
KW  - Torque
KW  - Thigh
KW  - Hip
KW  - Knee
KW  - Energy efficiency
DO  - 10.1109/ICRA40945.2020.9197135
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Energy-efficient locomotion is of primary importance for legged robot to extend operation time in practical applications. This paper presents an approach to achieve energy-efficient locomotion for a quadrupedal robot walking over challenging terrains. Firstly, we optimize the nominal stance parameters based on the analysis of leg torque distribution. Secondly, we proposed the foothold planner and the center of gravity (COG) trajectory planner working together to guide the robot to place its standing legs in an energy-saving stance posture. We have validated the effectiveness of our method on a real quadrupedal robot in experiments including autonomously walking on plain ground and climbing stairs.
ER  - 

TY  - CONF
TI  - Extracting Legged Locomotion Heuristics with Regularized Predictive Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 406
EP  - 412
AU  - G. Bledt
AU  - S. Kim
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - predictive control
KW  - legged locomotion heuristics
KW  - regularized predictive control
KW  - legged robots
KW  - dynamic maneuvers
KW  - difficult terrains
KW  - meaningful cost functions
KW  - high-fidelity models
KW  - timing restrictions
KW  - principled regularization heuristics
KW  - legged locomotion optimization control
KW  - cost space offline
KW  - desired commands
KW  - optimal control actions
KW  - robot states
KW  - heuristic candidates
KW  - adaptation laws
KW  - models online
KW  - powerful heuristics
KW  - approximate complex dynamics
KW  - model simplifications
KW  - parameter uncertainty
KW  - parameter tuning process
KW  - increased capabilities
KW  - newly extracted heuristics
KW  - controller structure
KW  - mini cheetah robot
KW  - Cost function
KW  - Legged locomotion
KW  - Data models
KW  - Tuning
KW  - Predictive control
DO  - 10.1109/ICRA40945.2020.9197488
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Optimization based predictive control is a powerful tool that has improved the ability of legged robots to execute dynamic maneuvers and traverse increasingly difficult terrains. However, it is often challenging and unintuitive to design meaningful cost functions and build high-fidelity models while adhering to timing restrictions. A novel framework to extract and design principled regularization heuristics for legged locomotion optimization control is presented. By allowing a simulation to fully explore the cost space offline, certain states and actions can be constrained or isolated. Data is fit with simple models relating the desired commands, optimal control actions, and robot states to identify new heuristic candidates. Basic parameter learning and adaptation laws are then applied to the models online. This method extracts simple, but powerful heuristics that can approximate complex dynamics and account for errors stemming from model simplifications and parameter uncertainty without the loss of physical intuition while generalizing the parameter tuning process. Results on the Mini Cheetah robot verify the increased capabilities due to the newly extracted heuristics without any modification to the controller structure or gains.
ER  - 

TY  - CONF
TI  - Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 413
EP  - 419
AU  - T. Li
AU  - N. Lambert
AU  - R. Calandra
AU  - F. Meier
AU  - A. Rai
PY  - 2020
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - path planning
KW  - predictive control
KW  - robot dynamics
KW  - robot kinematics
KW  - generalizable locomotion skills
KW  - hierarchical reinforcement learning
KW  - arbitrary goals
KW  - hierarchical framework
KW  - sample-efficiency
KW  - generalizability
KW  - learned locomotion skills
KW  - real-world robots
KW  - goal-oriented locomotion
KW  - diverse primitives skills
KW  - freedom robot
KW  - coarse dynamics models
KW  - primitive cycles
KW  - model predictive control framework
KW  - Daisy hexapod hardware
KW  - size 12.0 m
KW  - Hardware
KW  - Legged locomotion
KW  - Training
KW  - Task analysis
KW  - Planning
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196642
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning to locomote to arbitrary goals on hardware remains a challenging problem for reinforcement learning. In this paper, we present a hierarchical framework that improves sample-efficiency and generalizability of learned locomotion skills on real-world robots. Our approach divides the problem of goal-oriented locomotion into two sub-problems: learning diverse primitives skills, and using model-based planning to sequence these skills. We parametrize our primitives as cyclic movements, improving sample-efficiency of learning from scratch on a 18 degrees of freedom robot. Then, we learn coarse dynamics models over primitive cycles and use them in a model predictive control framework. This allows us to learn to walk to arbitrary goals up to 12m away, after about two hours of training from scratch on hardware. Our results on a Daisy hexapod hardware and simulation demonstrate the efficacy of our approach at reaching distant targets, in different environments, and with sensory noise.
ER  - 

TY  - CONF
TI  - SoRX: A Soft Pneumatic Hexapedal Robot to Traverse Rough, Steep, and Unstable Terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 420
EP  - 426
AU  - Z. Liu
AU  - Z. Lu
AU  - K. Karydis
PY  - 2020
KW  - actuators
KW  - legged locomotion
KW  - motion control
KW  - pneumatic actuators
KW  - SoRX
KW  - soft pneumatic hexapedal robot
KW  - 2-degree-of-freedom soft pneumatic actuator
KW  - tripod gait
KW  - pneumatically-actuated legged robots
KW  - rough terrain
KW  - steep terrain
KW  - open-loop control
KW  - cyclic foot trajectories
KW  - legged locomotion
KW  - physical testing
KW  - Legged locomotion
KW  - Pneumatic systems
KW  - Soft robotics
KW  - Pneumatic actuators
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196731
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robotics technology creates new ways for legged robots to interact with and adapt to their environment. In this paper we develop i) a new 2-degree-of-freedom soft pneumatic actuator, and ii) a novel soft robotic hexapedal robot called SoRX that leverages the new actuators. Simulation and physical testing confirm that the proposed actuator can generate cyclic foot trajectories that are appropriate for legged locomotion. Consistent with other hexapedal robots (and animals), SoRX employs an alternating tripod gait to propel itself forward. Experiments reveal that SoRX can reach forward speeds of up to 0.44 body lengths per second, or equivalently 101 mm/s. With a size of 230 mm length, 140 mm width and 100 mm height, and weight of 650 grams, SoRX is among the fastest tethered soft pneumatically-actuated legged robots to date. The motion capabilities of SoRX are evaluated through five experiments: running, step climbing, and traversing rough terrain, steep terrain, and unstable terrain. Experimental results show that SoRX is able to operate over challenging terrains in open-loop control and by following the same alternating tripod gait across all experimental cases.
ER  - 

TY  - CONF
TI  - UBAT: On Jointly Optimizing UAV Trajectories and Placement of Battery Swap Stations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 427
EP  - 433
AU  - M. Won
PY  - 2020
KW  - ant colony optimisation
KW  - autonomous aerial vehicles
KW  - battery powered vehicles
KW  - electric vehicles
KW  - optimal number
KW  - charging stations
KW  - UBAT
KW  - ant colony optimization
KW  - UAV trajectories
KW  - battery swap stations
KW  - unmanned aerial vehicles
KW  - UAVs
KW  - flight time
KW  - charging station deployment problem
KW  - NP-hard problem
KW  - Charging stations
KW  - Batteries
KW  - Trajectory
KW  - Optimization
KW  - Sensors
KW  - Unmanned aerial vehicles
KW  - Euclidean distance
DO  - 10.1109/ICRA40945.2020.9197227
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unmanned aerial vehicles (UAVs) have been widely used in many applications. The limited flight time of UAVs, however, still remains as a major challenge. Although numerous approaches have been developed to recharge the battery of UAVs effectively, little is known about optimal methodologies to deploy charging stations. In this paper, we address the charging station deployment problem with an aim to find the optimal number and locations of charging stations such that the system performance is maximized. We show that the problem is NP-Hard and propose UBAT, a heuristic framework based on the ant colony optimization (ACO) to solve the problem. Additionally, a suite of algorithms are designed to enhance the execution time and the quality of the solutions for UBAT. Through extensive simulations, we demonstrate that UBAT effectively performs multi-objective optimization of generation of UAV trajectories and placement of charging stations that are within 8.3% and 7.3% of the true optimal solutions, respectively.
ER  - 

TY  - CONF
TI  - Efficient Multi-Agent Trajectory Planning with Feasibility Guarantee using Relative Bernstein Polynomial
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 434
EP  - 440
AU  - J. Park
AU  - J. Kim
AU  - I. Jang
AU  - H. J. Kim
PY  - 2020
KW  - collision avoidance
KW  - multi-agent systems
KW  - optimisation
KW  - polynomials
KW  - optimization-based approaches
KW  - erroneous optimization setup
KW  - infeasible collision constraints
KW  - sequential optimization method
KW  - dummy agents
KW  - relative Bernstein polynomial
KW  - nonconvex collision avoidance constraints
KW  - multiagent trajectory planning problems
KW  - obstacle-dense environments
KW  - grid-based approaches
KW  - Trajectory
KW  - Planning
KW  - Heuristic algorithms
KW  - Collision avoidance
KW  - Optimization
KW  - System recovery
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197162
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a new efficient algorithm which guarantees a solution for a class of multi-agent trajectory planning problems in obstacle-dense environments. Our algorithm combines the advantages of both grid-based and optimization-based approaches, and generates safe, dynamically feasible trajectories without suffering from an erroneous optimization setup such as imposing infeasible collision constraints. We adopt a sequential optimization method with dummy agents to improve the scalability of the algorithm, and utilize the convex hull property of Bernstein and relative Bernstein polynomial to replace non-convex collision avoidance constraints to convex ones. The proposed method can compute the trajectory for 64 agents on average 6.36 seconds with Intel Core i7-7700 @ 3.60GHz CPU and 16G RAM, and it reduces more than 50% of the objective cost compared to our previous work. We validate the proposed algorithm through simulation and flight tests.
ER  - 

TY  - CONF
TI  - Optimal Sequential Task Assignment and Path Finding for Multi-Agent Robotic Assembly Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 441
EP  - 447
AU  - K. Brown
AU  - O. Peltzer
AU  - M. A. Sehr
AU  - M. Schwager
AU  - M. J. Kochenderfer
PY  - 2020
KW  - assembly planning
KW  - collision avoidance
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - robotic assembly
KW  - nonholonomic differential-drive robots
KW  - optimal sequential task assignment
KW  - collision-free trajectories
KW  - robotic manufacturing
KW  - collision-free routing
KW  - multiagent robotic assembly planning
KW  - path finding
KW  - Robots
KW  - Task analysis
KW  - Schedules
KW  - Manufacturing
KW  - Collision avoidance
KW  - Routing
KW  - Production facilities
DO  - 10.1109/ICRA40945.2020.9197527
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study the problem of sequential task assignment and collision-free routing for large teams of robots in applications with inter-task precedence constraints (e.g., task A and task B must both be completed before task C may begin). Such problems commonly occur in assembly planning for robotic manufacturing applications, in which sub-assemblies must be completed before they can be combined to form the final product. We propose a hierarchical algorithm for computing makespan-optimal solutions to the problem. The algorithm is evaluated on a set of randomly generated problem instances where robots must transport objects between stations in a "factory" grid world environment. In addition, we demonstrate in high-fidelity simulation that the output of our algorithm can be used to generate collision-free trajectories for non-holonomic differential-drive robots.
ER  - 

TY  - CONF
TI  - Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 448
EP  - 454
AU  - R. Han
AU  - S. Chen
AU  - Q. Hao
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-robot systems
KW  - navigation
KW  - path planning
KW  - optimal paths
KW  - multiple robots
KW  - dynamics randomization
KW  - differential drive robots
KW  - dynamic environment
KW  - obstacle complexities
KW  - multirobot navigation problem
KW  - deep reinforcement learning framework
KW  - optimal target locations
KW  - DRL based framework
KW  - navigation policy
KW  - Collision avoidance
KW  - Navigation
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Training
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197209
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.
ER  - 

TY  - CONF
TI  - Adaptive Directional Path Planner for Real-Time, Energy-Efficient, Robust Navigation of Mobile Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 455
EP  - 461
AU  - M. R. Nimmagadda
AU  - S. Dattawadkar
AU  - S. Muthukumar
AU  - V. Honkote
PY  - 2020
KW  - energy conservation
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - robust control
KW  - sample based methods
KW  - sub-optimal memory-intensive
KW  - Adaptive Directional Planner algorithm
KW  - robust local path planning
KW  - autonomous navigation
KW  - form factor mobile robots
KW  - low memory footprint
KW  - robust navigation
KW  - unknown environments
KW  - complex environments
KW  - fundamental capability
KW  - robotic applications
KW  - optimal robot path planning
KW  - complex memory intensive task
KW  - adaptive directional path planner
KW  - ADP algorithm implementation
KW  - memory size 28.0 KByte
KW  - Mobile robots
KW  - Trajectory
KW  - Real-time systems
KW  - Navigation
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197417
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation through unknown and complex environments is a fundamental capability that is essential in almost all robotic applications. Optimal robot path planning is critical to enable efficient navigation. Path planning is a complex, compute and memory intensive task. Traditional methods employ either graph based search methods or sample based methods to implement path planning, which are sub-optimal and compute/memory-intensive. To this end, an Adaptive Directional Planner (ADP) algorithm is devised to achieve real-time, energy-efficient, memory-optimized, robust local path planning for enabling efficient autonomous navigation of mobile robots. The ADP algorithm ensures that the paths are optimal and kinematically-feasible. Further, the proposed algorithm is tested with different challenging scenarios verifying the functionality and robustness. The ADP algorithm implementation results demonstrate 40- 60X less number of nodes and 40 - 50X less execution time compared to the standard TP-RRT schemes, without compromising on accuracy. Finally, the algorithm has also been implemented as an accelerator for non-holonomic, multi-shape, small form factor mobile robots to provide a silicon solution with high performance and low memory footprint (28KB).
ER  - 

TY  - CONF
TI  - Exploiting sparsity in robot trajectory optimization with direct collocation and geometric algorithms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 469
EP  - 475
AU  - D. Cardona-Ortiz
AU  - A. Paz
AU  - G. Arechavaleta
PY  - 2020
KW  - geometry
KW  - Lie algebras
KW  - Lie groups
KW  - optimal control
KW  - optimisation
KW  - robots
KW  - trajectory control
KW  - geometric algorithm
KW  - robot trajectory optimization
KW  - Lie group method
KW  - floating-point operations
KW  - first-order information
KW  - sparsity exploitation
KW  - numerical differentiation
KW  - analytical differentiation
KW  - Lie algebras
KW  - state equations
KW  - recursive algorithms
KW  - articulated robots
KW  - direct collocation algorithm
KW  - Robots
KW  - Heuristic algorithms
KW  - Jacobian matrices
KW  - Optimal control
KW  - System dynamics
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196668
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robot trajectory optimization formulation that builds upon numerical optimal control and Lie group methods. In particular, the inherent sparsity of direct collocation is carefully analyzed to dramatically reduce the number of floating-point operations to get first-order information of the problem. We describe how sparsity exploitation is employed with both numerical and analytical differentiation. Furthermore, the use of geometric algorithms based on Lie groups and their associated Lie algebras allow to analytically evaluate the state equations and their derivatives with efficient recursive algorithms. We demonstrate the scalability of the proposed formulation with three different articulated robots, such as a finger, a mobile manipulator and a humanoid composed of five, eight and more than twenty degrees of freedom, respectively. The performance of our implementation in C++ is also validated and compared against a state-of-the-art general purpose numerical optimal control solver.
ER  - 

TY  - CONF
TI  - Bi-Convex Approximation of Non-Holonomic Trajectory Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 476
EP  - 482
AU  - A. K. Singh
AU  - R. Ram Theerthala
AU  - M. Babu
AU  - U. K. R. Nair
AU  - K. Madhava Krishna
PY  - 2020
KW  - approximation theory
KW  - convex programming
KW  - minimisation
KW  - quadratic programming
KW  - robot kinematics
KW  - nonholonomic trajectory optimization
KW  - nonholonomic kinematics
KW  - nonlinearly maps control input
KW  - nonconvex
KW  - bi-convex cost
KW  - constraint functions
KW  - bi-convex part
KW  - nonholonomic behavior
KW  - nonlinear penalty
KW  - nonlinear costs
KW  - bi-convex structure
KW  - bi-convex approximation
KW  - autonomous cars
KW  - fixed-wing aerial vehicles
KW  - computational tractability
KW  - alternating minimization
KW  - sequential quadratic programming
KW  - interior-point methods
KW  - Trajectory optimization
KW  - Minimization
KW  - Computational modeling
KW  - Collision avoidance
KW  - Robots
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9197092
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous cars and fixed-wing aerial vehicles have the so-called non-holonomic kinematics which non-linearly maps control input to states. As a result, trajectory optimization with such a motion model becomes highly non-linear and non-convex. In this paper, we improve the computational tractability of non-holonomic trajectory optimization by reformulating it in terms of a set of bi-convex cost and constraint functions along with a non-linear penalty. The bi-convex part acts as a relaxation for the non-holonomic trajectory optimization while the residual of the penalty dictates how well its output obeys the non-holonomic behavior. We adopt an alternating minimization approach for solving the reformulated problem and show that it naturally leads to the replacement of the challenging non-linear penalty with a globally valid convex surrogate. Along with the common cost functions modeling goal-reaching, trajectory smoothness, etc., the proposed optimizer can also accommodate a class of non-linear costs for modeling goal-sets, while retaining the bi-convex structure. We benchmark the proposed optimizer against off-the-shelf solvers implementing sequential quadratic programming and interior-point methods and show that it produces solutions with similar or better cost as the former while significantly outperforming the latter. Furthermore, as compared to both off-the-shelf solvers, the proposed optimizer achieves more than 20x reduction in computation time.
ER  - 

TY  - CONF
TI  - Fast, Versatile, and Open-loop Stable Running Behaviors with Proprioceptive-only Sensing using Model-based Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 483
EP  - 489
AU  - W. Gao
AU  - C. Young
AU  - J. Nicholson
AU  - C. Hubicki
AU  - J. Clark
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - open loop systems
KW  - optimisation
KW  - robot dynamics
KW  - stability
KW  - state feedback
KW  - model-based trajectory optimization
KW  - direct-drive robot
KW  - direct-collocation-formulated optimization
KW  - single-legged planar robot
KW  - open-loop stable motion primitives
KW  - proprioceptive-only sensing
KW  - versatile dynamic behaviors
KW  - expensive inertial sensors
KW  - agile control
KW  - stable control
KW  - model-based optimization
KW  - open-loop stable running behaviors
KW  - Legged locomotion
KW  - Force
KW  - Optimization
KW  - Springs
KW  - Modulation
KW  - Hip
DO  - 10.1109/ICRA40945.2020.9196542
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As we build our legged robots smaller and cheaper, stable and agile control without expensive inertial sensors becomes increasingly important. We seek to enable versatile dynamic behaviors on robots with limited modes of state feedback, specifically proprioceptive-only sensing. This work uses model-based trajectory optimization methods to design open-loop stable motion primitives. We specifically design running gaits for a single-legged planar robot, and can generate motion primitives in under 3 seconds, approaching online-capable speeds. A direct-collocation-formulated optimization generated axial force profiles for the direct-drive robot to achieve desired running speed and apex height. When implemented in hardware, these trajectories produced open-loop stable running. Further, the measured running achieved the desired speed within 10% of the speed specified for the optimization in spite of having no control loop actively measuring or controlling running speed. Additionally, we examine the shape of the optimized force profile and observe features that may be applicable to open-loop stable running in general.
ER  - 

TY  - CONF
TI  - Wasserstein Distributionally Robust Motion Planning and Control with Safety Constraints Using Conditional Value-at-Risk
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 490
EP  - 496
AU  - A. Hakobyan
AU  - I. Yang
PY  - 2020
KW  - collision avoidance
KW  - decision making
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - predictive control
KW  - probability
KW  - robust control
KW  - Wasserstein distributionally robust motion planning
KW  - safety constraints
KW  - conditional value-at-risk
KW  - optimization-based decision-making tool
KW  - safe motion planning
KW  - pre-specified threshold
KW  - probability distribution
KW  - obstacles
KW  - Wasserstein ball
KW  - available empirical distribution
KW  - out-of-sample performance guarantee
KW  - risk constraint
KW  - computationally tractable method
KW  - distributionally robust model predictive control problem
KW  - distributionally robust method
KW  - Robustness
KW  - Safety
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Probability distribution
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196857
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose an optimization-based decision-making tool for safe motion planning and control in an environment with randomly moving obstacles. The unique feature of the proposed method is that it limits the risk of unsafety by a pre-specified threshold even when the true probability distribution of the obstacles' movements deviates, within a Wasserstein ball, from an available empirical distribution. Another advantage is that it provides a probabilistic out-of-sample performance guarantee of the risk constraint. To develop a computationally tractable method for solving the distributionally robust model predictive control problem, we propose a set of reformulation procedures using (i) the Kantorovich duality principle, (ii) the extremal representation of conditional value-at-risk, and (iii) a geometric expression of the distance to the union of halfspaces. The performance and utility of this distributionally robust method are demonstrated through simulations using a 12D quadrotor model in a 3D environment.
ER  - 

TY  - CONF
TI  - Grasping Fragile Objects Using A Stress-Minimization Metric
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 517
EP  - 523
AU  - Z. Pan
AU  - X. Gao
AU  - D. Manocha
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - minimisation
KW  - stress-minimization metric
KW  - homogeneous isotopic materials
KW  - SM metric measures
KW  - fragile objects grasping
KW  - optimal grasp planning algorithms
KW  - Measurement
KW  - Planning
KW  - Force
KW  - Tensile stress
KW  - Grasping
KW  - Resilience
DO  - 10.1109/ICRA40945.2020.9196938
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a new method to generate optimal grasps for brittle and fragile objects using a novel stress- minimization (SM) metric. Our approach is designed for objects that are composed of homogeneous isotopic materials. Our SM metric measures the maximal resistible external wrenches that would not result in fractures in the target objects. In this paper, we propose methods to compute our new metric. We also use our SM metric to design optimal grasp planning algorithms. Finally, we compare the performance of our metric and conventional grasp metrics, including Q1,Q‚àû,QG11,QMSV,QVEW. Our experiments show that our SM metric takes into account the material characteristics and object shapes to indicate the fragile regions, where prior methods may not work well. We also show that the computational cost of our SM metric is on par with prior methods. Finally, we show that grasp planners guided by our metric can lower the probability of breaking target objects.
ER  - 

TY  - CONF
TI  - Grasp Control for Enhancing Dexterity of Parallel Grippers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 524
EP  - 530
AU  - M. Costanzo
AU  - G. De Maria
AU  - G. Lettera
AU  - C. Natale
PY  - 2020
KW  - dexterous manipulators
KW  - friction
KW  - grippers
KW  - manipulator dynamics
KW  - materials handling
KW  - motion control
KW  - path planning
KW  - tactile sensors
KW  - grasp control
KW  - enhancing dexterity
KW  - parallel grippers
KW  - robust grasp controller
KW  - slipping avoidance
KW  - model-based algorithm
KW  - modified LuGre friction model
KW  - rotational frictional sliding motions
KW  - limit surface concept
KW  - computationally efficient method
KW  - minimum grasping force
KW  - tangential loads
KW  - torsional loads
KW  - control modalities
KW  - robot motion
KW  - automatically generates robot motions
KW  - gripper commands
KW  - Force
KW  - Grippers
KW  - Friction
KW  - Dynamics
KW  - Mathematical model
KW  - Computational modeling
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196873
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A robust grasp controller for both slipping avoidance and controlled sliding is proposed based on force/tactile feedback only. The model-based algorithm exploits a modified LuGre friction model to consider rotational frictional sliding motions. The modification relies on the Limit Surface concept where a novel computationally efficient method is introduced to compute in real-time the minimum grasping force to balance tangential and torsional loads. The two control modalities are considered by the robot motion planning algorithm that automatically generates robot motions and gripper commands to solve complex manipulation tasks in a material handling application.
ER  - 

TY  - CONF
TI  - Theoretical Derivation and Realization of Adaptive Grasping Based on Rotational Incipient Slip Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 531
EP  - 537
AU  - T. Narita
AU  - S. Nagakari
AU  - W. Conus
AU  - T. Tsuboi
AU  - K. Nagasaka
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - friction
KW  - grippers
KW  - manipulators
KW  - robust control
KW  - object manipulation
KW  - grasp force control algorithm
KW  - adaptive grasping
KW  - rotational incipient slip detection
KW  - robotics
KW  - incipient slip robust detection
KW  - center of gravity
KW  - friction coefficient
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196615
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Manipulating objects whose physical properties are unknown remains one of the greatest challenges in robotics. Controlling grasp force is an essential aspect of handling unknown objects without slipping or crushing them. Although extensive research has been carried out on grasp force control, unknown object manipulation is still difficult because conventional approaches assume that object properties (mass, center of gravity, friction coefficient, etc.) are known for grasp force control. One of the approaches to address this issue is incipient slip detection. However, there has been few detailed investigations of robust detection and control of incipient slip on rotational case. This study makes contributions on deriving the theoretical model of incipient slip and proposes a new algorithm to detect incipient slip. Additionally, a novel sensor configuration and a grasp force control algorithm based on the derived theoretical model are proposed. Finally, the proposed algorithm is evaluated by grasping objects with different weights and moments including a fragile pastry (√©clair).
ER  - 

TY  - CONF
TI  - Grasp State Assessment of Deformable Objects Using Visual-Tactile Fusion Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 538
EP  - 544
AU  - S. Cui
AU  - R. Wang
AU  - J. Wei
AU  - F. Li
AU  - S. Wang
PY  - 2020
KW  - convolutional neural nets
KW  - deformation
KW  - dexterous manipulators
KW  - force control
KW  - grippers
KW  - image classification
KW  - image fusion
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - tactile sensors
KW  - dexterity grasping
KW  - automatic force control
KW  - classification
KW  - tactile sensor
KW  - wrist camera
KW  - robotic arm
KW  - deformable objects
KW  - 3D convolution based visual tactile fusion deep neural network
KW  - adaptive grasping
KW  - extensive grasping
KW  - C3D-VTFN
KW  - sliding deformation
KW  - grasp state assessment
KW  - Visualization
KW  - Grasping
KW  - Feature extraction
KW  - Task analysis
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9196787
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Humans can quickly determine the force required to grasp a deformable object to prevent its sliding or excessive deformation through vision and touch, which is still a challenging task for robots. To address this issue, we propose a novel 3D convolution-based visual-tactile fusion deep neural network (C3D-VTFN) to evaluate the grasp state of various deformable objects in this paper. Specifically, we divide the grasp states of deformable objects into three categories of sliding, appropriate and excessive. Also, a dataset for training and testing the proposed network is built by extensive grasping and lifting experiments with different widths and forces on 16 various deformable objects with a robotic arm equipped with a wrist camera and a tactile sensor. As a result, a classification accuracy as high as 99.97% is achieved. Furthermore, some delicate grasp experiments based on the proposed network are implemented in this paper. The experimental results demonstrate that the C3D-VTFN is accurate and efficient enough for grasp state assessment, which can be widely applied to automatic force control, adaptive grasping, and other visual-tactile spatiotemporal sequence learning problems.
ER  - 

TY  - CONF
TI  - Beyond Top-Grasps Through Scene Completion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 545
EP  - 551
AU  - J. Lundell
AU  - F. Verdoja
AU  - V. Kyrki
PY  - 2020
KW  - cameras
KW  - end effectors
KW  - grippers
KW  - image colour analysis
KW  - image sensors
KW  - path planning
KW  - position control
KW  - robot vision
KW  - camera images
KW  - grasp success rate
KW  - simulated images
KW  - top-grasps
KW  - scene completion
KW  - six-degree-of-freedom grasps
KW  - simulated viewpoints
KW  - generation method
KW  - fully convolutional grasp quality CNN
KW  - end-to-end grasp
KW  - Shape
KW  - Cameras
KW  - Grasping
KW  - Planning
KW  - Robot vision systems
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197320
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled. Code and video are available at https://irobotics.aalto.fi/beyond-topgrasps-through-scene-completion/.
ER  - 

TY  - CONF
TI  - Dex-Net AR: Distributed Deep Grasp Planning Using a Commodity Cellphone and Augmented Reality App
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 552
EP  - 558
AU  - H. Zhang
AU  - J. Ichnowski
AU  - Y. Avigal
AU  - J. Gonzales
AU  - I. Stoica
AU  - K. Goldberg
PY  - 2020
KW  - Apple computers
KW  - augmented reality
KW  - computational geometry
KW  - control engineering computing
KW  - dexterous manipulators
KW  - distributed processing
KW  - image colour analysis
KW  - image sequences
KW  - mobile computing
KW  - path planning
KW  - robot vision
KW  - smart phones
KW  - distributed deep grasp planning
KW  - commodity cellphone
KW  - augmented reality app
KW  - consumer demand
KW  - mobile phone applications
KW  - AR apps
KW  - RGB image sequence
KW  - point clouds
KW  - distributed pipeline
KW  - Dex-Net AR
KW  - Dex-Net grasp planner
KW  - error estimation
KW  - robot gripper
KW  - iPhone
KW  - ARKit
KW  - Three-dimensional displays
KW  - Cameras
KW  - Planning
KW  - Sensors
KW  - Smart phones
KW  - Feature extraction
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197247
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Consumer demand for augmented reality (AR) in mobile phone applications, such as the Apple ARKit. Such applications have potential to expand access to robot grasp planning systems such as Dex-Net. AR apps use structure from motion methods to compute a point cloud from a sequence of RGB images taken by the camera as it is moved around an object. However, the resulting point clouds are often noisy due to estimation errors. We present a distributed pipeline, Dex-Net AR, that allows point clouds to be uploaded to a server in our lab, cleaned, and evaluated by Dex-Net grasp planner to generate a grasp axis that is returned and displayed as an overlay on the object. We implement Dex-Net AR using the iPhone and ARKit and compare results with those generated with high-performance depth sensors. The success rates with AR on harder adversarial objects are higher than traditional depth images. The server URL is https://sites.google.com/berkeley.edu/dex-net-ar/home.
ER  - 

TY  - CONF
TI  - OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 559
EP  - 566
AU  - C. Won
AU  - H. Seok
AU  - Z. Cui
AU  - M. Pollefeys
AU  - J. Lim
PY  - 2020
KW  - cameras
KW  - computerised instrumentation
KW  - distance measurement
KW  - image matching
KW  - image reconstruction
KW  - image sequences
KW  - neural nets
KW  - stereo image processing
KW  - omnidirectional localization
KW  - wide-baseline multicamera systems
KW  - dense mapping system
KW  - wide-baseline multiview stereo setup
KW  - ultrawide field-of-view fisheye cameras
KW  - stereo observations
KW  - light-weighted deep neural networks
KW  - loop closing module
KW  - efficient feature matching process
KW  - omnidirectional depth maps
KW  - truncated signed distance function volume
KW  - rig estimation
KW  - omnidirectional depth map estimation
KW  - VO
KW  - FOV
KW  - TSDF
KW  - OmniSLAM
KW  - Cameras
KW  - Three-dimensional displays
KW  - Estimation
KW  - Feature extraction
KW  - Visual odometry
KW  - Sensors
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196695
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360¬∞ coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.
ER  - 

TY  - CONF
TI  - What‚Äôs in my Room? Object Recognition on Indoor Panoramic Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 567
EP  - 573
AU  - J. Guerrero-Viu
AU  - C. Fernandez-Labrador
AU  - C. Demonceaux
AU  - J. J. Guerrero
PY  - 2020
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - natural scenes
KW  - neural nets
KW  - object detection
KW  - object recognition
KW  - solid modelling
KW  - 3D model
KW  - instance segmentation masks
KW  - equirectangular images
KW  - deep learning
KW  - semantic segmentation tasks
KW  - object detection
KW  - object recognition system
KW  - indoor scenes
KW  - indoor panoramic images
KW  - Image segmentation
KW  - Object recognition
KW  - Three-dimensional displays
KW  - Semantics
KW  - Task analysis
KW  - Layout
KW  - Distortion
DO  - 10.1109/ICRA40945.2020.9197335
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the last few years, there has been a growing interest in taking advantage of the 360¬∞ panoramic images potential, while managing the new challenges they imply. While several tasks have been improved thanks to the contextual information these images offer, object recognition in indoor scenes still remains a challenging problem that has not been deeply investigated. This paper provides an object recognition system that performs object detection and semantic segmentation tasks by using a deep learning model adapted to match the nature of equirectangular images. From these results, instance segmentation masks are recovered, refined and transformed into 3D bounding boxes that are placed into the 3D model of the room. Quantitative and qualitative results support that our method outperforms the state of the art by a large margin and show a complete understanding of the main objects in indoor scenes.
ER  - 

TY  - CONF
TI  - FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 574
EP  - 581
AU  - V. R. Kumar
AU  - S. A. Hiremath
AU  - M. Bach
AU  - S. Milz
AU  - C. Witt
AU  - C. Pinard
AU  - S. Yogamani
AU  - P. M√§der
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image classification
KW  - image motion analysis
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - road vehicles
KW  - robot vision
KW  - traffic engineering computing
KW  - video signal processing
KW  - autonomous driving
KW  - nonlinear distortions
KW  - complex algorithms
KW  - Euclidean distance estimation
KW  - fisheye cameras
KW  - automotive scenes
KW  - accurate depth supervision
KW  - dense depth supervision
KW  - self-supervised learning approaches
KW  - self-supervised scale-aware framework
KW  - raw monocular fisheye videos
KW  - applying rectification
KW  - piece-wise linear approximation
KW  - fisheye projection surface
KW  - re-sampling distortion
KW  - monocular methods
KW  - unseen fisheye video
KW  - self-supervised scale-aware distance estimation
KW  - monocular fisheye camera
KW  - Cameras
KW  - Estimation
KW  - Training
KW  - Euclidean distance
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197319
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fisheye cameras are commonly used in applications like autonomous driving and surveillance to provide a large field of view (> 180o). However, they come at the cost of strong non-linear distortions which require more complex algorithms. In this paper, we explore Euclidean distance estimation on fisheye cameras for automotive scenes. Obtaining accurate and dense depth supervision is difficult in practice, but self-supervised learning approaches show promising results and could potentially overcome the problem. We present a novel self-supervised scale-aware framework for learning Euclidean distance and ego-motion from raw monocular fisheye videos without applying rectification. While it is possible to perform piece-wise linear approximation of fisheye projection surface and apply standard rectilinear models, it has its own set of issues like re-sampling distortion and discontinuities in transition regions. To encourage further research in this area, we will release our dataset as part of the WoodScape project [1]. We further evaluated the proposed algorithm on the KITTI dataset and obtained state-of-the-art results comparable to other self-supervised monocular methods. Qualitative results on an unseen fisheye video demonstrate impressive performance1.
ER  - 

TY  - CONF
TI  - 360SD-Net: 360¬∞ Stereo Depth Estimation with Learnable Cost Volume
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 582
EP  - 588
AU  - N. -H. Wang
AU  - B. Solarte
AU  - Y. -H. Tsai
AU  - W. -C. Chiu
AU  - M. Sun
PY  - 2020
KW  - neural nets
KW  - stereo image processing
KW  - 360SD-Net
KW  - stereo depth estimation
KW  - learnable cost volume
KW  - end-to-end trainable deep neural networks
KW  - perspective images
KW  - equirectangular projection
KW  - distortion issue
KW  - learnable shifting filter
KW  - stereo datasets
KW  - camera pairs
KW  - spherical disparity
KW  - Cameras
KW  - Estimation
KW  - Three-dimensional displays
KW  - Distortion
KW  - Feature extraction
KW  - Convolution
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196975
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, end-to-end trainable deep neural networks have significantly improved stereo depth estimation for perspective images. However, 360¬∞ images captured under equirectangular projection cannot benefit from directly adopting existing methods due to distortion introduced (i.e., lines in 3D are not projected onto lines in 2D). To tackle this issue, we present a novel architecture specifically designed for spherical disparity using the setting of top-bottom 360¬∞ camera pairs. Moreover, we propose to mitigate the distortion issue by (1) an additional input branch capturing the position and relation of each pixel in the spherical coordinate, and (2) a cost volume built upon a learnable shifting filter. Due to the lack of 360¬∞ stereo data, we collect two 360¬∞ stereo datasets from Matterport3D and Stanford3D for training and evaluation. Extensive experiments and ablation study are provided to validate our method against existing algorithms. Finally, we show promising results on real-world environments capturing images with two consumer-level cameras. Our project page is at https://albert100121.github.io/360SD-Net-Project-Page.
ER  - 

TY  - CONF
TI  - Omnidirectional Depth Extension Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 589
EP  - 595
AU  - X. Cheng
AU  - P. Wang
AU  - Y. Zhou
AU  - C. Guan
AU  - R. Yang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image reconstruction
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - stereo image processing
KW  - omnidirectional depth extension networks
KW  - omnidirectional 360¬∞ camera
KW  - autonomous robots
KW  - perception ability
KW  - FoV
KW  - depth sensors
KW  - perception system
KW  - low-cost 3D sensing system
KW  - omnidirectional camera
KW  - calibrated projective depth camera
KW  - recorded omnidirectional image
KW  - omnidirectional depth extension convolutional neural network
KW  - spherical feature
KW  - feature encoding layers
KW  - deformable convolutional spatial propagation network
KW  - feature decoding layers
KW  - omnidirectional coordination
KW  - projective coordination
KW  - feature learning
KW  - estimated depths
KW  - proposed ODE-CNN
KW  - popular 360D dataset
KW  - depth error
KW  - Convolution
KW  - Estimation
KW  - Sensors
KW  - Cameras
KW  - Transforms
KW  - Kernel
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197123
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Omnidirectional 360¬∞ camera proliferates rapidly for autonomous robots since it significantly enhances the perception ability by widening the field of view (FoV). However, corresponding 360¬∞ depth sensors, which are also critical for the perception system, are still difficult or expensive to have. In this paper, we propose a low-cost 3D sensing system that combines an omnidirectional camera with a calibrated projective depth camera, where the depth from the limited FoV can be automatically extended to the rest of recorded omnidirectional image. To accurately recover the missing depths, we design an omnidirectional depth extension convolutional neural network (ODE-CNN), in which a spherical feature transform layer (SFTL) is embedded at the end of feature encoding layers, and a deformable convolutional spatial propagation network (D-CSPN) is appended at the end of feature decoding layers. The former re-samples the neighborhood of each pixel in the omnidirectional coordination to the projective coordination, which reduce the difficulty of feature learning, and the later automatically finds a proper context to well align the structures in the estimated depths via CNN w.r.t. the reference image, which significantly improves the visual quality. Finally, we demonstrate the effectiveness of proposed ODE-CNN over the popular 360D dataset, and show that ODE-CNN significantly outperforms (relatively 33% reduction in depth error) other state-of-the-art (SoTA) methods.
ER  - 

TY  - CONF
TI  - 3D Orientation Estimation and Vanishing Point Extraction from Single Panoramas Using Convolutional Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 596
EP  - 602
AU  - Y. Shi
AU  - X. Tong
AU  - J. Wen
AU  - H. Zhao
AU  - X. Ying
AU  - H. Zha
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - regression analysis
KW  - stereo image processing
KW  - convolutional neural network
KW  - 3D orientation estimation
KW  - computer vision
KW  - 3D scene understanding
KW  - single spherical panorama
KW  - labeled 3D orientation
KW  - vanishing point extraction
KW  - single panoramas
KW  - VOP60K
KW  - Google Street View
KW  - pinhole cameras
KW  - panorama geometric information
KW  - two column vector regression loss
KW  - rotation matrix
KW  - CNN architecture
KW  - classification loss
KW  - edge extractor layer
KW  - Cameras
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Google
KW  - Estimation
KW  - Data mining
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9196966
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - 3D orientation estimation is a key component of many important computer vision tasks such as autonomous navigation and 3D scene understanding. This paper presents a new CNN architecture to estimate the 3D orientation of an omnidirectional camera with respect to the world coordinate system from a single spherical panorama. To train the proposed architecture, we leverage a dataset of panoramas named VOP60K from Google Street View with labeled 3D orientation, including 50 thousand panoramas for training and 10 thousand panoramas for testing. Previous approaches usually estimate 3D orientation under pinhole cameras. However, for a panorama, due to its larger field of view, previous approaches cannot be suitable. In this paper, we propose an edge extractor layer to utilize the low-level and geometric information of panorama, an attention module to fuse different features generated by previous layers. A regression loss for two column vectors of the rotation matrix and classification loss for the position of vanishing points are added to optimize our network simultaneously. The proposed algorithm is validated on our benchmark, and experimental results clearly demonstrate that it outperforms previous methods.
ER  - 

TY  - CONF
TI  - Curvature sensing with a spherical tactile sensor using the color-interference of a marker array
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 603
EP  - 609
AU  - X. Lin
AU  - L. Willemet
AU  - A. Bailleul
AU  - M. Wiertlewski
PY  - 2020
KW  - cameras
KW  - colour
KW  - elasticity
KW  - force sensors
KW  - image colour analysis
KW  - indentation
KW  - tactile sensors
KW  - subtractive color-mixing principle
KW  - planar manufacturing process
KW  - functional features
KW  - ChromaTouch
KW  - millimeter-size indentation
KW  - elastic membrane
KW  - 3-dimensional displacement field
KW  - distributed tactile sensor
KW  - fine distributed sense
KW  - robots
KW  - fine manipulation
KW  - local shape
KW  - soft fingers
KW  - cues
KW  - marker array
KW  - color-interference
KW  - spherical tactile sensor
KW  - curvature sensing
KW  - normal sensing
KW  - contact mechanics
KW  - Hertz contact theory
KW  - curved surface
KW  - relative motion
KW  - colored patches
KW  - local 3d displacement
KW  - measurement points
KW  - spherical shape
KW  - flat functional panels
KW  - flat surface
KW  - size 40 mm
KW  - Shape
KW  - Cameras
KW  - Tactile sensors
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197050
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The only way to perceive a small object held between our fingers is to trust our sense of touch. Touch provides cues about the state of the contact even if its view is occluded by the finger. The interaction between the soft fingers and the surface reveals crucial information, such as the local shape of the object, that plays a central role in fine manipulation. In this work, we present a new spherical sensor that endows robots with a fine distributed sense of touch. This sensor is an evolution of our distributed tactile sensor that measures the dense 3-dimensional displacement field of an elastic membrane, using the subtractive color-mixing principle. We leverage a planar manufacturing process that enables the design and manufacturing of the functional features on a flat surface. The flat functional panels are then folded to create a spherical shape able to sense a wide variety of objects.The resulting 40mm-diameter spherical sensor has 77 measurement points, each of which gives an estimation of the local 3d displacement, normal and tangential to the surface. Each marker is built around 2 sets of colored patches placed at different depths. The relative motion and resulting hue of each marker, easily captured by an embedded RGB camera, provides a measurement of their 3d motion. To benchmark the sensor, we compared the measurements obtained while pressing the sensor on a curved surface with Hertz contact theory, a hallmark of contact mechanics. While the mechanics did strictly follow Hertz contact theory, using the shear and normal sensing, ChromaTouch can estimate the curvature of an object after a millimeter-size indentation of the sensor.
ER  - 

TY  - CONF
TI  - Center-of-Mass-based Robust Grasp Planning for Unknown Objects Using Tactile-Visual Sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 610
EP  - 617
AU  - Q. Feng
AU  - Z. Chen
AU  - J. Deng
AU  - C. Gao
AU  - J. Zhang
AU  - A. Knoll
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - path planning
KW  - robot vision
KW  - tactile sensors
KW  - Franka Emika robot arm
KW  - tactile sensors
KW  - multisensor modules
KW  - regrasp planner
KW  - slip detection
KW  - visual sensors
KW  - center-of-mass-based robust grasp planning
KW  - Grasping
KW  - Tactile sensors
KW  - Force
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196815
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An unstable grasp pose can lead to slip, thus an unstable grasp pose can be predicted by slip detection. A regrasp is required afterwards to correct the grasp pose in order to finish the task. In this work, we propose a novel regrasp planner with multi-sensor modules to plan grasp adjustments with the feedback from a slip detector. Then a regrasp planner is trained to estimate the location of center of mass, which helps robots find an optimal grasp pose. The dataset in this work consists of 1 025 slip experiments and 1 347 regrasps collected by one pair of tactile sensors, an RGB-D camera and one Franka Emika robot arm equipped with joint force/torque sensors. We show that our algorithm can successfully detect and classify the slip for 5 unknown test objects with an accuracy of 76.88% and a regrasp planner increases the grasp success rate by 31.0% compared to the state-of-the-art vision-based grasping algorithm.
ER  - 

TY  - CONF
TI  - OmniTact: A Multi-Directional High-Resolution Touch Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 618
EP  - 624
AU  - A. Padmanabha
AU  - F. Ebert
AU  - S. Tian
AU  - R. Calandra
AU  - C. Finn
AU  - S. Levine
PY  - 2020
KW  - cameras
KW  - computer vision
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - gels
KW  - image resolution
KW  - microsensors
KW  - neurocontrollers
KW  - sensor fusion
KW  - state estimation
KW  - tactile sensors
KW  - dexterous robotic manipulation
KW  - deep convolutional neural networks
KW  - electrical connector
KW  - multidirectional high-resolution touch sensor
KW  - low-resolution signals
KW  - multidirectional high-resolution tactile sensor
KW  - robotic hands
KW  - multiple microcameras
KW  - multidirectional deformations
KW  - gel-based skin
KW  - contact state variables
KW  - image processing
KW  - computer vision methods
KW  - state estimation problem
KW  - robotic control task
KW  - OmniTact combination
KW  - Cameras
KW  - Tactile sensors
KW  - Sensitivity
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196712
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Incorporating touch as a sensing modality for robots can enable finer and more robust manipulation skills. Existing tactile sensors are either flat, have small sensitive fields or only provide low-resolution signals. In this paper, we introduce OmniTact, a multi-directional high-resolution tactile sensor. OmniTact is designed to be used as a fingertip for robotic manipulation with robotic hands, and uses multiple micro-cameras to detect multi-directional deformations of a gel-based skin. This provides a rich signal from which a variety of different contact state variables can be inferred using modern image processing and computer vision methods. We evaluate the capabilities of OmniTact on a challenging robotic control task that requires inserting an electrical connector into an outlet, as well as a state estimation problem that is representative of those typically encountered in dexterous robotic manipulation, where the goal is to infer the angle of contact of a curved finger pressing against an object. Both tasks are performed using only touch sensing and deep convolutional neural networks to process images from the sensor's cameras. We compare with a state-of-the-art tactile sensor that is only sensitive on one side, as well as a state-of-the-art multi-directional tactile sensor, and find that OmniTact's combination of high-resolution and multi-directional sensing is crucial for reliably inserting the electrical connector and allows for higher accuracy in the state estimation task. Videos and supplementary material can be found here4.
ER  - 

TY  - CONF
TI  - Highly sensitive bio-inspired sensor for fine surface exploration and characterization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 625
EP  - 631
AU  - P. Ribeiro
AU  - S. Cardoso
AU  - A. Bernardino
AU  - L. Jamone
PY  - 2020
KW  - assembling
KW  - magnetic field measurement
KW  - magnetic sensors
KW  - magnetoresistive devices
KW  - permanent magnets
KW  - signal detection
KW  - surface texture
KW  - surface topography measurement
KW  - tunnelling magnetoresistance
KW  - bioinspired sensor
KW  - fine surface exploration
KW  - texture sensing
KW  - robotics
KW  - texture topography sensor
KW  - ciliary structure
KW  - biological structure
KW  - permanent magnetization
KW  - tunneling magnetoresistance sensor
KW  - surface texture
KW  - electronic signal acquisition board
KW  - topography scanner
KW  - elastic cilia brush
KW  - TMR sensor
KW  - surface roughness
KW  - size 7.0 mum
KW  - size 9.2 mum to 213.0 mum
KW  - Robot sensing systems
KW  - Tunneling magnetoresistance
KW  - Magnetic separation
KW  - Substrates
KW  - Bridge circuits
KW  - Magnetic tunneling
DO  - 10.1109/ICRA40945.2020.9197305
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Texture sensing is one of the types of information sensed by humans through touch, and is thus of interest to robotics that this type of information can be acquired and processed. In this work we present a texture topography sensor based on a ciliary structure, a biological structure found in many organisms. The device consists of up to 9 elastic cilia with permanent magnetization assembled on top of a highly sensitive tunneling magnetoresistance (TMR) sensor, within a compact footprint of 6√ó6 mm2. When these cilia brush against some textured surface, their movement and vibrations give rise to a signal that can be correlated to the characteristics of the texture being measured. We also present an electronic signal acquisition board, used in this work. Various configurations of cilia sizes are tested, with the most precise being capable of differentiating different types of sandpaper from 9.2 Œºm to 213 Œºm average surface roughness with a 7 Œºm resolution. As a topography scanner the sensor was able to scan a 20 Œºm high step in a flat surface.
ER  - 

TY  - CONF
TI  - Implementing Tactile and Proximity Sensing for Crack Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 632
EP  - 637
AU  - F. Palermo
AU  - J. Konstantinova
AU  - K. Althoefer
AU  - S. Poslad
AU  - I. Farkhatdinov
PY  - 2020
KW  - crack detection
KW  - learning (artificial intelligence)
KW  - object detection
KW  - pattern classification
KW  - tactile sensors
KW  - telerobotics
KW  - video cameras
KW  - proximity sensing
KW  - remote characterisation
KW  - physical robot-environment interaction
KW  - automatic crack detection
KW  - proximity sensor
KW  - physical environment
KW  - fibre optics
KW  - cracks
KW  - bumps
KW  - undulations
KW  - machine learning
KW  - classifier
KW  - average crack detection accuracy
KW  - width classification accuracy
KW  - Kruskal-Wallis results
KW  - force data
KW  - proximity data
KW  - optical fibres
KW  - extreme environments
KW  - Robot sensing systems
KW  - Force
KW  - Surface cracks
KW  - Optics
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196936
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Remote characterisation of the environment during physical robot-environment interaction is an important task commonly accomplished in telerobotics. This paper demonstrates how tactile and proximity sensing can be efficiently used to perform automatic crack detection. A custom-designed integrated tactile and proximity sensor is implemented. It measures the deformation of its body when interacting with the physical environment and distance to the environment's objects with the help of fibre optics. This sensor was used to slide across different surfaces and the data recorded during the experiments was used to detect and classify cracks, bumps and undulations. The proposed method uses machine learning techniques (mean absolute value as feature and random forest as classifier) to detect cracks and determine their width. An average crack detection accuracy of 86.46% and width classification accuracy of 57.30% is achieved. Kruskal-Wallis results (p<; 0.001) indicate statistically significant differences among results obtained when analysing only force data, only proximity data and both force and proximity data. In contrast to previous techniques, which mainly rely on visual modality, the proposed approach based on optical fibres is suitable for operation in extreme environments, such as nuclear facilities in which nuclear radiation may damage the electronic components of video cameras.
ER  - 

TY  - CONF
TI  - Novel Proximity Sensor for Realizing Tactile Sense in Suction Cups
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 638
EP  - 643
AU  - S. Doi
AU  - H. Koga
AU  - T. Seki
AU  - Y. Okuno
PY  - 2020
KW  - capacitance measurement
KW  - capacitive sensors
KW  - deformation
KW  - mechanical variables measurement
KW  - object detection
KW  - tactile sensors
KW  - partial contact position estimation
KW  - push-in stroke detection
KW  - deformation detection
KW  - tactile sensor module
KW  - capacitive proximity sensor module
KW  - suction cup
KW  - Robot sensing systems
KW  - Capacitance
KW  - Electrodes
KW  - Capacitance measurement
KW  - Sensitivity
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196726
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a new capacitive proximity sensor that detects deformations of a suction cup as a tactile sense. We confirmed that one sensor module provides three applications for reliable picking and a simplified setup. The first application is the picking height decision. The second one is the placing height decision for detecting whether the grasped object is placed on the placement surface. These two applications are achieved by detecting the push-in stroke of the suction cup. The final application is detection of whether the suction cup is in partial contact or full contact with the object. This function can correct the picking posture as well as detect whether picking is possible before the pull-up motion. We also demonstrate that the partial contact position can be estimated in real time.
ER  - 

TY  - CONF
TI  - Constrained Filtering-based Fusion of Images, Events, and Inertial Measurements for Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 644
EP  - 650
AU  - J. H. Jung
AU  - C. Gook Park
PY  - 2020
KW  - cameras
KW  - image filtering
KW  - image sensors
KW  - image sequences
KW  - pose estimation
KW  - constrained filtering-based fusion
KW  - inertial measurements
KW  - pose estimation
KW  - camera poses
KW  - bio-inspired sensor
KW  - brightness changes
KW  - independent pixels
KW  - high dynamic range
KW  - optical flow
KW  - intensity images
KW  - pixel difference
KW  - inverse scene-depth
KW  - DVS dataset
KW  - filtering-based estimator
KW  - dynamic vision sensor
KW  - Optical imaging
KW  - Optical filters
KW  - Optical sensors
KW  - Integrated optics
KW  - Optical variables measurement
KW  - Adaptive optics
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197248
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a novel filtering-based method that fuses events from a dynamic vision sensor (DVS), images, and inertial measurements to estimate camera poses. A DVS is a bio-inspired sensor that generates events triggered by brightness changes. It can cover the drawbacks of a conventional camera by virtual of its independent pixels and high dynamic range. Specifically, we focus on optical flow obtained from both a stream of events and intensity images in which the former is much like a differential quantity, whereas the latter is a pixel difference in a much longer time interval than events. This nature characteristic motivates us to model optical flow estimated from events directly, but feature tracks for images in the filter design. An inequality constraint is considered in our method since the inverse scene-depth is larger than zero by its definition. Furthermore, we evaluate our proposed method in the benchmark DVS dataset and a dataset collected by the authors. The results reveal that the presented algorithm has reduced the position error by 49.9% on average and comparable accuracy only using events when compared to the state-of-the-art filtering-based estimator.
ER  - 

TY  - CONF
TI  - Schmidt-EKF-based Visual-Inertial Moving Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 651
EP  - 657
AU  - K. Eckenhoff
AU  - P. Geneva
AU  - N. Merrill
AU  - G. Huang
PY  - 2020
KW  - image motion analysis
KW  - image representation
KW  - Kalman filters
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - target tracking
KW  - tracking sensor
KW  - target motion model
KW  - Schmidt-EKF-based visual-inertial moving object tracking
KW  - tightly-coupled estimation
KW  - visual-inertial localization
KW  - joint estimation system
KW  - Schmidt-Kalman Filter
KW  - ego-motion accuracy degradation
KW  - robot-centric representation
KW  - object pose tracking
KW  - Target tracking
KW  - Robot sensing systems
KW  - Estimation
KW  - Three-dimensional displays
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197352
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we investigate the effect of tightly-coupled estimation on the performance of visual-inertial localization and dynamic object pose tracking. In particular, we show that while a joint estimation system outperforms its decoupled counterpart when given a "proper" model for the target's motion, inconsistent modeling, such as choosing improper levels for the target's propagation noises, can actually lead to a degradation in ego-motion accuracy. To address the realistic scenario where a good prior knowledge of the target's motion model is not available, we design a new system based on the Schmidt-Kalman Filter (SKF), in which target measurements do not update the navigation states, however all correlations are still properly tracked. This allows for both consistent modeling of the target errors and the ability to update target estimates whenever the tracking sensor receives non-target data such as bearing measurements to static, 3D environmental features. We show in extensive simulation that this system, along with a robot-centric representation of the target, leads to robust estimation performance even in the presence of an inconsistent target motion model. Finally, the system is validated in a real-world experiment, and is shown to offer accurate localization and object pose tracking performance.
ER  - 

TY  - CONF
TI  - Learning View and Target Invariant Visual Servoing for Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 658
EP  - 664
AU  - Y. Li
AU  - J. Ko≈°ecka
PY  - 2020
KW  - convolutional neural nets
KW  - feedback
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - visual servoing
KW  - deep reinforcement learning
KW  - mobile robot navigation
KW  - deep convolutional network controller
KW  - viewpoint invariant visual servoing
KW  - target invariant visual servoing
KW  - feedback control error
KW  - Visual servoing
KW  - Navigation
KW  - Visualization
KW  - Feature extraction
KW  - Task analysis
KW  - Semantics
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197136
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment [1]. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.
ER  - 

TY  - CONF
TI  - Tightly-Coupled Single-Anchor Ultra-wideband-Aided Monocular Visual Odometry System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 665
EP  - 671
AU  - T. H. Nguyen
AU  - T. -M. Nguyen
AU  - L. Xie
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - graph theory
KW  - least squares approximations
KW  - optimisation
KW  - pose estimation
KW  - position measurement
KW  - robot vision
KW  - Levenberg-Marquardt nonlinear least squares optimization scheme
KW  - scale factor
KW  - visual features
KW  - pose-graph optimization scheme
KW  - landmark reprojection errors
KW  - visual drift
KW  - monocular visual feature observations
KW  - distance measurements
KW  - ultrawideband-aided monocular visual odometry system
KW  - single-anchor monocular visual odometry system
KW  - tightly-coupled odometry framework
KW  - anchor position estimation
KW  - robot operating system
KW  - Cameras
KW  - Distance measurement
KW  - Robot sensing systems
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196794
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose a tightly-coupled odometry framework, which combines monocular visual feature observations with distance measurements provided by a single ultra-wideband (UWB) anchor with an initial guess for its location. Firstly, the scale factor and the anchor position in the vision frame will be simultaneously estimated using a variant of Levenberg-Marquardt non-linear least squares optimization scheme. Once the scale factor is obtained, the map of visual features is updated with the new scale. Subsequent ranging errors in a sliding window are continuously monitored and the estimation procedure will be reinitialized to refine the estimates. Lastly, range measurements and anchor position estimates are fused when needed into a pose-graph optimization scheme to minimize both the landmark reprojection errors and ranging errors, thus reducing the visual drift and improving the system robustness. The proposed method is implemented in Robot Operating System (ROS) and can function in real-time. The performance is validated on both public datasets and real-life experiments and compared with state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Scaling Local Control to Large-Scale Topological Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 672
EP  - 678
AU  - X. Meng
AU  - N. Ratliff
AU  - Y. Xiang
AU  - D. Fox
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - deep learning
KW  - robot perception
KW  - reliability issue
KW  - world images
KW  - mechanical constraints
KW  - local controller
KW  - large-scale visual topological navigation
KW  - large-scale environments
KW  - local control
KW  - large-scale topological navigation
KW  - Navigation
KW  - Trajectory
KW  - Robustness
KW  - Robot kinematics
KW  - Planning
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196644
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual topological navigation has been revitalized recently thanks to the advancement of deep learning that substantially improves robot perception. However, the scalability and reliability issue remain challenging due to the complexity and ambiguity of real world images and mechanical constraints of real robots. We present an intuitive approach to show that by accurately measuring the capability of a local controller, large-scale visual topological navigation can be achieved while being scalable and robust. Our approach achieves state-of-the-art results in trajectory following and planning in large-scale environments. It also generalizes well to real robots and new environments without retraining or finetuning.
ER  - 

TY  - CONF
TI  - Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 679
EP  - 685
AU  - X. Pan
AU  - T. Zhang
AU  - B. Ichter
AU  - A. Faust
AU  - J. Tan
AU  - S. Ha
PY  - 2020
KW  - graphical user interfaces
KW  - humanoid robots
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - cost-effective data collection
KW  - third-person demonstrations
KW  - camera perspectives
KW  - perspective-invariant state features
KW  - model-based imitation learning approach
KW  - action-labeled human demonstrations
KW  - effective policy
KW  - zero-shot imitation
KW  - legged robot visual navigation
KW  - training effective visual navigation policies
KW  - expert demonstrations
KW  - goal-driven visual navigation policy
KW  - high-quality navigation
KW  - Feature extraction
KW  - Navigation
KW  - Legged locomotion
KW  - Visualization
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196602
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Imitation learning is a popular approach for training effective visual navigation policies. However, collecting expert demonstrations for legged robots is challenging as these robots can be hard to control, move slowly, and cannot operate continuously for long periods of time. In this work, we propose a zero-shot imitation learning framework for training a goal-driven visual navigation policy on a legged robot from human demonstrations (third-person perspective), allowing for high-quality navigation and cost-effective data collection. However, imitation learning from third-person demonstrations raises unique challenges. First, these demonstrations are captured from different camera perspectives, which we address via a feature disentanglement network (FDN) that extracts perspective-invariant state features. Second, as transition dynamics vary between systems, we reconstruct missing action labels by either building an inverse model of the robot's dynamics in the feature space and applying it to the human demonstrations or developing a Graphic User Interface (GUI) to label human demonstrations. To train a navigation policy we use a model-based imitation learning approach with FDN and action-labeled human demonstrations. We show that our framework can learn an effective policy for a legged robot, Laikago, from human demonstrations in both simulated and real-world environments. Our approach is zero-shot as the robot never navigates the same paths during training as those at testing time. We justify our framework by performing a comparative study.
ER  - 

TY  - CONF
TI  - Pressure-Driven Manipulator with Variable Stiffness Structure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 696
EP  - 702
AU  - C. Sozer
AU  - L. Patern√≤
AU  - G. Tortora
AU  - A. Menciassi
PY  - 2020
KW  - actuators
KW  - controllability
KW  - deformation
KW  - fibre reinforced composites
KW  - manipulators
KW  - position control
KW  - rigidity
KW  - air pressure
KW  - controllability
KW  - position dependent stiffness
KW  - manipulator stiffness
KW  - variable stiffness structure
KW  - pressure driven manipulator
KW  - soft rigid stiffness control structure
KW  - unidirectional fiber reinforced actuators
KW  - soft robot deformability
KW  - soft robot compliance
KW  - bidirectional in-plane manipulator
KW  - Manipulators
KW  - Actuators
KW  - Encapsulation
KW  - Fabrication
KW  - Force
KW  - Controllability
DO  - 10.1109/ICRA40945.2020.9197401
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The high deformability and compliance of soft robots allow safer interaction with the environment. On the other hand, these advantages bring along controllability and predictability challenges which result in loss of force and stiffness output. Such challenges should be addressed in order to improve the overall functional performance and to meet the requirements of real-scenario applications. In this paper, we present a bidirectional in-plane manipulator which consists of two unidirectional fiber-reinforced actuators (FRAs) and a hybrid soft-rigid stiffness control structure (SCS), all of them controlled by air pressure. Both controllability and predictability of the manipulator are enhanced by the hybrid soft-rigid structure. While the FRAs provide positioning and position dependent stiffness, the SCS increases the stiffness of the manipulator without position dependency. The SCS is able to increase the manipulator stiffness by 35%, 30%, and 18%, when one FRA is pressurized at 150 kPa, 75 kPa, and 0 kPa, respectively. Experiments are carried out to present the feasibility of the proposed manipulator.
ER  - 

TY  - CONF
TI  - Human Interface for Teleoperated Object Manipulation with a Soft Growing Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 726
EP  - 732
AU  - F. Stroppa
AU  - M. Luo
AU  - K. Yoshida
AU  - M. M. Coad
AU  - L. H. Blumenschein
AU  - A. M. Okamura
PY  - 2020
KW  - biomechanics
KW  - human-robot interaction
KW  - manipulators
KW  - robots
KW  - service robots
KW  - telerobotics
KW  - soft growing robot manipulator
KW  - body-movement-based interface
KW  - pick-and-place manipulation task
KW  - human-centered interface
KW  - complex manipulation tasks
KW  - teleoperated object manipulation
KW  - human interface
KW  - Robot kinematics
KW  - Task analysis
KW  - Manipulators
KW  - Soft robotics
KW  - Tracking
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197094
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft growing robots are proposed for use in applications such as complex manipulation tasks or navigation in disaster scenarios. Safe interaction and ease of production promote the usage of this technology, but soft robots can be challenging to teleoperate due to their unique degrees of freedom. In this paper, we propose a human-centered interface that allows users to teleoperate a soft growing robot for manipulation tasks using arm movements. A study was conducted to assess the intuitiveness of the interface and the performance of our soft robot, involving a pick-and-place manipulation task. The results show that users were able to complete the task 97% of the time and achieve placement errors below 2 cm on average. These results demonstrate that our body-movement-based interface is an effective method for control of a soft growing robot manipulator.
ER  - 

TY  - CONF
TI  - Modulating hip stiffness with a robotic exoskeleton immediately changes gait
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 733
EP  - 739
AU  - J. Lee
AU  - H. R. Warren
AU  - V. Agarwal
AU  - M. E. Huber
AU  - N. Hogan
PY  - 2020
KW  - gait analysis
KW  - kinematics
KW  - medical control systems
KW  - medical robotics
KW  - patient rehabilitation
KW  - modulating hip stiffness
KW  - robotic exoskeleton
KW  - healthy kinematics
KW  - critical component
KW  - assisting rehabilitating impaired locomotion
KW  - spatiotemporal gait patterns
KW  - mechanical impedance
KW  - hip joints
KW  - Samsung GEMS-H
KW  - virtual spring
KW  - short repeated exposures
KW  - spatiotemporal measures
KW  - stiffness controller
KW  - gait behavior
KW  - mechanical effect
KW  - lower-limb assistive devices
KW  - healthy gait patterns
KW  - Hip
KW  - Exoskeletons
KW  - Legged locomotion
KW  - Read only memory
KW  - Kinematics
KW  - Springs
KW  - Time measurement
DO  - 10.1109/ICRA40945.2020.9197054
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Restoring healthy kinematics is a critical component of assisting and rehabilitating impaired locomotion. Here we tested whether spatiotemporal gait patterns can be modulated by applying mechanical impedance to hip joints. Using the Samsung GEMS-H exoskeleton, we emulated a virtual spring (positive and negative) between the user's legs. We found that applying positive stiffness with the exoskeleton decreased stride time and hip range of motion for healthy subjects during treadmill walking. Conversely, the application of negative stiffness increased stride time and hip range of motion. These effects did not vary over long nor short repeated exposures to applied stiffness. In addition, minimal transient behavior was observed in spatiotemporal measures of gait when the stiffness controller transitioned between on and off states. These results suggest that changes in gait behavior induced by applying hip stiffness were purely a mechanical effect. Together, our findings indicate that applying mechanical impedance using lower-limb assistive devices may be an effective, minimally-encumbering intervention to restore healthy gait patterns.
ER  - 

TY  - CONF
TI  - Swing-Assist for Enhancing Stair Ambulation in a Primarily-Passive Knee Prosthesis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 740
EP  - 746
AU  - J. T. Lee
AU  - M. Goldfarb
PY  - 2020
KW  - artificial limbs
KW  - gait analysis
KW  - kinematics
KW  - medical control systems
KW  - stair ascent
KW  - SCSA prosthesis prototype
KW  - unilateral transfemoral amputee
KW  - motion capture apparatus
KW  - microprocessor-controlled daily-use
KW  - passive prosthesis
KW  - SCSA knee
KW  - stair activity
KW  - swing-assist
KW  - enhancing stair ambulation
KW  - primarily-passive knee prosthesis
KW  - primarily-passive stance-controlled swing
KW  - Knee
KW  - Prosthetics
KW  - Valves
KW  - Hip
KW  - Torque
KW  - Actuators
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196974
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the design and implementation of a controller for stair ascent and descent in a primarily-passive stance-controlled swing-assist (SCSA) prosthesis. The prosthesis and controller enable users to perform both step-over and step-to stair ascent and descent. The efficacy of the controller and SCSA prosthesis prototype in providing improved stair ambulation was tested on a unilateral transfemoral amputee in experiments that employed motion capture apparatus to compare joint kinematics with the SCSA prosthesis, relative to performing the same activity with a microprocessor-controlled daily-use passive prosthesis. Results suggest that the SCSA knee significantly decreases compensatory motion during stair activity when compared to the passive prosthesis.
ER  - 

TY  - CONF
TI  - Proof-of-concept of a Pneumatic Ankle Foot Orthosis Powered by a Custom Compressor for Drop Foot Correction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 747
EP  - 753
AU  - S. J. Kim
AU  - J. Park
AU  - W. Shin
AU  - D. Y. Lee
AU  - J. Kim
PY  - 2020
KW  - compressors
KW  - gait analysis
KW  - medical control systems
KW  - orthopaedics
KW  - orthotics
KW  - pneumatic systems
KW  - valves
KW  - unilateral drop-foot patients
KW  - maximum assistive torque
KW  - unilateral active AFO
KW  - pressurized air
KW  - wearable custom compressor
KW  - assistive devices
KW  - stationary air compressors
KW  - pneumatic systems
KW  - mass distribution
KW  - pneumatic components
KW  - powered ankle foot orthosis systems
KW  - pneumatic transmission
KW  - drop foot correction
KW  - pneumatic ankle foot orthosis powered
KW  - pressure 1050.0 kPa
KW  - pressure 550.0 kPa
KW  - Foot
KW  - Torque
KW  - DC motors
KW  - Valves
KW  - Optical sensors
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196817
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pneumatic transmission has several advantages in developing powered ankle foot orthosis (AFO) systems, such as the flexibility in placing pneumatic components for mass distribution and providing high back-drivability via simple valve control. However, pneumatic systems are generally tethered to large stationary air compressors that restrict them for being used as daily assistive devices. In this study, we improved a previously developed wearable (untethered) custom compressor that can be worn (1.5 kg) at the waist of the body and can generate adequate amount of pressurized air (maximum pressure of 1050 kPa and a flow rate of 15.1 mL/sec at 550 kPa) to power a unilateral active AFO used to assist the dorsiflexion (DF) motion of drop-foot patients. The finalized system can provide a maximum assistive torque of 10 Nm and induces an average 0.03¬±0.06 Nm resistive torque when free movement is provided. The system was tested for two unilateral drop-foot patients. The proposed system showed an average improvement of 13.6¬∞ of peak dorsiflexion angle during the swing phase of the gait cycle.
ER  - 

TY  - CONF
TI  - Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 754
EP  - 760
AU  - X. Gao
AU  - J. Si
AU  - Y. Wen
AU  - M. Li
AU  - H. H. Huang
PY  - 2020
KW  - artificial limbs
KW  - biomechanics
KW  - gait analysis
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - neurophysiology
KW  - patient rehabilitation
KW  - knowledge transfer
KW  - control tuning performance
KW  - amputee subject
KW  - AB subjects
KW  - transfer knowledge
KW  - data requirements
KW  - RL controller
KW  - robotic prosthetic limb
KW  - control method
KW  - knowledge-guided Q-learning
KW  - RL agents learn
KW  - controlled prosthesis
KW  - prosthesis control
KW  - prosthesis device
KW  - trans-femoral amputees
KW  - passive prostheses
KW  - lost functions
KW  - robotic lower limb prosthesis
KW  - guided reinforcement learning control
KW  - Prosthetics
KW  - Task analysis
KW  - Impedance
KW  - Knee
KW  - Legged locomotion
KW  - Tuning
DO  - 10.1109/ICRA40945.2020.9196749
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic prostheses provide new opportunities to better restore lost functions than passive prostheses for trans-femoral amputees. But controlling a prosthesis device automatically for individual users in different task environments is an unsolved problem. Reinforcement learning (RL) is a naturally promising tool. For prosthesis control with a user in the loop, it is desirable that the controlled prosthesis can adapt to different task environments as quickly and smoothly as possible. However, most RL agents learn or relearn from scratch when the environment changes. To address this issue, we propose the knowledge-guided Q-learning (KG-QL) control method as a principled way for the problem. In this report, we collected and used data from two able-bodied (AB) subjects wearing a RL controlled robotic prosthetic limb walking on level ground. Our ultimate goal is to build an efficient RL controller with reduced time and data requirements and transfer knowledge from AB subjects to amputee subjects. Toward this goal, we demonstrate its feasibility by employing OpenSim, a well-established human locomotion simulator. Our results show the OpenSim simulated amputee subject improved control tuning performance over learning from scratch by utilizing knowledge transfer from AB subjects. Also in this paper, we will explore the possibility of information transfer from AB subjects to help tuning for the amputee subjects.
ER  - 

TY  - CONF
TI  - Development of a Twisted String Actuator-based Exoskeleton for Hip Joint Assistance in Lifting Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 761
EP  - 767
AU  - H. -S. Seong
AU  - D. -H. Kim
AU  - I. Gaponov
AU  - J. -H. Ryu
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - design engineering
KW  - electric motors
KW  - handicapped aids
KW  - patient rehabilitation
KW  - robot dynamics
KW  - robot kinematics
KW  - torque control
KW  - wearable robots
KW  - twisted string actuator-based exoskeleton
KW  - hip joint assistance
KW  - compliant cable-driven exoskeleton
KW  - injuries
KW  - vocational setting
KW  - powerful exoskeleton
KW  - inherent TSA advantages
KW  - typical torque-speed requirements
KW  - exoskeleton design
KW  - motor selection
KW  - practical exoskeleton prototype
KW  - Hip
KW  - Exoskeletons
KW  - Torque
KW  - Pulleys
KW  - Actuators
KW  - Kinematics
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197359
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a study on a compliant cable-driven exoskeleton for hip assistance in lifting tasks that is aimed at preventing low-back pain and injuries in the vocational setting. In the proposed concept, we used twisted string actuator (TSA) to design a light-weight and powerful exoskeleton that benefits from inherent TSA advantages. We have noted that nonlinear nature of twisted strings' transmission ratio (decreasing with twisting) closely matched typical torque-speed requirements for hip assistance during lifting tasks and tried to use this fact in the exoskeleton design and motor selection. Hip-joint torque and speed required to lift a 10-kg load from stoop to stand were calculated, which gave us a baseline that we used to design and manufacture a practical exoskeleton prototype. Preliminary experimental trials demonstrated that the proposed device was capable of generating required torque and speed at the hip joint while weighing under 6 kg, including battery.
ER  - 

TY  - CONF
TI  - A Novel Portable Lower Limb Exoskeleton for Gravity Compensation during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 768
EP  - 773
AU  - L. Zhou
AU  - W. Chen
AU  - W. Chen
AU  - S. Bai
AU  - J. Wang
PY  - 2020
KW  - gait analysis
KW  - gears
KW  - handicapped aids
KW  - man-machine systems
KW  - medical robotics
KW  - motion control
KW  - patient rehabilitation
KW  - robot kinematics
KW  - springs (mechanical)
KW  - torque
KW  - gravity compensation
KW  - walking assistance
KW  - spring mechanisms
KW  - hip
KW  - knee joints
KW  - gravity balancing
KW  - human leg
KW  - mating gears
KW  - tension force
KW  - springs
KW  - safety
KW  - user acceptance
KW  - design principle
KW  - limb joints
KW  - single leg exoskeleton
KW  - portable passive lower limb exoskeleton
KW  - driving torques
KW  - Springs
KW  - Exoskeletons
KW  - Gravity
KW  - Legged locomotion
KW  - Gears
KW  - Potential energy
KW  - Hip
DO  - 10.1109/ICRA40945.2020.9197422
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel portable passive lower limb exoskeleton for walking assistance. The exoskeleton is designed with built-in spring mechanisms at the hip and knee joints to realize gravity balancing of the human leg. A pair of mating gears is used to convert the tension force from the built-in springs into balancing torques at hip and knee joints for overcoming the influence of gravity. Such a design makes the exoskeleton has a compact layout with small protrusion, which improves its safety and user acceptance. In this paper, the design principle of gravity balancing is described. Simulation results show a significant reduction of driving torques at the limb joints. A prototype of single leg exoskeleton has been constructed and preliminary test results show the effectiveness of the exoskeleton.
ER  - 

TY  - CONF
TI  - Steerable Burrowing Robot: Design, Modeling and Experiments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 829
EP  - 835
AU  - M. Barenboim
AU  - A. Degani
PY  - 2020
KW  - drag
KW  - impact (mechanical)
KW  - mobile robots
KW  - numerical analysis
KW  - robot dynamics
KW  - vehicle dynamics
KW  - thrusting mechanism
KW  - depth dependent model
KW  - steerable burrowing robot
KW  - vibro-impact mechanism
KW  - rotating bevel-tip head
KW  - nonholonomic model
KW  - steering mechanism
KW  - hybrid dynamics model
KW  - S-shaped trajectory
KW  - Robot kinematics
KW  - Needles
KW  - Numerical models
KW  - Solid modeling
KW  - Trajectory
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196648
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper investigates a burrowing robot that can maneuver and steer while being submerged in a granular medium. The robot locomotes using an internal vibro-impact mechanism and steers using a rotating bevel-tip head. We formulate and investigate a non-holonomic model for the steering mechanism and a hybrid dynamics model for the thrusting mechanism. We perform a numerical analysis of the dynamics of the robot's thrusting mechanism using a simplified, orientation and depth dependent model for the drag forces acting on the robot. We first show, in simulation, that by carefully tuning various control input parameters, the thrusting mechanism can drive the robot both forward and backward. We present several experiments designed to evaluate and verify the simulative results using a proof-of-concept robot. We show that different input amplitudes indeed affect the direction of motion, as suggested by the simulation. We further demonstrate the ability of the robot to perform a simple S-shaped trajectory. These experiments demonstrate the feasibility of the robot's design and fidelity of the model.
ER  - 

TY  - CONF
TI  - High Force Density Gripping with UV Activation and Sacrificial Adhesion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 836
EP  - 842
AU  - E. Lee
AU  - Z. Goddard
AU  - J. Ngotiaoco
AU  - N. Monterrosa
AU  - A. Mazumdar
PY  - 2020
KW  - adhesion
KW  - adhesives
KW  - curing
KW  - electric motors
KW  - grippers
KW  - mobile robots
KW  - plastics
KW  - prototypes
KW  - ultraviolet sources
KW  - plastic parts
KW  - force-to-weight ratio
KW  - high force density gripping
KW  - UV activation
KW  - sacrificial adhesion
KW  - light-activated chemical adhesive
KW  - ultraviolet light sensitive acrylics
KW  - rapid curing
KW  - electric motor
KW  - proof-of concept prototypes
KW  - mobile robots
KW  - size 380.0 nm
KW  - time 15.0 s to 75.0 s
KW  - Grippers
KW  - Force
KW  - Curing
KW  - Adhesives
KW  - Chemicals
KW  - Mobile robots
KW  - Mechanism Design
KW  - Manipulation
DO  - 10.1109/ICRA40945.2020.9197246
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel physical gripping framework intended for controlled, high force density attachment on a range of surfaces. Our framework utilizes a light-activated chemical adhesive to attach to surfaces. The cured adhesive is part of a "sacrificial layer," which is shed when the gripper separates from the surface. In order to control adhesive behavior we utilize ultraviolet (UV) light sensitive acrylics which are capable of rapid curing when activated with 380nm light. Once cured, zero input power is needed to hold load. Thin plastic parts can be used as the sacrificial layers, and these can be released using an electric motor. This new gripping framework including the curing load capacity, adhesive deposition, and sacrificial methods are described in detail. Two proof-of concept prototypes are designed, built, and tested. The experimental results illustrate the response time (15-75s depending on load), high holding force-to-weight ratio (10-30), and robustness to material type. Additionally, two drawbacks of this design are discussed: corruption of the gripped surface and a limited number of layers.
ER  - 

TY  - CONF
TI  - Stiffness optimization of a cable driven parallel robot for additive manufacturing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 843
EP  - 849
AU  - D. Gueners
AU  - H. Chanal
AU  - B. C. Bouzgarrou
PY  - 2020
KW  - cables (mechanical)
KW  - industrial robots
KW  - optimisation
KW  - position control
KW  - rigidity
KW  - three-dimensional printing
KW  - vibration control
KW  - stiffness optimization
KW  - cable driven parallel robot
KW  - additive manufacturing
KW  - anchor points
KW  - robot stiffness
KW  - tool path
KW  - platform rigidity
KW  - CDPR
KW  - 3D printing
KW  - vibration modes
DO  - 10.1109/ICRA40945.2020.9197368
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, the optimization of the anchor points of a cable driven parallel robot (CDPR) for 3D printing is proposed in order to maximize the rigidity. Indeed, in the context of 3D printing, robot stiffness should guarantee a high level of tool path following accuracy. The optimized platform showed a rigidity improvement in simulation, but also experimentally with a first study of vibration modes. In the same time, this study illustrates the influence of preload in cables on the platform rigidity.
ER  - 

TY  - CONF
TI  - CAMI - Analysis, Design and Realization of a Force-Compliant Variable Cam System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 850
EP  - 856
AU  - D. Mannhart
AU  - F. Dubois
AU  - K. Bodie
AU  - V. Klemm
AU  - A. Morra
AU  - M. Hutter
PY  - 2020
KW  - cams (mechanical)
KW  - compliant mechanisms
KW  - end effectors
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - path planning
KW  - trajectory control
KW  - CAMI
KW  - multilegged locomotion
KW  - continuous gait transition
KW  - end effector trajectory
KW  - end effector motions
KW  - three dimensional cam system
KW  - force compliant variable cam system
KW  - bipedal robot
KW  - Legged locomotion
KW  - Trajectory
KW  - End effectors
KW  - Couplings
KW  - Shape
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9197019
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents a novel design concept that achieves multi-legged locomotion using a three-dimensional cam system. A computational framework has been developed to analyze and dimension this cam apparatus, that can perform arbitrary end effector motions within its design constraints. The mechanism enables continuous gait transition and inherent force compliance. With only two motors, any trajectory of a continuous set of gaits can be followed. One motor is used to actuate the system and a second one to morph its movement. To illustrate a possible application of this system, a working prototype of a bipedal robot is developed and validated in hardware. It showcases a smooth velocity change by transitioning through different gaits from standing still to walking fast at 124mm/s within 2.0s, while following the given end effector trajectory with an error of only 2.47mm.
ER  - 

TY  - CONF
TI  - Using Manipulation to Enable Adaptive Ground Mobility
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 857
EP  - 863
AU  - R. Kim
AU  - A. Debate
AU  - S. Balakirsky
AU  - A. Mazumdar
PY  - 2020
KW  - adhesion
KW  - legged locomotion
KW  - manipulators
KW  - permanent magnets
KW  - propulsion
KW  - road vehicles
KW  - wheels
KW  - swappable propulsors
KW  - adhesion forces
KW  - wheeled locomotion
KW  - legged locomotion
KW  - autonomous ground vehicles
KW  - terrain
KW  - whegs
KW  - physical adaptation
KW  - multipurpose manipulators
KW  - propulsion system
KW  - adaptive ground mobility
KW  - permanent magnets
KW  - functional prototype robot
KW  - Legged locomotion
KW  - Wheels
KW  - Manipulators
KW  - Steel
KW  - Force
KW  - Mechanism design
KW  - mobile manipulation
KW  - wheeled robots
DO  - 10.1109/ICRA40945.2020.9197061
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to accomplish various missions, autonomous ground vehicles must operate on a wide range of terrain. While many systems such as wheels and whegs can navigate some types of terrain, none are optimal across all. This creates a need for physical adaptation. This paper presents a broad new approach to physical adaptation that relies on manipulation. Specifically, we explore how multipurpose manipulators can enable ground vehicles to dramatically modify their propulsion system in order to optimize performance across various terrain. While this approach is general and widely applicable, this work focuses on physically switching between wheels and legs. We outline the design of "swappable propulsors" that combine the powerful adhesion forces of permanent magnets with geometric features for easy detachment. We provide analysis on how the swappable propulsors can be manipulated, and use these results to create a functional prototype robot. This robot can use its manipulator to change between wheeled and legged locomotion. Our experimental results illustrate how this approach can enhance energy efficiency and versatility.
ER  - 

TY  - CONF
TI  - SNIAE-SSE Deformation Mechanism Enabled Scalable Multicopter: Design, Modeling and Flight Performance Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 864
EP  - 870
AU  - T. Yang
AU  - Y. Zhang
AU  - P. Li
AU  - Y. Shen
AU  - Y. Liu
AU  - H. Chen
PY  - 2020
KW  - actuators
KW  - deformation
KW  - helicopters
KW  - symmetrical deformation
KW  - synchronous deformation
KW  - multicopter system
KW  - flight missions
KW  - stable flight behavior
KW  - folding
KW  - unfolding body deformations
KW  - SNIAE-SSE deformation mechanism
KW  - flight performance validation
KW  - modeling validating
KW  - straight scissor-like elements
KW  - simple non-intersecting angulated elements
KW  - actuation capability
KW  - Strain
KW  - Rotors
KW  - Servomotors
KW  - Prototypes
KW  - Deformable models
KW  - Task analysis
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197025
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on designing, modeling and validating a novel scalable multicopter whose deformation mechanism, called SNIAE-SSE, relies on a combination of simple non-intersecting angulated elements (SNIAEs) and straight scissor-like elements (SSEs). The proposed SNIAE-SSE mechanism has the advantages of single degree-of-freedom, fast actuation capability and large deformation ratio. In this work, enabled by the SNIAE-SSE mechanism, a quadcopter prototype with symmetrical and synchronous deformation is firstly developed, which facilitates a novel and controllably scalable multicopter system for us to analyze its modeling, as well as to validate its flight performance and dynamics during the deformation in several flight missions including hover, throwing, and morphing flying through a narrow window. Experimental results demonstrate that the developed scalable multicopter can maintain its stable flight behavior even both the folding and unfolding body deformations are fast performed, which indicates an excellent capability of the scalable multicopter to rapidly adapt to complex and dynamically changed environments.
ER  - 

TY  - CONF
TI  - Cooperative Autonomy and Data Fusion for Underwater Surveillance With Networked AUVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 871
EP  - 877
AU  - G. Ferri
AU  - P. Stinco
AU  - G. De Magistris
AU  - A. Tesei
AU  - K. D. LePage
PY  - 2020
KW  - autonomous underwater vehicles
KW  - mobile robots
KW  - sensor fusion
KW  - target tracking
KW  - underwater acoustic communication
KW  - AUV cooperative strategies
KW  - data fusion
KW  - realistic underwater surveillance scenarios
KW  - networked AUVs
KW  - data sharing
KW  - robotic networks
KW  - underwater surveillance applications
KW  - autonomous underwater vehicles
KW  - CMRE Anti-Submarine Warfare network
KW  - track management module
KW  - robot autonomy software
KW  - track classification
KW  - T2T association
KW  - Target tracking
KW  - Robot kinematics
KW  - Sonar
KW  - Receivers
KW  - Signal processing algorithms
DO  - 10.1109/ICRA40945.2020.9197367
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Cooperative autonomy and data sharing can largely improve the mission performance of robotic networks in underwater surveillance applications. In this paper, we describe the cooperative autonomy used to control the Autonomous Underwater Vehicles (AUVs) acting as sonar receiver nodes in the CMRE Anti-Submarine Warfare (ASW) network. The paper focuses on a track management module that was integrated in the robot autonomy software for enabling the share of information. Track to track (T2T) associations are used for improving track classification and for creating a common tactical picture, necessary for AUV cooperative strategies. We also present a new cooperative data-driven AUV behaviour that exploits the spatial diversity of multiple robots for improving target tracking and for facilitating T2T associations. We report results with real data collected at sea that validate the approach. The reported results are one of the first examples that show the potential of cooperative autonomy and data fusion in realistic underwater surveillance scenarios characterised by limited communications.
ER  - 

TY  - CONF
TI  - Bidirectional Resonant Propulsion and Localization for AUVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 878
EP  - 884
AU  - T. W. Secord
AU  - T. R. Louwagie
PY  - 2020
KW  - autonomous underwater vehicles
KW  - diaphragms
KW  - electromagnetic actuators
KW  - marine control
KW  - mobile robots
KW  - motion control
KW  - robot vision
KW  - SLAM (robots)
KW  - electromagnetic voice coil motor
KW  - bidirectional resonant propulsion
KW  - AUV localization
KW  - thrust vectors
KW  - diaphragm pump mechanism
KW  - resonant motion
KW  - actuator design
KW  - bidirectional resonant pump
KW  - autonomous underwater vehicles
KW  - Propulsion
KW  - Resonant frequency
KW  - Strain
KW  - Standards
KW  - Damping
KW  - Reliability engineering
DO  - 10.1109/ICRA40945.2020.9197363
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Battery life, reliability, and localization are prominent challenges in the design of autonomous underwater vehicles (AUVs). This work aims to address facets of these challenges using a single system. We describe the design of a bidirectional resonant pump that uses a single electromagnetic voice coil motor (VCM) capable of rotation around a central two degree-of-freedom flexure stage axis. This actuator design produces highly efficient resonant motion that drives two orthogonally oriented diaphragms simultaneously. The operation of this diaphragm pump mechanism produces both adjustable thrust vectors at the aft surface of the AUV and a monotonic relationship between thrust vectors and operating frequency. We propose using the unique frequency to thrust relationship to enhance AUV localization capabilities. We construct a prototype and use it to experimentally demonstrate the feasibility of the directionally-tunable resonance concept.
ER  - 

TY  - CONF
TI  - Hierarchical Planning in Time-Dependent Flow Fields for Marine Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 885
EP  - 891
AU  - J. J. Heon Lee
AU  - C. Yoo
AU  - S. Anstee
AU  - R. Fitch
PY  - 2020
KW  - autonomous underwater vehicles
KW  - computational complexity
KW  - graph theory
KW  - marine vehicles
KW  - mobile robots
KW  - path planning
KW  - remotely operated vehicles
KW  - hierarchical planning
KW  - time-dependent flow fields
KW  - shortest paths
KW  - flow predictions
KW  - motion planning
KW  - slow marine robots
KW  - dynamic ocean currents
KW  - time-dependent graphs
KW  - polynomial-time algorithm
KW  - continuous trajectories
KW  - time-varying edge costs
KW  - underlying flow field
KW  - continuous algorithm
KW  - time complexity
KW  - path quality properties
KW  - autonomous marine vehicle
KW  - marine robotics
KW  - time-varying ocean predictions
KW  - East Australian Current
KW  - Planning
KW  - Robots
KW  - Vehicle dynamics
KW  - Oceans
KW  - Heuristic algorithms
KW  - Prediction algorithms
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197513
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an efficient approach for finding shortest paths in flow fields that vary as a sequence of flow predictions over time. This approach is applicable to motion planning for slow marine robots that are subject to dynamic ocean currents. Although the problem is NP-hard in general form, we incorporate recent results from the theory of finding shortest paths in time-dependent graphs to construct a polynomial-time algorithm that finds continuous trajectories in time-dependent flow fields. The algorithm has a hierarchical structure where a graph is constructed with time-varying edge costs that are derived from sets of continuous trajectories in the underlying flow field. We show that the continuous algorithm retains the time complexity and path quality properties of the discrete graph solution, and demonstrate its application to surface and underwater vehicles including a traversal along the East Australian Current with an autonomous marine vehicle. Results show that the algorithm performs efficiently in practice and can find paths that adapt to changing ocean currents. These results are significant to marine robotics because they allow for efficient use of time-varying ocean predictions for motion planning.
ER  - 

TY  - CONF
TI  - Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 892
EP  - 899
AU  - M. Xanthidis
AU  - N. Karapetyan
AU  - H. Damron
AU  - S. Rahman
AU  - J. Johnson
AU  - A. O‚ÄôConnell
AU  - J. M. O‚ÄôKane
AU  - I. Rekleitis
PY  - 2020
KW  - autonomous underwater vehicles
KW  - collision avoidance
KW  - feature extraction
KW  - mobile robots
KW  - navigation
KW  - optimisation
KW  - robot vision
KW  - stereo image processing
KW  - fly-overs
KW  - AUV
KW  - cluttered space
KW  - navigation framework
KW  - sampling-based correction procedure
KW  - obstacles detection
KW  - real-time 3D autonomous navigation
KW  - agile autonomous underwater vehicle
KW  - Trajopt
KW  - 3D path-optimization planning
KW  - visual features detection
KW  - Planning
KW  - Three-dimensional displays
KW  - Navigation
KW  - Optimization
KW  - Robots
KW  - Trajectory
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197558
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Navigation underwater traditionally is done by keeping a safe distance from obstacles, resulting in "fly-overs" of the area of interest. Movement of an autonomous underwater vehicle (AUV) through a cluttered space, such as a shipwreck or a decorated cave, is an extremely challenging problem that has not been addressed in the past. This paper proposes a novel navigation framework utilizing an enhanced version of Trajopt for fast 3D path-optimization planning for AUVs. A sampling-based correction procedure ensures that the planning is not constrained by local minima, enabling navigation through narrow spaces. Two different modalities are proposed: planning with a known map results in efficient trajectories through cluttered spaces; operating in an unknown environment utilizes the point cloud from the visual features detected to navigate efficiently while avoiding the detected obstacles. The proposed approach is rigorously tested, both on simulation and in-pool experiments, proven to be fast enough to enable safe real-time 3D autonomous navigation for an AUV.
ER  - 


TY  - CONF
TI  - Underwater Image Super-Resolution using Deep Residual Multipliers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 900
EP  - 906
AU  - M. J. Islam
AU  - S. Sakib Enan
AU  - P. Luo
AU  - J. Sattar
PY  - 2020
KW  - image resolution
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - robot vision
KW  - underwater vehicles
KW  - single image super-resolution
KW  - autonomous underwater robots
KW  - adversarial training pipeline
KW  - perceptual quality
KW  - global content
KW  - local style information
KW  - USR-248
KW  - SISR
KW  - state-of-the-art models
KW  - deep residual multipliers
KW  - deep residual network-based generative model
KW  - underwater image super-resolution
KW  - noisy visual conditions
KW  - Training
KW  - Image resolution
KW  - Robots
KW  - Data models
KW  - Cameras
KW  - Pipelines
KW  - Generators
DO  - 10.1109/ICRA40945.2020.9197213
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data. In order to supervise the training, we formulate an objective function that evaluates the perceptual quality of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of `high' (640√ó480) and `low' (80 √ó 60, 160 √ó 120, and 320√ó240) resolution. USR-248 contains paired instances for supervised training of 2√ó, 4√ó, or 8√ó SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions.
ER  - 

TY  - CONF
TI  - Nonlinear Synchronization Control for Short-Range Mobile Sensors Drifting in Geophysical Flows
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 907
EP  - 913
AU  - C. Wei
AU  - H. G. Tanner
AU  - M. Ani Hsieh
PY  - 2020
KW  - actuators
KW  - mobile radio
KW  - oceanographic techniques
KW  - synchronisation
KW  - telecommunication control
KW  - wireless sensor networks
KW  - short-range mobile sensors drifting
KW  - geophysical flows
KW  - ocean monitoring applications
KW  - minimal actuation capabilities
KW  - active drifters
KW  - gyre flows
KW  - data exchange
KW  - nonlinear synchronization control strategy
KW  - rendezvous regions
KW  - large-scale mobile sensor networks
KW  - numerical simulations
KW  - small-scale experiments
KW  - Synchronization
KW  - Sensors
KW  - Vehicle dynamics
KW  - Orbits
KW  - Robots
KW  - Oscillators
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196701
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a synchronization controller for mobile sensors that are minimally actuated and can only communicate with each other over a very short range. This work is motivated by ocean monitoring applications where large-scale sensor networks consisting of drifters with minimal actuation capabilities, i.e., active drifters, are employed. We assume drifters are tasked to monitor regions consisting of gyre flows where their trajectories are periodic. As drifters in neighboring regions move into each other's proximity, it presents an opportunity for data exchange and synchronization to ensure future rendezvous. We present a nonlinear synchronization control strategy to ensure that drifters will periodically rendezvous and maximize the time they are in their rendezvous regions. Numerical simulations and small-scale experiments validate the efficacy of the control strategy and hint at extensions to large-scale mobile sensor networks.
ER  - 

TY  - CONF
TI  - Energy-based Safety in Series Elastic Actuation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 914
EP  - 920
AU  - W. Roozing
AU  - S. S. Groothuis
AU  - S. Stramigioli
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - energy-based safety
KW  - series elastic actuation
KW  - generic actuation passivity
KW  - energy storage
KW  - power flow properties
KW  - power limits
KW  - Safety
KW  - Robots
KW  - Energy storage
KW  - Actuators
KW  - Impedance
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197448
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents the concept of energy-based safety for series-elastic actuation. Generic actuation passivity and safety is treated, defining several energy storage and power flow properties related to passivity. Safe behaviour is not guaranteed by passivity, but can be guaranteed by energy and power limits that adapt the nominal behaviour of an impedance controller. A discussion on power flows in series-elastic actuation is presented and an appropriate controller is developed. Experimental results validate the effectiveness of the energy-based safety in elastic actuation.
ER  - 

TY  - CONF
TI  - Safe high impedance control of a series-elastic actuator with a disturbance observer
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 921
EP  - 927
AU  - K. Haninger
AU  - A. Asignacion
AU  - S. Oh
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - feedforward
KW  - observers
KW  - torque control
KW  - high-stiffness environment
KW  - DOB approaches
KW  - feedforward controller
KW  - DOB controllers
KW  - maximum safe stiffness
KW  - DOB torque control
KW  - passivity conditions
KW  - load port passivity
KW  - safe impedance range
KW  - torque tracking performance
KW  - series-elastic actuator applications
KW  - disturbance observer
KW  - safe high impedance control
KW  - Impedance
KW  - Torque
KW  - Torque control
KW  - Springs
KW  - Stability analysis
KW  - Safety
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9197402
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many series-elastic actuator applications, the ability to safely render a wide range of impedance is important. Advanced torque control techniques such as the disturbance observer (DOB) can improve torque tracking performance, but their impact on safe impedance range is not established. Here, safety is defined with load port passivity, and passivity conditions are developed for two variants of DOB torque control. These conditions are used to determine the maximum safe stiffness and Z-region of the DOB controllers, which are analyzed and compared with the no DOB case. A feedforward controller is proposed which increases the maximum safe stiffness of the DOB approaches. The results are experimentally validated by manual excitation and in a high-stiffness environment.
ER  - 

TY  - CONF
TI  - Variable Stiffness Springs for Energy Storage Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 928
EP  - 933
AU  - S. Y. Kim
AU  - T. Zhang
AU  - D. J. Braun
PY  - 2020
KW  - actuators
KW  - energy storage
KW  - mathematical analysis
KW  - rigidity
KW  - robot dynamics
KW  - springs (mechanical)
KW  - variable stiffness actuation technology
KW  - variable stiffness springs
KW  - energy storage capacity
KW  - linear helical springs
KW  - variable stiffness actuators
KW  - human performance augmentation
KW  - spring exoskeleton
KW  - controllable volume air spring
KW  - mathematical conditions
KW  - Springs
KW  - Energy storage
KW  - Actuators
KW  - Potential energy
KW  - Strain
KW  - Force
KW  - Valves
DO  - 10.1109/ICRA40945.2020.9197245
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Theory suggests an inverse relation between the stiffness and the energy storage capacity for linear helical springs: reducing the active length of the spring by 50% increases its stiffness by 100%, but reduces its energy storage capacity by 50%. State-of-the-art variable stiffness actuators used to drive robots are characterized by a similar inverse relation, implying reduced energy storage capacity for increased spring stiffness. This relation limits the potential of the variable stiffness actuation technology when it comes to human performance augmentation in natural tasks, e.g., jumping, weight-bearing and running, which may necessitate a spring exoskeleton with large stiffness range and high energy storage capacity. In this paper, we theoretically show that the trade-off between stiffness range and energy storage capacity is not fundamental; it is possible to develop variable stiffness springs with simultaneously increasing stiffness and energy storage capacity. Consistent with the theory, we experimentally show that a controllable volume air spring, has a direct relation between its stiffness range and energy storage capacity. The mathematical conditions presented in this paper may be used to develop actuators that could bypass the limited energy storage capacity of current variable stiffness spring technology.
ER  - 

TY  - CONF
TI  - Parallel-motion Thick Origami Structure for Robotic Design
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 934
EP  - 939
AU  - S. Liu
AU  - H. Wu
AU  - Y. Yang
AU  - M. Y. Wang
PY  - 2020
KW  - art
KW  - control system synthesis
KW  - grippers
KW  - motion control
KW  - paper
KW  - parallel-motion thick origami structure
KW  - robotic design
KW  - three-dimensional shapes
KW  - zero-thickness flat paper sheets
KW  - origami facets
KW  - multiple layer origami structures
KW  - parallel-motion gripper
KW  - Fasteners
KW  - Grippers
KW  - Robots
KW  - Shape
KW  - Actuators
KW  - Force
KW  - Electronic mail
DO  - 10.1109/ICRA40945.2020.9197339
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Structures with origami design enable objects to transform into various three-dimensional shapes. Traditionally origami structures are designed with zero-thickness flat paper sheets. However, the thickness and intersection of origami facets are non-negligible in most cases, uniquely when integrating origami design with robotic design because of the more efficient force transfer between thick plates compared with zero-thickness paper-sheets. Meanwhile, the single-layer-paper oriented initial design limited the shape transformation potential as multiple layer origami structures could conduct more variety of deformation. In this article, we are proposing a general design method of parallel-motion thick origami structures that could apply in robotic design like a parallel-motion gripper.
ER  - 

TY  - CONF
TI  - Real-time Simulation of Non-Deformable Continuous Tracks with Explicit Consideration of Friction and Grouser Geometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 948
EP  - 954
AU  - Y. Okada
AU  - S. Kojima
AU  - K. Ohno
AU  - S. Tadokoro
PY  - 2020
KW  - friction
KW  - mobile robots
KW  - motion control
KW  - tracked vehicles
KW  - trajectory control
KW  - velocity control
KW  - nondeformable continuous tracks
KW  - grouser geometry
KW  - real-time simulation
KW  - circular segments
KW  - robot body
KW  - segment link
KW  - track rotation
KW  - friction
KW  - rough terrain
KW  - track trajectory
KW  - velocity constraints
KW  - tracked vehicles
KW  - Robots
KW  - Trajectory
KW  - Tracking
KW  - Friction
KW  - Collision avoidance
KW  - Real-time systems
KW  - Wheels
DO  - 10.1109/ICRA40945.2020.9196776
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, we developed a real-time simulation method for non-deformable continuous tracks having grousers for rough terrain by explicitly considering the collision and friction between the tracks and the ground. In the proposed simulation method, an arbitrary trajectory of a track is represented with multiple linear and circular segments, each of which is a link connected to a robot body. The proposed method sets velocity constraints between each segment link and the robot body, to simulate the track rotation around the body. To maintain the shape of a track, it also restores the positions of the segment links when required. Experimental comparisons with other existing real-time simulation methods demonstrated that while the proposed method considered the grousers and the friction with the ground, it was comparable to them in terms of the computational speed. Experimental comparison of the simulations based on the proposed method and a physical robot exhibited that the former was comparable to the precise motion of the robot on rough or uneven terrain.
ER  - 

TY  - CONF
TI  - Test Your SLAM! The SubT-Tunnel dataset and metric for mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 955
EP  - 961
AU  - J. G. Rogers
AU  - J. M. Gregory
AU  - J. Fink
AU  - E. Stump
PY  - 2020
KW  - mobile robots
KW  - public domain software
KW  - robot vision
KW  - SLAM (robots)
KW  - SLAM
KW  - open source tools
KW  - robotic mapping algorithms
KW  - DARPA Subterranean challenge
KW  - SubT-Tunnel dataset
KW  - subterranean mine rescue dataset
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Measurement
KW  - Laser radar
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197156
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.
ER  - 

TY  - CONF
TI  - Uncertainty Measured Markov Decision Process in Dynamic Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 962
EP  - 968
AU  - S. Dutta
AU  - B. Rekabdar
AU  - C. Ekenna
PY  - 2020
KW  - Markov processes
KW  - mobile robots
KW  - path planning
KW  - robot motion planning
KW  - path planning method
KW  - tracking robot
KW  - dynamic environments
KW  - robot path planning
KW  - visual occlusions
KW  - moving targets
KW  - visioning
KW  - perception algorithms
KW  - partially observable Markov decision
KW  - pursuit-evasion
KW  - robot tracking
KW  - predictive path planning
KW  - Uncertainty
KW  - Planning
KW  - Robots
KW  - Markov processes
KW  - Target tracking
KW  - Probabilistic logic
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9197064
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Successful robot path planning is challenging in the presence of visual occlusions and moving targets. Classical methods to solve this problem have used visioning and perception algorithms in addition to partially observable markov decision processes to aid in path planning for pursuit-evasion and robot tracking. We present a predictive path planning process that measures and utilizes the uncertainty present during robot motion planning. We develop a variant of subjective logic in combination with the Markov decision process (MDP) and provide a measure for belief, disbelief, and uncertainty in relation to feasible trajectories being generated. We then model the MDP to identify the best path planning method from a list of possible choices. Our results show a high percentage accuracy based on the closest acquired proximity between a target and a tracking robot and a simplified pursuer trajectory in comparison with related work.
ER  - 

TY  - CONF
TI  - Natural Scene Facial Expression Recognition with Dimension Reduction Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 987
EP  - 992
AU  - S. Hu
AU  - Y. Hu
AU  - J. Li
AU  - X. Long
AU  - M. Chen
AU  - Q. Gu
PY  - 2020
KW  - emotion recognition
KW  - face recognition
KW  - image classification
KW  - neural nets
KW  - natural scene facial expression recognition
KW  - dimension reduction network
KW  - human-computer interaction
KW  - expression recognition methods
KW  - natural scenes
KW  - expression classification
KW  - pattern recognition problem
KW  - intra-class distance
KW  - inter-class distance
KW  - neural networks
KW  - generalization error
KW  - data dimension reduction module
KW  - general classification network
KW  - classification tasks
KW  - Feature extraction
KW  - Faces
KW  - Face recognition
KW  - Neural networks
KW  - Training
KW  - Dimensionality reduction
DO  - 10.1109/ICRA40945.2020.9197547
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As an external manifestation of human emotions, expression recognition plays an important role in human-computer interaction. Although existing expression recognition methods performs perfectly on constrained frontal faces, there are still many challenges in expression recognition in natural scenes due to different unrestricted conditions. Expression classification belongs to a pattern recognition problem where intra-class distance is greater than the inter-class distance, which leads to severe over-fitting when using neural networks for expression recognition. This paper proposes a novel net-work structure called Dimension Reduction Network which can effectively reduce generalization error. By adding a data dimension reduction module before the general classification network, a lot of redundant information is filtered, and only useful information is left. This can reduce the interference by irrelevant information when performing classification tasks and reduce generalization error. The proposed method does not require any modification to the classification network, only a small dimension reduction module needs to be added in front of the classification network. However, it can effectively reduce generalization error. We designed big and tiny versions of Dimension Reduction Network, both exceeds our baseline on AffectNet data set. The big version of our proposed method surpassed the state-of-the-art methods by more than 1.2% on AffectNet data set. Our code will open source3 when the paper is accepted.
ER  - 

TY  - CONF
TI  - Hand Pose Estimation for Hand-Object Interaction Cases using Augmented Autoencoder
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 993
EP  - 999
AU  - S. Li
AU  - H. Wang
AU  - D. Lee
PY  - 2020
KW  - image capture
KW  - image coding
KW  - image representation
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - solid modelling
KW  - annotated hand-object samples
KW  - hand-object interaction cases
KW  - augmented autoencoder
KW  - deep learning method
KW  - 3D point cloud
KW  - latent representation
KW  - auxiliary point cloud decoder
KW  - augmented clean hand data
KW  - hand pose estimation
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Decoding
KW  - Image reconstruction
KW  - Task analysis
KW  - Shape
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197299
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hand pose estimation with objects is challenging due to object occlusion and the lack of large annotated datasets. To tackle these issues, we propose an Augmented Autoencoder based deep learning method using augmented clean hand data. Our method takes 3D point cloud of a hand with an augmented object as input and encodes the input to latent representation of the hand. From the latent representation, our method decodes 3D hand pose and we propose to use an auxiliary point cloud decoder to assist the formation of the latent space. Through quantitative and qualitative evaluation on both synthetic dataset and real captured data containing objects, we demonstrate state-of-the-art performance for hand pose estimation with objects, even using only a small number of annotated hand-object samples.
ER  - 

TY  - CONF
TI  - Accurate detection and 3D localization of humans using a novel YOLO-based RGB-D fusion approach and synthetic training data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1000
EP  - 1006
AU  - T. Linder
AU  - K. Y. Pfeiffer
AU  - N. Vaskevicius
AU  - R. Schirmer
AU  - K. O. Arras
PY  - 2020
KW  - feature extraction
KW  - image colour analysis
KW  - image fusion
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - synthetic training data
KW  - real-time detection
KW  - human 3D centroids
KW  - RGB-D data
KW  - image-based detection approach
KW  - YOLO v3 architecture
KW  - 3D centroid loss
KW  - mid-level feature fusion
KW  - transfer learning scheme
KW  - large-scale 2D object detection datasets
KW  - end-to-end 3D localization
KW  - precise 3D groundtruth
KW  - 3D localization accuracy
KW  - learning 3D localization
KW  - YOLO-based RGB-D fusion approach
KW  - depth-aware crop augmentation
KW  - intralogistics dataset
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Training
KW  - Detectors
KW  - Feature extraction
KW  - Robustness
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196899
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While 2D object detection has made significant progress, robustly localizing objects in 3D space under presence of occlusion is still an unresolved issue. Our focus in this work is on real-time detection of human 3D centroids in RGB-D data. We propose an image-based detection approach which extends the YOLO v3 architecture with a 3D centroid loss and mid-level feature fusion to exploit complementary information from both modalities. We employ a transfer learning scheme which can benefit from existing large-scale 2D object detection datasets, while at the same time learning end-to-end 3D localization from our highly randomized, diverse synthetic RGB-D dataset with precise 3D groundtruth. We further propose a geometrically more accurate depth-aware crop augmentation for training on RGB-D data, which helps to improve 3D localization accuracy. In experiments on our challenging intralogistics dataset, we achieve state-of-the-art performance even when learning 3D localization just from synthetic data.
ER  - 

TY  - CONF
TI  - Wide-range Load Sensor Using Vacuum Sealed Quartz Crystal Resonator for Simultaneous Biosignals Measurement on Bed
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1015
EP  - 1020
AU  - Y. Murozaki
AU  - F. Arai
PY  - 2020
KW  - biomechanics
KW  - biomedical equipment
KW  - biomedical measurement
KW  - cardiology
KW  - crystal resonators
KW  - electrocardiography
KW  - force measurement
KW  - force sensors
KW  - medical signal detection
KW  - medical signal processing
KW  - microfabrication
KW  - microsensors
KW  - patient monitoring
KW  - pneumodynamics
KW  - pressure sensors
KW  - quartz
KW  - sensors
KW  - wide-range load sensor
KW  - vacuum sealed quartz crystal resonator
KW  - biosignal measurement
KW  - QCR load sensor
KW  - measurement range
KW  - sensor structure
KW  - force sensor
KW  - QCR load sensing system
KW  - Bonding
KW  - Weight measurement
KW  - Robot sensing systems
KW  - Force
KW  - Heart beat
KW  - Resists
KW  - Stress
DO  - 10.1109/ICRA40945.2020.9196533
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Monitoring of biosignals on a daily basis plays important roles for the health management of elderly. The monitoring system for the daily life, the system should not require the subjects to take special effort like wearing a sensor. We propose biosignals measurement using wide-range load sensor on the bed. The sensing system can detect the body weight, heartbeat and respiration simultaneously by just lying on the bed. We have developed load sensor using quartz crystal resonator (QCR load sensor) as wide-range load sensor. However, the measurement range was not sufficient for the simultaneous measurement of biosgnals on bed. To realize such sensing system, we propose a QCR load sensor utilizing vacuum sealing technology for expanding the measurement range. We improved the oscillation characteristics of the QCR by the vacuum sealing to stabilize the sensor output. Accordingly, the resolution of the sensor was improved. Moreover, the load capacity of the sensor was increased by improving the bonding strength of sensor structure. The fabricated sensor had a measurement range of 0.27 mN - 1180 N (4.4 √ó 106). This wide enough compared with the conventional force sensor (103 - 104).Also, we developed mechanically robust jig of QCR load sensor for practical use of QCR load sensor. We succeed in simultaneous measurement of weight, heart rate, and respiration rate using fabricated QCR load sensing system. The accuracy of heart rate and respiration rate measurement are 0.4 bpm (0.6 %) and 1.1 brpm (6.1 %), respectively, in standard deviation of error compared with ECG signal.
ER  - 

TY  - CONF
TI  - Joint Pedestrian Detection and Risk-level Prediction with Motion-Representation-by-Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1021
EP  - 1027
AU  - H. Kataoka
AU  - T. Suzuki
AU  - K. Nakashima
AU  - Y. Satoh
AU  - Y. Aoki
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image representation
KW  - object detection
KW  - pedestrians
KW  - traffic engineering computing
KW  - risk-level prediction
KW  - pedestrian near-miss detection
KW  - risk-level assignment
KW  - motion-representation-by-detection
KW  - pedestrian near-miss dataset
KW  - single-shot multibox detector with motion representation
KW  - SSD-MR
KW  - motion-based features extraction
KW  - Videos
KW  - Databases
KW  - Detectors
KW  - Feature extraction
KW  - Object detection
KW  - Accidents
KW  - Autonomous automobiles
DO  - 10.1109/ICRA40945.2020.9197399
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents a pedestrian near-miss detector with temporal analysis that provides both pedestrian detection and risk-level predictions which are demonstrated on a self-collected database. Our work makes three primary contributions: (i) The framework of pedestrian near-miss detection is proposed by providing both a pedestrian detection and risk-level assignment. Specifically, we have created a Pedestrian Near-Miss (PNM) dataset that categorizes traffic near-miss incidents based on their risk levels (high-, low-, and no-risk). Unlike existing databases, our dataset also includes manually localized pedestrian labels as well as a large number of incident-related videos. (ii) Single-Shot MultiBox Detector with Motion Representation (SSD-MR) is implemented to effectively extract motion-based features in a detected pedestrian. (iii) Using the self-collected PNM dataset and SSD-MR, our proposed method achieved +19.38% (on risk-level prediction) and +13.00% (on joint pedestrian detection and risk-level prediction) higher scores than that of the baseline SSD and LSTM. Additionally, the running time of our system is over 50 fps on a graphics processing unit (GPU).
ER  - 

TY  - CONF
TI  - Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1070
EP  - 1076
AU  - P. Gao
AU  - H. Zhang
PY  - 2020
KW  - graph theory
KW  - image matching
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - robotics applications
KW  - simultaneously localization and mapping
KW  - spatial relationship similarities
KW  - spatial cues
KW  - visual cues
KW  - old landmarks
KW  - long-term environment changes
KW  - landmark information
KW  - integrate landmark appearances
KW  - worst-case graph matching
KW  - place recognition performance
KW  - long-term place recognition
KW  - worst appearance similarity
KW  - similar appearances
KW  - worst-case scenario
KW  - graph matching problem
KW  - visual appearances
KW  - angular spatial relationships
KW  - graph representation
KW  - Visualization
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Strain
KW  - Image recognition
KW  - Tensile stress
DO  - 10.1109/ICRA40945.2020.9196906
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.
ER  - 

TY  - CONF
TI  - Linear RGB-D SLAM for Atlanta World
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1077
EP  - 1083
AU  - K. Joo
AU  - T. -H. Oh
AU  - F. Rameau
AU  - J. -C. Bazin
AU  - I. S. Kweon
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - Kalman filters
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - SLAM (robots)
KW  - Manhattan world assumption
KW  - orthogonal directions
KW  - Atlanta world
KW  - vertical direction
KW  - horizontal directions
KW  - SLAM techniques
KW  - Atlanta representation
KW  - Atlanta frame-aware linear SLAM framework
KW  - Atlanta structure
KW  - linear Kalman filter
KW  - linear RGB-D SLAM
KW  - simultaneous localization and mapping
KW  - tracking-by-detection scheme
KW  - scene structure
KW  - camera motion
KW  - planar map
KW  - synthetic datasets
KW  - real datasets
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Three-dimensional displays
KW  - Tracking
KW  - Kalman filters
KW  - Visualization
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196561
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a new linear method for RGB-D based simultaneous localization and mapping (SLAM). Compared to existing techniques relying on the Manhattan world assumption defined by three orthogonal directions, our approach is designed for the more general scenario of the Atlanta world. It consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction and thus can represent a wider range of scenes. Our approach leverages the structural regularity of the Atlanta world to decouple the non-linearity of camera pose estimations. This allows us separately to estimate the camera rotation and then the translation, which bypasses the inherent non-linearity of traditional SLAM techniques. To this end, we introduce a novel tracking-by-detection scheme to estimate the underlying scene structure by Atlanta representation. Thereby, we propose an Atlanta frame-aware linear SLAM framework which jointly estimates the camera motion and a planar map supporting the Atlanta structure through a linear Kalman filter. Evaluations on both synthetic and real datasets demonstrate that our approach provides favorable performance compared to existing state-of-the-art methods while extending their working range to the Atlanta world.
ER  - 

TY  - CONF
TI  - Stereo Visual Inertial Odometry with Online Baseline Calibration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1084
EP  - 1090
AU  - Y. Fan
AU  - R. Wang
AU  - Y. Mao
PY  - 2020
KW  - calibration
KW  - cameras
KW  - distance measurement
KW  - inertial navigation
KW  - Jacobian matrices
KW  - Kalman filters
KW  - stereo image processing
KW  - update Jacobian sub-block
KW  - feature reprojection error
KW  - real-world outdoor dataset
KW  - EuRoC dataset
KW  - camera poses
KW  - IMU
KW  - inertial measurement unit
KW  - estimation performance
KW  - stereo-vision devices
KW  - stereo extrinsic parameters
KW  - multistate constraint Kalman filter
KW  - stereo VIO extrinsic parameters correction
KW  - online calibration method
KW  - camera extrinsic parameters
KW  - stereo visual inertial odometry
KW  - extrinsic parameter calibration
KW  - online baseline calibration
KW  - Cameras
KW  - Calibration
KW  - Estimation
KW  - Visualization
KW  - Acceleration
KW  - Jacobian matrices
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197581
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Stereo-vision devices have rigorous requirements for extrinsic parameter calibration. In Stereo Visual Inertial Odometry (VIO), inaccuracy in or changes to camera extrinsic parameters may lead to serious degradation in estimation performance. In this manuscript, we propose an online calibration method for stereo VIO extrinsic parameters correction. In particular, we focus on Multi-State Constraint Kalman Filter (MSCKF [1]) framework to implement our method. The key component is to formulate stereo extrinsic parameters as part of the state variables and model the Jacobian of feature reprojection error with respect to stereo extrinsic parameters as sub-block of update Jacobian. Therefore we can estimate stereo extrinsic parameters simultaneously with inertial measurement unit (IMU) states and camera poses. Experiments on EuRoC dataset and real-world outdoor dataset demonstrate that the proposed algorithm produce higher positioning accuracy than the original S-MSCKF [2], and the noise of camera extrinsic parameters are self-corrected within the system.
ER  - 

TY  - CONF
TI  - Lidar-Monocular Visual Odometry using Point and Line Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1091
EP  - 1097
AU  - S. -S. Huang
AU  - Z. -Y. Ma
AU  - T. -J. Mu
AU  - H. Fu
AU  - S. -M. Hu
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - optical radar
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - line features
KW  - lidar-visual odometry
KW  - pose estimation
KW  - line depth extraction
KW  - point-line bundle adjustment
KW  - purely visual motion tracking method
KW  - public KITTI odometry benchmark
KW  - lidar-monocular visual odometry approach
KW  - point-only based lidar-visual odometry
KW  - environment structure information
KW  - efficient lidar-monocular visual odometry system
KW  - Feature extraction
KW  - Cameras
KW  - Bundle adjustment
KW  - Laser radar
KW  - Image segmentation
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196613
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce a novel lidar-monocular visual odometry approach using point and line features. Compared to previous point-only based lidar-visual odometry, our approach leverages more environment structure information by introducing both point and line features into pose estimation. We provide a robust method for point and line depth extraction, and formulate the extracted depth as prior factors for point-line bundle adjustment. This method greatly reduces the features' 3D ambiguity and thus improves the pose estimation accuracy. Besides, we also provide a purely visual motion tracking method and a novel scale correction scheme, leading to an efficient lidar-monocular visual odometry system with high accuracy. The evaluations on the public KITTI odometry benchmark show that our technique achieves more accurate pose estimation than the state-of-the-art approaches, and is sometimes even better than those leveraging semantic information.
ER  - 

TY  - CONF
TI  - Probabilistic Data Association via Mixture Models for Robust Semantic SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1098
EP  - 1104
AU  - K. J. Doherty
AU  - D. P. Baxter
AU  - E. Schneeweiss
AU  - J. J. Leonard
PY  - 2020
KW  - Gaussian processes
KW  - image sensors
KW  - mobile robots
KW  - object detection
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - target tracking
KW  - probabilistic data association
KW  - mixture models
KW  - robust semantic SLAM
KW  - robotic systems
KW  - cameras
KW  - lidar
KW  - visual models
KW  - reliable navigation
KW  - semantic uncertainty inherent
KW  - geometric uncertainty inherent
KW  - object detection methods
KW  - data association ambiguity
KW  - nonlinear Gaussian formulation
KW  - data association variables
KW  - max-marginalization
KW  - standard Gaussian posterior assumptions
KW  - max-mixture-type model
KW  - multiple data association hypotheses
KW  - indoor navigation tasks
KW  - outdoor semantic navigation tasks
KW  - semantic SLAM approaches
KW  - simultaneous localization and mapping
KW  - noisy odometry
KW  - Semantics
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Optimization
KW  - Object detection
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197382
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern robotic systems sense the environment geometrically, through sensors like cameras, lidar, and sonar, as well as semantically, often through visual models learned from data, such as object detectors. We aim to develop robots that can use all of these sources of information for reliable navigation, but each is corrupted by noise. Rather than assume that object detection will eventually achieve near perfect performance across the lifetime of a robot, in this work we represent and cope with the semantic and geometric uncertainty inherent in object detection methods. Specifically, we model data association ambiguity, which is typically non-Gaussian, in a way that is amenable to solution within the common nonlinear Gaussian formulation of simultaneous localization and mapping (SLAM). We do so by eliminating data association variables from the inference process through max-marginalization, preserving standard Gaussian posterior assumptions. The result is a max-mixture-type model that accounts for multiple data association hypotheses. We provide experimental results on indoor and outdoor semantic navigation tasks with noisy odometry and object detection and find that the ability of the proposed approach to represent multiple hypotheses, including the "null" hypothesis, gives substantial robustness advantages in comparison to alternative semantic SLAM approaches.
ER  - 

TY  - CONF
TI  - Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1105
EP  - 1112
AU  - Y. Zhao
AU  - J. S. Smith
AU  - S. H. Karumanchi
AU  - P. A. Vela
PY  - 2020
KW  - closed loop systems
KW  - Global Positioning System
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - tracking
KW  - representative state-of-the-art visual-inertial SLAM systems
KW  - visual estimation module
KW  - stereo visual-inertial SLAM systems
KW  - open-loop analysis
KW  - closed-loop navigation tasks
KW  - accurate trajectory tracking
KW  - visualinertial SLAM systems
KW  - closed-loop benchmarking simulation
KW  - visual-inertial estimation
KW  - trajectory tracking performance
KW  - Visualization
KW  - Navigation
KW  - Simultaneous localization and mapping
KW  - Benchmark testing
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9197003
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.
ER  - 

TY  - CONF
TI  - PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1113
EP  - 1120
AU  - L. Pan
AU  - C. -M. Chew
AU  - G. H. Lee
PY  - 2020
KW  - codecs
KW  - convolutional neural nets
KW  - decoding
KW  - edge detection
KW  - graph theory
KW  - image coding
KW  - image filtering
KW  - image representation
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - stereo image processing
KW  - visual perception
KW  - deep hierarchical encoder-decoder
KW  - unorganized 3D points
KW  - multiscale contextual information
KW  - image analysis
KW  - PointAtrousGraph
KW  - PAG
KW  - multiscale edge features
KW  - point clouds
KW  - Point Atrous Convolution
KW  - PAC
KW  - Edge-preserved Unpooling
KW  - multiscale point features
KW  - nonoverlapping maxpooling operations
KW  - critical edge features
KW  - EU modules
KW  - deep permutation-invariant hierarchical encoder-decoder
KW  - edge preserved pooling
KW  - chained skip subsampling-upsampling modules
KW  - 3D semantic perception applications
KW  - Three-dimensional displays
KW  - Picture archiving and communication systems
KW  - Convolution
KW  - Semantics
KW  - Image edge detection
KW  - Task analysis
KW  - Decoding
DO  - 10.1109/ICRA40945.2020.9197499
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edgepreserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping maxpooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling/upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.
ER  - 

TY  - CONF
TI  - Learning error models for graph SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1121
EP  - 1127
AU  - C. Reymann
AU  - S. Lacroix
PY  - 2020
KW  - autonomous aerial vehicles
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - resistance distance
KW  - covisibility graph
KW  - simulated UAV coverage path
KW  - uncertainty models
KW  - monocular graph SLAM
KW  - topological features
KW  - error model learning
KW  - UAV coverage path planning trajectories
KW  - Simultaneous localization and mapping
KW  - Resistance
KW  - Uncertainty
KW  - Computational modeling
KW  - Computer architecture
KW  - Predictive models
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196864
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.
ER  - 

TY  - CONF
TI  - SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1128
EP  - 1134
AU  - M. Cornia
AU  - L. Baraldi
AU  - R. Cucchiara
PY  - 2020
KW  - human-robot interaction
KW  - natural language processing
KW  - robot vision
KW  - video signal processing
KW  - video captioning
KW  - computational requirements
KW  - fully-attentive captioning algorithm
KW  - language generation
KW  - transformer layers
KW  - decoding stages
KW  - image regions
KW  - caption quality
KW  - autonomous agents
KW  - domestic robots
KW  - SMArT
KW  - robotic explainability
KW  - natural language explanations
KW  - visual perception
KW  - shallow memory-aware transformer training
KW  - memory-aware encoding
KW  - image captioning
KW  - image captioning
KW  - Decoding
KW  - Computational modeling
KW  - Visualization
KW  - Magnetic heads
KW  - Robots
KW  - Natural languages
KW  - Encoding
DO  - 10.1109/ICRA40945.2020.9196653
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.
ER  - 

TY  - CONF
TI  - A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1135
EP  - 1141
AU  - L. K√§stner
AU  - V. C. Frasineanu
AU  - J. Lambrecht
PY  - 2020
KW  - augmented reality
KW  - calibration
KW  - cameras
KW  - control engineering computing
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - public domain software
KW  - robot vision
KW  - solid modelling
KW  - robotic environments
KW  - mobile robots
KW  - depth camera
KW  - deep learning-based calibration
KW  - open source 3D point cloud labeling tool
KW  - head mounted augmented reality device
KW  - 3D depth sensor data
KW  - Microsoft Hololens
KW  - neural network
KW  - VoteNet architecture
KW  - 3D-deep-learning-based augmented reality calibration
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Calibration
KW  - Neural networks
KW  - Augmented reality
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197155
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Augmented Reality and mobile robots are gaining increased attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate Augmented Reality devices using 3D depth sensor data. We use the depth camera of a Head Mounted Augmented Reality Device, the Microsoft Hololens, for deep learning-based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on raw point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.
ER  - 

TY  - CONF
TI  - Adversarial Feature Training for Generalizable Robotic Visuomotor Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1142
EP  - 1148
AU  - X. Chen
AU  - A. Ghadirzadeh
AU  - M. Bj√∂rkman
AU  - P. Jensfelt
PY  - 2020
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - robotic policy training
KW  - large-scale data collection
KW  - task-setup
KW  - task-irrelevant objects
KW  - interactive samples
KW  - adversarial training
KW  - deep RL capabilities
KW  - transfer learning
KW  - robotic tasks
KW  - adversarial feature training
KW  - generalizable robotic visuomotor control
KW  - deep reinforcement learning
KW  - action-selection policies
KW  - image pixels mapping
KW  - visuomotor robotic policy training
KW  - Task analysis
KW  - Training
KW  - Visualization
KW  - Feature extraction
KW  - Robots
KW  - Trajectory
KW  - Clutter
DO  - 10.1109/ICRA40945.2020.9197505
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep reinforcement learning (RL) has enabled training action-selection policies, end-to-end, by learning a function which maps image pixels to action outputs. However, it's application to visuomotor robotic policy training has been limited because of the challenge of large-scale data collection when working with physical hardware. A suitable visuomotor policy should perform well not just for the task-setup it has been trained for, but also for all varieties of the task, including novel objects at different viewpoints surrounded by task-irrelevant objects. However, it is impractical for a robotic setup to sufficiently collect interactive samples in a RL framework to generalize well to novel aspects of a task. In this work, we demonstrate that by using adversarial training for domain transfer, it is possible to train visuomotor policies based on RL frameworks, and then transfer the acquired policy to other novel task domains. We propose to leverage the deep RL capabilities to learn complex visuomotor skills for uncomplicated task setups, and then exploit transfer learning to generalize to new task domains provided only still images of the task in the target domain. We evaluate our method on two real robotic tasks, picking and pouring, and compare it to a number of prior works, demonstrating its superiority.
ER  - 

TY  - CONF
TI  - Efficient Bimanual Manipulation Using Learned Task Schemas
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1149
EP  - 1155
AU  - R. Chitnis
AU  - S. Tulsiani
AU  - S. Gupta
AU  - A. Gupta
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - parameterizations
KW  - sparse-reward tasks
KW  - robotic bimanual manipulation tasks
KW  - parameterized skills
KW  - state-independent task schema
KW  - model-free reinforcement learning
KW  - robotic systems
KW  - Task analysis
KW  - Learning (artificial intelligence)
KW  - Neural networks
KW  - Force
KW  - Geometry
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196958
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of effectively composing skills to solve sparse-reward tasks in the real world. Given a set of parameterized skills (such as exerting a force or doing a top grasp at a location), our goal is to learn policies that invoke these skills to efficiently solve such tasks. Our insight is that for many tasks, the learning process can be decomposed into learning a state-independent task schema (a sequence of skills to execute) and a policy to choose the parameterizations of the skills in a state-dependent manner. For such tasks, we show that explicitly modeling the schema's state-independence can yield significant improvements in sample efficiency for model-free reinforcement learning algorithms. Furthermore, these schemas can be transferred to solve related tasks, by simply re-learning the parameterizations with which the skills are invoked. We find that doing so enables learning to solve sparse-reward tasks on real-world robotic systems very efficiently. We validate our approach experimentally over a suite of robotic bimanual manipulation tasks, both in simulation and on real hardware. See videos at http://tinyurl.com/chitnis-schema.
ER  - 

TY  - CONF
TI  - Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1156
EP  - 1162
AU  - Q. Kuang
AU  - J. Wu
AU  - J. Pan
AU  - B. Zhou
PY  - 2020
KW  - autonomous aerial vehicles
KW  - computational geometry
KW  - image reconstruction
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - unmanned aerial vehicles
KW  - large-scale scene mapping
KW  - autonomous urban scene reconstruction
KW  - point cloud reconstruction
KW  - reconstruction quality
KW  - large-scale scene reconstruction
KW  - real-time UAV path planning
KW  - SLAM
KW  - Buildings
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Path planning
KW  - Drones
KW  - Cameras
KW  - Layout
DO  - 10.1109/ICRA40945.2020.9196558
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.
ER  - 

TY  - CONF
TI  - A Fast Marching Gradient Sampling Strategy for Motion Planning using an Informed Certificate Set
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1163
EP  - 1168
AU  - S. Shi
AU  - J. Chen
AU  - Y. Xiong
PY  - 2020
KW  - collision avoidance
KW  - gradient methods
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - sampling methods
KW  - convergence speed
KW  - safety certificate algorithms
KW  - fast marching gradient sampling strategy
KW  - sampling-based motion planning algorithms
KW  - marching seed
KW  - goal set
KW  - informed certificate set
KW  - planning space
KW  - RRT* algorithms
KW  - Planning
KW  - Safety
KW  - Convergence
KW  - Algorithms
KW  - Robots
KW  - Data structures
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196685
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel fast marching gradient sampling strategy to accelerate the convergence speed of sampling-based motion planning algorithms. This strategy is based on an informed certificate set which consists of the robot states with exact collision status as well as the minimum distance and the gradient to the nearest obstacle. The informed certificate set covers almost the whole planning space such that it contains rich information for the planner. The best quality point in this set is selected as the marching seed to guide the search graph move steadily to the goal set. The distance and gradient information of the marching seed helps to generate a new sample with almost sure collision status. When a feasible solution has been found, this set can construct the restricted subset that can improve current path quality. This marching gradient sampling strategy is applied to the RRT and RRT* algorithms. Simulation experiments demonstrate that the convergence speed to a feasible solution or to the optimal solution is almost twice faster than that of the safety certificate algorithms.
ER  - 

TY  - CONF
TI  - Privacy-Aware UAV Flights through Self-Configuring Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1169
EP  - 1175
AU  - Y. Luo
AU  - Y. Yu
AU  - Z. Jin
AU  - Y. Li
AU  - Z. Ding
AU  - Y. Zhou
AU  - Y. Liu
PY  - 2020
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - data privacy
KW  - decision making
KW  - mobile robots
KW  - motion control
KW  - privacy-aware UAV flights
KW  - unmanned aerial vehicle
KW  - uncertain obstacles
KW  - motion planning algorithms
KW  - privacy-preserving requirements
KW  - privacy risk aware motion planning method
KW  - privacy-sensitive sensor
KW  - safety
KW  - energy hard constraints
KW  - dynamically detected restricted areas
KW  - decision making method
KW  - test flights
KW  - DJI Matrice 100 UAV
KW  - self-configuring motion planning
KW  - Privacy
KW  - Planning
KW  - Cameras
KW  - Sensors
KW  - Trajectory
KW  - Safety
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA40945.2020.9197564
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - During flights, an unmanned aerial vehicle (UAV) may not be allowed to move across certain areas due to soft constraints such as privacy restrictions. Current methods on self-adaption focus mostly on motion planning such that the trajectory does not trespass predetermined restricted areas. When the environment is cluttered with uncertain obstacles, however, these motion planning algorithms are not flexible enough to find a trajectory that satisfies additional privacy-preserving requirements within a tight time budget during the flights. In this paper, we propose a privacy risk aware motion planning method through the reconfiguration of privacy-sensitive sensors. It minimises environmental impact by re-configuring the sensor during flight, while still guaranteeing the safety and energy hard constraints such as collision avoidance and timeliness. First, we formulate a model for assessing privacy risks of dynamically detected restricted areas. In case the UAV cannot find a feasible solution to satisfy both hard and soft constraints from the current configuration, our decision making method can then produce an optimal reconfiguration of the privacy-sensitive sensor with a more efficient trajectory. We evaluate the proposal through various simulations with different settings in a virtual environment and also validate the approach through real test flights on DJI Matrice 100 UAV.
ER  - 

TY  - CONF
TI  - Improved C-Space Exploration and Path Planning for Robotic Manipulators Using Distance Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1176
EP  - 1182
AU  - B. Lacevic
AU  - D. Osmankovic
PY  - 2020
KW  - collision avoidance
KW  - manipulators
KW  - path planning
KW  - robot kinematics
KW  - trees (mathematics)
KW  - generalized bur captures large portions
KW  - free C-space
KW  - accelerated exploration
KW  - exact collision-free paths
KW  - improved C-space exploration
KW  - path planning
KW  - robotic manipulators
KW  - distance information
KW  - geometrical structure
KW  - star-like tree
KW  - arbitrary number
KW  - guaranteed collision-free edges
KW  - simple forward kinematics
KW  - RRT-like planning algorithm
KW  - generalized burs
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Kinematics
KW  - Manipulators
KW  - Path planning
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196920
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a simple method to quickly explore C-spaces of robotic manipulators and thus facilitate path planning. The method is based on a novel geometrical structure called generalized bur. It is a star-like tree, rooted at a given point in free C-space, with an arbitrary number of guaranteed collision-free edges computed using distance information from the workspace and simple forward kinematics. Generalized bur captures large portions of free C-space, enabling accelerated exploration. The workspace is assumed to be decomposable into a finite set of (possibly overlapping) convex obstacles. When plugged in a suitable RRT-like planning algorithm, generalized burs enable significant performance improvements, while at the same time enabling exact collision-free paths.
ER  - 

TY  - CONF
TI  - Tuning-Free Contact-Implicit Trajectory Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1183
EP  - 1189
AU  - A. √ñ. √ñnol
AU  - R. Corcodel
AU  - P. Long
AU  - T. Padƒ±r
PY  - 2020
KW  - humanoid robots
KW  - manipulators
KW  - mobile robots
KW  - tuning-free contact-implicit trajectory optimization
KW  - contact-implicit trajectory optimization framework
KW  - contact-interaction trajectories
KW  - robot architectures
KW  - trivial initial guess
KW  - parameter tuning
KW  - relaxed contact model
KW  - automatic penalty adjustment loop
KW  - contact information
KW  - mobile robot
KW  - nonprehensile manipulation
KW  - 7-DOF arm
KW  - planar locomotion
KW  - Robots
KW  - Task analysis
KW  - Trajectory optimization
KW  - Tuning
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9196805
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a contact-implicit trajectory optimization framework that can plan contact-interaction trajectories for different robot architectures and tasks using a trivial initial guess and without requiring any parameter tuning. This is achieved by using a relaxed contact model along with an automatic penalty adjustment loop for suppressing the relaxation. Moreover, the structure of the problem enables us to exploit the contact information implied by the use of relaxation in the previous iteration, such that the solution is explicitly improved with little computational overhead. We test the proposed approach in simulation experiments for non-prehensile manipulation using a 7-DOF arm and a mobile robot and for planar locomotion using a humanoid-like robot in zero gravity. The results demonstrate that our method provides an out-of-the-box solution with good performance for a wide range of applications.
ER  - 

TY  - CONF
TI  - Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1208
EP  - 1214
AU  - B. Zhou
AU  - F. Gao
AU  - J. Pan
AU  - S. Shen
PY  - 2020
KW  - autonomous aerial vehicles
KW  - gradient methods
KW  - helicopters
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - search problems
KW  - trajectory control
KW  - quadrotor trajectory replanning
KW  - replanning method
KW  - GTO
KW  - path-guided optimization approach
KW  - topological path searching algorithm
KW  - independent trajectory optimization
KW  - output superior replanned trajectories
KW  - gradient-based trajectory optimization
KW  - UAV replanning
KW  - Splines (mathematics)
KW  - Linear programming
KW  - Trajectory optimization
KW  - Robustness
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196996
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gradient-based trajectory optimization (GTO) has gained wide popularity for quadrotor trajectory replanning. However, it suffers from local minima, which is not only fatal to safety but also unfavorable for smooth navigation. In this paper, we propose a replanning method based on GTO addressing this issue systematically. A path-guided optimization (PGO) approach is devised to tackle infeasible local minima, which improves the replanning success rate significantly. A topological path searching algorithm is developed to capture a collection of distinct useful paths in 3-D environments, each of which then guides an independent trajectory optimization. It activates a more comprehensive exploration of the solution space and output superior replanned trajectories. Benchmark evaluation shows that our method outplays state-of-the-art methods regarding replanning success rate and optimality. Challenging experiments of aggressive autonomous flight are presented to demonstrate the robustness of our method. We will release our implementation as an open-source package1.
ER  - 

TY  - CONF
TI  - Learning-based Path Planning for Autonomous Exploration of Subterranean Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1215
EP  - 1221
AU  - R. Reinhart
AU  - T. Dang
AU  - E. Hand
AU  - C. Papachristos
AU  - K. Alexis
PY  - 2020
KW  - autonomous aerial vehicles
KW  - graph theory
KW  - learning by example
KW  - mobile robots
KW  - optical radar
KW  - path planning
KW  - robot programming
KW  - sampled data systems
KW  - tunnels
KW  - autonomous exploration
KW  - subterranean environments
KW  - aerial robots
KW  - training expert
KW  - imitation learning
KW  - underground mine drifts
KW  - tunnels
KW  - graph based path planner
KW  - learning based path planning
KW  - LiDAR
KW  - range data sampling
KW  - Robot sensing systems
KW  - Path planning
KW  - Training
KW  - Training data
KW  - Planning
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196662
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a new methodology on learning-based path planning for autonomous exploration of subterranean environments using aerial robots. Utilizing a recently proposed graph-based path planner as a "training expert" and following an approach relying on the concepts of imitation learning, we derive a trained policy capable of guiding the robot to autonomously explore underground mine drifts and tunnels. The algorithm utilizes only a short window of range data sampled from the onboard LiDAR and achieves an exploratory behavior similar to that of the training expert with a more than an order of magnitude reduction in computational cost, while simultaneously relaxing the need to maintain a consistent and online reconstructed map of the environment. The trained path planning policy is extensively evaluated both in simulation and experimentally within field tests relating to the autonomous exploration of underground mines.
ER  - 

TY  - CONF
TI  - Visual-Inertial Telepresence for Aerial Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1222
EP  - 1229
AU  - J. Lee
AU  - R. Balachandran
AU  - Y. S. Sarkisov
AU  - M. De Stefano
AU  - A. Coelho
AU  - K. Shinde
AU  - M. J. Kim
AU  - R. Triebel
AU  - K. Kondak
PY  - 2020
KW  - aerospace robotics
KW  - distance measurement
KW  - feedback
KW  - grippers
KW  - haptic interfaces
KW  - human-robot interaction
KW  - manipulators
KW  - object tracking
KW  - robot vision
KW  - sensor fusion
KW  - telerobotics
KW  - virtual reality
KW  - visual-inertial telepresence
KW  - haptic device
KW  - virtual reality
KW  - 3D visual feedback
KW  - inertial sensors
KW  - object tracking algorithm
KW  - marker tracking algorithm
KW  - visual-inertial odometry
KW  - aerial manipulation
KW  - remotely located teleoperator
KW  - onboard visual sensors
KW  - human in the loop
KW  - Three-dimensional displays
KW  - Manipulators
KW  - Visualization
KW  - Cameras
KW  - Task analysis
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197394
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel telepresence system for enhancing aerial manipulation capabilities. It involves not only a haptic device, but also a virtual reality that provides a 3D visual feedback to a remotely-located teleoperator in real-time. We achieve this by utilizing onboard visual and inertial sensors, an object tracking algorithm and a pregenerated object database. As the virtual reality has to closely match the real remote scene, we propose an extension of a marker tracking algorithm with visual-inertial odometry. Both indoor and outdoor experiments show benefits of our proposed system in achieving advanced aerial manipulation tasks, namely grasping, placing, force exertion and peg-in-hole insertion.
ER  - 

TY  - CONF
TI  - Distributed Rotor-Based Vibration Suppression for Flexible Object Transport and Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1230
EP  - 1236
AU  - H. Yang
AU  - M. -S. Kim
AU  - D. Lee
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - controllability
KW  - helicopters
KW  - mechanical engineering computing
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - rotors
KW  - vibration control
KW  - RVM design
KW  - optimal placement
KW  - flexible object transport
KW  - manipulated object
KW  - object size
KW  - quadrotor usage
KW  - distributed RVMs
KW  - constrained optimization problem
KW  - aerial-ground manipulator system
KW  - robot-based vibration suppression module
KW  - distributed rotor-based vibration suppression
KW  - controllability gramian
KW  - multiple aerial-ground manipulator system
KW  - Rotors
KW  - Vibrations
KW  - Mathematical model
KW  - Manipulators
KW  - Controllability
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9196908
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The RVM (Robot-based Vibration Suppression Modules) is proposed for the manipulation and transport of a large flexible object. Since the RVM is easily attachable/detachable to the object, this RVM allows distributing over the manipulated object so that it is scalable to the object size. The composition of the system is partly motivated by the MAGMaS (Multiple Aerial-Ground Manipulator System) [1]- [3], however, since the quadrotor usage is mechanically too complicated and its design is not optimized for manipulation, thus we overcome these limitations using distributed RVMs and newly developed theory. For this, we first provide a constrained optimization problem of RVM design with the minimum number of rotors, so that the feasible thrust force is maximized while it minimizes undesirable wrench and its own weight. Then, we derive the full dynamics and elucidate a controllability condition with multiple distributed RVMs and show that even if multiple, their structures turn out similar to [2] composed with a single quadrotor. We also elucidate the optimal placement of the RVM via the usage of controllability gramian which is not even alluded in [2] and established for the first time here. Experiments are performed to demonstrate the effectiveness of the proposed theory.
ER  - 

TY  - CONF
TI  - Aerial Manipulation using Model Predictive Control for Opening a Hinged Door
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1237
EP  - 1242
AU  - D. Lee
AU  - H. Seo
AU  - D. Kim
AU  - H. J. Kim
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - control system synthesis
KW  - dynamic programming
KW  - manipulators
KW  - observers
KW  - position control
KW  - predictive control
KW  - robust control
KW  - three-term control
KW  - model predictive control
KW  - hinged door
KW  - environment interaction
KW  - aerial robot
KW  - multirotor-based aerial manipulator
KW  - daily-life moving structure
KW  - collision avoidance
KW  - differential dynamic programming
KW  - disturbance observer
KW  - robust controller
KW  - Manipulators
KW  - Vehicle dynamics
KW  - Mathematical model
KW  - Dynamics
KW  - Trajectory
KW  - Servomotors
DO  - 10.1109/ICRA40945.2020.9197524
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing studies for environment interaction with an aerial robot have been focused on interaction with static surroundings. However, to fully explore the concept of an aerial manipulation, interaction with moving structures should also be considered. In this paper, a multirotor-based aerial manipulator opening a daily-life moving structure, a hinged door, is presented. In order to address the constrained motion of the structure and to avoid collisions during operation, model predictive control (MPC) is applied to the derived coupled system dynamics between the aerial manipulator and the door involving state constraints. By implementing a constrained version of differential dynamic programming (DDP), MPC can generate position setpoints to the disturbance observer (DOB)-based robust controller in real-time, which is validated by our experimental results.
ER  - 

TY  - CONF
TI  - Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1243
EP  - 1249
AU  - B. Jeon
AU  - Y. Lee
AU  - H. J. Kim
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - object detection
KW  - quadratic programming
KW  - video recording
KW  - dense environment
KW  - drone
KW  - autonomous videography task
KW  - 3-D obstacle environment
KW  - moving object
KW  - target motion prediction module
KW  - hierarchical chasing planner
KW  - covariant optimization
KW  - bi-level structure
KW  - smooth planner
KW  - graph-search method
KW  - chasing corridor
KW  - subsequent phase
KW  - smooth trajectory
KW  - dynamically feasible trajectory
KW  - integrated motion planner
KW  - real-time aerial videography
KW  - autonomous videography task
KW  - source code
KW  - quadratic programming
KW  - Drones
KW  - Trajectory
KW  - Safety
KW  - Optimization
KW  - Measurement
KW  - Shape
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196703
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https://github.com/icsl-Jeon/traj_gen_vis and https://www.youtube.com/watch?v=_JSwXBwYRl8.
ER  - 

TY  - CONF
TI  - FG-GMM-based Interactive Behavior Estimation for Autonomous Driving Vehicles in Ramp Merging Control *
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1250
EP  - 1255
AU  - Y. Lyu
AU  - C. Dong
AU  - J. M. Dolan
PY  - 2020
KW  - automobiles
KW  - Gaussian processes
KW  - graph theory
KW  - mobile robots
KW  - probability
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - autonomous driving vehicles
KW  - autonomous driving cars
KW  - factor graph
KW  - human-designed models
KW  - FG-GMM-based interactive behavior estimation
KW  - ramp merging control
KW  - significant social interaction
KW  - probabilistic graphical model merging control model
KW  - Merging
KW  - Automobiles
KW  - Estimation
KW  - Autonomous vehicles
KW  - Probabilistic logic
KW  - Mathematical model
KW  - Roads
DO  - 10.1109/ICRA40945.2020.9197218
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Interactive behavior is important for autonomous driving vehicles, especially for scenarios like ramp merging which require significant social interaction between autonomous driving vehicles and human-driven cars. This paper enhances our previous Probabilistic Graphical Model (PGM) merging control model for the interactive behavior of autonomous driving vehicles. To better estimate the interactive behavior for autonomous driving cars, a Factor Graph (FG) is used to describe the dependency among observations and estimate other cars' intentions. Real trajectories are used to approximate the model instead of human-designed models or cost functions. Forgetting factors and a Gaussian Mixture Model (GMM) are also applied in the intention estimation process for stabilization, interpolation and smoothness. The advantage of the factor graph is that the relationship between its nodes can be described by self-defined functions, instead of probabilistic relationships as in PGM, giving more flexibility. Continuity of GMM also provides higher accuracy than the previous discrete speed transition model. The proposed method enhances the overall performance of intention estimation, in terms of collision rate and average distance between cars after merging, which means it is safer and more efficient.
ER  - 

TY  - CONF
TI  - Cooperative Perception and Localization for Cooperative Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1256
EP  - 1262
AU  - A. Miller
AU  - K. Rim
AU  - P. Chopra
AU  - P. Kelkar
AU  - M. Likhachev
PY  - 2020
KW  - cooperative systems
KW  - Kalman filters
KW  - location based services
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear filters
KW  - road vehicles
KW  - sensor fusion
KW  - vehicle sensors
KW  - extended Kalman filters
KW  - fully autonomous road vehicles
KW  - cooperative driving
KW  - cooperative perception
KW  - high fidelity sensors
KW  - low fidelity sensors
KW  - localization information
KW  - Sensor systems
KW  - Time measurement
KW  - Roads
KW  - Fuses
KW  - Current measurement
KW  - Bandwidth
DO  - 10.1109/ICRA40945.2020.9197463
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fully autonomous vehicles are expected to share the road with less advanced vehicles for a significant period of time. Furthermore, an increasing number of vehicles on the road are equipped with a variety of low-fidelity sensors which provide some perception and localization data, but not at a high enough quality for full autonomy. In this paper, we develop a perception and localization system that allows a vehicle with low-fidelity sensors to incorporate high-fidelity observations from a vehicle in front of it, allowing both vehicles to operate with full autonomy. The resulting system generates perception and localization information that is both low-noise in regions covered by high-fidelity sensors and avoids false negatives in areas only observed by low-fidelity sensors, while dealing with latency and dropout of the communication link between the two vehicles. At its core, the system uses a set of Extended Kalman filters which incorporate observations from both vehicles' sensors and extrapolate them using information about the road geometry. The perception and localization algorithms are evaluated both in simulation and on real vehicles as part of a full cooperative driving system.
ER  - 

TY  - CONF
TI  - Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1263
EP  - 1269
AU  - T. Manderson
AU  - S. Wapnick
AU  - D. Meger
AU  - G. Dudek
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - off-road vehicles
KW  - robot programming
KW  - robot vision
KW  - supervised learning
KW  - autonomous driving
KW  - vision based controllers
KW  - navigation learning
KW  - model robustmess
KW  - planning foresight
KW  - self supervised method
KW  - collision avoidance
KW  - sparse aerial images
KW  - off road driving
KW  - smooth terrain traversal
KW  - visual obstructions
KW  - on-board sensors
KW  - terrain roughness
KW  - model free reinforcement learning
KW  - unstructured outdoor environments
KW  - on-board camera
KW  - rough terrain
KW  - Predictive models
KW  - Navigation
KW  - Cameras
KW  - Planning
KW  - Computational modeling
KW  - Visualization
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196879
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both first-person and overhead aerial image inputs to our model. We nd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only first-person imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad_driving/.
ER  - 

TY  - CONF
TI  - RoadTrack: Realtime Tracking of Road Agents in Dense and Heterogeneous Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1270
EP  - 1277
AU  - R. Chandra
AU  - U. Bhattacharya
AU  - T. Randhavane
AU  - A. Bera
AU  - D. Manocha
PY  - 2020
KW  - collision avoidance
KW  - image motion analysis
KW  - object detection
KW  - object tracking
KW  - road traffic
KW  - traffic engineering computing
KW  - video signal processing
KW  - traffic videos
KW  - road agents
KW  - dense environments
KW  - heterogeneous environments
KW  - Road-Track
KW  - tracking-by-detection approach
KW  - bounding box region
KW  - Simultaneous Collision Avoidance and Interaction model
KW  - Tracking
KW  - Predictive models
KW  - Roads
KW  - Videos
KW  - Collision avoidance
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9196612
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a realtime tracking algorithm, Road-Track, to track heterogeneous road-agents in dense traffic videos. Our approach is designed for dense traffic scenarios that consist of different road-agents such as pedestrians, two-wheelers, cars, buses, etc. sharing the road. We use the tracking-by-detection approach where we track a road-agent by matching the appearance or bounding box region in the current frame with the predicted bounding box region propagated from the previous frame. Roadtrack uses a novel motion model called the Simultaneous Collision Avoidance and Interaction (SimCAI) model to predict the motion of road-agents by modeling collision avoidance and interactions between the road-agents for the next frame. We demonstrate the advantage of RoadTrack on a dataset of dense traffic videos and observe an accuracy of 75.8% on this dataset, outperforming prior state-of-the-art tracking algorithms by at least 5.2%. RoadTrack operates in realtime at approximately 30 fps and is at least 4√ó faster than prior tracking algorithms on standard tracking datasets.
ER  - 

TY  - CONF
TI  - Association-Free Multilateration Based on Times of Arrival
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1294
EP  - 1300
AU  - D. Frisch
AU  - U. D. Hanebeck
PY  - 2020
KW  - position measurement
KW  - radar tracking
KW  - target tracking
KW  - time-of-arrival estimation
KW  - spatially distributed receivers
KW  - static measurements
KW  - multitarget trackers
KW  - association-free multilateration
KW  - times of arrival estimation
KW  - uncorrelated measurement
KW  - initialization routine
KW  - Receivers
KW  - Noise measurement
KW  - Position measurement
KW  - Optimization
KW  - Acoustic measurements
KW  - Target tracking
KW  - Acoustics
DO  - 10.1109/ICRA40945.2020.9197455
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multilateration systems reconstruct the location of a target that transmits electromagnetic or acoustic signals. The employed measurements for localization are the times of arrival (TOAs) of the transmitted signal, measured by a number of spatially distributed receivers at known positions. We present a novel multilateration algorithm to localize multiple targets that transmit indistinguishable signals at unknown times. That is, each receiver measures merely a set of TOAs with no association to the targets. Our method does not need any prior information. Therefore, it can provide uncorrelated, static measurements to be introduced into a separate tracker subsequently, or an initialization routine for multi target trackers.
ER  - 

TY  - CONF
TI  - Adversarial Feature Disentanglement for Place Recognition Across Changing Appearance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1301
EP  - 1307
AU  - L. Tang
AU  - Y. Wang
AU  - Q. Luo
AU  - X. Ding
AU  - R. Xiong
PY  - 2020
KW  - feature extraction
KW  - image matching
KW  - image sequences
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - supervised learning
KW  - adversarial feature disentanglement
KW  - seasonal variation
KW  - visual place recognition
KW  - image descriptors
KW  - adversarial network
KW  - image sequences
KW  - domain related features
KW  - self supervised manner
KW  - image matching
KW  - Feature extraction
KW  - Training
KW  - Image reconstruction
KW  - Image recognition
KW  - Robustness
KW  - Machine learning
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196518
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When robots move autonomously for long-term, varied appearance such as the transition from day to night and seasonal variation brings challenges to visual place recognition. Defining an appearance condition (e.g. a season, a kind of weather) as a domain, we consider that the desired representation for place recognition (i) should be domain-unrelated so that images from different time can be matched regardless of varied appearance, (ii) should be learned in a self-supervised manner without the need of massive manually labeled data, and (iii) should be able to train among multiple domains in one model to keep limited model complexity. This paper sets to find domain-unrelated features across extremely changing appearance, which can be used as image descriptors to match between images collected at different conditions. We propose to use the adversarial network to disentangle domain-unrelated and domain-related features, which are named place and appearance features respectively. During training, only domain information is needed without requiring manually aligned image sequences. Experiments demonstrated that our method can disentangle place and appearance features in both toy case and images from the real world, and the place feature is qualified in place recognition tasks under different appearance conditions. The proposed network is also adaptable to multiple domains without increasing model capacity and shows favorable generalization.
ER  - 

TY  - CONF
TI  - A Fast and Accurate Solution for Pose Estimation from 3D Correspondences
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1308
EP  - 1314
AU  - L. Zhou
AU  - S. Wang
AU  - M. Kaess
PY  - 2020
KW  - approximation theory
KW  - computational geometry
KW  - computer vision
KW  - convex programming
KW  - least squares approximations
KW  - minimisation
KW  - pose estimation
KW  - pose estimation
KW  - point-to-plane correspondences
KW  - computer vision
KW  - least-squares problem
KW  - global minimizer
KW  - real-time applications
KW  - local minimizer
KW  - Cayley-Gibbs-Rodriguez parameterization
KW  - first-order optimality conditions
KW  - 3D correspondences
KW  - CGR parameterization
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Cost function
KW  - Approximation algorithms
KW  - Real-time systems
KW  - Iterative closest point algorithm
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197023
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Estimating pose from given 3D correspondences, including point-to-point, point-to-line and point-to-plane correspondences, is a fundamental task in computer vision with many applications. We present a fast and accurate solution for the least-squares problem of this task. Previous works mainly focus on studying the way to find the global minimizer of the least-squares problem. However, existing works that show the ability to achieve the global minimizer are still unsuitable for real-time applications. Furthermore, as one of contributions of this paper, we prove that there exist ambiguous configurations for any number of lines and planes. These configurations have several solutions in theory, which makes the correct solution may come from a local minimizer when the data are with noise. Previous works based on convex optimization which is unable to find local minimizers do not work in the ambiguous configuration. Our algorithm is efficient and able to reveal local minimizers. We employ the Cayley-Gibbs-Rodriguez (CGR) parameterization of the rotation to derive a general rational cost for the three cases of 3D correspondences. The main contribution of this paper is to solve the first-order optimality conditions of the least-squares problem, which are of a complicated rational form. The central idea of our algorithm is to introduce some intermediate unknowns to simplify the problem. Extensive experimental results show that our algorithm is more stable than previous algorithms when the number N of correspondences is small. Besides, when N is large, our algorithm achieves the same accuracy as the state-of-the-art algorithm [1], but our algorithm is about 7 times faster than [1] in real applications.
ER  - 

TY  - CONF
TI  - Ground Texture Based Localization Using Compact Binary Descriptors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1315
EP  - 1321
AU  - J. Fabian Schmid
AU  - S. F. Simon
AU  - R. Mester
PY  - 2020
KW  - image matching
KW  - image texture
KW  - pose estimation
KW  - robot vision
KW  - ground texture based localization
KW  - compact binary descriptors
KW  - global localization
KW  - subsequent local localization updates
KW  - compact binary feature descriptors
KW  - localization success rates
KW  - self-contained method
KW  - matching strategy
KW  - identity matching
KW  - Feature extraction
KW  - Cameras
KW  - Robots
KW  - Latches
KW  - Task analysis
KW  - Asphalt
KW  - Pose estimation
DO  - 10.1109/ICRA40945.2020.9197221
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ground texture based localization is a promising approach to achieve high-accuracy positioning of vehicles. We present a self-contained method that can be used for global localization as well as for subsequent local localization updates, i.e. it allows a robot to localize without any knowledge of its current whereabouts, but it can also take advantage of a prior pose estimate to reduce computation time significantly. Our method is based on a novel matching strategy, which we call identity matching, that is based on compact binary feature descriptors. Identity matching treats pairs of features as matches only if their descriptors are identical. While other methods for global localization are faster to compute, our method reaches higher localization success rates, and can switch to local localization after the initial localization.
ER  - 

TY  - CONF
TI  - Reliable Data Association for Feature-Based Vehicle Localization using Geometric Hashing Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1322
EP  - 1328
AU  - I. Hofstetter
AU  - M. Sprunk
AU  - F. Ries
AU  - M. Haueis
PY  - 2020
KW  - feature extraction
KW  - optical radar
KW  - reliability
KW  - road vehicles
KW  - sensor fusion
KW  - feature-based vehicle localization
KW  - data association
KW  - local environment
KW  - plausible feature associations
KW  - safe localization
KW  - localization features
KW  - geometric hashing methods
KW  - error propagation
KW  - cylindrical objects
KW  - LiDAR data
KW  - Feature extraction
KW  - Reliability
KW  - Data mining
KW  - Noise measurement
KW  - Standards
KW  - Visualization
KW  - Object recognition
DO  - 10.1109/ICRA40945.2020.9196601
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Reliable data association represents a main challenge of feature-based vehicle localization and is the key to integrity of localization. Independent of the type of features used, incorrect associations between detected and mapped features will provide erroneous position estimates. Only if the uniqueness of a local environment is represented by the features that are stored in the map, the reliability of localization is enhanced. In this work, a new approach based on Geometric Hashing is introduced to the field of data association for feature-based vehicle localization. Without any information on a prior position, the proposed method allows to efficiently search large map regions for plausible feature associations. Therefore, odometry and GNSS-based inputs can be neglected, which reduces the risk of error propagation and enables safe localization. The approach is demonstrated on approximately 10min of data recorded in an urban scenario. Cylindrical objects without distinctive descriptors, which were extracted from LiDAR data, serve as localization features. Experimental results both demonstrate the feasibility as well as limitations of the approach.
ER  - 

TY  - CONF
TI  - Context-Aware Task Execution Using Apprenticeship Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1329
EP  - 1335
AU  - A. F. Abdelrahman
AU  - A. Mitrevski
AU  - P. G. Pl√∂ger
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - service robots
KW  - ubiquitous computing
KW  - demonstrated motion
KW  - learned policy
KW  - perceived behaviour
KW  - context-aware task execution
KW  - apprenticeship learning
KW  - assistive service robots
KW  - human-oriented tasks
KW  - task parameters
KW  - optimal behaviour
KW  - robot-to-human object hand-over
KW  - reinforcement learning
KW  - demonstrator
KW  - contextualized variants
KW  - demonstrated action
KW  - dynamic movement primitives
KW  - compact motion representations
KW  - model-based C-REPS algorithm
KW  - hand-over position
KW  - context variables
KW  - simulated task executions
KW  - evaluating emergent behaviours
KW  - context-aware action generalization
KW  - Robots
KW  - Task analysis
KW  - Trajectory
KW  - Context modeling
KW  - Encoding
KW  - Adaptation models
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9197476
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An essential measure of autonomy in assistive service robots is adaptivity to the various contexts of human-oriented tasks, which are subject to subtle variations in task parameters that determine optimal behaviour. In this work, we propose an apprenticeship learning approach to achieving context-aware action generalization on the task of robot-to-human object hand-over. The procedure combines learning from demonstration and reinforcement learning: a robot first imitates a demonstrator's execution of the task and then learns contextualized variants of the demonstrated action through experience. We use dynamic movement primitives as compact motion representations, and a model-based C-REPS algorithm for learning policies that can specify hand-over position, conditioned on context variables. Policies are learned using simulated task executions, before transferring them to the robot and evaluating emergent behaviours. We additionally conduct a user study involving participants assuming different postures and receiving an object from a robot, which executes hand-overs by either imitating a demonstrated motion, or adapting its motion to hand-over positions suggested by the learned policy. The results confirm the hypothesized improvements in the robot's perceived behaviour when it is context-aware and adaptive, and provide useful insights that can inform future developments.
ER  - 

TY  - CONF
TI  - Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1336
EP  - 1342
AU  - R. Rayyes
AU  - H. Donat
AU  - J. Steil
PY  - 2020
KW  - adaptive control
KW  - hierarchical systems
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - radial basis function networks
KW  - stability
KW  - hierarchical interest-driven goal babbling
KW  - bootstrapping
KW  - sensorimotor skills
KW  - time-dependent changes
KW  - intrinsic motivation signal
KW  - online associative radial basis function network
KW  - associative dynamic network
KW  - parameter-sharing technique
KW  - exhaustive parameter tuning
KW  - learning process
KW  - physical robot manipulator
KW  - data-driven robot model learning
KW  - stability
KW  - Task analysis
KW  - Current measurement
KW  - Robot sensing systems
KW  - Service robots
KW  - Stability analysis
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196763
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel hierarchical online learning scheme for fast and efficient bootstrapping of sensorimotor skills. Our scheme permits rapid data-driven robot model learning in a "learning while behaving" fashion. It is updated continuously to adapt to time-dependent changes and driven by an intrinsic motivation signal. It utilizes an online associative radial basis function network, which is the first associative dynamic network to be constructed from scratch with high stability. Moreover, we propose a parameter-sharing technique to increase efficiency, stabilize the online scheme, avoid exhaustive parameter tuning, and speed up the learning process. We apply our proposed algorithms on a 7-DoF physical robot manipulator and demonstrate their performance and efficiency.
ER  - 

TY  - CONF
TI  - Robot-Supervised Learning for Object Segmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1343
EP  - 1349
AU  - V. Florence
AU  - J. J. Corso
AU  - B. Griffin
PY  - 2020
KW  - computer vision
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - robot manipulator
KW  - human supervision
KW  - foreground segmentation technique
KW  - grasped object
KW  - state-of-the-art adaptable in-hand object segmentation
KW  - segmentation performance
KW  - robot-supervised
KW  - unstructured changing environments
KW  - deep learning
KW  - object detection
KW  - human annotators
KW  - learning-based segmentation methods
KW  - robotics applications
KW  - annotated training data
KW  - pixelwise segmentation
KW  - Robot kinematics
KW  - Manipulators
KW  - Image segmentation
KW  - Robot sensing systems
KW  - Object segmentation
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196543
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To be effective in unstructured and changing environments, robots must learn to recognize new objects. Deep learning has enabled rapid progress for object detection and segmentation in computer vision; however, this progress comes at the price of human annotators labeling many training examples. This paper addresses the problem of extending learning-based segmentation methods to robotics applications where annotated training data is not available. Our method enables pixelwise segmentation of grasped objects. We factor the problem of segmenting the object from the background into two sub-problems: (1) segmenting the robot manipulator and object from the background and (2) segmenting the object from the manipulator. We propose a kinematics-based foreground segmentation technique to solve (1). To solve (2), we train a self-recognition network that segments the robot manipulator. We train this network without human supervision, leveraging our foreground segmentation technique from (1) to label a training set of images containing the robot manipulator without a grasped object. We demonstrate experimentally that our method outperforms state-of-the-art adaptable in-hand object segmentation. We also show that a training set composed of automatically labelled images of grasped objects improves segmentation performance on a test set of images of the same objects in the environment.
ER  - 

TY  - CONF
TI  - Gradient and Log-based Active Learning for Semantic Segmentation of Crop and Weed for Agricultural Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1350
EP  - 1356
AU  - R. Sheikh
AU  - A. Milioto
AU  - P. Lottes
AU  - C. Stachniss
AU  - M. Bennewitz
AU  - T. Schultz
PY  - 2020
KW  - agriculture
KW  - convolutional neural nets
KW  - entropy
KW  - image sampling
KW  - image segmentation
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - crop
KW  - weed
KW  - agricultural robots
KW  - annotated datasets
KW  - supervised learning
KW  - tedious time-intensive task
KW  - active learning
KW  - image data
KW  - existing semantic segmentation CNN
KW  - growth stage
KW  - rough foreground segmentation
KW  - substantially different field
KW  - challenging datasets
KW  - agricultural robotics domain
KW  - entropy based sampling
KW  - human labeling effort
KW  - Image segmentation
KW  - Semantics
KW  - Task analysis
KW  - Agriculture
KW  - Robots
KW  - Training
KW  - Sugar industry
DO  - 10.1109/ICRA40945.2020.9196722
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Annotated datasets are essential for supervised learning. However, annotating large datasets is a tedious and time-intensive task. This paper addresses active learning in the context of semantic segmentation with the goal of reducing the human labeling effort. Our application is agricultural robotics and we focus on the task of distinguishing between crop and weed plants from image data. A key challenge in this application is the transfer of an existing semantic segmentation CNN to a new field, in which growth stage, weeds, soil, and weather conditions differ. We propose a novel approach that, given a trained model on one field together with rough foreground segmentation, refines the network on a substantially different field providing an effective method of selecting samples to annotate for supporting the transfer. We evaluated our approach on two challenging datasets from the agricultural robotics domain and show that we achieve a higher accuracy with a smaller number of samples compared to random sampling as well as entropy based sampling, which consequently reduces the required human labeling effort.
ER  - 

TY  - CONF
TI  - Learning How to Walk: Warm-starting Optimal Control Solver with Memory of Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1357
EP  - 1363
AU  - T. S. Lembono
AU  - C. Mastalli
AU  - P. Fernbach
AU  - N. Mansard
AU  - S. Calinon
PY  - 2020
KW  - humanoid robots
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - optimal control
KW  - path planning
KW  - regression analysis
KW  - trajectory control
KW  - optimal control solver
KW  - locomotion task
KW  - humanoid robot
KW  - HPP Loco3D
KW  - versatile locomotion planner
KW  - whole-body trajectory
KW  - regression problem
KW  - single-step motion
KW  - multistep motion
KW  - predicted motion
KW  - Crocoddyl control solver
KW  - Trajectory
KW  - Databases
KW  - Optimal control
KW  - Task analysis
KW  - Legged locomotion
KW  - Ground penetrating radar
DO  - 10.1109/ICRA40945.2020.9196727
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a framework to build a memory of motion for warm-starting an optimal control solver for the locomotion task of a humanoid robot. We use HPP Loco3D, a versatile locomotion planner, to generate offline a set of dynamically consistent whole-body trajectory to be stored as the memory of motion. The learning problem is formulated as a regression problem to predict a single-step motion given the desired contact locations, which is used as a building block for producing multi-step motions. The predicted motion is then used as a warm-start for the fast optimal control solver Crocoddyl. We have shown that the approach manages to reduce the required number of iterations to reach the convergence from ~9.5 to only ~3.0 iterations for the single-step motion and from ~6.2 to ~4.5 iterations for the multi-step motion, while maintaining the solution's quality.
ER  - 

TY  - CONF
TI  - Feedback Linearization for Uncertain Systems via Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1364
EP  - 1371
AU  - T. Westenbroek
AU  - D. Fridovich-Keil
AU  - E. Mazumdar
AU  - S. Arora
AU  - V. Prabhu
AU  - S. S. Sastry
AU  - C. J. Tomlin
PY  - 2020
KW  - approximation theory
KW  - continuous time systems
KW  - control system synthesis
KW  - feedback
KW  - function approximation
KW  - learning (artificial intelligence)
KW  - linearisation techniques
KW  - nonlinear control systems
KW  - optimisation
KW  - uncertain systems
KW  - model-free policy optimization techniques
KW  - feedback linearization
KW  - nonlinear control
KW  - nonlinear plant
KW  - feedback controller
KW  - linear control techniques
KW  - exact linearizing controllers
KW  - learned linearizing controller
KW  - model-free policy optimization algorithms
KW  - Feedback linearization
KW  - Conferences
KW  - Automation
KW  - Uncertain systems
KW  - Learning (artificial intelligence)
KW  - Control design
KW  - Nonlinear systems
DO  - 10.1109/ICRA40945.2020.9197158
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel approach to control design for nonlinear systems which leverages model-free policy optimization techniques to learn a linearizing controller for a physical plant with unknown dynamics. Feedback linearization is a technique from nonlinear control which renders the input-output dynamics of a nonlinear plant linear under application of an appropriate feedback controller. Once a linearizing controller has been constructed, desired output trajectories for the nonlinear plant can be tracked using a variety of linear control techniques. However, the calculation of a linearizing controller requires a precise dynamics model for the system. As a result, model-based approaches for learning exact linearizing controllers generally require a simple, highly structured model of the system with easily identifiable parameters. In contrast, the model-free approach presented in this paper is able to approximate the linearizing controller for the plant using general function approximation architectures. Specifically, we formulate a continuous-time optimization problem over the parameters of a learned linearizing controller whose optima are the set of parameters which best linearize the plant. We derive conditions under which the learning problem is (strongly) convex and provide guarantees which ensure the true linearizing controller for the plant is recovered. We then discuss how model-free policy optimization algorithms can be used to solve a discrete-time approximation to the problem using data collected from the real-world plant. The utility of the framework is demonstrated in simulation and on a real-world robotic platform.
ER  - 

TY  - CONF
TI  - Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1380
EP  - 1386
AU  - B. van Amsterdam
AU  - M. J. Clarkson
AU  - D. Stoyanov
PY  - 2020
KW  - feature extraction
KW  - gesture recognition
KW  - image segmentation
KW  - medical robotics
KW  - recurrent neural nets
KW  - robot dynamics
KW  - robot kinematics
KW  - surgery
KW  - multitask recurrent neural network
KW  - surgical gesture recognition
KW  - surgical data science
KW  - computer-aided intervention
KW  - robotic kinematic information
KW  - robot kinematic data
KW  - Task analysis
KW  - Training
KW  - Kinematics
KW  - Estimation
KW  - Needles
KW  - Gesture recognition
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9197301
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical gesture recognition is important for surgical data science and computer-aided intervention. Even with robotic kinematic information, automatically segmenting surgical steps presents numerous challenges because surgical demonstrations are characterized by high variability in style, duration and order of actions. In order to extract discriminative features from the kinematic signals and boost recognition accuracy, we propose a multi-task recurrent neural network for simultaneous recognition of surgical gestures and estimation of a novel formulation of surgical task progress. To show the effectiveness of the presented approach, we evaluate its application on the JIGSAWS dataset, that is currently the only publicly available dataset for surgical gesture recognition featuring robot kinematic data. We demonstrate that recognition performance improves in multi-task frameworks with progress estimation without any additional manual labelling and training.
ER  - 

TY  - CONF
TI  - Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1387
EP  - 1393
AU  - N. Yilmaz
AU  - J. Y. Wu
AU  - P. Kazanzides
AU  - U. Tumerdem
PY  - 2020
KW  - force control
KW  - force sensors
KW  - mean square error methods
KW  - medical robotics
KW  - neurocontrollers
KW  - robot dynamics
KW  - surgical robotic systems
KW  - internal joint torques
KW  - robot inverse dynamics
KW  - da Vinci surgical robot
KW  - environment forces
KW  - model-based approaches
KW  - external force sensor
KW  - external force estimation
KW  - da Vinci research kit
KW  - neural network based inverse dynamics identification
KW  - normalized rootmean-square error
KW  - NRMSE
KW  - tool/tissue interaction forces
KW  - Robots
KW  - Dynamics
KW  - Force
KW  - Estimation
KW  - Training
KW  - Biological neural networks
DO  - 10.1109/ICRA40945.2020.9197445
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Most current surgical robotic systems lack the ability to sense tool/tissue interaction forces, which motivates research in methods to estimate these forces from other available measurements, primarily joint torques. These methods require the internal joint torques, due to the robot inverse dynamics, to be subtracted from the measured joint torques. This paper presents the use of neural networks to estimate the inverse dynamics of the da Vinci surgical robot, which enables estimation of the external environment forces. Experiments with motions in free space demonstrate that the neural networks can estimate the internal joint torques within 10% normalized rootmean-square error (NRMSE), which outperforms model-based approaches in the literature. Comparison with an external force sensor shows that the method is able to estimate environment forces within about 10% NRMSE.
ER  - 

TY  - CONF
TI  - Reliable Trajectories for Dynamic Quadrupeds using Analytical Costs and Learned Initializations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1410
EP  - 1416
AU  - O. Melon
AU  - M. Geisert
AU  - D. Surovik
AU  - I. Havoutis
AU  - M. Fallon
PY  - 2020
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - navigation
KW  - nonlinear programming
KW  - path planning
KW  - predictive control
KW  - reliability
KW  - robot dynamics
KW  - robust control
KW  - trajectory control
KW  - dynamic quadrupeds
KW  - dynamic traversal
KW  - legged robotics
KW  - robust dynamic motion
KW  - uneven terrain navigation
KW  - TOWR
KW  - learning based scheme
KW  - whole body tracking controller
KW  - trajectory optimization for walking robots
KW  - model predictive control
KW  - dynamic trajectory reliability
KW  - nonlinear program
KW  - dynamic motions
KW  - Legged locomotion
KW  - Dynamics
KW  - Foot
KW  - Trajectory optimization
DO  - 10.1109/ICRA40945.2020.9196562
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dynamic traversal of uneven terrain is a major objective in the field of legged robotics. The most recent model predictive control approaches for these systems can generate robust dynamic motion of short duration; however, planning over a longer time horizon may be necessary when navigating complex terrain. A recently-developed framework, Trajectory Optimization for Walking Robots (TOWR), computes such plans but does not guarantee their reliability on real platforms, under uncertainty and perturbations. We extend TOWR with analytical costs to generate trajectories that a state-of-the-art whole-body tracking controller can successfully execute. To reduce online computation time, we implement a learning-based scheme for initialization of the nonlinear program based on offline experience. The execution of trajectories as long as 16 footsteps and 5.5 s over different terrains by a real quadruped demonstrates the effectiveness of the approach on hardware. This work builds toward an online system which can efficiently and robustly replan dynamic trajectories.
ER  - 

TY  - CONF
TI  - On the Hardware Feasibility of Nonlinear Trajectory Optimization for Legged Locomotion based on a Simplified Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1417
EP  - 1423
AU  - A. Bratta
AU  - R. Orsolino
AU  - M. Focchi
AU  - V. Barasuol
AU  - G. G. Muscolo
AU  - C. Semini
PY  - 2020
KW  - hydraulic actuators
KW  - legged locomotion
KW  - motion control
KW  - path planning
KW  - position control
KW  - robot dynamics
KW  - HyQ robot
KW  - Hydraulically actuated Quadruped robot
KW  - simplified nonlinear nonconvex trajectory optimization
KW  - single rigid body dynamics-based trajectory optimizer
KW  - leg collision
KW  - leg model
KW  - joint positions
KW  - admissible contact forces
KW  - joint-torque limits
KW  - challenging terrain
KW  - robust motions
KW  - feasibility constraints
KW  - motion planning
KW  - computational efficiency
KW  - simplified dynamics
KW  - legged locomotion
KW  - nonlinear trajectory optimization
KW  - hardware feasibility
KW  - Legged locomotion
KW  - Foot
KW  - Collision avoidance
KW  - Force
KW  - Trajectory
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9196903
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simplified models are useful to increase the computational efficiency of a motion planning algorithm, but their lack of accuracy have to be managed. We propose two feasibility constraints to be included in a Single Rigid Body Dynamics-based trajectory optimizer in order to obtain robust motions in challenging terrain. The first one finds an approximate relationship between joint-torque limits and admissible contact forces, without requiring the joint positions. The second one proposes a leg model to prevent leg collision with the environment. Such constraints have been included in a simplified nonlinear non-convex trajectory optimization problem. We demonstrate the feasibility of the resulting motion plans both in simulation and on the Hydraulically actuated Quadruped (HyQ) robot, considering experiments on an irregular terrain.
ER  - 

TY  - CONF
TI  - Agile Legged-Wheeled Reconfigurable Navigation Planner Applied on the CENTAURO Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1424
EP  - 1430
AU  - V. S. Raghavan
AU  - D. Kanoulas
AU  - D. G. Caldwell
AU  - N. G. Tsagarakis
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - navigation
KW  - path planning
KW  - search problems
KW  - CENTAURO robot
KW  - agile legged wheeled reconfigurable navigation planner
KW  - hybrid legged-wheeled robots
KW  - Theta* based planner
KW  - trapezium-like search
KW  - Robot kinematics
KW  - Mobile robots
KW  - Planning
KW  - Wheels
KW  - Navigation
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197407
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid legged-wheeled robots such as the CEN-TAURO, are capable of varying their footprint polygon to carry out various agile motions. This property can be advantageous for wheeled-only planning in cluttered spaces, which is our focus. In this paper, we present an improved algorithm that builds upon our previously introduced preliminary footprint varying A* planner, which was based on the rectangular symmetry of the foot support polygon. In particular, we introduce a Theta* based planner with trapezium-like search, which aims to further reduce the limitations imposed upon the wheeled-only navigation of the CENTAURO robot by the low-dimensional search space, maintaining the real-time computational efficiency. The method is tested on the simulated and real full-size CENTAURO robot in cluttered environments.
ER  - 

TY  - CONF
TI  - Bounded haptic teleoperation of a quadruped robot‚Äôs foot posture for sensing and manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1431
EP  - 1437
AU  - G. Xin
AU  - J. Smith
AU  - D. Rytz
AU  - W. Wolfslag
AU  - H. -C. Lin
AU  - M. Mistry
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - legged locomotion
KW  - mechanical engineering computing
KW  - motion control
KW  - position control
KW  - quadratic programming
KW  - robot dynamics
KW  - telerobotics
KW  - quadruped robot ANYmal
KW  - force feedback
KW  - bounded haptic teleoperation
KW  - control framework
KW  - operator-guided haptic exploration
KW  - torso
KW  - foot posture control
KW  - whole-body controller
KW  - analytical Cartesian impedance controllers
KW  - null space projector
KW  - contact forces
KW  - force-feedback
KW  - 7D haptic joystick
KW  - Impedance
KW  - Aerospace electronics
KW  - Robot kinematics
KW  - Haptic interfaces
KW  - Torso
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197501
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a control framework to teleoperate a quadruped robot's foot for operator-guided haptic exploration of the environment. Since one leg of a quadruped robot typically only has 3 actuated degrees of freedom (DoFs), the torso is employed to assist foot posture control via a hierarchical whole-body controller. The foot and torso postures are controlled by two analytical Cartesian impedance controllers cascaded by a null space projector. The contact forces acting on supporting feet are optimized by quadratic programming (QP). The foot's Cartesian impedance controller may also estimate contact forces from trajectory tracking errors, and relay the force-feedback to the operator. A 7D haptic joystick, Sigma.7, transmits motion commands to the quadruped robot ANYmal, and renders the force feedback. Furthermore, the joystick's motion is bounded by mapping the foot's feasible force polytope constrained by the friction cones and torque limits in order to prevent the operator from driving the robot to slipping or falling over. Experimental results demonstrate the efficiency of the proposed framework.
ER  - 

TY  - CONF
TI  - Pinbot: A Walking Robot with Locking Pin Arrays for Passive Adaptability to Rough Terrains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1438
EP  - 1444
AU  - S. Noh
AU  - A. M. Dollar
PY  - 2020
KW  - design engineering
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - legged robots
KW  - passive adaptability
KW  - locking pin arrays
KW  - pin array mechanisms
KW  - walking robot
KW  - legged robot design
KW  - rough terrain locomotion
KW  - unstructured terrains
KW  - rough terrains
KW  - stable locomotion
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9197342
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To date, many control strategies for legged robots have been proposed for stable locomotion over rough and unstructured terrains. However, these approaches require sensing information throughout locomotion, which may be noisy or unavailable at times. An alternative solution to rough terrain locomotion is a legged robot design that can passively adapt to the variations in the terrain without requiring knowledge of them. This paper presents one such solution in the design of a walking robot that employs pin array mechanisms to passively adapt to rough terrains. The pins are passively dropped over the terrain to conform to its variations and then locked to provide a statically stable stance. Locomotion is achieved with parallel four-bar linkages that swing forward the platforms in an alternating manner. Experimental evaluation of the robot demonstrates that the pin arrays enable legged locomotion over rough terrains under open-loop control.
ER  - 

TY  - CONF
TI  - Planning for the Unexpected: Explicitly Optimizing Motions for Ground Uncertainty in Running
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1445
EP  - 1451
AU  - K. Green
AU  - R. L. Hatton
AU  - J. Hurst
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - robot dynamics
KW  - ground uncertainty
KW  - actuation plans
KW  - dynamic model
KW  - bipedal running
KW  - fixed body trajectory
KW  - passive dynamics
KW  - reduced order model
KW  - emergent robustness
KW  - legged robots
KW  - legged locomotion
KW  - linked inputs
KW  - input linking
KW  - hybrid dynamics
KW  - running model
KW  - optimization procedure
KW  - standard trajectory optimization
KW  - robust gaits
KW  - Legged locomotion
KW  - Springs
KW  - Dynamics
KW  - Foot
KW  - Robustness
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9197049
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a method to generate actuation plans for a reduced order, dynamic model of bipedal running. This method explicitly enforces robustness to ground uncertainty. The plan generated is not a fixed body trajectory that is aggressively stabilized: instead, the plan interacts with the passive dynamics of the reduced order model to create emergent robustness. The goal is to create plans for legged robots that will be robust to imperfect perception of the environment, and to work with dynamics that are too complex to optimize in real-time. Working within this dynamic model of legged locomotion, we optimize a set of disturbance cases together with the nominal case, all with linked inputs. The input linking is nontrivial due to the hybrid dynamics of the running model but our solution is effective and has analytical gradients. The optimization procedure proposed is significantly slower than a standard trajectory optimization, but results in robust gaits that reject disturbances extremely effectively without any replanning required.
ER  - 

TY  - CONF
TI  - One-Shot Multi-Path Planning for Robotic Applications Using Fully Convolutional Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1460
EP  - 1466
AU  - T. Kulvicius
AU  - S. Herzog
AU  - T. L√ºddecke
AU  - M. Tamosiunaite
AU  - F. W√∂rg√∂tter
PY  - 2020
KW  - convolutional neural nets
KW  - iterative methods
KW  - mobile robots
KW  - neurocontrollers
KW  - path planning
KW  - robot action execution
KW  - motion trajectory
KW  - iterative methods
KW  - fully convolutional neural network
KW  - network prediction iteration
KW  - optimal paths
KW  - single path predictions
KW  - simultaneously generated paths
KW  - shot multipath planning
KW  - robotic applications
KW  - Two dimensional displays
KW  - Three-dimensional displays
KW  - Training
KW  - Robots
KW  - Path planning
KW  - Prediction algorithms
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196719
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning is important for robot action execution, since a path or a motion trajectory for a particular action has to be defined first before the action can be executed. Most of the current approaches are iterative methods where the trajectory is generated by predicting the next state based on the current state. Here we propose a novel method by utilising a fully convolutional neural network, which allows generation of complete paths even for several agents with one network prediction iteration. We demonstrate that our method is able to successfully generate optimal or close to optimal paths (less than 10% longer) in more than 99% of the cases for single path predictions in 2D and 3D environments. Furthermore, we show that the network is - without specific training on such cases - able to create (close to) optimal paths in 96% of the cases for two and in 84% of the cases for three simultaneously generated paths.
ER  - 

TY  - CONF
TI  - Efficient Iterative Linear-Quadratic Approximations for Nonlinear Multi-Player General-Sum Differential Games
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1475
EP  - 1481
AU  - D. Fridovich-Keil
AU  - E. Ratner
AU  - L. Peters
AU  - A. D. Dragan
AU  - C. J. Tomlin
PY  - 2020
KW  - approximation theory
KW  - decision making
KW  - differential games
KW  - iterative methods
KW  - linear quadratic control
KW  - multi-agent systems
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - iterative linear-quadratic regulator
KW  - linear dynamics
KW  - quadratic costs
KW  - linear-quadratic games
KW  - complex interactive behavior
KW  - efficient iterative linear-quadratic approximations
KW  - nonlinear multiplayer general-sum differential games
KW  - robotics
KW  - multiple decision making agents
KW  - expressive theoretical framework
KW  - multiagent problems
KW  - numerical solution techniques
KW  - state dimension
KW  - single agent optimal control problem
KW  - ILQR
KW  - repeated approximations
KW  - three-player 14-state simulated intersection problem
KW  - hardware collision-avoidance test
KW  - time 0.25 s
KW  - time 50.0 ms
KW  - Games
KW  - Heuristic algorithms
KW  - Approximation algorithms
KW  - Optimal control
KW  - Iterative methods
KW  - Trajectory
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9197129
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many problems in robotics involve multiple decision making agents. To operate efficiently in such settings, a robot must reason about the impact of its decisions on the behavior of other agents. Differential games offer an expressive theoretical framework for formulating these types of multi-agent problems. Unfortunately, most numerical solution techniques scale poorly with state dimension and are rarely used in real-time applications. For this reason, it is common to predict the future decisions of other agents and solve the resulting decoupled, i.e., single-agent, optimal control problem. This decoupling neglects the underlying interactive nature of the problem; however, efficient solution techniques do exist for broad classes of optimal control problems. We take inspiration from one such technique, the iterative linear-quadratic regulator (ILQR), which solves repeated approximations with linear dynamics and quadratic costs. Similarly, our proposed algorithm solves repeated linear-quadratic games. We experimentally benchmark our algorithm in several examples with a variety of initial conditions and show that the resulting strategies exhibit complex interactive behavior. Our results indicate that our algorithm converges reliably and runs in real-time. In a three-player, 14-state simulated intersection problem, our algorithm initially converges in <; 0.25 s. Receding horizon invocations converge in <; 50 ms in a hardware collision-avoidance test.
ER  - 

TY  - CONF
TI  - Path-Following Model Predictive Control of Ballbots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1498
EP  - 1504
AU  - T. K. Jespersen
AU  - M. al Ahdab
AU  - J. d. Dios F. Mendez
AU  - M. R. Damgaard
AU  - K. D. Hansen
AU  - R. Pedersen
AU  - T. Bak
PY  - 2020
KW  - mobile robots
KW  - predictive control
KW  - robot dynamics
KW  - model predictive control
KW  - path-following tasks
KW  - dynamically unstable mobile robots
KW  - single ball
KW  - simplied version
KW  - physical ballbot system
KW  - high fidelity model
KW  - online implementation
KW  - quaternion-based model
KW  - Robots
KW  - Solid modeling
KW  - Friction
KW  - Quaternions
KW  - Planning
KW  - Acceleration
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196634
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a novel approach for model predictive control of ballbots for path-following tasks. Ballbots are dynamically unstable mobile robots which are designed to balance on a single ball. The model presented in this paper is a simplied version of a full quaternion-based model of ballbots' underactuated dynamics which is suited for online implementation. Furthermore, the approach is extended to handle nearby obstacles directly in the MPC formulation. The presented controller is validated through simulation on a high fidelity model as well as through real-world experiments on a physical ballbot system.
ER  - 

TY  - CONF
TI  - Underactuated Waypoint Trajectory Optimization for Light Painting Photography
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1505
EP  - 1510
AU  - C. Eilers
AU  - J. Eschmann
AU  - R. Menzenbach
AU  - B. Belousov
AU  - F. Muratore
AU  - J. Peters
PY  - 2020
KW  - control system synthesis
KW  - nonlinear control systems
KW  - optimisation
KW  - pendulums
KW  - photography
KW  - trajectory control
KW  - underactuated waypoint trajectory optimization
KW  - light painting photography
KW  - control engineering
KW  - auxiliary optimization variables
KW  - waypoint activations
KW  - letter drawing task
KW  - long exposure photography
KW  - r control engineering
KW  - Linear programming
KW  - Task analysis
KW  - Trajectory optimization
KW  - Painting
KW  - Photography
DO  - 10.1109/ICRA40945.2020.9196516
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Despite their abundance in robotics and nature, underactuated systems remain a challenge for control engineering. Trajectory optimization provides a generally applicable solution, however its efficiency strongly depends on the skill of the engineer to frame the problem in an optimizer-friendly way. This paper proposes a procedure that automates such problem reformulation for a class of tasks in which the desired trajectory is specified by a sequence of waypoints. The approach is based on introducing auxiliary optimization variables that represent waypoint activations. To validate the proposed method, a letter drawing task is set up where shapes traced by the tip of a rotary inverted pendulum are visualized using long exposure photography.
ER  - 

TY  - CONF
TI  - Whole-Body Walking Generation using Contact Parametrization: A Non-Linear Trajectory Optimization Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1511
EP  - 1517
AU  - S. Dafarra
AU  - G. Romualdi
AU  - G. Metta
AU  - D. Pucci
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - optimal control
KW  - optimisation
KW  - robot dynamics
KW  - trajectory control
KW  - humanoid robot model
KW  - walking surface
KW  - contact parametrization
KW  - complementarity-free
KW  - predefined contact sequence
KW  - optimal control
KW  - walking trajectories
KW  - dynamic equations
KW  - optimization problem
KW  - direct multiple shooting approach
KW  - body walking generation
KW  - nonlinear trajectory optimization
KW  - centroidal dynamics
KW  - humanoid robot kinematics
KW  - humanoid robot dynamics
KW  - Legged locomotion
KW  - Foot
KW  - Force
KW  - Trajectory
KW  - Robot kinematics
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196801
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we describe a planner capable of generating walking trajectories by using the centroidal dynamics and the full kinematics of a humanoid robot model. The interaction between the robot and the walking surface is modeled explicitly through a novel contact parametrization. The approach is complementarity-free and does not need a predefined contact sequence. By solving an optimal control problem we obtain walking trajectories. In particular, through a set of constraints and dynamic equations, we model the robot in contact with the ground. We describe the objective the robot needs to achieve with a set of tasks. The whole optimal control problem is transcribed into an optimization problem via a Direct Multiple Shooting approach and solved with an off-the-shelf solver. We show that it is possible to achieve walking motions automatically by specifying a minimal set of references, such as a constant desired Center of Mass velocity and a reference point on the ground.
ER  - 

TY  - CONF
TI  - Controlling Fast Height Variation of an Actively Articulated Wheeled Humanoid Robot Using Center of Mass Trajectory
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1518
EP  - 1524
AU  - M. V. Otubela
AU  - C. McGinn
PY  - 2020
KW  - adaptive control
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - nonlinear dynamical systems
KW  - optimal control
KW  - path planning
KW  - quadratic programming
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - splines (mathematics)
KW  - stability criteria
KW  - trajectory control
KW  - task-space inverse dynamics controller
KW  - optimal 7th order spline coefficients
KW  - dynamic stability
KW  - TSID controller
KW  - simplified passive dynamics model
KW  - Aerobot platform
KW  - fast height adaptation
KW  - actively articulated wheeled humanoid robot
KW  - center of mass trajectory
KW  - hybrid wheel-legged robots
KW  - complex terrain
KW  - purely wheeled morphologies
KW  - highly adaptive behaviours
KW  - nonlinear dynamics control problem
KW  - hybrid humanoid platform
KW  - offline trajectory optimisation
KW  - optimal center of mass kinematic trajectories
KW  - fast height variation control
KW  - nonlinear zero moment point
KW  - optimal control
KW  - sequential quadratic programming
KW  - robot kinematics
KW  - stability criterion
KW  - motion plan
KW  - task Jacobians
KW  - robust control
KW  - Aerodynamics
KW  - Mathematical model
KW  - Acceleration
KW  - Robot kinematics
KW  - Stability analysis
DO  - 10.1109/ICRA40945.2020.9196569
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid wheel-legged robots have begun to demonstrate the ability to adapt to complex terrain traditionally inaccessible to purely wheeled morphologies. Further research is needed into how their dynamics can be optimally controlled for developing highly adaptive behaviours on challenging terrain. Using optimal center of mass (COM) kinematic trajectories, this work examines the nonlinear dynamics control problem for fast height adaptation on the hybrid humanoid platform known as Aerobot. We explore the dynamics control problem through experimentation with an offline trajectory optimisation (TO) method and a task-space inverse dynamics (TSID) controller for varying the robot's height. Our TO approach uses sequential quadratic programming (SQP) to solve optimal 7th order spline coefficients for the robot's kinematics. The nonlinear Zero Moment Point (ZMP) is used to model a stability criterion that is constrained in the TO problem to ensure dynamic stability. Our TSID controller follows motion plans based on using task jacobians and a simplified passive dynamics model of the Aerobot platform. Results exhibit fast height adaptation on the Aerobot platform with significantly differing results between the control methods that prompts new research into how it may be controlled online.
ER  - 

TY  - CONF
TI  - Contact-Aware Controller Design for Complementarity Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1525
EP  - 1531
AU  - A. Aydinoglu
AU  - V. M. Preciado
AU  - M. Posa
PY  - 2020
KW  - control system synthesis
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - optimisation
KW  - robot dynamics
KW  - robust control
KW  - tactile sensors
KW  - multicontact motion
KW  - combinatoric structure
KW  - real-time control
KW  - tactile sensors
KW  - robust control
KW  - complementarity structure
KW  - contact dynamics
KW  - control framework
KW  - multicontact robotics problems
KW  - contact-aware controller design
KW  - robotic tasks
KW  - locomotion
KW  - Lyapunov methods
KW  - Control systems
KW  - Force
KW  - Dynamics
KW  - Task analysis
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9197568
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While many robotic tasks, like manipulation and locomotion, are fundamentally based in making and breaking contact with the environment, state-of-the-art control policies struggle to deal with the hybrid nature of multi-contact motion. Such controllers often rely heavily upon heuristics or, due to the combinatoric structure in the dynamics, are unsuitable for real-time control. Principled deployment of tactile sensors offers a promising mechanism for stable and robust control, but modern approaches often use this data in an ad hoc manner, for instance to guide guarded moves. In this work, by exploiting the complementarity structure of contact dynamics, we propose a control framework which can close the loop on rich, tactile sensors. Critically, this framework is non-combinatoric, enabling optimization algorithms to automatically synthesize provably stable control policies. We demonstrate this approach on three different underactuated, multi-contact robotics problems.
ER  - 

TY  - CONF
TI  - Learning to Generate 6-DoF Grasp Poses with Reachability Awareness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1532
EP  - 1538
AU  - X. Lou
AU  - Y. Yang
AU  - C. Choi
PY  - 2020
KW  - convolutional neural nets
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - stability
KW  - 3D CNN
KW  - 6-DoF grasp poses
KW  - reachability awareness
KW  - voxel-based deep 3D convolutional neural network
KW  - reachability predictor
KW  - robot
KW  - grasp pose stability
KW  - Grasping
KW  - Three-dimensional displays
KW  - Robot kinematics
KW  - Planning
KW  - Measurement
KW  - Data models
KW  - Grasping
KW  - Deep Learning in Robotics and Automation
KW  - Perception for Grasping and Manipulation
DO  - 10.1109/ICRA40945.2020.9197413
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motivated by the stringent requirements of unstructured real-world where a plethora of unknown objects reside in arbitrary locations of the surface, we propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that generates feasible 6-DoF grasp poses in unrestricted workspace with reachability awareness. Unlike the majority of works that predict if a proposed grasp pose within the restricted workspace will be successful solely based on grasp pose stability, our approach further learns a reachability predictor that evaluates if the grasp pose is reachable or not from robot's own experience. To avoid the laborious real training data collection, we exploit the power of simulation to train our networks on a large-scale synthetic dataset. This work is an early attempt that simultaneously learns grasping reachability while proposing feasible grasp poses with 3D CNN. Experimental results in both simulation and real-world demonstrate that our approach outperforms several other methods and achieves 82.5% grasping success rate on unknown objects.
ER  - 

TY  - CONF
TI  - Enhancing Grasp Pose Computation in Gripper Workspace Spheres
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1539
EP  - 1545
AU  - M. Sorour
AU  - K. Elgeneidy
AU  - M. Hanheide
AU  - M. Abdalmjed
AU  - A. Srinivasan
AU  - G. Neumann
PY  - 2020
KW  - computational geometry
KW  - dexterous manipulators
KW  - grippers
KW  - path planning
KW  - pose estimation
KW  - position control
KW  - robot vision
KW  - grasp pose computation
KW  - gripper workspace spheres
KW  - registered point cloud
KW  - gripper position sampling
KW  - orientation sampling
KW  - object orientation estimation
KW  - jaw gripper
KW  - Franka Panda gripper
KW  - geometric based methods
KW  - multifingered hands
KW  - Intel RealSense-D435 depth camera
KW  - Grippers
KW  - Three-dimensional displays
KW  - Ellipsoids
KW  - Grasping
KW  - Measurement
KW  - Shape
KW  - Planning
KW  - grasping
KW  - manipulation
DO  - 10.1109/ICRA40945.2020.9196863
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, enhancement to the novel grasp planning algorithm based on gripper workspace spheres is presented. Our development requires a registered point cloud of the target from different views, assuming no prior knowledge of the object, nor any of its properties. This work features a new set of metrics for grasp pose candidates evaluation, as well as exploring the impact of high object sampling on grasp success rates. In addition to gripper position sampling, we now perform orientation sampling about the x, y, and z-axes, hence the grasping algorithm no longer require object orientation estimation. Successful experiments have been conducted on a simple jaw gripper (Franka Panda gripper) as well as a complex, high Degree of Freedom (DoF) hand (Allegro hand) as a proof of its versatility. Higher grasp success rates of 76% and 85.5% respectively has been reported by real world experiments.
ER  - 

TY  - CONF
TI  - Minimal Work: A Grasp Quality Metric for Deformable Hollow Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1546
EP  - 1552
AU  - J. Xu
AU  - M. Danielczuk
AU  - J. Ichnowski
AU  - J. Mahler
AU  - E. Steinbach
AU  - K. Goldberg
PY  - 2020
KW  - deformation
KW  - elasticity
KW  - grippers
KW  - linear programming
KW  - Robotiq gripper
KW  - UR5 robot
KW  - object empirical stiffness
KW  - linear program
KW  - manipulation task
KW  - wrench resistance
KW  - physical grasps
KW  - work quality metric
KW  - wrench-based quality metrics
KW  - real-world grasps
KW  - gripper jaw displacements
KW  - grasp force
KW  - external wrench
KW  - object deformation
KW  - robot grasping
KW  - deformable hollow objects
KW  - grasp quality metric
KW  - Measurement
KW  - Force
KW  - Task analysis
KW  - Strain
KW  - Computational modeling
KW  - Friction
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9197062
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot grasping of deformable hollow objects such as plastic bottles and cups is challenging, as the grasp should resist disturbances while minimally deforming the object so as not to damage it or dislodge liquids. We propose minimal work as a novel grasp quality metric that combines wrench resistance and object deformation. We introduce an efficient algorithm to compute the work required to resist an external wrench for a manipulation task by solving a linear program. The algorithm first computes the minimum required grasp force and an estimation of the gripper jaw displacements based on the object's empirical stiffness at different locations. The work done by the jaws is the product of the grasp force and the displacements. Grasps requiring minimal work are considered to be of high quality. We collect 460 physical grasps with a UR5 robot and a Robotiq gripper. We consider a grasp to be successful if it completes the task without damaging the object or dislodging the content. Physical experiments suggest that the minimal work quality metric reaches 74.2% balanced accuracy, a metric that is the raw accuracy normalized by the number of successful and failed real-world grasps, and is up to 24.2% higher than classical wrench-based quality metrics.
ER  - 

TY  - CONF
TI  - Hierarchical 6-DoF Grasping with Approaching Direction Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1553
EP  - 1559
AU  - Y. Choi
AU  - H. Kee
AU  - K. Lee
AU  - J. Choy
AU  - J. Min
AU  - S. Lee
AU  - S. Oh
PY  - 2020
KW  - convolutional neural nets
KW  - entropy
KW  - geometry
KW  - grippers
KW  - hierarchical systems
KW  - iterative methods
KW  - learning systems
KW  - neurocontrollers
KW  - optimisation
KW  - position control
KW  - cluttered objects
KW  - cross entropy method
KW  - iterative direction optimization
KW  - derivative-free optimization
KW  - geometry-based prior
KW  - point clouds
KW  - input grasp representations
KW  - robot arm
KW  - detection problem
KW  - hierarchical approach
KW  - robot grasping
KW  - hierarchical 6-DoF grasping
KW  - surface normal directions
KW  - approaching direction selection method
KW  - grasp quality
KW  - fully convolutional grasp quality network
KW  - Grasping
KW  - Three-dimensional displays
KW  - Grippers
KW  - Service robots
KW  - Manipulators
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196678
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we tackle the problem of 6-DoF grasp detection which is crucial for robot grasping in cluttered real-world scenes. Unlike existing approaches which synthesize 6-DoF grasp data sets and train grasp quality networks with input grasp representations based on point clouds, we rather take a novel hierarchical approach which does not use any 6-DoF grasp data. We cast the 6-DoF grasp detection problem as a robot arm approaching direction selection problem using the existing 4-DoF grasp detection algorithm, by exploiting a fully convolutional grasp quality network for evaluating the quality of an approaching direction. To select the best approaching direction with the highest grasp quality, we propose an approaching direction selection method which leverages a geometry-based prior and a derivative-free optimization method. Specifically, we optimize the direction iteratively using the cross entropy method with initial samples of surface normal directions. Our algorithm efficiently finds diverse 6-DoF grasps by the novel way of evaluating and optimizing approaching directions. We validate that the proposed method outperforms other selection methods in scenarios with cluttered objects in a physics-based simulator. Finally, we show that our method outperforms the state-of-the-art grasp detection method in real-world experiments with robots.
ER  - 

TY  - CONF
TI  - Geometric Characterization of Two-Finger Basket Grasps of 2-D Objects: Contact Space Formulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1560
EP  - 1566
AU  - E. D. Rimon
AU  - F. T. Pokorny
AU  - W. Wan
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - contact space formulation
KW  - two-finger basket grasps
KW  - high-dimensional configuration space
KW  - low-dimensional contact space
KW  - two-finger contacts
KW  - object boundary
KW  - critical finger opening
KW  - two-finger robot hand
KW  - geometric techniques
KW  - depth and drop-off finger opening
KW  - Robots
KW  - Gravity
KW  - Search problems
KW  - Security
KW  - Layout
KW  - Safety
KW  - Air pollution
DO  - 10.1109/ICRA40945.2020.9196946
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers basket grasps, where a two-finger robot hand forms a basket that can safely lift and carry rigid objects in a 2-D gravitational environment. The two-finger basket grasps form special points in a high-dimensional configuration space of the object and two-finger robot hand. This paper establishes that all two-finger basket grasps can be found in a low-dimensional contact space that parametrizes the two-finger contacts along the supported object boundary. Using contact space, each basket grasp is associated with its depth that provides a security measure while carrying the object, as well as its safety margin away from a critical finger opening where the object drops-off into its intended destination. Geometric techniques that compute the depth and drop-off finger opening are described and illustrated with detailed graphical and numerical examples.
ER  - 

TY  - CONF
TI  - Robust Sound Source Localization considering Similarity of Back-Propagation Signals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1574
EP  - 1580
AU  - I. An
AU  - B. Jo
AU  - Y. Kwon
AU  - J. -w. Choi
AU  - S. -e. Yoon
PY  - 2020
KW  - acoustic generators
KW  - acoustic radiators
KW  - acoustic wave propagation
KW  - backpropagation
KW  - Monte Carlo methods
KW  - ray tracing
KW  - signal processing
KW  - transient response
KW  - ground-truth sound source position
KW  - robust sound source localization
KW  - backpropagation signals
KW  - direct reflection acoustic ray paths
KW  - backward sound propagation path estimation
KW  - ray tracing
KW  - impulse response
KW  - Monte Carlo localization method
KW  - size 3.0 m
KW  - noise figure 77.0 dB
KW  - noise figure 67.0 dB
KW  - Acoustics
KW  - Direction-of-arrival estimation
KW  - Array signal processing
KW  - Three-dimensional displays
KW  - Microphone arrays
KW  - Position measurement
DO  - 10.1109/ICRA40945.2020.9196743
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel, robust sound source localization algorithm considering back-propagation signals. Sound propagation paths are estimated by generating direct and reflection acoustic rays based on ray tracing in a backward manner. We then compute the back-propagation signals by designing and using the impulse response of the backward sound propagation based on the acoustic ray paths. For identifying the 3D source position, we use a well-established Monte Carlo localization method. Candidates for a source position are determined by identifying convergence regions of acoustic ray paths. Those candidates are validated by measuring similarities between back-propagation signals, under the assumption that the back-propagation signals of different acoustic ray paths should be similar near the ground-truth sound source position. Thanks to considering similarities of back-propagation signals, our approach can localize a source position with an averaged error of 0.55 m in a room of 7 m by 7 m area with 3 m height in tested environments. We also place additional 67 dB and 77 dB white noise at the background, to test the robustness of our approach. Overall, we observe a 7 % to 100 % improvement in accuracy over the state-of-the-art method.
ER  - 

TY  - CONF
TI  - BatVision: Learning to See 3D Spatial Layout with Two Ears
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1581
EP  - 1587
AU  - J. H. Christensen
AU  - S. Hornauer
AU  - S. X. Yu
PY  - 2020
KW  - acoustic signal processing
KW  - audio signal processing
KW  - bioacoustics
KW  - cameras
KW  - ear
KW  - image colour analysis
KW  - image sensors
KW  - mechanoception
KW  - mobile robots
KW  - object detection
KW  - path planning
KW  - robot vision
KW  - stereo image processing
KW  - visual perception
KW  - machine vision
KW  - 3D spatial layout
KW  - ears
KW  - nonvisual perception
KW  - artificial systems
KW  - ultrasound complement camera-based vision
KW  - information gain
KW  - harness sound
KW  - machine perception
KW  - low- cost BatVision system
KW  - short chirps
KW  - artificial human pinnae pair
KW  - stereo camera
KW  - color images
KW  - scene depths
KW  - trained BatVision
KW  - 2D visual scenes
KW  - vision system
KW  - robot navigation
KW  - Microphones
KW  - Visualization
KW  - Ear
KW  - Chirp
KW  - Cameras
KW  - Three-dimensional displays
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196934
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception.Inspired by bats' echolocation mechanism, we design a low- cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.
ER  - 

TY  - CONF
TI  - Self-Supervised Learning for Alignment of Objects and Sound
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1588
EP  - 1594
AU  - X. Liu
AU  - X. Liu
AU  - D. Guo
AU  - H. Liu
AU  - F. Sun
AU  - H. Min
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - object detection
KW  - source separation
KW  - human-robot interaction
KW  - scene understanding
KW  - sound source separation task
KW  - self-supervised learning framework
KW  - object detection
KW  - sound separation modules
KW  - sound components
KW  - visual information
KW  - audio information
KW  - Visualization
KW  - Videos
KW  - Feature extraction
KW  - Object detection
KW  - Spectrogram
KW  - Training
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197566
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The sound source separation problem has many useful applications in the field of robotics, such as human-robot interaction, scene understanding, etc. However, it remains a very challenging problem. In this paper, we utilize both visual and audio information of videos to perform the sound source separation task. A self-supervised learning framework is proposed to implement the object detection and sound separation modules simultaneously. Such an approach is designed to better find the alignment between the detected objects and separated sound components. Our experiments, conducted on both the synthetic and real datasets, validate this approach and demonstrate the effectiveness of the proposed model in the task of object and sound alignment.
ER  - 

TY  - CONF
TI  - The OmniScape Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1603
EP  - 1608
AU  - A. R. Sekkat
AU  - Y. Dupuis
AU  - P. Vasseur
AU  - P. Honeine
PY  - 2020
KW  - cameras
KW  - image segmentation
KW  - motorcycles
KW  - object detection
KW  - stereo image processing
KW  - traffic engineering computing
KW  - omnidirectional images
KW  - semantic segmentation
KW  - depth map
KW  - ground truth images
KW  - CARLA Simulator
KW  - open-source simulator
KW  - catadioptric images
KW  - OmniScape dataset
KW  - autonomous driving research
KW  - Grand Theft Auto V
KW  - two-wheeled vehicles
KW  - motorcycle
KW  - Cameras
KW  - Semantics
KW  - Vehicle dynamics
KW  - Motorcycles
KW  - Image segmentation
KW  - Virtual environments
KW  - Roads
DO  - 10.1109/ICRA40945.2020.9197144
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.
ER  - 

TY  - CONF
TI  - An ERT-based Robotic Skin with Sparsely Distributed Electrodes: Structure, Fabrication, and DNN-based Signal Processing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1617
EP  - 1624
AU  - K. Park
AU  - H. Park
AU  - H. Lee
AU  - S. Park
AU  - J. Kim
PY  - 2020
KW  - biomedical electrodes
KW  - carbon nanotubes
KW  - manipulators
KW  - neural nets
KW  - tactile sensors
KW  - tomography
KW  - cylindrical surface
KW  - sensor output images
KW  - 3D-shaped sensors
KW  - ERT-based robotic skin
KW  - sparsely distributed electrodes
KW  - DNN-based signal processing
KW  - electrical resistance tomography
KW  - large-scale tactile sensor
KW  - conductivity distribution
KW  - physical model
KW  - curved surface
KW  - electrode configuration
KW  - edge region
KW  - sensor performance
KW  - carbon nanotube-dispersed solution
KW  - conductive sensing domain
KW  - Robot sensing systems
KW  - Electrodes
KW  - Conductivity
KW  - Image reconstruction
KW  - Inverse problems
DO  - 10.1109/ICRA40945.2020.9197361
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electrical resistance tomography (ERT) has previously been utilized to develop a large-scale tactile sensor because this approach enables the estimation of the conductivity distribution among the electrodes based on a known physical model. Such a sensor made with a stretchable material can conform to a curved surface. However, this sensor cannot fully cover a cylindrical surface because in such a configuration, the edges of the sensor must meet each other. The electrode configuration becomes irregular in this edge region, which may degrade the sensor performance. In this paper, we introduce an ERT-based robotic skin with evenly and sparsely distributed electrodes. For implementation, we sprayed a carbon nanotube (CNT)-dispersed solution to form a conductive sensing domain on a cylindrical surface. The electrodes were firmly embedded in the surface so that the wires were not exposed to the outside. The sensor output images were estimated using a deep neural network (DNN), which was trained with noisy simulation data. An indentation experiment revealed that the localization error of the sensor was 5.2 ¬± 3.3 mm, which is remarkable performance with only 30 electrodes. A frame rate of up to 120 Hz could be achieved with a sensing domain area of 90 cm2. The proposed approach simplifies the fabrication of 3D-shaped sensors, allowing them to be easily applied to existing robot arms in a seamless and robust manner.
ER  - 

TY  - CONF
TI  - FBG-Based Triaxial Force Sensor Integrated with an Eccentrically Configured Imaging Probe for Endoluminal Optical Biopsy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1625
EP  - 1631
AU  - Z. Wu
AU  - A. Gao
AU  - N. Liu
AU  - Z. Jin
AU  - G. -Z. Yang
PY  - 2020
KW  - biological tissues
KW  - biomedical optical imaging
KW  - Bragg gratings
KW  - fibre optic sensors
KW  - force measurement
KW  - force sensors
KW  - medical image processing
KW  - support vector machines
KW  - temperature sensors
KW  - FBG-based triaxial force sensor integrated
KW  - eccentrically configured imaging probe
KW  - endoluminal optical biopsy
KW  - endoluminal intervention
KW  - lesion
KW  - robotic bronchoscopy
KW  - FBG sensors
KW  - conical substrate
KW  - eccentric inner lumen
KW  - flexible imaging probe
KW  - laser-profiled continuum robot
KW  - temperature sensors
KW  - sensor substrate
KW  - developed triaxial force sensor
KW  - Robot sensing systems
KW  - Temperature sensors
KW  - Force
KW  - Force sensors
KW  - Optical fibers
DO  - 10.1109/ICRA40945.2020.9197128
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate force sensing is important for endoluminal intervention in terms of both safety and lesion targeting. This paper develops an FBG-based force sensor for robotic bronchoscopy by configuring three FBG sensors at the lateral side of a conical substrate. It allows a large and eccentric inner lumen for the interventional instrument, enabling a flexible imaging probe inside to perform optical biopsy. The force sensor is embodied with a laser-profiled continuum robot and thermo drift is fully compensated by three temperature sensors integrated on the circumference surface of the sensor substrate. Different decoupling approaches are investigated, and nonlinear decoupling is adopted based on the cross-validation SVM and a Gaussian kernel function, achieving an accuracy of 10.58 mN, 14.57 mN and 26.32 mN along X, Y and Z axis, respectively. The tissue test is also investigated to further demonstrate the feasibility of the developed triaxial force sensor.
ER  - 

TY  - CONF
TI  - Calibrating a Soft ERT-Based Tactile Sensor with a Multiphysics Model and Sim-to-real Transfer Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1632
EP  - 1638
AU  - H. Lee
AU  - H. Park
AU  - G. Serhat
AU  - H. Sun
AU  - K. J. Kuchenbecker
PY  - 2020
KW  - calibration
KW  - inverse problems
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - robots
KW  - tactile sensors
KW  - tomography
KW  - soft ERT-based tactile sensor
KW  - sim-to-real transfer learning
KW  - electrical resistance tomography
KW  - finite element multiphysics model
KW  - contact pressure distributions
KW  - voltage measurements
KW  - model parameters
KW  - single-point dataset
KW  - contact force
KW  - calibration method
KW  - ERT-based tactile sensors
KW  - Fabrics
KW  - Computational modeling
KW  - Tactile sensors
KW  - Mathematical model
KW  - Electrodes
KW  - Force
KW  - Conductivity
DO  - 10.1109/ICRA40945.2020.9196732
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tactile sensors based on electrical resistance tomography (ERT) have shown many advantages for implementing a soft and scalable whole-body robotic skin; however, calibration is challenging because pressure reconstruction is an ill-posed inverse problem. This paper introduces a method for calibrating soft ERT-based tactile sensors using sim-to-real transfer learning with a finite element multiphysics model. The model is composed of three simple models that together map contact pressure distributions to voltage measurements. We optimized the model parameters to reduce the gap between the simulation and reality. As a preliminary study, we discretized the sensing points into a 6 by 6 grid and synthesized single- and two-point contact datasets from the multiphysics model. We obtained another single-point dataset using the real sensor with the same contact location and force used in the simulation. Our new deep neural network architecture uses a de-noising network to capture the simulation-to-real gap and a reconstruction network to estimate contact force from voltage measurements. The proposed approach showed 82% hit rate for localization and 0.51 N of force estimation error performance in singlecontact tests and 78.5% hit rate for localization and 5.0 N of force estimation error in two-point contact tests. We believe this new calibration method has the possibility to improve the sensing performance of ERT-based tactile sensors.
ER  - 

TY  - CONF
TI  - Sim-to-Real Transfer for Optical Tactile Sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1639
EP  - 1645
AU  - Z. Ding
AU  - N. F. Lepora
AU  - E. Johns
PY  - 2020
KW  - cameras
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - tactile sensors
KW  - sim-to-real transfer methods
KW  - TacTip optical tactile sensor
KW  - deformable tip
KW  - soft body simulation
KW  - Unity physics engine
KW  - domain randomisation techniques
KW  - real-world data
KW  - optical tactile sensing
KW  - deep learning
KW  - reinforcement learning methods
KW  - flexible robot controllers
KW  - complex robot controllers
KW  - training data
KW  - data collection
KW  - size 1.0 mm
KW  - Robot sensing systems
KW  - Pins
KW  - Force
KW  - Strain
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197512
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning and reinforcement learning methods have been shown to enable learning of flexible and complex robot controllers. However, the reliance on large amounts of training data often requires data collection to be carried out in simulation, with a number of sim-to-real transfer methods being developed in recent years. In this paper, we study these techniques for tactile sensing using the TacTip optical tactile sensor, which consists of a deformable tip with a camera observing the positions of pins inside this tip. We designed a model for soft body simulation which was implemented using the Unity physics engine, and trained a neural network to predict the locations and angles of edges when in contact with the sensor. Using domain randomisation techniques for sim-to-real transfer, we show how this framework can be used to accurately predict edges with less than 1 mm prediction error in real-world testing, without any real-world data at all.
ER  - 

TY  - CONF
TI  - Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1646
EP  - 1652
AU  - Y. Zhu
AU  - K. Lu
AU  - K. Hauser
PY  - 2020
KW  - elastic deformation
KW  - image representation
KW  - robot vision
KW  - elastically deformable objects
KW  - data-driven models
KW  - point-based surface representation
KW  - inhomogeneous force response model
KW  - nonlinear force response model
KW  - robotic arm
KW  - arbitrary rigid object
KW  - Hertzian contact model
KW  - heterogeneous elastic objects
KW  - semiempirical method
KW  - point stiffness models
KW  - Force
KW  - Probes
KW  - Deformable models
KW  - Data models
KW  - Robot sensing systems
KW  - Strain
DO  - 10.1109/ICRA40945.2020.9197077
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a semi-empirical method for simulating contact with elastically deformable objects whose force response is learned using entirely data-driven models. A point-based surface representation and an inhomogeneous, nonlinear force response model are learned from a robotic arm acquiring force-displacement curves from a small number of poking interactions. The simulator then estimates displacement and force response when the deformable object is in contact with an arbitrary rigid object. It does so by estimating displacements by solving a Hertzian contact model, and sums the expected forces at individual surface points through querying the learned point stiffness models as a function of their expected displacements. Experiments on a variety of challenging objects show that our approach learns force response with sufficient accuracy to generate plausible contact response for novel rigid objects.
ER  - 

TY  - CONF
TI  - Low-Cost Fiducial-based 6-Axis Force-Torque Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1653
EP  - 1659
AU  - R. Ouyang
AU  - R. Howe
PY  - 2020
KW  - force sensors
KW  - strain gauges
KW  - 6-axis force-torque sensor
KW  - six-axis force-torque sensors
KW  - hard-touse
KW  - fiducial-based design
KW  - inexpensive webcam
KW  - consumer-grade 3D printer
KW  - open-source software
KW  - applied force-torque
KW  - browser-based interface
KW  - open source design files
KW  - human-computer interfaces
KW  - six-axis force-torque sensing
KW  - traditional strain-gauge based sensors
KW  - open-source sensor design
KW  - Robot sensing systems
KW  - Force
KW  - Three-dimensional displays
KW  - Webcams
KW  - Sensitivity
KW  - Prototypes
DO  - 10.1109/ICRA40945.2020.9196925
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Commercial six-axis force-torque sensors suffer from being some combination of expensive, fragile, and hard-touse. We propose a new fiducial-based design which addresses all three points. The sensor uses an inexpensive webcam and can be fabricated using a consumer-grade 3D printer. Open-source software is used to estimate the 3D pose of the fiducials on the sensor, which is then used to calculate the applied force-torque. A browser-based (installation free) interface demonstrates ease-of-use. The sensor is very light and can be dropped or thrown with little concern. We characterize our prototype in dynamic conditions under compound loading, finding a mean R2 of over 0.99 for the Fx, Fy, Mx, and My axes, and over 0.87 and 0.90 for the Fz and Mz axes respectively. The open source design files allow the sensor to be adapted for diverse applications ranging from robot fingers to human-computer interfaces, while the sdesign principle allows for quick changes with minimal technical expertise. This approach promises to bring six-axis force-torque sensing to new applications where the precision, cost, and fragility of traditional strain-gauge based sensors are not appropriate. The open-source sensor design can be viewed at http://sites.google.com/view/fiducialforcesensor.
ER  - 

TY  - CONF
TI  - Reliable frame-to-frame motion estimation for vehicle-mounted surround-view camera systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1660
EP  - 1666
AU  - Y. Wang
AU  - K. Huang
AU  - X. Peng
AU  - H. Li
AU  - L. Kneip
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - mobile robots
KW  - motion estimation
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - frame-to-frame visual odometry
KW  - vehicle-mounted surround-view camera system
KW  - reliable frame-to-frame motion estimation
KW  - vehicle-mounted surround-view camera systems
KW  - surround-view multicamera system
KW  - autonomous driving
KW  - 3D point related optimization variables
KW  - two-view optimization scheme
KW  - nonholonomic characteristics
KW  - relative displacements
KW  - nonholonomic vehicle motion
KW  - overly simplified assumptions
KW  - single camera
KW  - existing camera
KW  - relative vehicle displacement
KW  - Conferences
KW  - Automation
KW  - Reliability
KW  - Motion estimation
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197176
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern vehicles are often equipped with a surround-view multi-camera system. The current interest in autonomous driving invites the investigation of how to use such systems for a reliable estimation of relative vehicle displacement. Existing camera pose algorithms either work for a single camera, make overly simplified assumptions, are computationally expensive, or simply become degenerate under non-holonomic vehicle motion. In this paper, we introduce a new, reliable solution able to handle all kinds of relative displacements in the plane despite the possibly non-holonomic characteristics. We furthermore introduce a novel two-view optimization scheme which minimizes a geometrically relevant error without relying on 3D point related optimization variables. Our method leads to highly reliable and accurate frame-to-frame visual odometry with a full-size, vehicle-mounted surround-view camera system.
ER  - 

TY  - CONF
TI  - Enabling Topological Planning with Monocular Vision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1667
EP  - 1673
AU  - G. J. Stein
AU  - C. Bradley
AU  - V. Preston
AU  - N. Roy
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - path planning
KW  - robot vision
KW  - sensors
KW  - SLAM (robots)
KW  - heuristic priors
KW  - intelligent planning
KW  - monocular SLAM
KW  - low texture
KW  - highly cluttered environments
KW  - robust sparse map representation
KW  - monocular vision
KW  - learned sensor
KW  - high-level structure
KW  - sparse vertices
KW  - known free space
KW  - mapping technique
KW  - subgoal planning applications
KW  - enabling topological planning
KW  - topological strategies
KW  - navigation
KW  - possible actions
KW  - Planning
KW  - Image edge detection
KW  - Navigation
KW  - Robot sensing systems
KW  - Buildings
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197484
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Topological strategies for navigation meaningfully reduce the space of possible actions available to a robot, allowing use of heuristic priors or learning to enable computationally efficient, intelligent planning. The challenges in estimating structure with monocular SLAM in low texture or highly cluttered environments have precluded its use for topological planning in the past. We propose a robust sparse map representation that can be built with monocular vision and overcomes these shortcomings. Using a learned sensor, we estimate high-level structure of an environment from streaming images by detecting sparse "vertices" (e.g., boundaries of walls) and reasoning about the structure between them. We also estimate the known free space in our map, a necessary feature for planning through previously unknown environments. We show that our mapping technique can be used on real data and is sufficient for planning and exploration in simulated multi-agent search and learned subgoal planning applications.
ER  - 

TY  - CONF
TI  - DeepMEL: Compiling Visual Multi-Experience Localization into a Deep Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1674
EP  - 1681
AU  - M. Gridseth
AU  - T. D. Barfoot
PY  - 2020
KW  - distance measurement
KW  - image colour analysis
KW  - mobile robots
KW  - neurocontrollers
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - robust control
KW  - stereo image processing
KW  - outdoor driving
KW  - deep neural network
KW  - visual odometry
KW  - vision-based path following
KW  - unstructured outdoor environments
KW  - visual multiexperience localization
KW  - colour-constant imaging
KW  - multiexperience VT&R
KW  - DeepMEL
KW  - stereo visual teach and repeat
KW  - robust long-range path following
KW  - environmental conditions
KW  - pose estimates
KW  - in-the-loop path following
KW  - Pose estimation
KW  - Image edge detection
KW  - Neural networks
KW  - Robots
KW  - Lighting
KW  - Snow
DO  - 10.1109/ICRA40945.2020.9197362
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Vision-based path following allows robots to autonomously repeat manually taught paths. Stereo Visual Teach and Repeat (VT&R) [1] accomplishes accurate and robust long-range path following in unstructured outdoor environments across changing lighting, weather, and seasons by relying on colour-constant imaging [2] and multi-experience localization [3]. We leverage multi-experience VT&R together with two datasets of outdoor driving on two separate paths spanning different times of day, weather, and seasons to teach a deep neural network to predict relative pose for visual odometry (VO) and for localization with respect to a path. In this paper we run experiments exclusively on datasets to study how the network generalizes across environmental conditions. Based on the results we believe that our system achieves relative pose estimates sufficiently accurate for in-the-loop path following and that it is able to localize radically different conditions against each other directly (i.e. winter to spring and day to night), a capability that our hand-engineered system does not have.
ER  - 

TY  - CONF
TI  - SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1682
EP  - 1688
AU  - L. Xie
AU  - A. Markham
AU  - N. Trigoni
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neurocontrollers
KW  - robot vision
KW  - robust control
KW  - SnapNav
KW  - mapless visual navigation
KW  - sparse directional guidance
KW  - visual reference
KW  - robotics
KW  - deep neural network
KW  - visual navigation system
KW  - two-level hierarchy
KW  - directional commands
KW  - real-time control
KW  - obstacle avoidance
KW  - autonomous navigation
KW  - learning-based visual navigation
KW  - robust control
KW  - Robots
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Training
KW  - Collision avoidance
KW  - Turning
DO  - 10.1109/ICRA40945.2020.9197523
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.
ER  - 

TY  - CONF
TI  - Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1689
EP  - 1696
AU  - A. Rosinol
AU  - M. Abate
AU  - Y. Chang
AU  - L. Carlone
PY  - 2020
KW  - C++ language
KW  - control engineering computing
KW  - graph theory
KW  - image reconstruction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - public domain software
KW  - robot vision
KW  - SLAM (robots)
KW  - open-source C++ library
KW  - visual-inertial SLAM libraries
KW  - ORB-SLAM
KW  - VINS-Mono
KW  - semantic labeling
KW  - visual-inertial odometry module
KW  - state estimation
KW  - robust pose graph optimizer
KW  - global trajectory estimation
KW  - lightweight 3D mesher module
KW  - fast mesh reconstruction
KW  - 3D metric-semantic reconstruction module
KW  - semantically labeled images
KW  - metric-semantic SLAM
KW  - real-time metric-semantic localization and mapping
KW  - Kimera
KW  - deep learning
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Semantics
KW  - Libraries
KW  - Visualization
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196885
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.
ER  - 

TY  - CONF
TI  - CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1697
EP  - 1704
AU  - M. Chanc√°n
AU  - M. Milford
PY  - 2020
KW  - decision making
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neural nets
KW  - robot vision
KW  - high-dimensional data
KW  - decision-making problems
KW  - deep reinforcement learning
KW  - place recognition feedback
KW  - visual navigation tasks
KW  - sample-efficient navigation policy learning
KW  - CityLearn environments
KW  - visual place recognition
KW  - extreme visual appearance changes
KW  - realistic environments
KW  - navigation algorithms
KW  - bimodal image representations
KW  - compact image representations
KW  - goal destination feedback
KW  - deep learning techniques
KW  - sample complexity
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Robot sensing systems
KW  - Machine learning
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197336
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.
ER  - 

TY  - CONF
TI  - High Resolution Soft Tactile Interface for Physical Human-Robot Interaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1705
EP  - 1711
AU  - I. Huang
AU  - R. Bajcsy
PY  - 2020
KW  - cameras
KW  - control engineering computing
KW  - haptic interfaces
KW  - human-robot interaction
KW  - image processing
KW  - tactile sensors
KW  - touch (physiological)
KW  - high resolution soft tactile interface
KW  - physical human-robot interaction
KW  - tactile interactions
KW  - intuitive communication tool
KW  - fundamental method
KW  - tactile abilities
KW  - mechanical safety
KW  - sensory intelligence
KW  - human-sized geometries
KW  - soft tactile interfaces
KW  - intrinsically safe mechanical properties
KW  - nonlinear characteristics
KW  - robotic system
KW  - completely soft interface
KW  - human upper limbs
KW  - high resolution tactile sensory readings
KW  - human finger
KW  - tactile input
KW  - human forearm
KW  - safe tactile interface
KW  - Robot sensing systems
KW  - Cameras
KW  - Safety
KW  - Task analysis
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197365
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - If robots and humans are to coexist and cooperate in society, it would be useful for robots to be able to engage in tactile interactions. Touch is an intuitive communication tool as well as a fundamental method by which we assist each other physically. Tactile abilities are challenging to engineer in robots, since both mechanical safety and sensory intelligence are imperative. Existing work reveals a trade-off between these principles- tactile interfaces that are high in resolution are not easily adapted to human-sized geometries, nor are they generally compliant enough to guarantee safety. On the other hand, soft tactile interfaces deliver intrinsically safe mechanical properties, but their non-linear characteristics render them difficult for use in timely sensing and control. We propose a robotic system that is equipped with a completely soft and therefore safe tactile interface that is large enough to interact with human upper limbs, while producing high resolution tactile sensory readings via depth camera imaging of the soft interface. We present and validate a data-driven model that maps point cloud data to contact forces, and verify its efficacy by demonstrating two real-world applications. In particular, the robot is able to react to a human finger's pokes and change its pose based on the tactile input. In addition, we also demonstrate that the robot can act as an assistive device that dynamically supports and follows a human forearm from underneath.
ER  - 

TY  - CONF
TI  - Design and Validation of a Soft Robotic Ankle-Foot Orthosis (SR-AFO) Exosuit for Inversion and Eversion Ankle Support
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1735
EP  - 1741
AU  - C. M. Thalman
AU  - H. Lee
PY  - 2020
KW  - actuators
KW  - biomedical equipment
KW  - biomedical measurement
KW  - finite element analysis
KW  - gait analysis
KW  - mechanoception
KW  - medical robotics
KW  - muscle
KW  - orthotics
KW  - patient rehabilitation
KW  - frontal plane
KW  - sagittal plane
KW  - SR-AFO exosuit
KW  - wearable ankle robot
KW  - ankle stiffness
KW  - soft robotic ankle-foot orthosis exosuit
KW  - eversion ankle support
KW  - pressure 50.0 kPa
KW  - pressure 30.0 kPa
KW  - Actuators
KW  - Fabrics
KW  - Solids
KW  - Structural beams
KW  - Load modeling
KW  - Soft robotics
KW  - Soft Robotics
KW  - Wearable Robots
KW  - Assistive Robots
KW  - Rehabilitation
DO  - 10.1109/ICRA40945.2020.9197531
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a soft robotic ankle-foot orthosis (SR-AFO) exosuit designed to provide support to the human ankle in the frontal plane without restricting natural motion in the sagittal plane. The SR-AFO exosuit incorporates inflatable fabric-based actuators with a hollow cylinder design which requires less volume than the commonly used solid cylinder design for the same deflection. The actuators were modeled and characterized using finite element analysis techniques and experimentally validated. The SR-AFO exosuit was evaluated on healthy participants in both a sitting position using a wearable ankle robot and a standing position using a dual-axis robotic platform to characterize the effect of the exosuit on the change of 2D ankle stiffness in the sagittal and frontal planes. For both sitting and standing test protocols, a trend of increasing ankle stiffness in the frontal plane was observed up to 50 kPa while stiffness in the sagittal plane remained relatively constant over pressure levels. During quiet standing, the exosuit could effectively change eversion stiffness at the ankle joint from about 20 to 70 Nm/rad at relatively low- pressure levels (<; 30 kPa). Eversion stiffness was 84.9 Nm/rad at 50 kPa, an increase of 387.5% from the original free foot stiffness.
ER  - 

TY  - CONF
TI  - Velocity Field based Active-Assistive Control for Upper Limb Rehabilitation Exoskeleton Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1742
EP  - 1748
AU  - E. -Y. Chia
AU  - Y. -L. Chen
AU  - T. -C. Chien
AU  - M. -L. Chiang
AU  - L. -C. Fu
AU  - J. -S. Lai
AU  - L. Lu
PY  - 2020
KW  - biomechanics
KW  - Kalman filters
KW  - medical robotics
KW  - motion control
KW  - observers
KW  - path planning
KW  - patient rehabilitation
KW  - torque control
KW  - wearable robots
KW  - upper limb rehabilitation exoskeleton robot
KW  - time-dependent trajectories
KW  - task-based rehabilitation exercise
KW  - multijoint motion
KW  - assistive mechanism
KW  - active-assistive control system
KW  - joint-position-dependent velocity field
KW  - task motion pattern
KW  - time-independent assistance
KW  - active motions
KW  - assistive motions
KW  - rehabilitation task
KW  - single joint tasks
KW  - Kalman filter based interactive torque observer
KW  - subject active motion intention
KW  - subject torque exertion
KW  - Task analysis
KW  - Torque
KW  - Robots
KW  - Kalman filters
KW  - Observers
KW  - Control systems
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9196766
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - There are limitations of conventional active-assistive control for upper limb rehabilitation exoskeleton robot, such as 1). prior time-dependent trajectories are generally required, 2). task-based rehabilitation exercise involving multi-joint motion is hard to implement, and 3). assistive mechanism normally is so inflexible that the resulting exercise performed by the subjects becomes inefficient. In this paper, we propose a novel velocity field based active-assistive control system to address these issues. First, we design a Kalman filter based interactive torque observer to obtain subjects' active intention of motion. Next, a joint-position-dependent velocity field which can be automatically generated via the task motion pattern is proposed to provide the time-independent assistance to the subjects. We further propose a novel integration method that combines the active and assistive motions based on the performance and the involvement of subjects to guide them to perform the task more voluntarily and precisely. The experiment results show that both the execution time and the subjects' torque exertion are reduced while performing both given single joint tasks and task-oriented multi-joint tasks as compared with the related work in the literature. To sum up, the proposed system not only can efficiently retain subjects' active intention but also can assist them to accomplish the rehabilitation task more precisely.
ER  - 

TY  - CONF
TI  - Design, Development, and Control of a Tendon-actuated Exoskeleton for Wrist Rehabilitation and Training
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1749
EP  - 1754
AU  - M. Dragusanu
AU  - T. L. Baldi
AU  - Z. Iqbal
AU  - D. Prattichizzo
AU  - M. Malvezzi
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - diseases
KW  - medical robotics
KW  - neurophysiology
KW  - patient rehabilitation
KW  - patient treatment
KW  - robot-mediated therapies
KW  - robotic technologies
KW  - social contexts
KW  - motion training
KW  - device design
KW  - wrist mobility
KW  - tendon-actuated exoskeleton
KW  - wrist rehabilitation
KW  - robot rehabilitation
KW  - emerging promising topic
KW  - neuroscience
KW  - neurological diseases
KW  - rehabilitation process
KW  - Exoskeletons
KW  - Wrist
KW  - Robots
KW  - Sensors
KW  - Tracking
KW  - Magnetometers
KW  - Medical treatment
DO  - 10.1109/ICRA40945.2020.9197013
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot rehabilitation is an emerging and promising topic that incorporates robotics with neuroscience and rehabilitation to define new methods for supporting patients with neurological diseases. As a consequence, the rehabilitation process could increase the efficacy exploiting the potentialities of robot-mediated therapies. Nevertheless, nowadays clinical effectiveness is not enough to widely introduce robotic technologies in such social contexts. In this paper we propose a step further, presenting an innovative exoskeleton for wrist flexion/extension and adduction/abduction motion training. It is designed to be wearable and easy to control and manage. It can be used by the patient in collaboration with the therapist or autonomously. The paper introduces the main steps of device design and development and presents some tests conducted with an user with limited wrist mobility.
ER  - 

TY  - CONF
TI  - Impedance Control of a Transfemoral Prosthesis using Continuously Varying Ankle Impedances and Multiple Equilibria
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1755
EP  - 1761
AU  - N. A. Kumar
AU  - W. Hong
AU  - P. Hur
PY  - 2020
KW  - artificial limbs
KW  - gait analysis
KW  - least squares approximations
KW  - medical robotics
KW  - prosthetics
KW  - springs (mechanical)
KW  - vibration control
KW  - squares estimation
KW  - impedance controller
KW  - squares optimization method
KW  - knee impedance
KW  - impedance control
KW  - lower limb prostheses
KW  - human joint torque
KW  - perturbation studies
KW  - least squares estimates
KW  - ankle impedance parameters
KW  - powered transfemoral prosthesis
KW  - Impedance
KW  - Damping
KW  - Torque
KW  - Optimization
KW  - Knee
KW  - Prosthetics
KW  - Perturbation methods
DO  - 10.1109/ICRA40945.2020.9197565
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Impedance controllers are popularly used in the field of lower limb prostheses and exoskeleton development. Such controllers assume the joint to be a spring-damper system described by a discrete set of equilibria and impedance parameters. These parameters are estimated via a least squares optimization that minimizes the difference between the controller's output torque and human joint torque. Other researchers have used perturbation studies to determine empirical values for ankle impedance. The resulting values vary greatly from the prior least squares estimates. While perturbation studies are more credible, they require immense investment. This paper extended the least squares approach to reproduce the results of perturbation studies. The resulting ankle impedance parameters were successfully tested on a powered transfemoral prosthesis, AMPRO II. Further, the paper investigated the effect of multiple equilibria on the least squares estimation and the performance of the impedance controller. Finally, the paper uses the proposed least squares optimization method to estimate knee impedance.
ER  - 

TY  - CONF
TI  - Gait patterns generation based on basis functions interpolation for the TWIN lower-limb exoskeleton*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1778
EP  - 1784
AU  - C. Vassallo
AU  - S. De Giuseppe
AU  - C. Piezzo
AU  - S. Maludrottu
AU  - G. Cerruti
AU  - M. L. D‚ÄôAngelo
AU  - E. Gruppioni
AU  - C. Marchese
AU  - S. Castellano
AU  - E. Guanziroli
AU  - F. Molteni
AU  - M. Laffranchi
AU  - L. De Michieli
PY  - 2020
KW  - gait analysis
KW  - injuries
KW  - interpolation
KW  - legged locomotion
KW  - medical robotics
KW  - neurophysiology
KW  - orthotics
KW  - patient rehabilitation
KW  - gait trajectory pattern
KW  - TWIN exoskeleton
KW  - spinal-cord injury patient
KW  - spinal cord injuries
KW  - rehabilitation centers
KW  - basis function interpolation method
KW  - stable trajectory pattern
KW  - feasible trajectory pattern
KW  - exoskeletons
KW  - biomedical orthotic devices
KW  - TWIN lower-limb exoskeleton
KW  - basis function interpolation
KW  - gait pattern generation
KW  - Trajectory
KW  - Legged locomotion
KW  - Exoskeletons
KW  - Torso
KW  - Hip
KW  - Foot
KW  - Knee
DO  - 10.1109/ICRA40945.2020.9197250
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Since the uprising of new biomedical orthotic devices, exoskeletons have been put in the spotlight for their possible use in rehabilitation. Even if these products might share some commonalities among them in terms of overall structure, degrees of freedom and possible actions, they quite often differ in their approach on how to generate a feasible, stable and comfortable gait trajectory pattern. This paper introduces three proposed trajectories that were generated by using a basis function interpolation method and by working closely with two major rehabilitation centers in Italy. The whole procedure has been focused on the concepts of a configurable walk for patients that suffer from spinal cord injuries. We tested the solutions on a group of healthy volunteers and on a spinal-cord injury patient with the use of the new TWIN exoskeleton developed at the Rehab Technologies Lab at the Italian Institute of Technology.
ER  - 

TY  - CONF
TI  - Human-Centric Active Perception for Autonomous Observation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1785
EP  - 1791
AU  - D. Kent
AU  - S. Chernova
PY  - 2020
KW  - aerospace robotics
KW  - Markov processes
KW  - mobile communication
KW  - mobile robots
KW  - optimisation
KW  - space vehicles
KW  - autonomous observation systems
KW  - human activity
KW  - multiobjective optimization
KW  - autonomous human observation problem
KW  - robot-centric costs
KW  - scalarization-based MultiObjective MDP methods
KW  - NASA Astrobee robot operating
KW  - human-centric active perception
KW  - robot autonomy
KW  - SemiMDP formulation
KW  - constrained MDP method
KW  - NASA Astrobee robot
KW  - Task analysis
KW  - Cameras
KW  - Robot vision systems
KW  - Collision avoidance
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9197201
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As robot autonomy improves, robots are increasingly being considered in the role of autonomous observation systems - free-flying cameras capable of actively tracking human activity within some predefined area of interest. In this work, we formulate the autonomous observation problem through multi-objective optimization, presenting a novel Semi-MDP formulation of the autonomous human observation problem that maximizes observation rewards while accounting for both human- and robot-centric costs. We demonstrate that the problem can be solved with both scalarization-based Multi-Objective MDP methods and Constrained MDP methods, and discuss the relative benefits of each approach. We validate our work on activity tracking using a NASA Astrobee robot operating within a simulated International Space Station environment.
ER  - 

TY  - CONF
TI  - Prediction of Human Full-Body Movements with Motion Optimization and Recurrent Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1792
EP  - 1798
AU  - P. Kratzer
AU  - M. Toussaint
AU  - J. Mainprice
PY  - 2020
KW  - gradient methods
KW  - human-robot interaction
KW  - image motion analysis
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - recurrent neural nets
KW  - robot vision
KW  - robot trajectory planning
KW  - gradient-based trajectory optimization
KW  - human full-body movement prediction
KW  - motion prediction
KW  - trajectory optimization
KW  - environmental constraints
KW  - short-term dynamics
KW  - long-term prediction
KW  - internal body dynamics
KW  - short-term prediction
KW  - recurrent neural network
KW  - motion optimization
KW  - Optimization
KW  - Trajectory
KW  - Predictive models
KW  - Recurrent neural networks
KW  - Collision avoidance
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197290
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human movement prediction is difficult as humans naturally exhibit complex behaviors that can change drastically from one environment to the next. In order to alleviate this issue, we propose a prediction framework that decouples short-term prediction, linked to internal body dynamics, and long-term prediction, linked to the environment and task constraints. In this work we investigate encoding short-term dynamics in a recurrent neural network, while we account for environmental constraints, such as obstacle avoidance, using gradient-based trajectory optimization. Experiments on real motion data demonstrate that our framework improves the prediction with respect to state-of-the-art motion prediction methods, as it accounts to beforehand unseen environmental structures. Moreover we demonstrate on an example, how this framework can be used to plan robot trajectories that are optimized to coordinate with a human partner.
ER  - 

TY  - CONF
TI  - Predicting and Optimizing Ergonomics in Physical Human-Robot Cooperation Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1799
EP  - 1805
AU  - L. v. der Spaa
AU  - M. Gienger
AU  - T. Bates
AU  - J. Kober
PY  - 2020
KW  - biomechanics
KW  - ergonomics
KW  - graph theory
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - telerobotics
KW  - 32 DoF bimanual mobile robot
KW  - ergonomic-enhanced planner
KW  - reduced ergonomic cost
KW  - physical human-robot cooperation tasks
KW  - action sequences
KW  - continuous physical interaction
KW  - computational model
KW  - ergonomics assessment
KW  - human motion capture data
KW  - prediction model
KW  - informed graph search algorithm
KW  - ergonomic assessment
KW  - bimanual human-robot cooperation tasks
KW  - Ergonomics
KW  - Robots
KW  - Task analysis
KW  - Optimization
KW  - Predictive models
KW  - Computational modeling
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197296
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to incorporate ergonomics into the optimization of action sequences for bi-manual human-robot cooperation tasks with continuous physical interaction. Our first contribution is a novel computational model of the human that allows prediction of an ergonomics assessment corresponding to each step in a task. The model is learned from human motion capture data in order to predict the human pose as realistically as possible. The second contribution is a combination of this prediction model with an informed graph search algorithm, which allows computation of human-robot cooperative plans with improved ergonomics according to the incorporated method for ergonomic assessment. The concepts have been evaluated in simulation and in a small user study in which the subjects manipulate a large object with a 32 DoF bimanual mobile robot as partner. For all subjects, the ergonomic-enhanced planner shows their reduced ergonomic cost compared to a baseline planner.
ER  - 

TY  - CONF
TI  - Active Reward Learning for Co-Robotic Vision Based Exploration in Bandwidth Limited Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1806
EP  - 1812
AU  - S. Jamieson
AU  - J. P. How
AU  - Y. Girdhar
PY  - 2020
KW  - decision theory
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - mobile robots
KW  - query processing
KW  - robot vision
KW  - making queries
KW  - regret-based criterion
KW  - active reward learning strategy
KW  - co-robotic vision based exploration
KW  - robotic explorer
KW  - bandwidth-limited environments
KW  - autonomous visual exploration
KW  - high-dimensional observation space
KW  - communication strategy
KW  - reward model
KW  - observation model
KW  - human operator
KW  - scientifically relevant images
KW  - POMDP problem formulation
KW  - Robot sensing systems
KW  - Semantics
KW  - Bandwidth
KW  - Computational modeling
KW  - Visualization
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196922
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel POMDP problem formulation for a robot that must autonomously decide where to go to collect new and scientifically relevant images given a limited ability to communicate with its human operator. From this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data. We introduce a novel active reward learning strategy based on making queries to help the robot minimize path "regret" online, and evaluate it for suitability in autonomous visual exploration through simulations. We demonstrate that, in some bandwidth-limited environments, this novel regret-based criterion enables the robotic explorer to collect up to 17% more reward per mission than the next-best criterion.
ER  - 

TY  - CONF
TI  - VariPath: A Database for Modelling the Variance of Human Pathways in Manual and HRC Processes with Heavy-Duty Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1821
EP  - 1826
AU  - M. Bdiwi
AU  - A. -K. Harsch
AU  - P. Reindel
AU  - M. Putz
PY  - 2020
KW  - human-robot interaction
KW  - industrial robots
KW  - occupational safety
KW  - path planning
KW  - heavy-duty robots
KW  - human pathway variations
KW  - human-robot collaboration
KW  - HRC process
KW  - women walking pathways
KW  - varipath database
KW  - planning process
KW  - safety
KW  - Legged locomotion
KW  - Service robots
KW  - Task analysis
KW  - Planning
KW  - Atmospheric measurements
KW  - Particle measurements
DO  - 10.1109/ICRA40945.2020.9196699
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unlike robots, humans do not have constant movements. Their pathways are individually changeable and influenced by circumstances. This paper presents a method to investigate human pathway variations in a real study. In systematically selected tasks, human pathways are examined for 100 participants in manual and human-robot collaboration (HRC) scenarios. As a result, the variations of pathways are presented depending on various features: e.g. in nearly all cases the variance of women's walking pathways is smaller than that of men. VariPath database can be used in any planning process of manual or HRC scenarios to ensure safety and efficiency.
ER  - 

TY  - CONF
TI  - A Compact and Low-cost Robotic Manipulator Driven by Supercoiled Polymer Actuators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1827
EP  - 18533
AU  - Y. Yang
AU  - Z. Liu
AU  - Y. Wang
AU  - S. Liu
AU  - M. Y. Wang
PY  - 2020
KW  - coils
KW  - design engineering
KW  - dexterous manipulators
KW  - electroactive polymer actuators
KW  - grippers
KW  - mobile robots
KW  - pneumatic actuators
KW  - fragile fruit
KW  - pick and place demonstration
KW  - Fin Ray Effect inspired soft gripper
KW  - ball-and-socket joints
KW  - Joule heating
KW  - electrical activation
KW  - bio-inspired robotic manipulator design
KW  - bio-inspired arm
KW  - manipulator prototype
KW  - SCP actuators
KW  - robotic arm
KW  - muscle-like form
KW  - twisting coiling polymer fibers
KW  - artificial muscle
KW  - supercoiled polymer actuator
KW  - low-cost robotic manipulator
KW  - Actuators
KW  - Manipulators
KW  - Muscles
KW  - Grippers
KW  - Polymers
DO  - 10.1109/ICRA40945.2020.9197390
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The supercoiled polymer (SCP) actuator is a novel artificial muscle, which is manufactured by twisting and coiling polymer fibers. This new artificial muscle is soft, low-cost and shows good linearity. Being utilized as an actuator, the artificial muscle could generate significant mechanical power in a muscle-like form upon electrical activation by Joule heating. In this study, we adopt this new artificial muscle to actuate a novel designed robotic manipulator, which is composed of two parts. The first part is a robotic arm based on the inspiration of the musculoskeletal system. The arm is fabricated with two ball-and-socket joints as skeleton and SCP actuators as driven muscles. The second part is a Fin Ray Effect inspired soft gripper that can perform grasping tasks on fragile objects. The manipulator prototype is fabricated and experimental tests are conducted including both simple but effective control of the bio-inspired arm as well as characterization of the gripper. Lastly, a pick and place demonstration of a fragile fruit is performed utilizing the proposed manipulator. We envision that the bio-inspired robotic manipulator design driven by SCP actuators could potentially be used in other robotic applications.
ER  - 


TY  - CONF
TI  - Internally-Balanced Magnetic Mechanisms Using a Magnetic Spring for Producing a Large Amplified Clamping Force
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1840
EP  - 1846
AU  - T. Shimizu
AU  - K. Tadakuma
AU  - M. Watanabe
AU  - E. Takane
AU  - M. Konyo
AU  - S. Tadokoro
PY  - 2020
KW  - clamps
KW  - control system synthesis
KW  - force control
KW  - magnetic devices
KW  - mobile robots
KW  - multi-robot systems
KW  - permanent magnets
KW  - springs (mechanical)
KW  - magnetic spring
KW  - magnetic mechanisms
KW  - clamping force
KW  - permanent magnet
KW  - force control
KW  - attractive force
KW  - internally-balanced magnetic unit
KW  - magnetic devices
KW  - internal force
KW  - internally-balanced magnetic mechanisms
KW  - IB magnet
KW  - nonlinear spring
KW  - unlike-pole pair
KW  - wall-climbing robots
KW  - ceiling-dangling drones
KW  - modular swarm robots
KW  - robotic clamp
KW  - Springs
KW  - Force
KW  - Magnetic noise
KW  - Magnetic shielding
KW  - Magnetic levitation
KW  - Magnetic liquids
KW  - Magnetic separation
KW  - Mechanism Design of Manipulators
KW  - Force Control
DO  - 10.1109/ICRA40945.2020.9197151
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To detach a permanent magnet using a control force much smaller than its original attractive force, the internally-balanced magnetic unit (IB Magnet) was invented. It has been applied to magnetic devices such as wall-climbing robots, ceiling-dangling drones, and modular swarm robots. In contrast to its significant reduction rate with regard to the control force, the IB Magnet has two major problems in its nonlinear spring, which serves the purpose of cancelling out the internal force on the magnet. These problems include the complicated design procedure and the trade-off relationship between balancing the precision and the volume of the mechanism. This paper proposes a principle for a new balancing method for the IB Magnet. This method uses a like-pole pair of magnets as a magnetic spring, whose repulsive force should equal the attractive force of an unlike-pole pair. To verify the proposed principle, a prototype of the IB Magnet was designed using a magnetic spring and verified through experiments such that its reduction rate is comparable to those of conventional IB Magnets. Moreover, a robotic clamp was developed as an application example that contains the proposed IB Magnets as its internal mechanism.
ER  - 

TY  - CONF
TI  - A Continuum Manipulator with Closed-form Inverse Kinematics and Independently Tunable Stiffness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1847
EP  - 1853
AU  - B. Zhao
AU  - L. Zeng
AU  - B. Wu
AU  - K. Xu
PY  - 2020
KW  - control system synthesis
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - rigidity
KW  - continuum manipulator
KW  - closed-form inverse kinematics
KW  - independently tunable stiffness
KW  - compliant structures
KW  - articulated manipulator design
KW  - manipulator end-effector
KW  - analytical inverse kinematics
KW  - Manipulators
KW  - Kinematics
KW  - Shape
KW  - Electron tubes
KW  - Friction
KW  - Payloads
DO  - 10.1109/ICRA40945.2020.9196688
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Continuum manipulators can accomplish various tasks in confined spaces, benefiting from their compliant structures and improved dexterity. Confined and unstructured spaces may require both enhanced stiffness of a continuum manipulator for precision and payload, as well as compliance for safe interaction. Thus, studies have been consistently dedicated to design continuum or articulated manipulators with tunable stiffness to adapt to different operating conditions. This paper presents a continuum manipulator with independently tunable stiffness where the stiffness variation does not affect the movement of the manipulator's end-effector. Moreover, the proposed continuum manipulator is found to have analytical inverse kinematics. The design concept, analytical kinematics, system construction and experimental characterizations are presented. The results showed that the manipulator's stiffness can be increased up to 3.61 times of the minimal value, demonstrating the effectiveness of the proposed idea.
ER  - 

TY  - CONF
TI  - Design and Compensation Control of a Flexible Instrument for Endoscopic Surgery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1860
EP  - 1866
AU  - W. Hong
AU  - A. Schmitz
AU  - W. Bai
AU  - P. Berthet-Rayne
AU  - L. Xie
AU  - G. -Z. Yang
PY  - 2020
KW  - actuators
KW  - compensation
KW  - end effectors
KW  - endoscopes
KW  - grippers
KW  - medical robotics
KW  - surgery
KW  - flexible instrument
KW  - endoscopic surgery
KW  - snake-like robots
KW  - flexible tendon-driven instruments
KW  - microsurgical tasks
KW  - standard endoscopic surgeries
KW  - articulated wrists
KW  - distal-roll gripper
KW  - compensation control scheme
KW  - end-effector rolling motion
KW  - Tendons
KW  - Instruments
KW  - Grippers
KW  - Gears
KW  - Robots
KW  - Joints
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196955
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Snake-like robots for endoscopic surgery make it possible to reach deep-seated lesions. With the use of small flexible tendon-driven instruments, it is possible to perform bimanual micro-surgical tasks that are challenging for standard endoscopic surgeries. Existing devices, however, lack articulated wrists and rolling motion of the end-effector. This paper presents a new instrument design with a distal-roll gripper for snake-like robots. The developed 5 DoFs miniaturized instruments with a diameter of 3 mm enable the deployment into narrow endoluminal channels. Issues related to actuation coupling, tendon slack, and backlash are addressed. Experimental results show that the distal-roll gripper can rotate 106¬∞, and the actuated joints can achieve good repeatability and accuracy with the proposed compensation control scheme.
ER  - 

TY  - CONF
TI  - Distance and Steering Heuristics for Streamline-Based Flow Field Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1867
EP  - 1873
AU  - K. Y. Cadmus To
AU  - C. Yoo
AU  - S. Anstee
AU  - R. Fitch
PY  - 2020
KW  - computational fluid dynamics
KW  - flow simulation
KW  - marine robots
KW  - mobile robots
KW  - path planning
KW  - robot dynamics
KW  - artificial flow field
KW  - East Australian current
KW  - streamline-based flow field planning
KW  - motion planning
KW  - streamline-based planning
KW  - fluid dynamics
KW  - travel distance
KW  - incompressible flows
KW  - ocean currents
KW  - distance functions
KW  - Euclidean distance
KW  - stream function
KW  - steering heuristics
KW  - ocean prediction data
KW  - autonomous marine robots
KW  - Planning
KW  - Aerospace electronics
KW  - Vehicle dynamics
KW  - Two dimensional displays
KW  - Space exploration
KW  - Space vehicles
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9196555
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion planning for vehicles under the influence of flow fields can benefit from the idea of streamline-based planning, which exploits ideas from fluid dynamics to achieve computational efficiency. Important to such planners is an efficient means of computing the travel distance and direction between two points in free space, but this is difficult to achieve in strong incompressible flows such as ocean currents. We propose two useful distance functions in analytical form that combine Euclidean distance with values of the stream function associated with a flow field, and with an estimation of the strength of the opposing flow between two points. Further, we propose steering heuristics that are useful for steering towards a sampled point. We evaluate these ideas by integrating them with RRT* and comparing the algorithm's performance with state-of-the-art methods in an artificial flow field and in actual ocean prediction data in the region of the dominant East Australian Current between Sydney and Brisbane. Results demonstrate the method's computational efficiency and ability to find high-quality paths outperforming state-of-the-art methods, and show promise for practical use with autonomous marine robots.
ER  - 

TY  - CONF
TI  - Enhancing Coral Reef Monitoring Utilizing a Deep Semi-Supervised Learning Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1874
EP  - 1880
AU  - M. Modasshir
AU  - I. Rekleitis
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - marine engineering
KW  - object detection
KW  - supervised learning
KW  - deep neural network
KW  - sample extraction
KW  - coral object dataset
KW  - coral reef monitoring
KW  - deep semisupervised learning approach
KW  - coral species detection
KW  - convolutional neural network-based object detector
KW  - Detectors
KW  - Training
KW  - Object detection
KW  - Monitoring
KW  - Tracking
KW  - Predictive models
KW  - Semantics
DO  - 10.1109/ICRA40945.2020.9196528
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Coral species detection underwater is a challenging problem. There are many cases when even the experts (marine biologists) fail to recognize corals, hence limiting ground truth annotation for training a robust detection system. Identifying coral species is fundamental for enabling the monitoring of coral reefs, a task currently performed by humans, which can be automated with the use of underwater robots. By employing temporal cues using a tracker on a high confidence prediction by a convolutional neural network-based object detector, we augment the collected dataset for the retraining of the object detector. However, using trackers to extract examples also introduces hard or mislabelled samples, which is counterproductive and will deteriorate the performance of the detector. In this work, we show that employing a simple deep neural network to filter out hard or mislabelled samples can help regulate sample extraction. We empirically evaluate our approach in a coral object dataset, collected via an Autonomous Underwater Vehicle (AUV) and human divers, that shows the benefit of incorporating extracted examples obtained from tracking. This work also demonstrates how controlling sample generation by tracking using a simple deep neural network can further improve an object detector.
ER  - 

TY  - CONF
TI  - DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1881
EP  - 1887
AU  - T. Wang
AU  - W. Lu
AU  - Z. Yan
AU  - D. Liu
PY  - 2020
KW  - control system synthesis
KW  - feedback
KW  - learning (artificial intelligence)
KW  - neurocontrollers
KW  - observers
KW  - optimal control
KW  - position control
KW  - recurrent neural nets
KW  - robots
KW  - robot control capabilities
KW  - disturbance dynamics observer network
KW  - controller network
KW  - conventional DOB mechanisms
KW  - recurrent neural networks
KW  - optimal control signals
KW  - conventional feedback controllers
KW  - DOB-Net
KW  - disturbance OB-server network
KW  - observer-integrated reinforcement learning
KW  - Observers
KW  - History
KW  - Vehicle dynamics
KW  - Robots
KW  - Optimization
KW  - Optimal control
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196641
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an observer-integrated Reinforcement Learning (RL) approach, called Disturbance OB-server Network (DOB-Net), for robots operating in environments where disturbances are unknown and time-varying, and may frequently exceed robot control capabilities. The DOB-Net integrates a disturbance dynamics observer network and a controller network. Originated from conventional DOB mechanisms, the observer is built and enhanced via Recurrent Neural Networks (RNNs), encoding estimation of past values and prediction of future values of unknown disturbances in RNN hidden state. Such encoding allows the controller generate optimal control signals to actively reject disturbances, under the constraints of robot control capabilities. The observer and the controller are jointly learned within policy optimization by advantage actor critic. Numerical simulations on position regulation tasks have demonstrated that the proposed DOB-Net significantly outperforms conventional feedback controllers and classical RL policy.
ER  - 

TY  - CONF
TI  - Demonstration of Autonomous Nested Search for Local Maxima Using an Unmanned Underwater Vehicle
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1888
EP  - 1895
AU  - A. Branch
AU  - J. McMahon
AU  - G. Xu
AU  - M. V. Jakuba
AU  - C. R. German
AU  - S. Chien
AU  - J. C. Kinsey
AU  - A. D. Bowen
AU  - K. P. Hand
AU  - J. S. Seewald
PY  - 2020
KW  - autonomous underwater vehicles
KW  - oceanographic equipment
KW  - oceanographic techniques
KW  - search problems
KW  - hydrothermal plume model
KW  - hydrothermal vent emissions
KW  - local maxima
KW  - autonomous nested search method
KW  - hydrothermal venting
KW  - sufficient autonomy
KW  - mission concept
KW  - solar system
KW  - extra-terrestrial life
KW  - Ocean World
KW  - unmanned underwater vehicle
KW  - Vents
KW  - Oceans
KW  - Vehicle dynamics
KW  - Underwater vehicles
KW  - Earth
KW  - Numerical models
KW  - Base stations
DO  - 10.1109/ICRA40945.2020.9196625
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ocean Worlds represent one of the best chances for extra-terrestrial life in our solar system. A new mission concept must be developed to explore these oceans. This mission would require traversing the 10s of km thick icy shell and releasing a submersible into the ocean below. During the transit of the icy shell and the exploration of the ocean, the vehicle(s) would be out of contact with Earth for weeks or potentially months at a time. During this time the vehicle must have sufficient autonomy to locate and study scientific targets of interest. One such target of interest is hydrothermal venting. We have previously developed an autonomous nested search method to locate and investigate sources of hydrothermal venting by locating local maxima in hydrothermal vent emissions. In this work we demonstrate this approach on board an OceanServer Iver2 AUV in Chesapeake Bay, MD using simulated sensor data from a hydrothermal plume model. This represents the first step towards the deployment of this approach in conditions analogous to those that we might expect on an Ocean World.
ER  - 

TY  - CONF
TI  - Towards distortion based underwater domed viewport camera calibration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1896
EP  - 1902
AU  - E. Iscar
AU  - M. Johnson-Roberson
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - geometry
KW  - image motion analysis
KW  - image reconstruction
KW  - motion estimation
KW  - optical transfer function
KW  - photogrammetry
KW  - stereo image processing
KW  - image-world correspondences
KW  - refractive calibration methods
KW  - photogrammetry techniques
KW  - motion estimation
KW  - projective geometry
KW  - image formation process
KW  - refraction
KW  - light rays
KW  - housing interface
KW  - nonlinear effects
KW  - systematic errors
KW  - spherical domes
KW  - camera centers
KW  - distortion based underwater domed viewport camera calibration
KW  - point spread function
KW  - PSF
KW  - optical system
KW  - 3D reconstructions
KW  - Cameras
KW  - Calibration
KW  - Three-dimensional displays
KW  - Optical distortion
KW  - Solid modeling
KW  - Distortion
KW  - Adaptive optics
DO  - 10.1109/ICRA40945.2020.9197036
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Photogrammetry techniques used for 3D reconstructions and motion estimation from images are based on projective geometry that models the image formation process. However, in the underwater setting, refraction of light rays at the housing interface introduce non-linear effects in the image formation. These effects produce systematic errors if not accounted for, and severely degrade the quality of the acquired images. In this paper, we present a novel approach to the calibration of cameras inside spherical domes with large offsets between dome and camera centers. Such large offsets not only amplify the effect of refraction, but also introduce blur in the image that corrupts feature extractors used to establish image-world correspondences in existing refractive calibration methods. We propose using the point spread function (PSF) as a complete description of the optical system and introduce a procedure to recover the camera pose inside the dome based on the measurement of the distortions. Results on a collected dataset show the method is capable of recovering the camera pose with high accuracy.
ER  - 

TY  - CONF
TI  - How far are Pneumatic Artificial Muscles from biological muscles?
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1909
EP  - 1915
AU  - O. Mohseni
AU  - F. Gagey
AU  - G. Zhao
AU  - A. Seyfarth
AU  - M. A. Sharbafi
PY  - 2020
KW  - biomechanics
KW  - elasticity
KW  - electroactive polymer actuators
KW  - legged locomotion
KW  - muscle
KW  - pneumatic actuators
KW  - pneumatic artificial muscles
KW  - biological muscles
KW  - artificial copies
KW  - force generation mechanism
KW  - PAM force-length
KW  - additive passive parallel elastic element
KW  - PAM dynamic behaviors
KW  - dynamic muscle-like model
KW  - living creatures
KW  - multiplicative formulation
KW  - two-segmented leg
KW  - legged robots
KW  - Muscles
KW  - Mathematical model
KW  - Force
KW  - Biological system modeling
KW  - Robots
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197177
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - There is a long history demonstrating humans' tendency to create artificial copies of living creatures. For moving machines called robots, actuators play a key role in developing human-like movements. Among different types of actuation, PAMs (pneumatic artificial muscles) are known as the most similar ones to biological muscles. In addition to similarities in force generation mechanism (tension based), the well-accepted argumentation from Klute et al., states that the PAM force-length (fl) behavior is close to biological muscles, while the force-velocity (fv) pattern is different. Using the multiplicative formulation of the pressure (as an activation term), fl and fv beside an additive passive parallel elastic element, we present a new model of PAM. This muscle-based model can predict PAM dynamic behaviors with high precision. With a second experiment on a two-segmented leg, the proposed model is verified to predict the generated forces of PAMs in an antagonistic arrangement. Such a dynamic muscle-like model of artificial muscles can be used for the design and control of legged robots to generate robust, efficient and versatile gaits.
ER  - 

TY  - CONF
TI  - Shared Control Templates for Assistive Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1956
EP  - 1962
AU  - G. Quere
AU  - A. Hagengruber
AU  - M. Iskandar
AU  - S. Bustamante
AU  - D. Leidner
AU  - F. Stulp
AU  - J. Vogel
PY  - 2020
KW  - assisted living
KW  - handicapped aids
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - path planning
KW  - service robots
KW  - constraint-based shared control
KW  - specific user command mappings
KW  - task execution
KW  - impairments
KW  - control interface
KW  - motor disability
KW  - light-weight robotic manipulators
KW  - assistive robotics
KW  - shared control templates
KW  - low-dimensional interface
KW  - high-dimensional tasks
KW  - human-readable format
KW  - state transitions
KW  - Task analysis
KW  - Robot kinematics
KW  - Manipulators
KW  - Wheelchairs
KW  - Manifolds
KW  - Rehabilitation robotics
DO  - 10.1109/ICRA40945.2020.9197041
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Light-weight robotic manipulators can be used to restore the manipulation capability of people with a motor disability. However, manipulating the environment poses a complex task, especially when the control interface is of low bandwidth, as may be the case for users with impairments. Therefore, we propose a constraint-based shared control scheme to define skills which provide support during task execution. This is achieved by representing a skill as a sequence of states, with specific user command mappings and different sets of constraints being applied in each state. New skills are defined by combining different types of constraints and conditions for state transitions, in a human-readable format. We demonstrate its versatility in a pilot experiment with three activities of daily living. Results show that even complex, high-dimensional tasks can be performed with a low-dimensional interface using our shared control approach.
ER  - 

TY  - CONF
TI  - Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1963
EP  - 1969
AU  - H. Chen
AU  - H. Tan
AU  - A. Kuntz
AU  - M. Bansal
AU  - R. Alterovitz
PY  - 2020
KW  - common-sense reasoning
KW  - control engineering computing
KW  - human-robot interaction
KW  - natural language processing
KW  - text analysis
KW  - environmental context
KW  - natural language instruction
KW  - unconstrained natural language
KW  - language-model-based commonsense reasoning
KW  - commonsense knowledge
KW  - spoken natural language
KW  - LMCR
KW  - parsing
KW  - verb frames
KW  - unstructured textual corpora
KW  - robot
KW  - Natural languages
KW  - Robot sensing systems
KW  - Cognition
KW  - Task analysis
KW  - Semantics
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197315
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling robots to understand instructions provided via spoken natural language would facilitate interaction between robots and people in a variety of settings in homes and workplaces. However, natural language instructions are often missing information that would be obvious to a human based on environmental context and common sense, and hence does not need to be explicitly stated. In this paper, we introduce Language-Model-based Commonsense Reasoning (LMCR), a new method which enables a robot to listen to a natural language instruction from a human, observe the environment around it, and automatically fill in information missing from the instruction using environmental context and a new commonsense reasoning approach. Our approach first converts an instruction provided as unconstrained natural language into a form that a robot can understand by parsing it into verb frames. Our approach then fills in missing information in the instruction by observing objects in its vicinity and leveraging commonsense reasoning. To learn commonsense reasoning automatically, our approach distills knowledge from large unstructured textual corpora by training a language model. Our results show the feasibility of a robot learning commonsense knowledge automatically from web-based textual corpora, and the power of learned commonsense reasoning models in enabling a robot to autonomously perform tasks based on incomplete natural language instructions.
ER  - 

TY  - CONF
TI  - A Holistic Approach in Designing Tabletop Robot‚Äôs Expressivity
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1970
EP  - 1976
AU  - R. Gomez
AU  - D. Szapiro
AU  - L. Merino
AU  - K. Nakamura
PY  - 2020
KW  - computer animation
KW  - control engineering computing
KW  - control system synthesis
KW  - humanoid robots
KW  - robot modalities
KW  - zoomorphic-designed robots
KW  - robot design
KW  - animated characters
KW  - table top robot
KW  - animation techniques
KW  - robot hardware
KW  - Haru
KW  - Animation
KW  - Hardware
KW  - Neck
KW  - Light emitting diodes
KW  - Mouth
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197016
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Defining a robot's expressivity is a difficult task that requires thoughtful consideration of the potential of various robot modalities and a model of communication that humans understand. Humanoid and zoomorphic-designed robots can easily take cues from human and animals, respectively when designing their expressivity. However, a robot design that is neither human nor animal-like does not have a clear model to follow in terms of designing expressivity. Animation presents a potential model in these circumstances as animated characters in movies take various forms, sizes, shapes and styles, and are successful in defining expressivity that is widely accepted across different languages and cultures. In this paper, we discuss the development and design of the expressivity of Haru, a table top robot that is neither human nor animal-like and the application of animation expertise to the holistic treatment of the different modalities. The method maximizes animation techniques and expertise normally applied to movies to generate expressivity that is then transferred to the robot hardware. Experimental results show that the robot's expressivity generated using our method is easily understood and are preferred to the conventional approach of generating expressions.
ER  - 

TY  - CONF
TI  - DirtNet: Visual Dirt Detection for Autonomous Cleaning Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1977
EP  - 1983
AU  - R. Bormann
AU  - X. Wang
AU  - J. Xu
AU  - J. Schmidt
PY  - 2020
KW  - cleaning
KW  - feature extraction
KW  - object detection
KW  - robot vision
KW  - service robots
KW  - office item detection system
KW  - YOLOv3 framework
KW  - visual dirt detection
KW  - autonomous cleaning
KW  - vacuum cleaning
KW  - professional cleaning robots
KW  - DirtNet
KW  - Cleaning
KW  - Object detection
KW  - Robots
KW  - Training
KW  - Task analysis
KW  - Image resolution
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196559
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual dirt detection is becoming an important capability of modern professional cleaning robots both for optimizing their wet cleaning results and for facilitating demand-oriented daily vacuum cleaning. This paper presents a robust, fast, and reliable dirt and office item detection system for these tasks based on an adapted YOLOv3 framework. Its superiority over state-of-the-art dirt detection systems is demonstrated in several experiments. The paper furthermore features a dataset generator for creating any number of realistic training images from a small set of real scene, dirt, and object examples.
ER  - 

TY  - CONF
TI  - Semantic Linking Maps for Active Visual Object Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1984
EP  - 1990
AU  - Z. Zeng
AU  - A. R√∂fer
AU  - O. C. Jenkins
PY  - 2020
KW  - inference mechanisms
KW  - manipulators
KW  - mobile robots
KW  - robot vision
KW  - search problems
KW  - Semantic Linking Maps model
KW  - target object
KW  - landmark objects
KW  - probabilistic inter-object spatial relations
KW  - hybrid search strategy
KW  - SLiM-based search strategy
KW  - Fetch mobile manipulation robot
KW  - mobile robots
KW  - common human environments
KW  - unseen target objects
KW  - reasoning
KW  - search space
KW  - common spatial relations
KW  - active visual object search strategy
KW  - Search problems
KW  - Robots
KW  - Probabilistic logic
KW  - Semantics
KW  - Buildings
KW  - Inference algorithms
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196830
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim for mobile robots to function in a variety of common human environments. Such robots need to be able to reason about the locations of previously unseen target objects. Landmark objects can help this reasoning by narrowing down the search space significantly. More specifically, we can exploit background knowledge about common spatial relations between landmark and target objects. For example, seeing a table and knowing that cups can often be found on tables aids the discovery of a cup. Such correlations can be expressed as distributions over possible pairing relationships of objects. In this paper, we propose an active visual object search strategy method through our introduction of the Semantic Linking Maps (SLiM) model. SLiM simultaneously maintains the belief over a target object's location as well as landmark objects' locations, while accounting for probabilistic inter-object spatial relations. Based on SLiM, we describe a hybrid search strategy that selects the next best view pose for searching for the target object based on the maintained belief. We demonstrate the efficiency of our SLiM-based search strategy through comparative experiments in simulated environments. We further demonstrate the realworld applicability of SLiM-based search in scenarios with a Fetch mobile manipulation robot.
ER  - 

TY  - CONF
TI  - Active Depth Estimation: Stability Analysis and its Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2002
EP  - 2008
AU  - R. T. Rodrigues
AU  - P. Miraldo
AU  - D. V. Dimarogonas
AU  - A. Pedro Aguiar
PY  - 2020
KW  - cameras
KW  - image sequences
KW  - Lyapunov methods
KW  - mobile robots
KW  - robot vision
KW  - solid modelling
KW  - stability
KW  - SfM
KW  - incremental active depth estimation
KW  - chronological sequence
KW  - image frames
KW  - camera actuation
KW  - stability analysis
KW  - control inputs
KW  - image plane
KW  - vision-controlled structure-from-motion scheme
KW  - depth estimation filter
KW  - Lyapunov theory
KW  - Cameras
KW  - Three-dimensional displays
KW  - Stability analysis
KW  - Asymptotic stability
KW  - Convergence
KW  - Estimation error
DO  - 10.1109/ICRA40945.2020.9196670
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recovering the 3D structure of the surrounding environment is an essential task in any vision-controlled Structure-from-Motion (SfM) scheme. This paper focuses on the theoretical properties of the SfM, known as the incremental active depth estimation. The term incremental stands for estimating the 3D structure of the scene over a chronological sequence of image frames. Active means that the camera actuation is such that it improves estimation performance. Starting from a known depth estimation filter, this paper presents the stability analysis of the filter in terms of the control inputs of the camera. By analyzing the convergence of the estimator using the Lyapunov theory, we relax the constraints on the projection of the 3D point in the image plane when compared to previous results. Nonetheless, our method is capable of dealing with the cameras' limited field-of-view constraints. The main results are validated through experiments with simulated data.
ER  - 

TY  - CONF
TI  - VALID: A Comprehensive Virtual Aerial Image Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2009
EP  - 2016
AU  - L. Chen
AU  - F. Liu
AU  - Y. Zhao
AU  - W. Wang
AU  - X. Yuan
AU  - J. Zhu
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image classification
KW  - image segmentation
KW  - object detection
KW  - stereo image processing
KW  - aerial imagery
KW  - unmanned aerial vehicle tasks
KW  - single ground truth type
KW  - virtual environment
KW  - high-resolution images
KW  - virtual scenes
KW  - comprehensive virtual aerial image dataset
KW  - visual ground truth data
KW  - Image segmentation
KW  - Semantics
KW  - Task analysis
KW  - Object detection
KW  - Image color analysis
KW  - Benchmark testing
KW  - Labeling
DO  - 10.1109/ICRA40945.2020.9197186
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Aerial imagery plays an important role in land-use planning, population analysis, precision agriculture, and unmanned aerial vehicle tasks. However, existing aerial image datasets generally suffer from the problem of inaccurate labeling, single ground truth type, and few category numbers. In this work, we implement a simulator that can simultaneously acquire diverse visual ground truth data in the virtual environment. Based on that, we collect a comprehensive Virtual AeriaL Image Dataset named VALID, consisting of 6690 high-resolution images, all annotated with panoptic segmentation on 30 categories, object detection with oriented bounding box, and binocular depth maps, collected in 6 different virtual scenes and 5 various ambient conditions (sunny, dusk, night, snow and fog). To our knowledge, VALID is the first aerial image dataset that can provide panoptic level segmentation and complete dense depth maps. We analyze the characteristics of VALID and evaluate state-of-the-art methods for multiple tasks to provide reference baselines. The experiment results demonstrate that VALID is well presented and challenging. The dataset is available at https://sites.google.com/view/valid-dataset/.
ER  - 

TY  - CONF
TI  - Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2095
EP  - 2101
AU  - H. Wang
AU  - C. Wang
AU  - L. Xie
PY  - 2020
KW  - computational geometry
KW  - feature extraction
KW  - image matching
KW  - mobile robots
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - intensity scan context
KW  - light detection and ranging sensor
KW  - discard intensity reading
KW  - geometric relation
KW  - intensity structure re-identification
KW  - coding intensity
KW  - simultaneous localization and mapping
KW  - LiDAR sensor
KW  - 3D loop closure detection
KW  - geometrical only descriptor matching
KW  - place recognition
KW  - robot navigation
KW  - fast point feature histogram
KW  - Geometry
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Histograms
KW  - Simultaneous localization and mapping
KW  - Rough surfaces
KW  - Surface roughness
DO  - 10.1109/ICRA40945.2020.9196764
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage on matching of local or global geometrical-only descriptors which discard intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. We propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including binary-operation based fast geometric relation retrieval and intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves better recall rate and recall precision than existing geometric-only methods.
ER  - 

TY  - CONF
TI  - TextSLAM: Visual SLAM with Planar Text Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2102
EP  - 2108
AU  - B. Li
AU  - D. Zou
AU  - D. Sartori
AU  - L. Pei
AU  - W. Yu
PY  - 2020
KW  - augmented reality
KW  - data visualisation
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - text analysis
KW  - text detection
KW  - text object integration
KW  - augmented reality
KW  - navigation
KW  - scene understanding
KW  - illumination-invariant photometric error
KW  - TextSLAM
KW  - text detection
KW  - text-based visual SLAM
KW  - 3D text maps
KW  - visual SLAM pipeline
KW  - planar text features
KW  - visual SLAM system
KW  - planar feature
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Visualization
KW  - Feature extraction
KW  - Navigation
KW  - Cameras
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197233
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.
ER  - 

TY  - CONF
TI  - FlowNorm: A Learning-based Method for Increasing Convergence Range of Direct Alignment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2109
EP  - 2115
AU  - K. Wang
AU  - K. Wang
AU  - S. Shen
PY  - 2020
KW  - image registration
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - patch alignments
KW  - global information
KW  - learning-based network
KW  - local information
KW  - learning-based method
KW  - direct alignment
KW  - camera poses
KW  - photometric error
KW  - nonconvex property
KW  - outlier terms
KW  - local error term
KW  - global image registration information
KW  - FlowNorm
KW  - Huber norm
KW  - DSO
KW  - BA-Net
KW  - Optimization
KW  - Convergence
KW  - Cameras
KW  - Robustness
KW  - Optical imaging
KW  - Learning systems
KW  - Nonlinear optics
DO  - 10.1109/ICRA40945.2020.9197118
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many approaches have been proposed to estimate camera poses by directly minimizing photometric error. However, due to the non-convex property of direct alignment, proper initialization is still required for these methods. Many robust norms (e.g. Huber norm) have been proposed to deal with the outlier terms caused by incorrect initializations. These robust norms are solely defined on the magnitude of each error term. In this paper, we propose a novel robust norm, named FlowNorm, that exploits the information from both the local error term and the global image registration information. While the local information is defined on patch alignments, the global information is estimated using a learning-based network. Using both the local and global information, we achieve a large convergence range in which images can be aligned given large view angle changes or small overlaps. We further demonstrate the usability of the proposed robust norm by integrating it into the direct methods DSO and BA-Net, and generate more robust and accurate results in real-time.
ER  - 

TY  - CONF
TI  - Redesigning SLAM for Arbitrary Multi-Camera Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2116
EP  - 2122
AU  - J. Kuo
AU  - M. Muglikar
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - sensor-specific modifications
KW  - SLAM systems
KW  - robustness
KW  - camera configurations
KW  - adaptive SLAM system
KW  - multicamera setup
KW  - visual SLAM
KW  - adaptive initialization
KW  - scalable voxel-based map
KW  - sensor-agnostic information-theoretic keyframe selection algorithm
KW  - visual front-end design
KW  - visual-inertial odometry
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Uncertainty
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197553
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.
ER  - 

TY  - CONF
TI  - Dynamic SLAM: The Need For Speed
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2123
EP  - 2129
AU  - M. Henein
AU  - J. Zhang
AU  - R. Mahony
AU  - V. Ila
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image segmentation
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - rigid moving objects
KW  - static structure
KW  - dynamic structure
KW  - rigid objects
KW  - object-aware dynamic SLAM algorithm
KW  - model-free
KW  - significant motion constraints
KW  - 3D models
KW  - SLAM based approaches
KW  - unstructured dynamic environments
KW  - autonomous systems
KW  - increased deployment
KW  - simultaneous localisation
KW  - static world assumption
KW  - Simultaneous localization and mapping
KW  - Heuristic algorithms
KW  - Dynamics
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9196895
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.
ER  - 

TY  - CONF
TI  - ‚àáSLAM: Dense SLAM meets Automatic Differentiation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2130
EP  - 2137
AU  - K. M. Jatavallabhula
AU  - G. Iyer
AU  - L. Paull
PY  - 2020
KW  - gradient methods
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - SLAM (robots)
KW  - automatic differentiation
KW  - dense simultaneous localization
KW  - learning-based approaches
KW  - representation learning approaches
KW  - classical SLAM systems
KW  - differentiable function
KW  - optimize task performance
KW  - typical dense SLAM system
KW  - ‚àáSLAM
KW  - posing SLAM systems
KW  - differentiable computational graphs
KW  - differentiable trust-region optimizers
KW  - task-based error signals
KW  - Simultaneous localization and mapping
KW  - Optimization
KW  - Three-dimensional displays
KW  - Damping
KW  - Task analysis
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197519
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Learning-based approaches have the potential to leverage data or task performance to directly inform the representation. However, blending representation learning approaches with "classical" SLAM systems has remained an open question, because of their highly modular and complex nature. A SLAM system transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals over the outputs of this function to learn representations that optimize task performance. However, this is infeasible as several components of a typical dense SLAM system are non-differentiable. In this work, we propose ‚àáSLAM (gradSLAM), a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM1.
ER  - 

TY  - CONF
TI  - Learning local behavioral sequences to better infer non-local properties in real multi-robot systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2138
EP  - 2144
AU  - T. Choi
AU  - S. Kang
AU  - T. P. Pavlic
PY  - 2020
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - two-wheeled robotic platform
KW  - local behavioral sequences
KW  - multirobot systems
KW  - multirobot team
KW  - traditional observer-based approach
KW  - machine learning methods
KW  - remote teammate localization modules
KW  - long-short-term-memory
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Training
KW  - Multi-robot systems
KW  - Machine learning
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9196728
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When members of a multi-robot team follow regular motion rules sensitive to robots and other environmental factors within sensing range, the team itself may become an informational fabric for gaining situational awareness without explicit signalling among robots. In our previous work [1], we used machine learning to develop a scalable module, trained only on data from 3-robot teams, that could predict the positions of all robots in larger multi-robot teams based only on observations of the movement of a robot's nearest neighbor. Not only was this approach scalable from 3-to-many robots, but it did not require knowledge of the control laws of the robots under observation, as would a traditional observer-based approach. However, performance was only tested in simulation and could only be a substitute for explicit communication for short periods of time or in cases of very low sensing noise. In this work, we apply more sophisticated machine learning methods to data from a physically realized robotic team to develop Remote Teammate Localization (ReTLo) modules that can be used in realistic environments. To be specific, we adopt Long-Short-Term-Memory (LSTM) [2] to learn the evolution of behaviors in a modular team, which has the effect of greatly reducing errors from regression outcomes. In contrast with our previous work in simulation, all of the experiments conducted in this work were conducted on the Thymio physical, two-wheeled robotic platform.
ER  - 

TY  - CONF
TI  - Unsupervised Geometry-Aware Deep LiDAR Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2145
EP  - 2152
AU  - Y. Cho
AU  - G. Kim
AU  - A. Kim
PY  - 2020
KW  - distance measurement
KW  - geometry
KW  - optical radar
KW  - radar computing
KW  - reliability
KW  - supervised learning
KW  - unsupervised learning
KW  - unsupervised geometry-aware deep LiDAR odometry
KW  - visual perception
KW  - supervised learning-based approaches
KW  - supervised training
KW  - ground-truth pose labels
KW  - trainable LO
KW  - uncertainty-aware loss
KW  - LeGO-LOAM
KW  - unsupervised learning-based approaches
KW  - egomotion estimation approaches
KW  - Stereo-VO datasets
KW  - complex urban datasets
KW  - Oxford RobotCar datasets
KW  - KITTI
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Training
KW  - Estimation
KW  - Two dimensional displays
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197366
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based ego-motion estimation approaches have recently drawn strong interest from researchers, mostly focusing on visual perception. A few learning-based approaches using Light Detection and Ranging (LiDAR) have been re-ported; however, they heavily rely on a supervised learning manner. Despite the meaningful performance of these approaches, supervised training requires ground-truth pose labels, which is the bottleneck for real-world applications. Differing from these approaches, we focus on unsupervised learning for LiDAR odometry (LO) without trainable labels. Achieving trainable LO in an unsupervised manner, we introduce the uncertainty-aware loss with geometric confidence, thereby al-lowing the reliability of the proposed pipeline. Evaluation on the KITTI, Complex Urban, and Oxford RobotCar datasets demonstrate the prominent performance of the proposed method compared to conventional model-based methods. The proposed method shows a comparable result against SuMa (in KITTI), LeGO-LOAM (in Complex Urban), and Stereo-VO (in Oxford RobotCar). The video and extra-information of the paper are described in https://sites.google.com/view/deeplo.
ER  - 

TY  - CONF
TI  - SA-Net: Robust State-Action Recognition for Learning from Observations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2153
EP  - 2159
AU  - N. Soans
AU  - E. Asali
AU  - Y. Hong
AU  - P. Doshi
PY  - 2020
KW  - control engineering computing
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - robust state-action recognition
KW  - LfO methods
KW  - SA-Net
KW  - RGB-D data streams
KW  - replicated robotic applications
KW  - mobile ground robots
KW  - robotic manipulator
KW  - physical robot
KW  - deep neural network architecture
KW  - learning from observation
KW  - Image recognition
KW  - Task analysis
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Robot kinematics
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197393
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning from observation (LfO) offers a new paradigm for transferring task behavior to robots. LfO requires the robot to observe the task being performed and decompose the sensed streaming data into sequences of state-action pairs, which are then input to LfO methods. Thus, recognizing the state-action pairs correctly and quickly in sensed data is a crucial prerequisite. We present SA-Net a deep neural network architecture that recognizes state-action pairs from RGB-D data streams. SA-Net performs well in two replicated robotic applications of LfO - one involving mobile ground robots and another involving a robotic manipulator - which demonstrates that the architecture could generalize well to differing contexts. Comprehensive evaluations including deployment on a physical robot show that SA-Net significantly improves on the accuracy of the previous methods under various conditions.
ER  - 

TY  - CONF
TI  - A Generative Approach for Socially Compliant Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2160
EP  - 2166
AU  - C. -E. Tsai
AU  - J. Oh
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - socially compliant navigation
KW  - robots navigation
KW  - socially compliant behavior
KW  - optimization objectives
KW  - inverse reinforcement learning approaches
KW  - natural behavior
KW  - generative navigation algorithm
KW  - navigation path
KW  - latent social rules
KW  - trained social navigation behavior
KW  - NaviGAN
KW  - Navigation
KW  - Robots
KW  - Force
KW  - Generators
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197497
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots navigating in human crowds need to optimize their paths not only for their task performance but also for their compliance to social norms. One of the key challenges in this context is the lack of standard metrics for evaluating and optimizing a socially compliant behavior. Existing works in social navigation can be grouped according to the differences in their optimization objectives. For instance, the reinforcement learning approaches tend to optimize on the comfort aspect of the socially compliant navigation, whereas the inverse reinforcement learning approaches are designed to achieve natural behavior. In this paper, we propose NaviGAN, a generative navigation algorithm that jointly optimizes both of the comfort and naturalness aspects. Our approach is designed as an adversarial training framework that can learn to generate a navigation path that is both optimized for achieving a goal and for complying with latent social rules. A set of experiments has been carried out on multiple datasets to demonstrate the strengths of the proposed approach quantitatively. We also perform extensive experiments using a physical robot in a realworld environment to qualitatively evaluate the trained social navigation behavior. The video recordings of the robot experiments can be found in the link: https://youtu.be/61blDymjCpw.
ER  - 

TY  - CONF
TI  - Scalable Multi-Task Imitation Learning with Autonomous Improvement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2167
EP  - 2173
AU  - A. Singh
AU  - E. Jang
AU  - A. Irpan
AU  - D. Kappler
AU  - M. Dalal
AU  - S. Levinev
AU  - M. Khansari
AU  - C. Finn
PY  - 2020
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - robots
KW  - task analysis
KW  - deploying learning-based systems
KW  - scalable multitask imitation learning
KW  - sparse task-agnostic reward signals
KW  - reinforcement learning algorithms
KW  - continuous improvement
KW  - prior imitation learning approaches
KW  - initial demonstration dataset
KW  - learned latent space
KW  - multitask demonstration data
KW  - multitask setting
KW  - autonomous improvement
KW  - supervised imitation
KW  - autonomous data collection
KW  - imitation learning system
KW  - robot learning
KW  - stable approach
KW  - Task analysis
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Standards
KW  - Trajectory
KW  - Data collection
KW  - Learning systems
DO  - 10.1109/ICRA40945.2020.9197020
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While robot learning has demonstrated promising results for enabling robots to automatically acquire new skills, a critical challenge in deploying learning-based systems is scale: acquiring enough data for the robot to effectively generalize broadly. Imitation learning, in particular, has remained a stable and powerful approach for robot learning, but critically relies on expert operators for data collection. In this work, we target this challenge, aiming to build an imitation learning system that can continuously improve through autonomous data collection, while simultaneously avoiding the explicit use of reinforcement learning, to maintain the stability, simplicity, and scalability of supervised imitation. To accomplish this, we cast the problem of imitation with autonomous improvement into a multi-task setting. We utilize the insight that, in a multi-task setting, a failed attempt at one task might represent a successful attempt at another task. This allows us to leverage the robot's own trials as demonstrations for tasks other than the one that the robot actually attempted. Using an initial dataset of multitask demonstration data, the robot autonomously collects trials which are only sparsely labeled with a binary indication of whether the trial accomplished any useful task or not. We then embed the trials into a learned latent space of tasks, trained using only the initial demonstration dataset, to draw similarities between various trials, enabling the robot to achieve one-shot generalization to new tasks. In contrast to prior imitation learning approaches, our method can autonomously collect data with sparse supervision for continuous improvement, and in contrast to reinforcement learning algorithms, our method can effectively improve from sparse, task-agnostic reward signals.
ER  - 

TY  - CONF
TI  - Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2174
EP  - 2181
AU  - A. K. Tanwani
AU  - P. Sermanet
AU  - A. Yan
AU  - R. Anand
AU  - M. Phielipp
AU  - K. Goldberg
PY  - 2020
KW  - image segmentation
KW  - medical image processing
KW  - recurrent neural nets
KW  - supervised learning
KW  - video signal processing
KW  - visual representations
KW  - embedding space
KW  - downstream tasks
KW  - action segmentation
KW  - motion-centric representation
KW  - surgical video demonstrations
KW  - deep embedding feature space
KW  - video observations
KW  - Siamese network
KW  - action segment
KW  - randomly sampled images
KW  - recurrent neural network
KW  - labeled video segments
KW  - learned model parameters
KW  - surgical suturing kinematic motions
KW  - kinematic pose imitation
KW  - semisupervised representation learning
KW  - JIGSAWS dataset
KW  - Videos
KW  - Motion segmentation
KW  - Image segmentation
KW  - Measurement
KW  - Hidden Markov models
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197324
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning meaningful visual representations in an embedding space can facilitate generalization in downstream tasks such as action segmentation and imitation. In this paper, we learn a motion-centric representation of surgical video demonstrations by grouping them into action segments/subgoals/options in a semi-supervised manner. We present Motion2Vec, an algorithm that learns a deep embedding feature space from video observations by minimizing a metric learning loss in a Siamese network: images from the same action segment are pulled together while pushed away from randomly sampled images of other segments, while respecting the temporal ordering of the images. The embeddings are iteratively segmented with a recurrent neural network for a given parametrization of the embedding space after pre-training the Siamese network. We only use a small set of labeled video segments to semantically align the embedding space and assign pseudo-labels to the remaining unlabeled data by inference on the learned model parameters. We demonstrate the use of this representation to imitate surgical suturing kinematic motions from publicly available videos of the JIGSAWS dataset. Results give 85.5% segmentation accuracy on average suggesting performance improvement over several state-of-the-art baselines, while kinematic pose imitation gives 0.94 centimeter error in position per observation on the test set. Videos, code and data are available at: https://sites.google.com/view/motion2vec.
ER  - 

TY  - CONF
TI  - A New Path Planning Architecture to Consider Motion Uncertainty in Natural Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2182
EP  - 2188
AU  - M. Mizuno
AU  - T. Kubota
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - probability
KW  - random processes
KW  - trees (mathematics)
KW  - uncertain systems
KW  - rough environments
KW  - uncertainty propagation
KW  - rapidly-exploring random tree
KW  - position uncertainty
KW  - motion uncertainty
KW  - natural environment
KW  - wheeled robots
KW  - path planning architecture
KW  - path-following
KW  - path replanning
KW  - probability
KW  - collision avoidance
KW  - Uncertainty
KW  - Robot kinematics
KW  - Path planning
KW  - Mobile robots
KW  - Planning
KW  - Global Positioning System
DO  - 10.1109/ICRA40945.2020.9197238
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a new path planning algorithm to consider motion uncertainty for wheeled robots in rough environments. The proposed method uses particles to express the uncertainty propagation in complicated environments constructed with various types of terrain. Also, RRT (Rapidly-exploring Random Tree) is expanded based on the uncertainty of each node in order to prevent increasing the accumulated position uncertainty. As a result, the generated path reduces the times of path-following and re-planning based on inaccurate localization information. The effectiveness of the proposed method is evaluated in simulation using motion uncertainty models obtained by experiments. The results show that the proposed method decreases the position uncertainty while keeping the probability to avoid collisions and to reach the goal area compared with conventional approaches.
ER  - 

TY  - CONF
TI  - Revisiting the Asymptotic Optimality of RRT*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2189
EP  - 2195
AU  - K. Solovey
AU  - L. Janson
AU  - E. Schmerling
AU  - E. Frazzoli
AU  - M. Pavone
PY  - 2020
KW  - computational complexity
KW  - sampling methods
KW  - search problems
KW  - trees (mathematics)
KW  - mathematically-rigorous proof
KW  - asymptotic optimality
KW  - RRT*
KW  - asymptotically-optimal motion planning
KW  - optimality proof
KW  - sampling-based algorithms
KW  - connection radius
KW  - Robustness
KW  - Robots
KW  - Heuristic algorithms
KW  - Manganese
KW  - Planning
KW  - Aerodynamics
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196553
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - RRT* is one of the most widely used sampling-based algorithms for asymptotically-optimal motion planning. RRT* laid the foundations for optimality in motion planning as a whole, and inspired the development of numerous new algorithms in the field, many of which build upon RRT* itself. In this paper, we first identify a logical gap in the optimality proof of RRT*, which was developed by Karaman and Frazzoli (2011). Then, we present an alternative and mathematically-rigorous proof for asymptotic optimality. Our proof suggests that the connection radius used by RRT* should be increased from Œ≥ (log n/n)1/d to Œ≥' (log n/n)1/(d+1) in order to account n n for the additional dimension of time that dictates the samples' ordering. Here Œ≥, Œ≥' are constants, and n, d are the number of samples and the dimension of the problem, respectively.
ER  - 

TY  - CONF
TI  - Sample Complexity of Probabilistic Roadmaps via Œµ-nets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2196
EP  - 2202
AU  - M. Tsao
AU  - K. Solovey
AU  - M. Pavone
PY  - 2020
KW  - computational complexity
KW  - deterministic algorithms
KW  - graph theory
KW  - probability
KW  - shortest Œ¥-clear path
KW  - sample complexity
KW  - probabilistic roadmaps
KW  - Œµ-nets
KW  - optimality guarantees
KW  - deterministic sampling distribution
KW  - motion planning problem
KW  - parameter completeness
KW  - Planning
KW  - Complexity theory
KW  - Probabilistic logic
KW  - Two dimensional displays
KW  - Robots
KW  - Collision avoidance
KW  - Benchmark testing
DO  - 10.1109/ICRA40945.2020.9196917
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study fundamental theoretical aspects of probabilistic roadmaps (PRM) in the finite time (non-asymptotic) regime. In particular, we investigate how completeness and optimality guarantees of the approach are influenced by the underlying deterministic sampling distribution X and connection radius r > 0. We develop the notion of (Œ¥, Œµ)-completeness of the parameters X, r, which indicates that for every motion-planning problem of clearance at least Œ¥ > 0, PRM using X, r returns a solution no longer than 1+Œµ times the shortest Œ¥-clear path. Leveraging the concept of e-nets, we characterize in terms of lower and upper bounds the number of samples needed to guarantee (Œ¥, Œµ)-completeness. This is in contrast with previous work which mostly considered the asymptotic regime in which the number of samples tends to infinity. In practice, we propose a sampling distribution inspired by e-nets that achieves nearly the same coverage as grids while using fewer samples.
ER  - 

TY  - CONF
TI  - Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2203
EP  - 2208
AU  - H. Su
AU  - Y. Hu
AU  - Z. Li
AU  - A. Knoll
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - end effectors
KW  - Gaussian processes
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - medical robotics
KW  - motion control
KW  - regression analysis
KW  - surgery
KW  - complex tasks demonstrations
KW  - reinforcement learning algorithm based manipulation skill transferring technique
KW  - robot-assisted minimally invasive surgery
KW  - Gaussian mixture model
KW  - Gaussian mixture regression
KW  - multiple demonstrations
KW  - trial phase performed offline
KW  - practical surgical operation
KW  - KUKA LWR4+ robot
KW  - human manipulation skill
KW  - surgical robots
KW  - Learning (artificial intelligence)
KW  - Robots
KW  - Surgery
KW  - Trajectory
KW  - Task analysis
KW  - Shape
KW  - Education
DO  - 10.1109/ICRA40945.2020.9196588
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The complexity of surgical operation can be released significantly if surgical robots can learn the manipulation skills by imitation from complex tasks demonstrations such as puncture, suturing, and knotting, etc.. This paper proposes a reinforcement learning algorithm based manipulation skill transferring technique for robot-assisted Minimally Invasive Surgery by Teaching by Demonstration. It employed Gaussian mixture model and Gaussian mixture Regression based dynamic movement primitive to model the high-dimensional human-like manipulation skill after multiple demonstrations. Furthermore, this approach fascinates the learning and trial phase performed offline, which reduces the risks and cost for the practical surgical operation. Finally, it is demonstrated by transferring manipulation skills for reaching and puncture using a KUKA LWR4+ robot in a lab setup environment. The results show the effectiveness of the proposed approach for modelling and learning of human manipulation skill.
ER  - 

TY  - CONF
TI  - Safe Mission Planning under Dynamical Uncertainties
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2209
EP  - 2215
AU  - Y. Lu
AU  - M. Kamgarpour
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - Monte Carlo methods
KW  - uncertain systems
KW  - uncertainty model
KW  - path planning
KW  - dynamical uncertainties
KW  - probabilistic model
KW  - safe robot mission planning
KW  - Monte Carlo method
KW  - collision free path
KW  - Uncertainty
KW  - Planning
KW  - Automata
KW  - Computational modeling
KW  - Hazards
DO  - 10.1109/ICRA40945.2020.9196515
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers safe robot mission planning in uncertain dynamical environments. This problem arises in applications such as surveillance, emergency rescue, and autonomous driving. It is a challenging problem due to mod-eling and integrating dynamical uncertainties into a safe planning framework, and finding a solution in a computationally tractable way. In this work, we first develop a probabilistic model for dynamical uncertainties. Then, we provide a framework to generate a path that maximizes safety for complex missions by incorporating the uncertainty model. We also devise a Monte Carlo method to obtain a safe path efficiently. Finally, we evaluate the performance of our approach and compare it to potential alternatives in several case studies.
ER  - 

TY  - CONF
TI  - An Iterative Quadratic Method for General-Sum Differential Games with Feedback Linearizable Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2216
EP  - 2222
AU  - D. Fridovich-Keil
AU  - V. Rubies-Royo
AU  - C. J. Tomlin
PY  - 2020
KW  - control system synthesis
KW  - convergence of numerical methods
KW  - differential games
KW  - feedback
KW  - game theory
KW  - iterative methods
KW  - linear quadratic control
KW  - linearisation techniques
KW  - nonlinear control systems
KW  - path planning
KW  - feedback linearizable dynamics
KW  - nonlinear optimal control community
KW  - multiplayer general-sum differential games
KW  - ILQ methods
KW  - local equilibria
KW  - interactive motion planning problems
KW  - iterative procedures
KW  - initial conditions
KW  - hyperparameter choices
KW  - unsafe trajectories
KW  - dynamical systems
KW  - algorithmic reliability
KW  - feedback linearizable structure
KW  - iterative linear-quadratic method
KW  - Games
KW  - Heuristic algorithms
KW  - Planning
KW  - Feedback linearization
KW  - Iterative methods
KW  - Vehicle dynamics
KW  - Optimal control
DO  - 10.1109/ICRA40945.2020.9196517
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Iterative linear-quadratic (ILQ) methods are widely used in the nonlinear optimal control community. Recent work has applied similar methodology in the setting of multi-player general-sum differential games. Here, ILQ methods are capable of finding local equilibria in interactive motion planning problems in real-time. As in most iterative procedures, however, this approach can be sensitive to initial conditions and hyperparameter choices, which can result in poor computational performance or even unsafe trajectories. In this paper, we focus our attention on a broad class of dynamical systems which are feedback linearizable, and exploit this structure to improve both algorithmic reliability and runtime. We showcase our new algorithm in three distinct traffic scenarios, and observe that in practice our method converges significantly more often and more quickly than was possible without exploiting the feedback linearizable structure.
ER  - 

TY  - CONF
TI  - A Morphable Aerial-Aquatic Quadrotor with Coupled Symmetric Thrust Vectoring
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2223
EP  - 2229
AU  - Y. H. Tan
AU  - B. M. Chen
PY  - 2020
KW  - autonomous aerial vehicles
KW  - autonomous underwater vehicles
KW  - design engineering
KW  - helicopters
KW  - mobile robots
KW  - stability
KW  - single design difficult
KW  - normal aerial vehicles
KW  - rotational acceleration
KW  - quadrotor based vehicle
KW  - vehicle body
KW  - design considerations
KW  - aerial-aquatic quadrotor
KW  - coupled symmetric thrust vectoring
KW  - aerial-aquatic vehicles
KW  - fluid resistance
KW  - energy efficient position
KW  - morphable aerial-aquatic quadrotor
KW  - mechanical actuation
KW  - static stability
KW  - Buoyancy
KW  - Robots
KW  - Force
KW  - Torque
KW  - Prototypes
KW  - Propellers
DO  - 10.1109/ICRA40945.2020.9196687
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid aerial-aquatic vehicles have the unique ability of travelling in both air and water and can benefit from both lower fluid resistance in air and energy efficient position holding in water. However, they have to address the differing requirements which make optimising a single design difficult. While existing examples have shown the possibility of such vehicles, they are mostly structurally identical to normal aerial vehicles with minor adjustments to work underwater. Instead of using rotational acceleration to direct a component of thrust in surge and sway, we propose a quadrotor based vehicle that tilts its rotors about the respective arm so that a larger component of thrust can be directed in the lateral plane or in the opposite direction without rotating the vehicle body. A small-scale prototype of this design is presented here, detailing the design considerations including mechanical actuation, static stability and waterproofing.
ER  - 

TY  - CONF
TI  - An Autonomous Intercept Drone with Image-based Visual Servo
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2230
EP  - 2236
AU  - K. Yang
AU  - Q. Quan
PY  - 2020
KW  - autonomous aerial vehicles
KW  - cameras
KW  - robot vision
KW  - visual servoing
KW  - autonomous intercept drone
KW  - unwanted drone
KW  - radio wave gun
KW  - image-based visual servo algorithm
KW  - Cameras
KW  - Visualization
KW  - Mathematical model
KW  - Drones
KW  - Channel models
KW  - Servomotors
KW  - Angular velocity
DO  - 10.1109/ICRA40945.2020.9197539
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For most people on the ground, facing an unwanted drone buzzing around overhead, there is not a lot that we can do, especially if it is out of gun (radio wave gun or shotgun) range. A solution to this is to use intercept drones that seek out and bring down other drones. In order to make the interception autonomous, an image-based visual servo algorithm is designed with a forward-looking monocular camera. The control command, namely the angular velocity and thrust, is generated for intercept drones to implement accurate and fast interception. The proposed method is demonstrated in both hardware-in-the-loop simulation and demonstrative flight experiments.
ER  - 

TY  - CONF
TI  - On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2253
EP  - 2258
AU  - P. Prajapati
AU  - S. Parekh
AU  - V. Vashista
PY  - 2020
KW  - actuators
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - cable-suspended payload system
KW  - human control
KW  - multiple quadcopters system
KW  - leader quadcopter
KW  - payload attitude controller
KW  - cable attitude controller
KW  - quadcopter-payload system
KW  - Payloads
KW  - Oscillators
KW  - Angular velocity
KW  - Attitude control
KW  - Vehicle dynamics
KW  - Trajectory
KW  - Quadcopters
KW  - Human control
KW  - Cable-suspended payload
KW  - Collaborative transportation
KW  - Multi-agents
DO  - 10.1109/ICRA40945.2020.9197279
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A quadcopter is an under-actuated system with only four control inputs for six degrees of freedom, and yet the human control of a quadcopter is simple enough to be learned with some practice. In this work, we consider the problem of human control of a multiple quadcopters system to transport a cable-suspended payload. The coupled dynamics of the system, due to the inherent physical constraints, is used to develop a leader-follower architecture where the leader quadcopter is controlled directly by a human operator and the followers are controlled with the proposed Payload Attitude Controller and Cable Attitude Controller. Experiments, where a human operator flew a two quadcopters system to transport a cable-suspended payload, were conducted to study the performance of proposed controller. The results demonstrated successful implementation of human control in these systems. This work presents the possibility of enabling manual control for on-the-go maneuvering of the quadcopter-payload system which motivates aerial transportation in the unknown environments.
ER  - 

TY  - CONF
TI  - A*3D Dataset: Towards Autonomous Driving in Challenging Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2267
EP  - 2273
AU  - Q. -H. Pham
AU  - P. Sevestre
AU  - R. S. Pahwa
AU  - H. Zhan
AU  - C. H. Pang
AU  - Y. Chen
AU  - A. Mustafa
AU  - V. Chandrasekhar
AU  - J. Lin
PY  - 2020
KW  - image annotation
KW  - image colour analysis
KW  - mobile robots
KW  - object detection
KW  - optical radar
KW  - road vehicle radar
KW  - stereo image processing
KW  - traffic engineering computing
KW  - A*3D dataset
KW  - self-driving cars
KW  - 3D object detection
KW  - 3D object annotations
KW  - autonomous driving research
KW  - nuScenes dataset
KW  - KITTI dataset
KW  - high-density images
KW  - LiDAR data
KW  - RGB images
KW  - computer vision tasks
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Autonomous vehicles
KW  - Cameras
KW  - Calibration
KW  - Object detection
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197385
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the increasing global popularity of self-driving cars, there is an immediate need for challenging real-world datasets for benchmarking and training various computer vision tasks such as 3D object detection. Existing datasets either represent simple scenarios or provide only day-time data. In this paper, we introduce a new challenging A*3D dataset which consists of RGB images and LiDAR data with a significant diversity of scene, time, and weather. The dataset consists of high-density images (‚âà 10 times more than the pioneering KITTI dataset), heavy occlusions, a large number of nighttime frames (‚âà 3 times the nuScenes dataset), addressing the gaps in the existing datasets to push the boundaries of tasks in autonomous driving research to more challenging highly diverse environments. The dataset contains 39K frames, 7 classes, and 230K 3D object annotations. An extensive 3D object detection benchmark evaluation on the A*3D dataset for various attributes such as high density, day-time/night-time, gives interesting insights into the advantages and limitations of training and testing 3D object detection in real-world setting.
ER  - 

TY  - CONF
TI  - SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2274
EP  - 2280
AU  - H. Yi
AU  - S. Shi
AU  - M. Ding
AU  - J. Sun
AU  - K. Xu
AU  - H. Zhou
AU  - Z. Wang
AU  - S. Li
AU  - G. Wang
PY  - 2020
KW  - image denoising
KW  - image segmentation
KW  - object detection
KW  - optical radar
KW  - stereo image processing
KW  - traffic engineering computing
KW  - depth-aware features
KW  - 3D vehicle detection
KW  - point cloud distribution
KW  - semantic context information
KW  - ambiguous vehicles
KW  - semantic context encoder
KW  - target detection range
KW  - emantic segmentation masks
KW  - depth-aware head
KW  - SegVoxelNet
KW  - LiDAR
KW  - noisy region suppression
KW  - Three-dimensional displays
KW  - Semantics
KW  - Feature extraction
KW  - Two dimensional displays
KW  - Vehicle detection
KW  - Head
KW  - Convolution
DO  - 10.1109/ICRA40945.2020.9196556
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only.
ER  - 

TY  - CONF
TI  - Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2281
EP  - 2287
AU  - K. Nishi
AU  - M. Shimosaka
PY  - 2020
KW  - behavioural sciences computing
KW  - driver information systems
KW  - learning (artificial intelligence)
KW  - road accidents
KW  - road safety
KW  - road traffic
KW  - unexpected VRU movements
KW  - residential roads
KW  - proficient acceleration
KW  - deceleration
KW  - road width
KW  - traffic direction
KW  - multilinear reward function
KW  - contextual information
KW  - long-term prediction
KW  - defensive driving strategy
KW  - context-aware multitask inverse reinforcement learning
KW  - advanced driver assistance systems
KW  - vulnerable road users
KW  - traffic accident reduction rate
KW  - multitask IRL approach
KW  - fine-grained driving behavior prediction
KW  - inverse reinforcement learning
KW  - Roads
KW  - Context modeling
KW  - Task analysis
KW  - Vehicles
KW  - Hidden Markov models
KW  - Safety
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9197126
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Research on advanced driver assistance systems for reducing risks to vulnerable road users (VRUs) has recently gained popularity because the traffic accident reduction rate for VRUs is still small. Dealing with unexpected VRU movements on residential roads requires proficient acceleration and deceleration. Although fine-grained prediction of driving behavior through inverse reinforcement learning (IRL) has been reported with promising results in recent years, learning of a precise model fails when driving strategies vary with contextual factors, i.e., weather, time of day, road width, and traffic direction. In this work, we propose a novel multi-task IRL approach with a multilinear reward function to incorporate contextual information into the model. This approach can provide precise long-term prediction of fine-grained driving behavior while adjusting to context. Experimental results using actual driving data over 141 km with various contexts and roads confirm the success of this approach in terms of predicting defensive driving strategy even in unknown situations.
ER  - 

TY  - CONF
TI  - How to Keep HD Maps for Automated Driving Up To Date
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2288
EP  - 2294
AU  - D. Pannen
AU  - M. Liebner
AU  - W. Hempel
AU  - W. Burgard
PY  - 2020
KW  - cartography
KW  - data privacy
KW  - road vehicles
KW  - traffic engineering computing
KW  - dedicated mapping vehicles
KW  - low traversal frequencies
KW  - anonymized data
KW  - up-to-dateness
KW  - crowdsourced data
KW  - automatically trigger map update jobs
KW  - map patches
KW  - date HD map
KW  - automated driving functions
KW  - HD maps
KW  - automotive high definition digital map generation
KW  - automated driving up to date
KW  - Roads
KW  - Vehicle dynamics
KW  - Topology
KW  - Robot sensing systems
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197419
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The current state of the art in automotive high definition digital (HD) map generation based on dedicated mapping vehicles cannot reliably keep these maps up to date because of the low traversal frequencies. Anonymized data collected from the fleet of vehicles that is already on the road provides a huge potential to outperform such state of the art solutions in robustness, safety and up-to-dateness of the map while achieving comparable quality. We thus present a solution based on crowdsourced data to (i) detect changes in the map independent of the type of change, (ii) automatically trigger map update jobs for parts of the map, and (iii) create and integrate map patches to keep the map always up to date. The developed solution provides a crowdsourced up to date HD map to make reliable prior information on lane markings and road edges available to automated driving functions.
ER  - 

TY  - CONF
TI  - Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2295
EP  - 2301
AU  - A. Frickenstein
AU  - M. -R. Vemparala
AU  - J. Mayr
AU  - N. -S. Nagaraja
AU  - C. Unger
AU  - F. Tombari
AU  - W. Stechele
PY  - 2020
KW  - embedded systems
KW  - field programmable gate arrays
KW  - image segmentation
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - traffic engineering computing
KW  - binarized driveable area detection network
KW  - autonomous driving
KW  - ground-plane detection
KW  - obstacle detection
KW  - maneuver planning
KW  - over-parameterized networks
KW  - slim binary networks
KW  - binary weights
KW  - binary dilated convolutions
KW  - binary DAD-Net
KW  - semantic segmentation networks
KW  - FPGA
KW  - memory size 0.9 MByte
KW  - Convolutional codes
KW  - Task analysis
KW  - Semantics
KW  - Decoding
KW  - Training
KW  - Autonomous vehicles
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197119
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Driveable area detection is a key component for various applications in the field of autonomous driving (AD), such as ground-plane detection, obstacle detection and maneuver planning. Additionally, bulky and over-parameterized networks can be easily forgone and replaced with smaller networks for faster inference on embedded systems. The driveable area detection, posed as a two class segmentation task, can be efficiently modeled with slim binary networks. This paper proposes a novel binarized driveable area detection network (binary DAD-Net), which uses only binary weights and activations in the encoder, the bottleneck, and the decoder part. The latent space of the bottleneck is efficiently increased (√ó32‚Üí√ó16 downsampling) through binary dilated convolutions, learning more complex features. Along with automatically generated training data, the binary DAD-Net outperforms state-of-the-art semantic segmentation networks on public datasets. In comparison to a full-precision model, our approach has a √ó14.3 reduced compute complexity on an FPGA and it requires only 0.9MB memory resources. Therefore, commodity SIMD-based AD-hardware is capable of accelerating the binary DAD-Net.
ER  - 

TY  - CONF
TI  - UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2310
EP  - 2316
AU  - W. Wen
AU  - Y. Zhou
AU  - G. Zhang
AU  - S. Fahandezh-Saadi
AU  - X. Bai
AU  - W. Zhan
AU  - M. Tomizuka
AU  - L. -T. Hsu
PY  - 2020
KW  - cameras
KW  - image matching
KW  - image registration
KW  - inertial navigation
KW  - optical radar
KW  - satellite navigation
KW  - urban canyon
KW  - urban terrain
KW  - Hong Kong
KW  - San Francisco
KW  - IMU
KW  - GNSS-based solutions
KW  - LIDAR
KW  - global navigation satellite system
KW  - urban scene localization
KW  - urban scene mapping
KW  - inertia measurement units
KW  - camera-based methods
KW  - inertia navigation
KW  - visual feature matching
KW  - point cloud registration
KW  - full sensor suite dataset
KW  - UrbanLoco
KW  - Global navigation satellite system
KW  - Cameras
KW  - Laser radar
KW  - Urban areas
KW  - Robot sensing systems
KW  - Trajectory
KW  - Satellites
DO  - 10.1109/ICRA40945.2020.9196526
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mapping and localization is a critical module of autonomous driving, and significant achievements have been reached in this field. Beyond Global Navigation Satellite System (GNSS), research in point cloud registration, visual feature matching, and inertia navigation has greatly enhanced the accuracy and robustness of mapping and localization in different scenarios. However, highly urbanized scenes are still challenging: LIDAR- and camera-based methods perform poorly with numerous dynamic objects; the GNSS-based solutions experience signal loss and multi-path problems; the inertia measurement units (IMU) suffer from drifting. Unfortunately, current public datasets either do not adequately address this urban challenge or do not provide enough sensor information related to map-ping and localization. Here we present UrbanLoco: a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers. Our dataset includes a wide variety of urban terrains: urban canyons, bridges, tunnels, sharp turns, etc. More importantly, our dataset includes information from LIDAR, cameras, IMU, and GNSS receivers. Now the dataset is publicly available through the link in the footnote 1.
ER  - 

TY  - CONF
TI  - Map As the Hidden Sensor: Fast Odometry-Based Global Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2317
EP  - 2323
AU  - C. Peng
AU  - D. Weikersdorfer
PY  - 2020
KW  - distance measurement
KW  - mobile robots
KW  - path planning
KW  - sensors
KW  - tensors
KW  - odometry-based global localization
KW  - ambiguous observations
KW  - odometry drift
KW  - blind robots
KW  - robot state
KW  - belief tensor
KW  - map-corrected odometry localization
KW  - map traversability
KW  - robotics applications
KW  - hidden sensor
KW  - Robot sensing systems
KW  - Tensile stress
KW  - Trajectory
KW  - Robustness
KW  - Uncertainty
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197225
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.
ER  - 

TY  - CONF
TI  - Joint Human Pose Estimation and Stereo 3D Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2324
EP  - 2330
AU  - W. Deng
AU  - L. Bertoni
AU  - S. Kreiss
AU  - A. Alahi
PY  - 2020
KW  - neural nets
KW  - pose estimation
KW  - stereo image processing
KW  - 3D localization task
KW  - KITTI dataset
KW  - stereo 3D localization
KW  - neural network architecture
KW  - stereo imaging
KW  - human body
KW  - joint human pose estimation
KW  - image stereo pair
KW  - stereo pose dataset
KW  - Three-dimensional displays
KW  - Correlation
KW  - Two dimensional displays
KW  - Pose estimation
KW  - Uncertainty
KW  - Decoding
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197069
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an end-to-end trainable Neural Network architecture for stereo imaging that jointly locates and estimates human body poses in 3D. Our method defines a 2D pose for each human in a stereo pair of images and uses a correlation layer with a composite field to associate each left-right pair of joints. In absence of a stereo pose dataset, we show that we can train our method with synthetic data only and test it on real-world images (i.e., our training stage is domain invariant). Our method is particularly suitable for autonomous vehicles. We achieve state-of-the-art results for the 3D localization task on the challenging real-world KITTI dataset while running four times faster.
ER  - 

TY  - CONF
TI  - Self-Supervised Deep Pose Corrections for Robust Visual Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2331
EP  - 2337
AU  - B. Wagstaff
AU  - V. Peretroukhin
AU  - J. Kelly
PY  - 2020
KW  - distance measurement
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - regression analysis
KW  - stereo image processing
KW  - self-supervised deep pose corrections
KW  - robust visual odometry
KW  - data-driven learning
KW  - six-degrees-of-freedom ground truth
KW  - self-supervised DPC network
KW  - stereo odometry estimators
KW  - pose corrections regression
KW  - monocular odometry estimators
KW  - Image reconstruction
KW  - Cameras
KW  - Training
KW  - Lighting
KW  - Pipelines
KW  - Robustness
KW  - Visual odometry
DO  - 10.1109/ICRA40945.2020.9197562
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a self-supervised deep pose correction (DPC) network that applies pose corrections to a visual odometry estimator to improve its accuracy. Instead of regressing inter-frame pose changes directly, we build on prior work that uses data-driven learning to regress pose corrections that account for systematic errors due to violations of modelling assumptions. Our self-supervised formulation removes any requirement for six-degrees-of-freedom ground truth and, in contrast to expectations, often improves overall navigation accuracy compared to a supervised approach. Through extensive experiments, we show that our self-supervised DPC network can significantly enhance the performance of classical monocular and stereo odometry estimators and substantially out-performs state-of-the-art learning-only approaches.
ER  - 

TY  - CONF
TI  - Ultra-High-Accuracy Visual Marker for Indoor Precise Positioning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2338
EP  - 2343
AU  - H. Tanaka
PY  - 2020
KW  - attitude control
KW  - attitude measurement
KW  - autonomous aerial vehicles
KW  - indoor navigation
KW  - mobile robots
KW  - position control
KW  - robot vision
KW  - local positioning
KW  - marker coordinate system
KW  - high-accuracy global positioning
KW  - ultra-high-accuracy visual marker
KW  - indoor precise positioning
KW  - indoor positioning
KW  - indoor mobile robots
KW  - drones
KW  - general-purpose technology
KW  - attitude measurement
KW  - multiple dynamic moires
KW  - lenticular lens
KW  - attitude estimation error
KW  - marker position error
KW  - reprojection error
KW  - size 10.0 m
KW  - size 1.0 cm
KW  - size 10.0 cm
KW  - attitude accuracy
KW  - Visualization
KW  - Lenses
KW  - Position measurement
KW  - Cameras
KW  - Measurement uncertainty
KW  - Pose estimation
DO  - 10.1109/ICRA40945.2020.9196535
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Indoor positioning technology is essential for indoor mobile robots and drones. However, there has never been a general-purpose technology or infrastructure that enables indoor positioning with an accuracy of less than 10 cm. We have developed an attitude measurement method using multiple dynamic moires with a lenticular lens and developed an ultra- high-accuracy visual marker with an attitude estimation error of less than 0.1¬∞. We also developed a calculation method that minimizes the marker position error by reminimizing reprojection error using its good attitude accuracy. We proved that accurate local positioning with a position error of about 1 cm in a marker coordinate system is possible even when a marker is shot from a distance of 10 m. In addition, a demonstration test was performed in a public space, and it was shown that high-accuracy global positioning with a position error of about 10 cm is possible.
ER  - 

TY  - CONF
TI  - Accurate position tracking with a single UWB anchor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2344
EP  - 2350
AU  - Y. Cao
AU  - C. Yang
AU  - R. Li
AU  - A. Knoll
AU  - G. Beltrame
PY  - 2020
KW  - inertial systems
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - object tracking
KW  - observability
KW  - position measurement
KW  - SLAM (robots)
KW  - ultra wideband technology
KW  - velocity measurement
KW  - ultrawideband technology
KW  - UWB anchor
KW  - UWB range
KW  - moving robot tracking
KW  - position tracking
KW  - robotic applications
KW  - localization systems
KW  - optical tracking
KW  - 9 DoF inertial measurement unit
KW  - UWB ranging source
KW  - UWB technology
KW  - robot speed estimation
KW  - orientation estimation
KW  - IMU sensor
KW  - observability
KW  - extended Kalman filter
KW  - EKF
KW  - robot pose estimation
KW  - Robot sensing systems
KW  - Estimation
KW  - Observability
KW  - Velocity measurement
KW  - Distance measurement
KW  - Mobile robots
DO  - 10.1109/ICRA40945.2020.9197345
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate localization and tracking are a fundamental requirement for robotic applications. Localization systems like GPS, optical tracking, simultaneous localization and mapping (SLAM) are used for daily life activities, research, and commercial applications. Ultra-wideband (UWB) technology provides another venue to accurately locate devices both indoors and outdoors. In this paper, we study a localization solution with a single UWB anchor, instead of the traditional multi-anchor setup. Besides the challenge of a single UWB ranging source, the only other sensor we require is a low-cost 9 DoF inertial measurement unit (IMU). Under such a configuration, we propose continuous monitoring of UWB range changes to estimate the robot speed when moving on a line. Combining speed estimation with orientation estimation from the IMU sensor, the system becomes temporally observable. We use an Extended Kalman Filter (EKF) to estimate the pose of a robot. With our solution, we can effectively correct the accumulated error and maintain accurate tracking of a moving robot.
ER  - 

TY  - CONF
TI  - Preference-Based Learning for Exoskeleton Gait Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2351
EP  - 2357
AU  - M. Tucker
AU  - E. Novoseller
AU  - C. Kann
AU  - Y. Sui
AU  - Y. Yue
AU  - J. W. Burdick
AU  - A. D. Ames
PY  - 2020
KW  - gait analysis
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - optimisation
KW  - wearable robots
KW  - personalized gait optimization framework
KW  - lower-body exoskeleton
KW  - numerical objectives
KW  - preference-based interactive learning
KW  - CoSpar algorithm
KW  - pairwise preferences
KW  - exoskeleton walking
KW  - nonintuitive behavior
KW  - numerical feedback
KW  - human walking trajectory features
KW  - user-preferred parameters
KW  - adapting personalizing exoskeletons
KW  - exoskeleton gait optimization
KW  - Exoskeletons
KW  - Legged locomotion
KW  - Optimization
KW  - Bayes methods
KW  - Reliability
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196661
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a personalized gait optimization framework for lower-body exoskeletons. Rather than optimizing numerical objectives such as the mechanical cost of transport, our approach directly learns from user prefer-ences, e.g., for comfort. Building upon work in preference-based interactive learning, we present the CoSpar algorithm. CoSpar prompts the user to give pairwise preferences between trials and suggest improvements; as exoskeleton walking is a non-intuitive behavior, users can provide preferences more easily and reliably than numerical feedback. We show that CoSpar performs competitively in simulation and demonstrate a prototype implementation of CoSpar on a lower-body exoskeleton to optimize human walking trajectory features. In the experiments, CoSpar consistently found user-preferred parameters of the exoskeleton's walking gait, which suggests that it is a promising starting point for adapting and personalizing exoskeletons (or other assistive devices) to individual users.
ER  - 

TY  - CONF
TI  - Adaptive Neural Trajectory Tracking Control for Flexible-Joint Robots with Online Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2358
EP  - 2364
AU  - S. Chen
AU  - J. T. Wen
PY  - 2020
KW  - actuators
KW  - adaptive control
KW  - backpropagation
KW  - control system synthesis
KW  - feedforward
KW  - flexible manipulators
KW  - manipulator dynamics
KW  - neurocontrollers
KW  - nonlinear control systems
KW  - position control
KW  - stability
KW  - online backpropagation
KW  - collaborative robots
KW  - multilayer neural network
KW  - control architecture
KW  - flexible joint dynamics
KW  - control bandwidth
KW  - control design
KW  - space manipulators
KW  - online learning
KW  - flexible-joint robots
KW  - adaptive neural trajectory tracking control
KW  - series-elastic joint actuators
KW  - joint flexibility
KW  - Baxter robot
KW  - commanded joint position
KW  - outer loop control
KW  - nonlinear basis functions
KW  - internal weights
KW  - tracking error
KW  - output layer weights
KW  - linear output layer
KW  - robot dynamics
KW  - linear-in-parameter representation
KW  - feedforward control
KW  - approximate unknown dynamics
KW  - Artificial neural networks
KW  - Trajectory
KW  - Manipulator dynamics
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9197051
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collaborative robots and space manipulators contain significant joint flexibility. It complicates the control design, compromises the control bandwidth, and limits the tracking accuracy. The imprecise knowledge of the flexible joint dynamics compounds the challenge. In this paper, we present a new control architecture for controlling flexible-joint robots. Our approach uses a multi-layer neural network to approximate unknown dynamics needed for the feedforward control. The network may be viewed as a linear-in-parameter representation of the robot dynamics, with the nonlinear basis of the robot dynamics connected to the linear output layer. The output layer weights are updated based on the tracking error and the nonlinear basis. The internal weights of the nonlinear basis are updated by online backpropagation to further reduce the tracking error. To use time scale separation to reduce the coupling of the two steps - the update of the internal weights is at a lower rate compared to the update of the output layer weights. With the update of the output layer weights, our controller adapts quickly to the unknown dynamics change and disturbances (such as attaching a load). The update of the internal weights would continue to improve the converge of the nonlinear basis functions. We show the stability of the proposed scheme under the "outer loop" control, where the commanded joint position is considered as the control input. Simulation and physical experiments are conducted to demonstrate the performance of the proposed controller on a Baxter robot, which exhibits significant joint flexibility due to the series-elastic joint actuators.
ER  - 

TY  - CONF
TI  - BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2365
EP  - 2371
AU  - F. Lin
AU  - C. Fu
AU  - Y. He
AU  - F. Guo
AU  - Q. Tang
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - remotely operated vehicles
KW  - target tracking
KW  - video signal processing
KW  - unmanned aerial vehicle tracking scenarios
KW  - high computational efficiency
KW  - UAV tracking process
KW  - viewpoint variations
KW  - background appearance
KW  - CF-based trackers
KW  - ideal tracker
KW  - object position
KW  - response-based errors
KW  - forward errors
KW  - backward errors
KW  - current training sample
KW  - historical training samples
KW  - BiCF
KW  - response-based bidirectional incongruity error
KW  - UAV datasets
KW  - UAV123
KW  - bidirectional incongruity-aware correlation filter
KW  - Unmanned aerial vehicles
KW  - Training
KW  - Target tracking
KW  - Correlation
KW  - Robustness
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196530
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filters (CFs) have shown excellent performance in unmanned aerial vehicle (UAV) tracking scenarios due to their high computational efficiency. During the UAV tracking process, viewpoint variations are usually accompanied by changes in the object and background appearance, which poses a unique challenge to CF-based trackers. Since the appearance is gradually changing over time, an ideal tracker can not only forward predict the object position but also backtrack to locate its position in the previous frame. There exist response-based errors in the reversibility of the tracking process containing the information on the changes in appearance. However, some existing methods do not consider the forward and backward errors based on while using only the current training sample to learn the filter. For other ones, the applicants of considerable historical training samples impose a computational burden on the UAV. In this work, a novel bidirectional incongruity-aware correlation filter (BiCF) is proposed. By integrating the response-based bidirectional incongruity error into the CF, BiCF can Efficiently learn the changes in appearance and suppress the inconsistent error. Extensive experiments on 243 challenging sequences from three UAV datasets (UAV123, UAVDT, and DTB70) are conducted to demonstrate that BiCF favorably outperforms other 25 state-of-the-art trackers and achieves a real-time speed of 45.4 FPS on a single CPU, which can be applied in UAV Efficiently.
ER  - 

TY  - CONF
TI  - Adaptive Unknown Object Rearrangement Using Low-Cost Tabletop Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2372
EP  - 2378
AU  - C. -Y. Chai
AU  - W. -H. Peng
AU  - S. -L. Tsao
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - tabletop environment
KW  - object rearrangement
KW  - low-cost tabletop robot
KW  - object rearrangement planning
KW  - learning-based methods
KW  - single-step interaction
KW  - adaptive learning procedure
KW  - size 3.5 cm
KW  - Physics
KW  - Engines
KW  - Task analysis
KW  - Optimization
KW  - Robots
KW  - Adaptation models
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197356
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Studies on object rearrangement planning typically consider known objects. Some learning-based methods can predict the movement of an unknown object after single-step interaction, but require intermediate targets, which are generated manually, to achieve the rearrangement task. In this work, we propose a framework for unknown object rearrangement. Our system first models an object through a small-amount of identification actions and adjust the model parameters during task execution. We implement the proposed framework based on a low-cost tabletop robot (under 180 USD) to demonstrate the advantages of using a physics engine to assist action prediction. Experimental results reveal that after running our adaptive learning procedure, the robot can successfully arrange a novel object using an average of five discrete pushes on our tabletop environment and satisfy a precise 3.5 cm translation and 5¬∞ rotation criterion.
ER  - 

TY  - CONF
TI  - Unsupervised Learning and Exploration of Reachable Outcome Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2379
EP  - 2385
AU  - G. Paolo
AU  - A. Laflaqui√®re
AU  - A. Coninx
AU  - S. Doncieux
PY  - 2020
KW  - search problems
KW  - unsupervised learning
KW  - reachable outcome space
KW  - sparse rewards settings
KW  - reinforcement learning
KW  - learning process
KW  - search strategy
KW  - TAXONS
KW  - task agnostic exploration
KW  - population-based divergent-search approach
KW  - diverse policies
KW  - high-dimensional observation
KW  - task-specific information
KW  - low-dimensional outcome space
KW  - learned outcome space
KW  - ground-truth outcome space
KW  - unsupervised learning
KW  - Task analysis
KW  - Robots
KW  - Training
KW  - Space exploration
KW  - Aerospace electronics
KW  - Extraterrestrial measurements
DO  - 10.1109/ICRA40945.2020.9196819
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Performing Reinforcement Learning in sparse rewards settings, with very little prior knowledge, is a challenging problem since there is no signal to properly guide the learning process. In such situations, a good search strategy is fundamental. At the same time, not having to adapt the algorithm to every single problem is very desirable. Here we introduce TAXONS, a Task Agnostic eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on a population-based divergent-search approach, it learns a set of diverse policies directly from high-dimensional observations, without any task-specific information. TAXONS builds a repertoire of policies while training an autoencoder on the high-dimensional observation of the final state of the system to build a low-dimensional outcome space. The learned outcome space, combined with the reconstruction error, is used to drive the search for new policies. Results show that TAXONS can find a diverse set of controllers, covering a good part of the ground-truth outcome space, while having no information about such space.
ER  - 

TY  - CONF
TI  - Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2386
EP  - 2392
AU  - C. D. McKinnon
AU  - A. P. Schoellig
PY  - 2020
KW  - mobile robots
KW  - predictive control
KW  - probability
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - stochastic systems
KW  - model error
KW  - receding horizon control
KW  - repetitive path-following task
KW  - robot dynamics
KW  - simple learned dynamics model
KW  - MPC horizon
KW  - stochastic MPC
KW  - prediction horizon
KW  - online model learning
KW  - ground robot
KW  - context-aware cost shaping
KW  - stochastic model predictive control
KW  - Robots
KW  - Computational modeling
KW  - Predictive models
KW  - Aerodynamics
KW  - Cost function
KW  - Task analysis
KW  - Stochastic processes
DO  - 10.1109/ICRA40945.2020.9197521
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to enable a robot using stochastic Model Predictive Control (MPC) to achieve high performance on a repetitive path-following task. In particular, we consider the case where the accuracy of the model for robot dynamics varies significantly over the path-motivated by the fact that the models used in MPC must be computationally efficient, which limits their expressive power. Our approach is based on correcting the cost predicted using a simple learned dynamics model over the MPC horizon. This discourages the controller from taking actions that lead to higher cost than would have been predicted using the dynamics model. In addition, stochastic MPC provides a quantitative measure of safety by limiting the probability of violating state and input constraints over the prediction horizon. Our approach is unique in that it combines both online model learning and cost learning over the prediction horizon and is geared towards operating a robot in changing conditions. We demonstrate our algorithm in simulation and experiment on a ground robot that uses a stereo camera for localization.
ER  - 

TY  - CONF
TI  - Aortic 3D Deformation Reconstruction using 2D X-ray Fluoroscopy and 3D Pre-operative Data for Endovascular Interventions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2393
EP  - 2399
AU  - Y. Zhang
AU  - L. Zhao
AU  - S. Huang
PY  - 2020
KW  - biomedical MRI
KW  - catheters
KW  - computerised tomography
KW  - deformation
KW  - diagnostic radiography
KW  - image reconstruction
KW  - image registration
KW  - medical image processing
KW  - phantoms
KW  - live 3D aortic deformation
KW  - static 3D model
KW  - stereo images
KW  - reconstruction process
KW  - deformation graph approach
KW  - reconstruction accuracy
KW  - aortic 3D deformation reconstruction
KW  - 2D X-ray fluoroscopy
KW  - current clinical endovascular interventions
KW  - catheter manipulation
KW  - aortic 3D surface
KW  - deformation reconstruction frameworks
KW  - 3D intraoperative guidance
KW  - Three-dimensional displays
KW  - Strain
KW  - Solid modeling
KW  - Image reconstruction
KW  - X-ray imaging
KW  - Two dimensional displays
KW  - Deformable models
KW  - aortic deformation reconstruction
KW  - fluoroscopy
KW  - endovascular interventions
DO  - 10.1109/ICRA40945.2020.9197410
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current clinical endovascular interventions rely on 2D guidance for catheter manipulation. Although an aortic 3D surface is available from the pre-operative CT/MRI imaging, it cannot be used directly as a 3D intra-operative guidance since the vessel will deform during the procedure. This paper aims to reconstruct the live 3D aortic deformation by fusing the static 3D model from the pre-operative data and the 2D live imaging from fluoroscopy. In contrast to some existing deformation reconstruction frameworks which require 3D observations such as RGB-D or stereo images, fluoroscopy only presents 2D information. In the proposed framework, a 2D-3D registration is performed and the reconstruction process is formulated as a non-linear optimization problem based on the deformation graph approach. Detailed simulations and phantom experiments are conducted and the result demonstrates the reconstruction accuracy and robustness, as well as the potential clinical value of this framework.
ER  - 

TY  - CONF
TI  - Design and Kinematic Modeling of a Novel Steerable Needle for Image-Guided Insertion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2400
EP  - 2406
AU  - Y. Chen
AU  - H. Yang
AU  - X. Liu
AU  - K. Xu
PY  - 2020
KW  - biomedical ultrasonics
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - path planning
KW  - pose estimation
KW  - surgery
KW  - tumours
KW  - image-guided insertion
KW  - novel steerable needle
KW  - kinematics model
KW  - passive needle tip articulation
KW  - needle path consistency
KW  - articulated tip
KW  - needle design
KW  - tissue mechanics
KW  - needle-tissue interaction
KW  - needle placement
KW  - percutaneous tumor ablation
KW  - biopsy tumor ablation
KW  - Needles
KW  - Kinematics
KW  - Fasteners
KW  - Path planning
KW  - Shape
KW  - Electron tubes
KW  - Laser beam cutting
DO  - 10.1109/ICRA40945.2020.9196960
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Needle-based procedures, such as biopsy and percutaneous tumor ablation, highly depend on the accuracy of needle placement. The accuracy is significantly affected by the needle-tissue interaction no matter what needles (straight or steerable) are used. Due to the unknown tissue mechanics, it is challenging to achieve high accuracy in practice. This paper hence proposes a needle design with an articulated tip for increased steerability and improved needle path consistency. Due to the passive needle tip articulation, tissue mechanics always plays a dominant role such that the needle creates similar paths with approximately piece-wise constant curvature in different tissues. Kinematics model for the proposed needle is presented. The algorithms of path planning and needle tip pose estimation under external imaging modality are developed. Experimental verifications were conducted to demonstrate the needle's steerability as well as the target-reaching capability with obstacles avoidance.
ER  - 

TY  - CONF
TI  - Robotic needle insertion in moving soft tissues using constraint-based inverse Finite Element simulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2407
EP  - 2413
AU  - P. Baksic
AU  - H. Courtecuisse
AU  - C. Duriez
AU  - B. Bayle
PY  - 2020
KW  - biological tissues
KW  - end effectors
KW  - finite element analysis
KW  - inverse problems
KW  - medical image processing
KW  - medical robotics
KW  - needles
KW  - robotic needle insertion
KW  - soft tissues
KW  - robotic steering
KW  - flexible needle
KW  - predefined path
KW  - inverse problem
KW  - robot end effector
KW  - constraint-based formulation
KW  - simulation-guided needle insertion
KW  - direct simulation
KW  - respiratory motion
KW  - numerical simulation
KW  - constraint-based inverse finite element simulation
KW  - Needles
KW  - Robots
KW  - Mathematical model
KW  - Linear programming
KW  - Computational modeling
KW  - Numerical models
KW  - Inverse problems
DO  - 10.1109/ICRA40945.2020.9197515
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a method for robotic steering of a flexible needle inside moving and deformable tissues. The method relies on a set of objective functions allowing to automatically steer the needle along a predefined path. In order to follow the desired trajectory, an inverse problem linking the motion of the robot end effector with the objective functions is solved using a Finite Element simulation. The main contribution of the article is the new constraint-based formulation of the objective functions allowing to: 1) significantly reduce the computation time; 2) increase the accuracy and stability of the simulation-guided needle insertion. The method is illustrated, and its performances are characterized in a realistic framework, using a direct simulation of the respiratory motion generated from in vivo data of a pig. Despite the highly non-linear behavior of the numerical simulation and the significant deformations occurring during the insertion, the obtained performances enable the possibility to follow the trajectory with the desired accuracy for medical purpose.
ER  - 

TY  - CONF
TI  - Collaborative Robot-Assisted Endovascular Catheterization with Generative Adversarial Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2414
EP  - 2420
AU  - W. Chi
AU  - G. Dagnino
AU  - T. M. Y. Kwok
AU  - A. Nguyen
AU  - D. Kundrat
AU  - M. E. M. K. Abdelaziz
AU  - C. Riga
AU  - C. Bicknell
AU  - G. -Z. Yang
PY  - 2020
KW  - blood vessels
KW  - catheters
KW  - diagnostic radiography
KW  - haemodynamics
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - pulsatile flow
KW  - surgery
KW  - reduced procedural duration
KW  - deep reinforcement learning technologies
KW  - complex endovascular tasks
KW  - reduced fatigue
KW  - cognitive workload
KW  - model-based approaches
KW  - model-free generative adversarial imitation learning
KW  - standard arterial catherization task
KW  - automation policies
KW  - catheter motions
KW  - collaborative robot-assisted endovascular catheterization
KW  - master-slave systems
KW  - clinical benefits
KW  - radiation doses
KW  - vascular anatomical model
KW  - Catheters
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Catheterization
KW  - Instruments
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196912
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Master-slave systems for endovascular catheterization have brought major clinical benefits including reduced radiation doses to the operators, improved precision and stability of the instruments, as well as reduced procedural duration. Emerging deep reinforcement learning (RL) technologies could potentially automate more complex endovascular tasks with enhanced success rates, more consistent motion and reduced fatigue and cognitive workload of the operators. However, the complexity of the pulsatile flows within the vasculature and non-linear behavior of the instruments hinder the use of model-based approaches for RL. This paper describes model-free generative adversarial imitation learning to automate a standard arterial catherization task. The automation policies have been trained in a pre-clinical setting. Detailed validation results show high success rates after skill transfer to a different vascular anatomical model. The quality of the catheter motions also shows less mean and maximum contact forces compared to manual-based approaches.
ER  - 

TY  - CONF
TI  - GA3C Reinforcement Learning for Surgical Steerable Catheter Path Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2429
EP  - 2435
AU  - A. Segato
AU  - L. Sestini
AU  - A. Castellano
AU  - E. De Momi
PY  - 2020
KW  - biomedical MRI
KW  - catheters
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - path planning
KW  - robot kinematics
KW  - trajectory smoothness
KW  - GA3C Reinforcement Learning
KW  - surgical steerable catheter path planning
KW  - path planning algorithms
KW  - steerable catheters
KW  - anatomical obstacles avoidance
KW  - insertion length
KW  - needle kinematics
KW  - smooth trajectories
KW  - path planning problem
KW  - reinforcement learning problem
KW  - trajectory planning model
KW  - optimal trajectories
KW  - obstacle clearance
KW  - kinematic constraints
KW  - MRI images processing
KW  - path planning model
KW  - curvilinear trajectories
KW  - RRT* algorithms
KW  - obstacle avoidance
KW  - Trajectory
KW  - Needles
KW  - Three-dimensional displays
KW  - Learning (artificial intelligence)
KW  - Catheters
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196954
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning algorithms for steerable catheters, must guarantee anatomical obstacles avoidance, reduce the insertion length and ensure the compliance with needle kinematics. The majority of the solutions in literature focuses on graph based or sampling based methods, both limited by the impossibility to directly obtain smooth trajectories. In this work we formulate the path planning problem as a reinforcement learning problem and show that the trajectory planning model, generated from the training, can provide the user with optimal trajectories in terms of obstacle clearance and kinematic constraints. We obtain 2D and 3D environments from MRI images processing and we implement a GA3C algorithm to create a path planning model, able to generalize on different patients anatomies. The curvilinear trajectories obtained from the model in 2D and 3D environments are compared to the ones obtained by A* and RRT* algorithms. Our method achieves state-of-the-art performances in terms of obstacle avoidance, trajectory smoothness and computational time proving this algorithm as valid planning method for complex environments.
ER  - 

TY  - CONF
TI  - MPC-based Controller with Terrain Insight for Dynamic Legged Locomotion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2436
EP  - 2442
AU  - O. Villarreal
AU  - V. Barasuol
AU  - P. M. Wensing
AU  - D. G. Caldwell
AU  - C. Semini
PY  - 2020
KW  - convolutional neural nets
KW  - hydraulic actuators
KW  - legged locomotion
KW  - neurocontrollers
KW  - predictive control
KW  - robot dynamics
KW  - on-board mapping
KW  - contact sequence task
KW  - convolutional neural network
KW  - model predictive controller
KW  - on-board sensing
KW  - MPC-based controller
KW  - terrain insight
KW  - dynamic legged locomotion
KW  - hydraulically actuated quadruped robot HyQReal
KW  - Legged locomotion
KW  - Trajectory
KW  - Task analysis
KW  - Computational modeling
KW  - Dynamics
KW  - Foot
DO  - 10.1109/ICRA40945.2020.9197312
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel control strategy for dynamic legged locomotion in complex scenarios that considers information about the morphology of the terrain in contexts when only on-board mapping and computation are available. The strategy is built on top of two main elements: first a contact sequence task that provides safe foothold locations based on a convolutional neural network to perform fast and continuous evaluation of the terrain in search of safe foothold locations; then a model predictive controller that considers the foothold locations given by the contact sequence task to optimize target ground reaction forces. We assess the performance of our strategy through simulations of the hydraulically actuated quadruped robot HyQReal traversing rough terrain under realistic on-board sensing and computing conditions.
ER  - 

TY  - CONF
TI  - An Adaptive Supervisory Control Approach to Dynamic Locomotion Under Parametric Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2443
EP  - 2449
AU  - P. Chand
AU  - S. Veer
AU  - I. Poulakakis
PY  - 2020
KW  - adaptive control
KW  - control system synthesis
KW  - feedback
KW  - legged locomotion
KW  - parameter estimation
KW  - robot dynamics
KW  - uncertain systems
KW  - feedback control
KW  - estimator design
KW  - dynamic locomotion
KW  - parametric uncertainty
KW  - robotic systems
KW  - parameter estimation
KW  - adaptive supervisory control
KW  - dynamically walking bipedal robot
KW  - Switches
KW  - Uncertainty
KW  - Robots
KW  - Supervisory control
KW  - Adaptive control
KW  - Libraries
DO  - 10.1109/ICRA40945.2020.9197120
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an adaptive control scheme for robotic systems that operate in the face of-potentially large-structured uncertainty. The proposed adaptive controller employs an on-line supervisor that utilizes logic-based switching among a finite set of controllers to identify uncertain parameters, and adapt the behavior of the system based on a current estimate of their value. To achieve this, the adaptive control approach in this paper combines on-line parameter estimation and feedback control while avoiding some of the inherent difficulties of classical adaptive control strategies. Furthermore, the proposed supervisory control architecture is modular as it relies on established "off-the-shelf" feedback control law and estimator design approaches, instead of cus-tomizing the overall design to the specific requirements of an adaptive control algorithm. We demonstrate the efficacy of the method on the problem of a dynamically-walking bipedal robot delivering a payload of unknown mass, and show that, by switching to the controller that is the "best" according to a current estimate of the uncertainty, the system maintains a low energy cost during its operation.
ER  - 

TY  - CONF
TI  - Joint Space Position/Torque Hybrid Control of the Quadruped Robot for Locomotion and Push Reaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2450
EP  - 2456
AU  - O. Sim
AU  - H. Jeong
AU  - J. Oh
AU  - M. Lee
AU  - K. Kyu Lee
AU  - H. -W. Park
AU  - J. -H. Oh
PY  - 2020
KW  - force control
KW  - legged locomotion
KW  - motion control
KW  - position control
KW  - robot dynamics
KW  - torque control
KW  - torque control
KW  - contact-force-control
KW  - joint space hybrid control
KW  - legged robot platform
KW  - hybrid control algorithm
KW  - robot displayed stability
KW  - mammal-type quadruped robot
KW  - dynamic locomotion
KW  - push reaction abilities
KW  - Cartesian space
KW  - external push disturbances
KW  - Force
KW  - Aerospace electronics
KW  - Legged locomotion
KW  - Torque
KW  - Robot kinematics
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197230
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a novel algorithm for joint space position/torque hybrid control of a mammal-type quadruped robot. With this control algorithm, the robot demonstrated both dynamic locomotion and push reaction abilities without the need for torque control in the ab/ad joints. Based on the tipping and slipping condition of the legged robot, we showed that reaction to a typical push in the horizontal direction does not require full contact-force-control in the frontal plane. Furthermore, we showed that position/torque hybrid control in Cartesian space is directly applicable to joint space hybrid control due to the joint configuration of the quadruped robot. We conducted experiments on our legged robot platform to verify the performance of our hybrid control algorithm. With this approach, the robot displayed stability while walking and reacting to external push disturbances.
ER  - 

TY  - CONF
TI  - Improved Performance on Moving-Mass Hopping Robots with Parallel Elasticity
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2457
EP  - 2463
AU  - E. Ambrose
AU  - A. D. Ames
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - legged locomotion
KW  - robot dynamics
KW  - springs (mechanical)
KW  - stability
KW  - mechanical design
KW  - robotic hopping
KW  - moving-mass hopping robots
KW  - hop heights
KW  - single-spring model
KW  - double-spring model
KW  - hopping effort
KW  - parallel spring
KW  - hybrid systems models
KW  - rigorous trajectory optimization method
KW  - one-dimensional hopping robot
KW  - actuator
KW  - single spring
KW  - moving-mass robot
KW  - stable hopping
KW  - ground contact
KW  - Springs
KW  - Actuators
KW  - Force
KW  - Robot kinematics
KW  - Dynamics
KW  - Jacobian matrices
DO  - 10.1109/ICRA40945.2020.9197070
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic Hopping is challenging from the perspective of both modeling the dynamics as well as the mechanical design due to the short period of ground contact in which to actuate on the world. Previous work has demonstrated stable hopping on a moving-mass robot, wherein a single spring was utilized below the body of the robot. This paper finds that the addition of a spring in parallel to the actuator greatly improves the performance of moving mass hopping robots. This is demonstrated through the design of a novel one-dimensional hopping robot. For this robot, a rigorous trajectory optimization method is developed using hybrid systems models with experimentally tuned parameters. Simulation results are used to study the effects of a parallel spring on energetic efficiency, stability, and hopping effort. We find that the double-spring model had 2.5x better energy efficiency than the single-spring model, and was able to hop using 40% less peak force from the actuator. Furthermore, the double-spring model produces stable hopping without the need for stabilizing controllers. These concepts are demonstrated experimentally on a novel hopping robot, wherein hop heights up to 40cm were achieved with comparable efficiency and stability.
ER  - 

TY  - CONF
TI  - Vision Aided Dynamic Exploration of Unstructured Terrain with a Small-Scale Quadruped Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2464
EP  - 2470
AU  - D. Kim
AU  - D. Carballo
AU  - J. Di Carlo
AU  - B. Katz
AU  - G. Bledt
AU  - B. Lim
AU  - S. Kim
PY  - 2020
KW  - collision avoidance
KW  - gait analysis
KW  - legged locomotion
KW  - robot dynamics
KW  - fully sensorized Mini-Cheetah quadruped robot
KW  - unstructured terrain
KW  - dynamic exploration
KW  - dynamic trotting
KW  - highly irregular terrain
KW  - obstacle avoidance
KW  - foothold adjustment
KW  - evaluation algorithms
KW  - effective filtering
KW  - MIT Mini-Cheetah
KW  - Intel RealSense sensors
KW  - jumping
KW  - dynamic locomotion
KW  - effective sensor integration
KW  - walking speed
KW  - obstacle clearance
KW  - narrow paths
KW  - cluttered environments
KW  - rough terrain locomotion capability
KW  - rescue scenarios
KW  - disaster response
KW  - mobile platforms
KW  - legged robots
KW  - small-scale quadruped robot
KW  - size 0.3 m
KW  - mass 9.0 kg
KW  - Cameras
KW  - Robot vision systems
KW  - Robot kinematics
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196777
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Legged robots have been highlighted as promising mobile platforms for disaster response and rescue scenarios because of their rough terrain locomotion capability. In cluttered environments, small robots are desirable as they can maneuver through small gaps, narrow paths, or tunnels. However small robots have their own set of difficulties such as limited space for sensors, limited obstacle clearance, and scaled-down walking speed. In this paper, we extensively address these difficulties via effective sensor integration and exploitation of dynamic locomotion and jumping. We integrate two Intel RealSense sensors into the MIT Mini-Cheetah, a 0.3 m tall, 9 kg quadruped robot. Simple and effective filtering and evaluation algorithms are used for foothold adjustment and obstacle avoidance. We showcase the exploration of highly irregular terrain using dynamic trotting and jumping with the small-scale, fully sensorized Mini-Cheetah quadruped robot.
ER  - 

TY  - CONF
TI  - Distributed Attack-Robust Submodular Maximization for Multi-Robot Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2479
EP  - 2485
AU  - L. Zhou
AU  - V. Tzoumas
AU  - G. J. Pappas
AU  - P. Tokekar
PY  - 2020
KW  - control system security
KW  - distributed algorithms
KW  - divide and conquer methods
KW  - multi-robot systems
KW  - optimisation
KW  - path planning
KW  - target tracking
KW  - distributed attack-robust submodular maximization
KW  - multirobot planning
KW  - swarm-robotics applications
KW  - multirobot motion
KW  - attack-robust algorithms
KW  - robust optimization
KW  - distributed robust maximization
KW  - DRM performance
KW  - multiple robots
KW  - denial-of-service attacks
KW  - DoS attacks
KW  - large-scale robotic applications
KW  - general-purpose distributed algorithm
KW  - divide-and-conquer approach
KW  - robot communication range
KW  - close-to-optimal performance
KW  - active target tracking
KW  - Planning
KW  - Target tracking
KW  - Robot kinematics
KW  - Partitioning algorithms
KW  - Robot sensing systems
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197243
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim to guard swarm-robotics applications against denial-of-service (DoS) attacks that result in withdrawals of robots. We focus on applications requiring the selection of actions for each robot, among a set of available ones, e.g., which trajectory to follow. Such applications are central in large-scale robotic applications, e.g., multi-robot motion planning for target tracking. But the current attack-robust algorithms are centralized, and scale quadratically with the problem size (e.g., number of robots). In this paper, we propose a general-purpose distributed algorithm towards robust optimization at scale, with local communications only. We name it distributed robust maximization (DRM). DRM proposes a divide-and-conquer approach that distributively partitions the problem among K cliques of robots. The cliques optimize in parallel, independently of each other. That way, DRM also offers computational speed-ups up to 1/K2 the running time of its centralized counterparts. K depends on the robots' communication range, which is given as input to DRM. DRM also achieves a close-to-optimal performance. We demonstrate DRM's performance in Gazebo and MATLAB simulations, in scenarios of active target tracking with multiple robots. We observe DRM achieves significant computational speed-ups (it is 3 to 4 orders faster) and, yet, nearly matches the tracking performance of its centralized counterparts.
ER  - 

TY  - CONF
TI  - Multirobot Patrolling Against Adaptive Opponents with Limited Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2486
EP  - 2492
AU  - C. D. Alvarenga
AU  - N. Basilico
AU  - S. Carpin
PY  - 2020
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - optimisation
KW  - multirobot patrolling
KW  - adaptive opponents
KW  - patrolling problem
KW  - multiple agents
KW  - time consuming
KW  - Robot kinematics
KW  - Task analysis
KW  - Delays
KW  - Optimization
KW  - Games
DO  - 10.1109/ICRA40945.2020.9197287
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study a patrolling problem where multiple agents are tasked with protecting an environment where one or more adversaries are trying to compromise targets of varying value. The objective of the patrollers is to move between targets to quickly spot when an attack is taking place and then diffuse it. Differently from most related literature, we do not assume that attackers have full knowledge of the strategies followed by the patrollers, but rather build a model at run time through repeated observations of how often they visit certain targets. We study three different solutions to this problem. The first two partition the environment using either a fast heuristic or an exact method that is significantly more time consuming. The third method, instead does not partition the environment, but rather lets every patroller roam over the entire environment. After having identified strengths and weaknesses of each method, we contrast their performances against attackers using different algorithms to decide whether to attack or not.
ER  - 

TY  - CONF
TI  - Distributed Optimization of Nonlinear, Non-Gaussian, Communication-Aware Information using Particle Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2493
EP  - 2499
AU  - S. Moon
AU  - E. W. Frew
PY  - 2020
KW  - entropy
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - wireless sensor networks
KW  - mobile robotic sensor networks
KW  - neighbor robots
KW  - conditional mutual information
KW  - communication properties
KW  - specific measurement set
KW  - particle methods
KW  - information computation
KW  - distributed optimization
KW  - local utility design
KW  - communication-aware information gathering
KW  - sampling procedures
KW  - entropy reduction
KW  - Robot sensing systems
KW  - Optimization
KW  - Mutual information
KW  - Planning
KW  - Atmospheric measurements
KW  - Particle measurements
DO  - 10.1109/ICRA40945.2020.9197404
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a distributed optimization framework and its local utility design for communication-aware information gathering by mobile robotic sensor networks. The main idea of the optimization is that each robot decides based on its local utility that considers the decisions of other neighbor robots higher in a given hierarchy. The local utility is designed as conditional mutual information that captures sensing and communication properties. Sampling procedures using a specific measurement set and particle methods are applied to compute the designed utility, which allows nonlinear, non-Gaussian properties of targets, sensing, and communication. Simulation results describe the presented distributed optimization shows more improved estimates and entropy reduction than another approach that does not consider communication properties. Simulation results also verify the presented distributed optimization using the described approach for information computation has better results than using other approaches that simplify the communication-aware information.
ER  - 

TY  - CONF
TI  - Targeted Drug Delivery: Algorithmic Methods for Collecting a Swarm of Particles with Uniform, External Forces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2508
EP  - 2514
AU  - A. T. Becker
AU  - S. P. Fekete
AU  - L. Huang
AU  - P. Keldenich
AU  - L. Kleist
AU  - D. Krupke
AU  - C. Rieck
AU  - A. Schmidt
PY  - 2020
KW  - computational complexity
KW  - deterministic algorithms
KW  - drug delivery systems
KW  - drugs
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optimisation
KW  - actuation steps
KW  - deterministic algorithms
KW  - targeted drug delivery
KW  - maze-like environment
KW  - vascular system
KW  - basic scenario
KW  - global external force
KW  - fluidic flow
KW  - deep learning
KW  - Magnetic resonance imaging
KW  - Robots
KW  - Tumors
KW  - Force
KW  - Blood
KW  - Electromagnetics
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9196551
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We investigate algorithmic approaches for targeted drug delivery in a complex, maze-like environment, such as a vascular system. The basic scenario is given by a large swarm of micro-scale particles ("agents") and a particular target region ("tumor") within a system of passageways. Agents are too small to contain on-board power or computation and are instead controlled by a global external force that acts uniformly on all particles, such as an applied fluidic flow or electromagnetic field. The challenge is to deliver all agents to the target region with a minimum number of actuation steps. We provide a number of results for this challenge. We show that the underlying problem is NP-hard, which explains why previous work did not provide provably efficient algorithms. We also develop a number of algorithmic approaches that greatly improve the worst-case guarantees for the number of required actuation steps. We evaluate our algorithmic approaches by a number of simulations, both for deterministic algorithms and searches supported by deep learning, which show that the performance is practically promising.
ER  - 

TY  - CONF
TI  - Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2515
EP  - 2521
AU  - G. Tang
AU  - W. Sun
AU  - K. Hauser
PY  - 2020
KW  - autonomous aerial vehicles
KW  - bang-bang control
KW  - duality (mathematics)
KW  - mobile robots
KW  - Newton method
KW  - nonlinear programming
KW  - path planning
KW  - sensitivity analysis
KW  - time optimal control
KW  - sensitivity analysis
KW  - NLP solvers
KW  - parametric optimization problems
KW  - quasiNewton method
KW  - geometric path
KW  - time-optimal velocity profile
KW  - hierarchical optimization
KW  - bang-bang control structure
KW  - nonlinear programming solvers
KW  - dynamic robotic vehicles
KW  - time-optimal trajectories
KW  - duality gap approach
KW  - UAV time-optimal trajectory
KW  - gap-free bilevel optimization
KW  - interior-point method
KW  - Trajectory
KW  - Cost function
KW  - Switches
KW  - Acceleration
KW  - Robustness
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196789
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Time-optimal trajectories for dynamic robotic vehicles are difficult to compute even for state-of-the-art nonlinear programming (NLP) solvers, due to nonlinearity and bang-bang control structure. This paper presents a bilevel optimization framework that addresses these problems by decomposing the spatial and temporal variables into a hierarchical optimization. Specifically, the original problem is divided into an inner layer, which computes a time-optimal velocity profile along a given geometric path, and an outer layer, which refines the geometric path by a Quasi-Newton method. The inner optimization is convex and efficiently solved by interior-point methods. The gradients of the outer layer can be analytically obtained using sensitivity analysis of parametric optimization problems. A novel contribution is to introduce a duality gap in the inner optimization rather than solving it to optimality; this lets the optimizer realize warm-starting of the interior-point method, avoids non-smoothness of the outer cost function caused by active inequality constraint switching. Like prior bilevel frameworks, this method is guaranteed to return a feasible solution at any time, but converges faster than gap-free bilevel optimization. Numerical experiments on a drone model with velocity and acceleration limits show that the proposed method performs faster and more robustly than gap-free bilevel optimization and general NLP solvers.
ER  - 

TY  - CONF
TI  - Constrained Sampling-based Trajectory Optimization using Stochastic Approximation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2522
EP  - 2528
AU  - G. I. Boutselis
AU  - Z. Wang
AU  - E. A. Theodorou
PY  - 2020
KW  - approximation theory
KW  - discrete systems
KW  - gradient methods
KW  - optimal control
KW  - optimisation
KW  - sampling methods
KW  - stochastic processes
KW  - constrained problems
KW  - stochastic search
KW  - box control constraints
KW  - nonlinear state constraints
KW  - discrete dynamical systems
KW  - sampling-based trajectory optimization methodology
KW  - stochastic approximation
KW  - constrained sampling-based trajectory optimization
KW  - nonsmooth penalty functions
KW  - control inputs
KW  - truncated parameterized distributions
KW  - Heuristic algorithms
KW  - Trajectory optimization
KW  - Convergence
KW  - Approximation algorithms
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197284
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a sampling-based trajectory optimization methodology for constrained problems. We extend recent works on stochastic search to deal with box control constraints, as well as nonlinear state constraints for discrete dynamical systems. Regarding the former, our strategy is to optimize over truncated parameterized distributions on control inputs. Furthermore, we show how non-smooth penalty functions can be incorporated into our framework to handle state constraints. Simulations on cartpole and quadcopter show that our approach outperforms previous methods on constrained sampling-based optimization, in terms of quality of solutions and convergence speed.
ER  - 

TY  - CONF
TI  - Learning Control Policies from Optimal Trajectories
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2529
EP  - 2535
AU  - C. Zelch
AU  - J. Peters
AU  - O. von Stryk
PY  - 2020
KW  - feedback
KW  - Gaussian processes
KW  - industrial robots
KW  - learning systems
KW  - mobile robots
KW  - nonlinear control systems
KW  - optimal control
KW  - optimisation
KW  - pendulums
KW  - trajectory control
KW  - time-dependent optimal trajectories
KW  - optimal feedback control policies
KW  - real-world systems
KW  - frequent correction
KW  - model errors
KW  - optimal reference trajectories
KW  - high dimensional state space
KW  - learning control policies
KW  - optimally control robotic systems
KW  - high dimensional nonlinear system dynamic models
KW  - Gaussian processes
KW  - swing-up problem
KW  - underactuated pendulum
KW  - energy-minimal point-to-point movement
KW  - 3-DOF industrial robot
KW  - Trajectory
KW  - Computational modeling
KW  - Task analysis
KW  - Numerical models
KW  - Robots
KW  - Feedback control
KW  - Optimal control
DO  - 10.1109/ICRA40945.2020.9196791
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to optimally control robotic systems offers significant advantages for their performance. While time-dependent optimal trajectories can numerically be computed for high dimensional nonlinear system dynamic models, constraints and objectives, finding optimal feedback control policies for such systems is hard. This is unfortunate, as without a policy, the control of real-world systems requires frequent correction or replanning to compensate for disturbances and model errors.In this paper, a feedback control policy is learned from a set of optimal reference trajectories using Gaussian processes. Information from existing trajectories and the current policy is used to find promising start points for the computation of further optimal trajectories. This aspect is important as it avoids exhaustive sampling of the complete state space, which is impractical due to the high dimensional state space, and to focus on the relevant region.The presented method has been applied in simulation to a swing-up problem of an underactuated pendulum and an energy-minimal point-to-point movement of a 3-DOF industrial robot.
ER  - 

TY  - CONF
TI  - Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2536
EP  - 2542
AU  - C. Mastalli
AU  - R. Budhiraja
AU  - W. Merkt
AU  - G. Saurel
AU  - B. Hammoud
AU  - M. Naveau
AU  - J. Carpentier
AU  - L. Righetti
AU  - S. Vijayakumar
AU  - N. Mansard
PY  - 2020
KW  - dynamic programming
KW  - iterative methods
KW  - motion control
KW  - optimal control
KW  - Crocoddyl
KW  - contact robot control by differential dynamic library
KW  - open-source framework
KW  - multicontact optimal control
KW  - state trajectory
KW  - control policy
KW  - sparse analytical derivatives
KW  - differential geometry
KW  - floating-base systems
KW  - FDDP
KW  - computation time
KW  - infeasible state-control trajectories
KW  - highly-dynamic maneuvers
KW  - feasibility-driven differential dynamic programming
KW  - Optimal control
KW  - Trajectory
KW  - Optimization
KW  - Heuristic algorithms
KW  - Dynamic programming
KW  - Robots
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9196673
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl efficiently computes the state trajectory and the control policy for a given predefined sequence of contacts. Its efficiency is due to the use of sparse analytical derivatives, exploitation of the problem structure, and data sharing. It employs differential geometry to properly describe the state of any geometrical system, e.g. floating-base systems. Additionally, we propose a novel optimal control algorithm called Feasibility-driven Differential Dynamic Programming (FDDP). Our method does not add extra decision variables which often increases the computation time per iteration due to factorization. FDDP shows a greater globalization strategy compared to classical Differential Dynamic Programming (DDP) algorithms. Concretely, we propose two modifications to the classical DDP algorithm. First, the backward pass accepts infeasible state-control trajectories. Second, the rollout keeps the gaps open during the early "exploratory" iterations (as expected in multipleshooting methods with only equality constraints). We showcase the performance of our framework using different tasks. With our method, we can compute highly-dynamic maneuvers (e.g. jumping, front-flip) within few milliseconds.
ER  - 

TY  - CONF
TI  - Grasp for Stacking via Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2543
EP  - 2549
AU  - J. Zhang
AU  - W. Zhang
AU  - R. Song
AU  - L. Ma
AU  - Y. Li
PY  - 2020
KW  - grippers
KW  - industrial manipulators
KW  - learning systems
KW  - neurocontrollers
KW  - optimal control
KW  - stacking
KW  - deep reinforcement learning
KW  - integrated robotic arm system
KW  - object grasping
KW  - model-free deep Q-learning method
KW  - grasping-stacking task
KW  - GSN
KW  - grasping for stacking network
KW  - industrial environments
KW  - GNet
KW  - optimal location
KW  - long-range planning
KW  - Grasping
KW  - Stacking
KW  - Task analysis
KW  - Feature extraction
KW  - Manipulators
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197508
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.
ER  - 

TY  - CONF
TI  - CAGE: Context-Aware Grasping Engine
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2550
EP  - 2556
AU  - W. Liu
AU  - A. Daruna
AU  - S. Chernova
PY  - 2020
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - ubiquitous computing
KW  - context-aware grasping engine
KW  - semantic grasping
KW  - stable grasps
KW  - specific object manipulation tasks
KW  - task constraints
KW  - semantic representation
KW  - grasp contexts
KW  - object states
KW  - memorization
KW  - semantic grasps
KW  - CAGE
KW  - statistically significant margins
KW  - memorization balancing
KW  - robot
KW  - Semantics
KW  - Task analysis
KW  - Grasping
KW  - Feature extraction
KW  - Robots
KW  - Cognition
KW  - Context modeling
DO  - 10.1109/ICRA40945.2020.9197289
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.
ER  - 

TY  - CONF
TI  - Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2588
EP  - 2594
AU  - A. Wolff
AU  - S. Praisler
AU  - I. Tcenov
AU  - G. Gilboa
PY  - 2020
KW  - image colour analysis
KW  - image reconstruction
KW  - image sampling
KW  - random processes
KW  - data-driven approach
KW  - depth sampling
KW  - depth acquisition
KW  - active illumination
KW  - autonomous navigation
KW  - robotic navigation
KW  - mechanical templates
KW  - sampling templates
KW  - autonomous vehicles
KW  - solid-state depth sensors
KW  - adaptive framework
KW  - simple reconstruction algorithm
KW  - generic reconstruction algorithm
KW  - sampling reconstruction algorithm
KW  - random sampling
KW  - depth completion algorithms
KW  - single-pixel prototype sampler
KW  - RGB sampling strategies
KW  - single depth sampling strategies
KW  - piecewise planar depth model
KW  - superpixel sampler
KW  - SPS
KW  - Image reconstruction
KW  - Adaptation models
KW  - Estimation
KW  - Cameras
KW  - Robot sensing systems
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197191
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast and adaptive scans. In this paper, we propose an adaptive, image-driven, fast, sampling and reconstruction strategy. First, we formulate a piece-wise planar depth model and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, adaptive sampling strategies with about 20-60 piece-wise planar structures can approximate well a depth map. This translates to requiring a single depth sample for every 1200 RGB samples (less than 0.1%), providing strong motivation to investigate an adaptive framework. Second, we introduce SPS (Super-Pixel Sampler), a simple, generic, sampling and reconstruction algorithm, based on super-pixels. Our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Third, we propose an extremely simple and fast reconstruction for our sampler. It achieves state-of-the-art results, compared to complex image- guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel prototype sampler built in our lab illustrates the concept.
ER  - 

TY  - CONF
TI  - Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2595
EP  - 2601
AU  - E. Heiden
AU  - Z. Liu
AU  - R. K. Ramachandran
AU  - G. S. Sukhatme
PY  - 2020
KW  - calibration
KW  - computerised instrumentation
KW  - differentiation
KW  - gradient methods
KW  - optical radar
KW  - optical sensors
KW  - optical tracking
KW  - optimisation
KW  - radar receivers
KW  - parameter estimation
KW  - 2D continuous-wave LIDAR sensors
KW  - light detection and ranging sensors
KW  - depth measurements
KW  - localization pipelines
KW  - autonomous robots
KW  - perception stack
KW  - physics-based simulation
KW  - sensor measurements
KW  - gradient-based optimization
KW  - Hokuyo URG-04LX LIDAR
KW  - surface-light interactions
KW  - physically plausible model
KW  - laser light
KW  - calibration
KW  - time-of-flight cameras
KW  - depth sensors
KW  - Laser radar
KW  - Measurement by laser beam
KW  - Mathematical model
KW  - Sensor phenomena and characterization
KW  - Surface emitting lasers
KW  - Laser modes
DO  - 10.1109/ICRA40945.2020.9197138
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.
ER  - 

TY  - CONF
TI  - A Spatial-temporal Multiplexing Method for Dense 3D Surface Reconstruction of Moving Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2602
EP  - 2608
AU  - C. Sui
AU  - K. He
AU  - Z. Wang
AU  - C. Lyu
AU  - H. Guo
AU  - Y. -H. Liu
PY  - 2020
KW  - image coding
KW  - image filtering
KW  - image matching
KW  - image restoration
KW  - image texture
KW  - multiplexing
KW  - object recognition
KW  - robot vision
KW  - stereo image processing
KW  - surface reconstruction
KW  - high reconstruction accuracy
KW  - fast image acquisition
KW  - spatial-temporal multiplexing method
KW  - moving objects
KW  - three-dimensional reconstruction
KW  - robotic applications
KW  - robotic recognition
KW  - spatial-multiplexing time-multiplexing structured-light techniques
KW  - image acquisition time
KW  - spatial-temporal encoded patterns
KW  - dense 3D surface reconstruction
KW  - texture map
KW  - image blur
KW  - high-frequency phase-shifting fringes
KW  - spatial-coded texture
KW  - Image reconstruction
KW  - Multiplexing
KW  - Three-dimensional displays
KW  - Surface reconstruction
KW  - Robots
KW  - Encoding
KW  - Reliability
DO  - 10.1109/ICRA40945.2020.9197462
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Three-dimensional reconstruction of dynamic objects is important for robotic applications, for example, the robotic recognition and manipulation. In this paper, we present a novel 3D surface reconstruction method for moving objects. The proposed method combines the spatial-multiplexing and time-multiplexing structured-light techniques that have advantages of less image acquisition time and accurate 3D reconstruction, respectively. A set of spatial-temporal encoded patterns are designed, where a spatial-encoded texture map is embedded into the temporal-encoded three-step phase-shifting fringes. The specifically designed spatial-coded texture assigns high-uniqueness codeword to any window on the image which helps to eliminate the phase ambiguity. In addition, the texture is robust to noise and image blur. Combining this texture with high-frequency phase-shifting fringes, high reconstruction accuracy would be ensured. This method only requires 3 patterns to uniquely encode a surface, which facilitates the fast image acquisition for each reconstruction step. A filtering stereo matching algorithm is proposed for the spatial-temporal multiplexing method to improve the matching reliability. Moreover, the reconstruction precision is further enhanced by a correspondence refinement algorithm. Experiments validate the performance of the proposed method including the high accuracy, the robustness to noise and the ability to reconstruct moving objects.
ER  - 

TY  - CONF
TI  - PhaRaO: Direct Radar Odometry using Phase Correlation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2617
EP  - 2623
AU  - Y. S. Park
AU  - Y. -S. Shin
AU  - A. Kim
PY  - 2020
KW  - distance measurement
KW  - Fourier transforms
KW  - image matching
KW  - motion estimation
KW  - radar imaging
KW  - PhaRaO
KW  - direct radar odometry
KW  - scanning radar-based odometry methods
KW  - radar image
KW  - feature-based methods
KW  - radar scans
KW  - log-polar radar images
KW  - large-scale radar data
KW  - odometry estimation
KW  - radar-based navigation
KW  - Fourier Mellin transform
KW  - Correlation
KW  - Radar imaging
KW  - Estimation
KW  - Image resolution
KW  - Feature extraction
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197231
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent studies in radar-based navigation present promising navigation performance using scanning radars. These scanning radar-based odometry methods are mostly feature-based; they detect and match salient features within a radar image. Differing from existing feature-based methods, this paper reports on a method using direct radar odometry, PhaRaO, which infers relative motion from a pair of radar scans via phase correlation. Specifically, we apply the Fourier Mellin transform (FMT) for Cartesian and log-polar radar images to sequentially estimate rotation and translation. In doing so, we decouple rotation and translation estimations in a coarse-to-fine manner to achieve real-time performance. The proposed method is evaluated using large-scale radar data obtained from various environments. The inferred trajectory yields a 2.34% (translation) and 2.93¬∞ (rotation) Relative Error (RE) over a 4km path length on average for the odometry estimation.
ER  - 

TY  - CONF
TI  - DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2624
EP  - 2630
AU  - A. Dewan
AU  - W. Burgard
PY  - 2020
KW  - Bayes methods
KW  - control engineering computing
KW  - convolutional neural nets
KW  - filtering theory
KW  - image segmentation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural net architecture
KW  - object detection
KW  - optical radar
KW  - path planning
KW  - recursive estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - DeepTemporalSeg
KW  - temporally consistent semantic segmentation
KW  - 3d LiDAR scans
KW  - semantic characteristics
KW  - autonomous robot operation
KW  - deep convolutional neural network
KW  - DCNN
KW  - LiDAR scan
KW  - pedestrian
KW  - bicyclist
KW  - dense blocks
KW  - depth separable convolutions
KW  - current semantic state
KW  - recursive estimation
KW  - isolated erroneous predictions
KW  - neural network architectures
KW  - Bayes filter approach
KW  - KITTI tracking benchmark
KW  - Semantics
KW  - Laser radar
KW  - Image segmentation
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Neural networks
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9197193
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Understanding the semantic characteristics of the environment is a key enabler for autonomous robot operation. In this paper, we propose a deep convolutional neural network (DCNN) for semantic segmentation of a LiDAR scan into the classes car, pedestrian and bicyclist. This architecture is based on dense blocks and efficiently utilizes depth separable convolutions to limit the number of parameters while still maintaining the state-of-the-art performance. To make the predictions from the DCNN temporally consistent, we propose a Bayes filter based method. This method uses the predictions from the neural network to recursively estimate the current semantic state of a point in a scan. This recursive estimation uses the knowledge gained from previous scans, thereby making the predictions temporally consistent and robust towards isolated erroneous predictions. We compare the performance of our proposed architecture with other state-of-the-art neural network architectures and report substantial improvement. For the proposed Bayes filter approach, we shows results on various sequences in the KITTI tracking benchmark.
ER  - 

TY  - CONF
TI  - Discrete Bimanual Manipulation for Wrench Balancing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2631
EP  - 2637
AU  - S. Cruciani
AU  - D. Almeida
AU  - D. Kragic
AU  - Y. Karayiannidis
PY  - 2020
KW  - manipulators
KW  - motion control
KW  - payload limitations
KW  - single arm
KW  - grasped object
KW  - wrenches
KW  - payload limits
KW  - dual-arm robot
KW  - robot arms changes
KW  - wrench imbalance
KW  - discrete bimanual manipulation
KW  - grasp points
KW  - balanced configuration
KW  - robot experiments
KW  - grasping force
KW  - wrench balancing
KW  - Manipulators
KW  - Force
KW  - Grippers
KW  - Planning
KW  - Grasping
KW  - Payloads
DO  - 10.1109/ICRA40945.2020.9196527
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dual-arm robots can overcome grasping force and payload limitations of a single arm by jointly grasping an object. However, if the distribution of mass of the grasped object is not even, each arm will experience different wrenches that can exceed its payload limits. In this work, we consider the problem of balancing the wrenches experienced by a dual-arm robot grasping a rigid tray. The distribution of wrenches among the robot arms changes due to objects being placed on the tray. We present an approach to reduce the wrench imbalance among arms through discrete bimanual manipulation. Our approach is based on sequential sliding motions of the grasp points on the surface of the object, to attain a more balanced configuration. We validate our modeling approach and system design through a set of robot experiments.
ER  - 

TY  - CONF
TI  - NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2654
EP  - 2660
AU  - B. Ward-Cherrier
AU  - N. Pestell
AU  - N. F. Lepora
PY  - 2020
KW  - biomedical equipment
KW  - biomedical optical imaging
KW  - biomimetics
KW  - feedback
KW  - haptic interfaces
KW  - image classification
KW  - image texture
KW  - medical image processing
KW  - prosthetics
KW  - skin
KW  - tactile sensors
KW  - touch (physiological)
KW  - NeuroTac
KW  - texture recognition
KW  - artificial tactile sensing capabilities
KW  - rival human touch
KW  - robotics
KW  - prosthetics
KW  - biomimetic tactile sensors
KW  - grasping
KW  - manipulation tasks
KW  - biomimetic hardware design
KW  - TacTip sensor
KW  - layered papillae structure
KW  - human glabrous skin
KW  - event-based camera
KW  - spike trains
KW  - texture classification task
KW  - spike coding methods
KW  - artificial textures
KW  - spike-based output
KW  - biomimetic tactile perception algorithms
KW  - neuromorphic optical tactile sensor
KW  - Encoding
KW  - Neuromorphics
KW  - Tactile sensors
KW  - Cameras
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197046
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Developing artificial tactile sensing capabilities that rival human touch is a long-term goal in robotics and prosthetics. Gradually more elaborate biomimetic tactile sensors are being developed and applied to grasping and manipulation tasks to help achieve this goal. Here we present the neuroTac, a novel neuromorphic optical tactile sensor. The neuroTac combines the biomimetic hardware design from the TacTip sensor which mimicks the layered papillae structure of human glabrous skin, with an event-based camera (DAVIS240, iniVation) and algorithms which transduce contact information in the form of spike trains. The performance of the sensor is evaluated on a texture classification task, with four spike coding methods being implemented and compared: Intensive, Spatial, Temporal and Spatiotemporal. We found timing-based coding methods performed with the highest accuracy over both artificial and natural textures. The spike-based output of the neuroTac could enable the development of biomimetic tactile perception algorithms in robotics as well as non-invasive and invasive haptic feedback methods in prosthetics.
ER  - 

TY  - CONF
TI  - Reducing Uncertainty in Pose Estimation under Complex Contacts via Force Forecast
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2661
EP  - 2667
AU  - H. Mao
AU  - J. Xiao
PY  - 2020
KW  - haptic interfaces
KW  - manipulators
KW  - pose estimation
KW  - regression analysis
KW  - robotic assembly
KW  - trees (mathematics)
KW  - sphere-tree representation
KW  - least-uncertain estimate
KW  - relative contact
KW  - multiregion complex contacts
KW  - contact types
KW  - contact locations
KW  - object shapes
KW  - object poses
KW  - complex shapes
KW  - pose estimation
KW  - force forecast
KW  - autonomous robotic manipulation
KW  - simulated complex contacts
KW  - force sensing
KW  - constraint-based haptic simulation algorithm
KW  - three-pin peg-in-hole robotic assembly tasks
KW  - contact-rich two-pin peg-in-hole assembly tasks
KW  - calibration
KW  - regression model
KW  - Force
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Task analysis
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9197190
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - How to reduce uncertainty in object pose estimation under complex contacts is crucial to autonomous robotic manipulation and assembly. In this paper, we introduce an approach through forecasting contact force from simulated complex contacts with calibration based on real force sensing. A constraint-based haptic simulation algorithm is used with sphere-tree representation of contacting objects to compute contact poses and forces, and through matching the computed forces to measured real force data via a regression model, the least-uncertain estimate of the relative contact pose is obtained. Our approach can handle multi-region complex contacts and does not make any assumption about contact types or contact locations. It also does not have restriction on object shapes. We have applied the force forecast approach to reducing uncertainty in estimating object poses in challenging peg-in-hole robotic assembly tasks and demonstrate the effectiveness of the approach by successful completion of contact-rich two-pin and three-pin real peg-in-hole assembly tasks with complex shapes of pins and holes.
ER  - 

TY  - CONF
TI  - Comparison of Constrained and Unconstrained Human Grasp Forces Using Fingernail Imaging and Visual Servoing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2668
EP  - 2674
AU  - N. Fallahinia
AU  - S. A. Mascaro
PY  - 2020
KW  - dexterous manipulators
KW  - force measurement
KW  - grippers
KW  - robot vision
KW  - visual servoing
KW  - fingernail imaging
KW  - force measurement
KW  - visual tracking system
KW  - force collaboration
KW  - constrained human grasp forces
KW  - unconstrained human grasp forces
KW  - visual servoing
KW  - 3D fingertip forces
KW  - robotic arms
KW  - Grasping
KW  - Force
KW  - Estimation
KW  - Cameras
KW  - Mathematical model
KW  - Force measurement
DO  - 10.1109/ICRA40945.2020.9196963
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fingernail imaging has been proven to be effective in prior works [1], [2] for estimating the 3D fingertip forces with a maximum RMS estimation error of 7%. In the current research, fingernail imaging is used to perform unconstrained grasp force measurement on multiple fingers to study human grasping. Moreover, two robotic arms with mounted cameras and a visual tracking system have been devised to keep the human fingers in the camera frame during the experiments. Experimental tests have been conducted for six human subjects under both constrained and unconstrained grasping conditions, and the results indicate a significant difference in force collaboration among the fingers between the two grasping conditions. Another interesting result according to the experiments is that in comparison to constrained grasping, unconstrained grasp forces are more evenly distributed over the fingers and there is less force variation (more steadiness) in each finger force. These results validate the importance of measuring grasp forces in an unconstrained manner in order to study how humans naturally grasp objects.
ER  - 

TY  - CONF
TI  - Robust and Efficient Estimation of Absolute Camera Pose for Monocular Visual Odometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2675
EP  - 2681
AU  - H. Li
AU  - W. Chen
AU  - J. Zhao
AU  - J. -C. Bazin
AU  - L. Luo
AU  - Z. Liu
AU  - Y. -H. Liu
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - optimisation
KW  - pose estimation
KW  - high generality
KW  - absolute camera pose
KW  - monocular visual odometry
KW  - cost function
KW  - branch-and-bound
KW  - high outlier ratios
KW  - robust estimation
KW  - efficient estimation
KW  - 3D-to-2D point correspondences
KW  - projection constraint
KW  - local optimizer
KW  - effective function bounds
KW  - real-time applications
KW  - synthetic datasets
KW  - real-world datasets
KW  - Robustness
KW  - Pose estimation
KW  - Cameras
KW  - Cost function
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196814
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Given a set of 3D-to-2D point correspondences corrupted by outliers, we aim to robustly estimate the absolute camera pose. Existing methods robust to outliers either fail to guarantee high robustness and efficiency simultaneously, or require an appropriate initial pose and thus lack generality. In contrast, we propose a novel approach based on the robust "L2-minimizing estimate" (L2E) loss. We first define a novel cost function by integrating the projection constraint into the L2E loss. Then to efficiently obtain the global minimum of this function, we propose a hybrid strategy of a local optimizer and branch-and-bound. For branch-and-bound, we derive effective function bounds. Our approach can handle high outlier ratios, leading to high robustness. It can run reliably regardless of whether the initial pose is appropriate, providing high generality. Moreover, given a decent initial pose, it is suitable for real-time applications. Experiments on synthetic and real-world datasets showed that our approach outperforms state-of-the-art methods in terms of robustness and/or efficiency.
ER  - 

TY  - CONF
TI  - Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in Dynamic Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2682
EP  - 2688
AU  - J. Lin
AU  - H. Zhu
AU  - J. Alonso-Mora
PY  - 2020
KW  - collision avoidance
KW  - helicopters
KW  - mobile robots
KW  - predictive control
KW  - robot dynamics
KW  - robot vision
KW  - state estimation
KW  - robust vision-based obstacle avoidance
KW  - microaerial vehicle
KW  - dynamic environments
KW  - on-board vision-based approach
KW  - moving obstacle
KW  - efficient obstacle detection
KW  - tracking algorithm
KW  - depth image pairs
KW  - robust collision avoidance
KW  - chance-constrained model predictive controller
KW  - collision probability
KW  - account MAV dynamics
KW  - state estimation
KW  - obstacle sensing uncertainties
KW  - on-line collision avoidance
KW  - Collision avoidance
KW  - Uncertainty
KW  - Cameras
KW  - Robustness
KW  - Sensors
KW  - Predictive models
KW  - State estimation
DO  - 10.1109/ICRA40945.2020.9197481
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an on-board vision-based approach for avoidance of moving obstacles in dynamic environments. Our approach relies on an efficient obstacle detection and tracking algorithm based on depth image pairs, which provides the estimated position, velocity and size of the obstacles. Robust collision avoidance is achieved by formulating a chance-constrained model predictive controller (CC-MPC) to ensure that the collision probability between the micro aerial vehicle (MAV) and each moving obstacle is below a specified threshold. The method takes into account MAV dynamics, state estimation and obstacle sensing uncertainties. The proposed approach is implemented on a quadrotor equipped with a stereo camera and is tested in a variety of environments, showing effective on-line collision avoidance of moving obstacles.
ER  - 

TY  - CONF
TI  - Proximity Estimation Using Vision Features Computed On Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2689
EP  - 2695
AU  - J. Chen
AU  - Y. Liu
AU  - S. J. Carey
AU  - P. Dudek
PY  - 2020
KW  - collision avoidance
KW  - computer vision
KW  - feature extraction
KW  - image sensors
KW  - microcontrollers
KW  - mobile robots
KW  - recurrent neural nets
KW  - robot vision
KW  - sensor arrays
KW  - VLSI
KW  - neural network output
KW  - image capture
KW  - control system
KW  - trained neural network
KW  - fully connected layer-recurrent
KW  - training data
KW  - infrared proximity sensors
KW  - vision output
KW  - sparse feature description data
KW  - feature algorithms
KW  - pixel processor array chip
KW  - embedded 256√ó256 processor SIMD array
KW  - image sensor
KW  - RC model car
KW  - microcontroller
KW  - SCAMP-5 vision chip
KW  - vision system integrating
KW  - experimental vehicle
KW  - blobs
KW  - corner points
KW  - abstract features
KW  - monocular vision based proximity estimation system
KW  - vision features computed
KW  - velocity 0.64 m/s to 1.8 m/s
KW  - time 4.0 ms
KW  - Hardware
KW  - Estimation
KW  - Neural networks
KW  - Machine vision
KW  - Feature extraction
KW  - Registers
KW  - Image edge detection
DO  - 10.1109/ICRA40945.2020.9197370
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a monocular vision based proximity estimation system using abstract features, such as corner points, blobs and edges, as inputs to a neural network. An experimental vehicle was built using a vision system integrating the SCAMP-5 vision chip, a micro-controller, and an RC model car. The vision chip includes image sensor with embedded 256√ó256 processor SIMD array. The pixel processor array chip was programmed to capture images and run the feature algorithms directly on the focal plane, and then digest them so that only sparse feature description data were read-out in the form of 40 values. By logging the vision output and the output from three infrared proximity sensors, training data were obtained to train three fully connected layer-recurrent neural networks with fewer than 700 parameters each. The trained neural network was able to estimate the proximity to the level of accuracy sufficient for a reactive collision avoidance behaviour to be achieved. The latency of the control system, from image capture to neural network output, was under 4ms, enabling the vehicles to avoid obstacles while moving at 0.64m/s to 1.8m/s in the experiment.
ER  - 

TY  - CONF
TI  - Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2696
EP  - 2702
AU  - L. Gao
AU  - J. Su
AU  - J. Cui
AU  - X. Zeng
AU  - X. Peng
AU  - L. Kneip
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image registration
KW  - mobile robots
KW  - motion estimation
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - steering systems
KW  - tree searching
KW  - efficient globally-optimal correspondence-less visual odometry
KW  - planar ground vehicles
KW  - 2 DoF Ackermann steering model
KW  - downward facing camera
KW  - simple image registration problem
KW  - 2-parameter planar homography
KW  - ground-plane features
KW  - plane-based Ackermann motion estimation
KW  - correspondence-based hypothesise
KW  - test schemes
KW  - fronto-parallel motion
KW  - branch-and-bound optimisation technique
KW  - low-dimensional parametrisation
KW  - Cameras
KW  - Optimization
KW  - Transmission line matrix methods
KW  - Land vehicles
KW  - Motion estimation
KW  - Image registration
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196595
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The motion of planar ground vehicles is often non-holonomic, and as a result may be modelled by the 2 DoF Ackermann steering model. We analyse the feasibility of estimating such motion with a downward facing camera that exerts fronto-parallel motion with respect to the ground plane. This turns the motion estimation into a simple image registration problem in which we only have to identify a 2-parameter planar homography. However, one difficulty that arises from this setup is that ground-plane features are indistinctive and thus hard to match between successive views. We encountered this difficulty by introducing the first globally-optimal, correspondence-less solution to plane-based Ackermann motion estimation. The solution relies on the branch-and-bound optimisation technique. Through the low-dimensional parametrisation, a derivation of tight bounds, and an efficient implementation, we demonstrate how this technique is eventually amenable to accurate real-time motion estimation. We prove its property of global optimality and analyse the impact of assuming a locally constant centre of rotation. Our results on real data finally demonstrate a significant advantage over the more traditional, correspondence-based hypothesise-and-test schemes.
ER  - 

TY  - CONF
TI  - egoTEB: Egocentric, Perception Space Navigation Using Timed-Elastic-Bands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2703
EP  - 2709
AU  - J. S. Smith
AU  - R. Xu
AU  - P. Vela
PY  - 2020
KW  - aerospace navigation
KW  - aerospace robotics
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - Monte Carlo methods
KW  - motion control
KW  - optimisation
KW  - trajectory control
KW  - grid-based representations
KW  - optimization graph
KW  - local planning map
KW  - egoTEB
KW  - timed-elastic-bands
KW  - TEB hierarchical planner
KW  - real-time navigation
KW  - collision avoidance
KW  - goal directed motion
KW  - multitrajectory optimization based synthesis method
KW  - topologically distinct trajectory candidates
KW  - factor graph approach
KW  - egocentric perception space navigation
KW  - egocentric perception space representations
KW  - Monte Carlo evaluations
KW  - autonomous mobile robot
KW  - Trajectory
KW  - Optimization
KW  - Navigation
KW  - Planning
KW  - Robots
KW  - Collision avoidance
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196721
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The TEB hierarchical planner for real-time navigation through unknown environments is highly effective at balancing collision avoidance with goal directed motion. Designed over several years and publications, it implements a multi-trajectory optimization based synthesis method for identifying topologically distinct trajectory candidates through navigable space. Unfortunately, the underlying factor graph approach to the optimization problem induces a mismatch between grid-based representations and the optimization graph, which leads to several time and optimization inefficiencies. This paper explores the impact of using egocentric, perception space representations for the local planning map. Doing so alleviates many of the identified issues related to TEB and leads to a new method called egoTEB. Timing experiments and Monte Carlo evaluations in benchmark worlds quantify the benefits of egoTEB for navigation through uncertain environments.
ER  - 

TY  - CONF
TI  - Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2718
EP  - 2724
AU  - R. Jeong
AU  - Y. Aytar
AU  - D. Khosid
AU  - Y. Zhou
AU  - J. Kay
AU  - T. Lampe
AU  - K. Bousmalis
AU  - F. Nori
PY  - 2020
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - visual robotic manipulation
KW  - robotic visual data
KW  - reinforcement learning algorithms
KW  - robotic learning
KW  - state estimation
KW  - latent state representation
KW  - deep reinforcement learning
KW  - unlabeled real robot data
KW  - robot experience
KW  - time-contrastive techniques
KW  - learned state representation
KW  - vision-based reinforcement learning agent
KW  - standard visual domain adaptation techniques
KW  - self-supervised sim-to-real adaptation
KW  - sequence-based supervised objectives
KW  - contrastive forward dynamics loss
KW  - Robots
KW  - Task analysis
KW  - Adaptation models
KW  - Visualization
KW  - Stacking
KW  - Data models
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197326
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training reinforcement learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the accuracy and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep reinforcement learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self- supervised objectives. These use the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We further propose a novel such objective, the Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based reinforcement learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer.
ER  - 

TY  - CONF
TI  - Meta Reinforcement Learning for Sim-to-real Domain Adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2725
EP  - 2731
AU  - K. Arndt
AU  - M. Hazara
AU  - A. Ghadirzadeh
AU  - V. Kyrki
PY  - 2020
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - dynamic conditions
KW  - task-specific trajectory generation model
KW  - KUKA LBR 4+ robot
KW  - sim-to-real domain transfer
KW  - robotic policy training
KW  - meta reinforcement learning
KW  - Adaptation models
KW  - Task analysis
KW  - Trajectory
KW  - Robots
KW  - Training
KW  - Learning (artificial intelligence)
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196540
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.
ER  - 

TY  - CONF
TI  - Variational Auto-Regularized Alignment for Sim-to-Real Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2732
EP  - 2738
AU  - M. Hwasser
AU  - D. Kragic
AU  - R. Antonova
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - variational auto-regularized alignment
KW  - sim-to-real control
KW  - general-purpose simulators
KW  - variational autoencoder
KW  - black-box simulation
KW  - latent space
KW  - encoder training
KW  - simulation parameter distribution
KW  - matching parameter distributions
KW  - ABB YuMi robot hardware
KW  - Hardware
KW  - Decoding
KW  - Computational modeling
KW  - Neural networks
KW  - Training
KW  - Trajectory
KW  - Benchmark testing
DO  - 10.1109/ICRA40945.2020.9197130
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - General-purpose simulators can be a valuable data source for flexible learning and control approaches. However, training models or control policies in simulation and then directly applying to hardware can yield brittle control. Instead, we propose a novel way to use simulators as regularizers. Our approach regularizes a decoder of a variational autoencoder to a black-box simulation, with the latent space bound to a subset of simulator parameters. This enables successful encoder training from a small number of real-world trajectories (10 in our experiments), yielding a latent space with simulation parameter distribution that matches the real-world setting. We use a learnable mixture for the latent prior/posterior, which implies a highly flexible class of densities for the posterior fit. Our approach is scalable and does not require restrictive distributional assumptions. We demonstrate ability to recover matching parameter distributions on a range of benchmarks, challenging custom simulation environments and several real-world scenarios. Our experiments using ABB YuMi robot hardware show ability to help reinforcement learning approaches overcome cases of severe sim-to-real mismatch.
ER  - 

TY  - CONF
TI  - Experience Selection Using Dynamics Similarity for Efficient Multi-Source Transfer Learning Between Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2739
EP  - 2745
AU  - M. J. Sorocky
AU  - S. Zhou
AU  - A. P. Schoellig
PY  - 2020
KW  - control engineering computing
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - robot programming
KW  - robust control
KW  - user experience
KW  - robot systems
KW  - multisource inter-robot transfer learning
KW  - quadrotor experiments
KW  - real source quadrotor
KW  - virtual source quadrotor
KW  - experience selection
KW  - dynamics similarity
KW  - robotics literature
KW  - knowledge transfer
KW  - learning process
KW  - robust control theory
KW  - data-efficiency algorithm
KW  - Measurement
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Robot sensing systems
KW  - Robust control
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196744
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the robotics literature, different knowledge transfer approaches have been proposed to leverage the experience from a source task or robot-real or virtual-to accelerate the learning process on a new task or robot. A commonly made but infrequently examined assumption is that incorporating experience from a source task or robot will be beneficial. In practice, inappropriate knowledge transfer can result in negative transfer or unsafe behaviour. In this work, inspired by a system gap metric from robust control theory, the ŒΩ-gap, we present a data-efficient algorithm for estimating the similarity between pairs of robot systems. In a multi-source inter-robot transfer learning setup, we show that this similarity metric allows us to predict relative transfer performance and thus informatively select experiences from a source robot before knowledge transfer. We demonstrate our approach with quadrotor experiments, where we transfer an inverse dynamics model from a real or virtual source quadrotor to enhance the tracking performance of a target quadrotor on arbitrary hand-drawn trajectories. We show that selecting experiences based on the proposed similarity metric effectively facilitates the learning of the target quadrotor, improving performance by 62% compared to a poorly selected experience.
ER  - 

TY  - CONF
TI  - DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2746
EP  - 2754
AU  - B. Balaji
AU  - S. Mallya
AU  - S. Genc
AU  - S. Gupta
AU  - L. Dirac
AU  - V. Khare
AU  - G. Roy
AU  - T. Sun
AU  - Y. Tao
AU  - B. Townsend
AU  - E. Calleja
AU  - S. Muralidhara
AU  - D. Karuppasamy
PY  - 2020
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot dynamics
KW  - robot vision
KW  - reality gap
KW  - joint perception
KW  - on-demand compute architecture
KW  - training optimal policies
KW  - robust evaluation
KW  - deep reinforcement learning
KW  - robotic control agent
KW  - raw camera images
KW  - robust path planning
KW  - DeepRacer
KW  - autonomous racing platform
KW  - Sim2Real reinforcement learning
KW  - end-to-end experimentation
KW  - RL
KW  - intelligent control systems
KW  - monocular camera
KW  - physical world
KW  - robust reinforcement learning
KW  - model-free learning
KW  - Training
KW  - Automobiles
KW  - Robots
KW  - Computational modeling
KW  - Cameras
KW  - Robustness
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197465
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.
ER  - 

TY  - CONF
TI  - A closed-loop and ergonomic control for prosthetic wrist rotation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2763
EP  - 2769
AU  - M. Legrand
AU  - N. Jarrass√©
AU  - F. Richer
AU  - G. Morel
PY  - 2020
KW  - artificial limbs
KW  - biomechanics
KW  - closed loop systems
KW  - electromyography
KW  - ergonomics
KW  - medical signal processing
KW  - open-loop scheme
KW  - ergonomic posture
KW  - control scheme
KW  - prostheses users
KW  - body compensation
KW  - closed-loop control
KW  - prosthetic level
KW  - control loop
KW  - correcting errors
KW  - upper-limb prostheses control
KW  - prosthetic wrist rotation
KW  - ergonomic control
KW  - Prosthetics
KW  - Task analysis
KW  - Wrist
KW  - Ergonomics
KW  - Wires
KW  - Robots
KW  - Muscles
DO  - 10.1109/ICRA40945.2020.9197554
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Beyond the ultimate goal of prosthetics, repairing all the capabilities of amputees, the development line of upper-limb prostheses control mainly relies on three aspects: the robustness, the intuitiveness and the reduction of mental fatigue. Many complex structures and algorithms are proposed but no one question a common open-loop nature, where the user is the one in charge of correcting errors. Yet, closing the control loop at the prosthetic level may help to improve the three main lines of research cited above. One major issue to build a closed-loop control is the definition of a reliable error signal; this paper proposes to use body compensations, naturally exhibited by prostheses users when the motion of their device is inaccurate, as such. The described control scheme measures these compensatory movements and makes the prosthesis move in order to bring back the user into an ergonomic posture. The function of the prosthesis is no longer to perform a given motion but rather to correct the posture of its user while s/he focus on performing an endpoint task. This concept was validated and compared to a standard open-loop scheme, for the control of a prosthetic wrist, with five healthy subjects completing a dedicated task with a customized transradial prosthesis. Results show that the presented closed-loop control allows for more intuitiveness and less mental burden without enhancing body compensation.
ER  - 

TY  - CONF
TI  - Comparison of online algorithms for the tracking of multiple magnetic targets in a myokinetic control interface*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2770
EP  - 2776
AU  - J. Montero
AU  - M. Gherardini
AU  - F. Clemente
AU  - C. Cipriani
PY  - 2020
KW  - Kalman filters
KW  - medical robotics
KW  - optimisation
KW  - prosthetics
KW  - surgery
KW  - telerobotics
KW  - tracking
KW  - localization algorithms
KW  - optimization
KW  - Levenberg-Marquardt algorithm
KW  - trust region reflective algorithm
KW  - robotics applications
KW  - remote tracking
KW  - multiple magnetic targets
KW  - myokinetic control interface
KW  - magnetic tracking algorithms
KW  - biomedical applications
KW  - teleoperated surgical robots
KW  - upper limb prostheses
KW  - Magnetostatics
KW  - Magnetic separation
KW  - Robots
KW  - Magnetic sensors
KW  - Magnetic devices
DO  - 10.1109/ICRA40945.2020.9196804
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Magnetic tracking algorithms can be used to determine the position and orientation of magnetic markers or devices. These techniques are particularly interesting for biomedical applications such as teleoperated surgical robots or the control of upper limb prostheses. The performance of different algorithms used for magnetic tracking was compared in the past. However, in most cases, those algorithms were required to track a single magnet.Here we investigated the performance of three localization algorithms in tracking up to 9 magnets: two optimization-based (Levenberg-Marquardt algorithm, LMA, and Trust Region Reflective algorithm, TRRA) and one recursion-based (Unscented Kalman Filter, UKF). The tracking accuracy of the algorithms and their computation time were investigated through simulations.The accuracy of the three algorithms, when tracking up to six magnets, was similar, leading to estimation errors varying from 0.06 ¬± 0.02 mm to 2.26 ¬± 0.07 mm within a 100 mm √ó 54 mm √ó 100 mm workspace, at the highest sampling frequency. In all cases, computation times under 300 ms for the UKF and 45 ms for the LMA/TRRA were obtained. The TRRA showed the best tracking performance overall.These outcomes are of interest for a wide range of robotics applications that require remote tracking.
ER  - 

TY  - CONF
TI  - Congestion-aware Evacuation Routing using Augmented Reality Devices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2798
EP  - 2804
AU  - Z. Zhang
AU  - H. Liu
AU  - Z. Jiao
AU  - Y. Zhu
AU  - S. -C. Zhu
PY  - 2020
KW  - augmented reality
KW  - emergency management
KW  - optimisation
KW  - congestion-aware evacuation
KW  - congestion-aware routing solution
KW  - indoor evacuation
KW  - real-time individual-customized evacuation routes
KW  - multiple destinations
KW  - population density map
KW  - obtained on-the-fly
KW  - congestion distribution
KW  - optimal solution
KW  - time-efficient evacuation route
KW  - AR devices
KW  - user-end augmented reality devices
KW  - Sociology
KW  - Statistics
KW  - Routing
KW  - Real-time systems
KW  - Path planning
KW  - Robots
KW  - Headphones
DO  - 10.1109/ICRA40945.2020.9197494
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a congestion-aware routing solution for indoor evacuation, which produces real-time individual-customized evacuation routes among multiple destinations while keeping tracks of all evacuees' locations. A population density map, obtained on-the-fly by aggregating locations of evacuees from user-end Augmented Reality (AR) devices, is used to model the congestion distribution inside a building. To efficiently search the evacuation route among all destinations, a variant of A* algorithm is devised to obtain the optimal solution in a single pass. In a series of simulated studies, we show that the proposed algorithm is more computationally optimized compared to classic path planning algorithms; it generates a more time-efficient evacuation route for each individual that minimizes the overall congestion. A complete system using AR devices is implemented for a pilot study in real-world environments, demonstrating the efficacy of the proposed approach.
ER  - 

TY  - CONF
TI  - Human-robot interaction for robotic manipulator programming in Mixed Reality
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2805
EP  - 2811
AU  - M. Ostanin
AU  - S. Mikhel
AU  - A. Evlampiev
AU  - V. Skvortsova
AU  - A. Klimchik
PY  - 2020
KW  - augmented reality
KW  - human-robot interaction
KW  - manipulators
KW  - robot programming
KW  - human-robot interaction
KW  - robotic manipulator programming
KW  - mixed reality
KW  - interactive programming
KW  - HoloLens glasses
KW  - robotic operation system
KW  - robotic manipulators
KW  - robot location
KW  - point cloud analysis
KW  - virtual markers
KW  - menus
KW  - pick-and-place operation
KW  - contact operations execution
KW  - UR10e robot
KW  - KUKA iiwa robot
KW  - Virtual reality
KW  - Trajectory
KW  - Collision avoidance
KW  - Manipulators
KW  - Programming
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196965
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents an approach for interactive programming of the robotic manipulator using mixed reality. The developed system is based on the HoloLens glasses connected through Robotic Operation System to Unity engine and robotic manipulators. The system gives a possibility to recognize the real robot location by the point cloud analysis, to use virtual markers and menus for the task creation, to generate a trajectory for execution in the simulator or on the real manipulator. It also provides the possibility of scaling virtual and real worlds for more accurate planning. The proposed framework has been tested on pick-and-place and contact operations execution by UR10e and KUKA iiwa robots.
ER  - 

TY  - CONF
TI  - Heart Rate Sensing with a Robot Mounted mmWave Radar
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2812
EP  - 2818
AU  - P. Zhao
AU  - C. X. Lu
AU  - B. Wang
AU  - C. Chen
AU  - L. Xie
AU  - M. Wang
AU  - N. Trigoni
AU  - A. Markham
PY  - 2020
KW  - cardiology
KW  - convolutional neural nets
KW  - health care
KW  - medical robotics
KW  - medical signal processing
KW  - patient monitoring
KW  - patient rehabilitation
KW  - telemedicine
KW  - mBeats features
KW  - deep neural network predictor
KW  - robust operation
KW  - post-operative rehabilitation
KW  - heart rate sensing
KW  - robot mounted mmWave radar
KW  - post-operative recovery
KW  - noncontact heart rate monitoring
KW  - static wall-mounted device
KW  - millimeter wave radar system
KW  - periodic heart rate measurements
KW  - users daily activities
KW  - mmWave servoing module
KW  - Heart rate
KW  - Robot sensing systems
KW  - Radar
KW  - Monitoring
KW  - Estimation
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197437
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Heart rate monitoring at home is a useful metric for assessing health e.g. of the elderly or patients in post-operative recovery. Although non-contact heart rate monitoring has been widely explored, typically using a static, wall-mounted device, measurements are limited to a single room and sensitive to user orientation and position. In this work, we propose mBeats, a robot mounted millimeter wave (mmWave) radar system that provide periodic heart rate measurements under different user poses, without interfering in a users daily activities. mBeats contains a mmWave servoing module that adaptively adjusts the sensor angle to the best reflection pro le. Furthermore, mBeats features a deep neural network predictor, which can estimate heart rate from the lower leg and additionally provides estimation uncertainty. Through extensive experiments, we demonstrate accurate and robust operation of mBeats in a range of scenarios. We believe by integrating mobility and adaptability, mBeats can empower many down-stream healthcare applications at home, such as palliative care, post-operative rehabilitation and telemedicine.
ER  - 

TY  - CONF
TI  - Prediction of Gait Cycle Percentage Using Instrumented Shoes with Artificial Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2834
EP  - 2840
AU  - A. Prado
AU  - X. Cao
AU  - X. Ding
AU  - S. K. Agrawal
PY  - 2020
KW  - biomedical measurement
KW  - gait analysis
KW  - medical computing
KW  - neural nets
KW  - patient rehabilitation
KW  - encoder-decoder
KW  - RMSE
KW  - root mean square error
KW  - inertial measurement unit
KW  - instrument standard footwear
KW  - overground walking
KW  - continuous gait cycle
KW  - walking over-ground
KW  - gait rehabilitation
KW  - gait parameters
KW  - traditional gait measurement systems
KW  - gait abnormalities
KW  - gait training
KW  - artificial neural networks
KW  - instrumented shoes
KW  - gait cycle percentage
KW  - Training
KW  - Instruments
KW  - Foot
KW  - Legged locomotion
KW  - Prediction algorithms
KW  - Footwear
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196747
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gait training is widely used to treat gait abnormalities. Traditional gait measurement systems are limited to instrumented laboratories. Even though gait measurements can be made in these settings, it is challenging to estimate gait parameters robustly in real-time for gait rehabilitation, especially when walking over-ground. In this paper, we present a novel approach to track the continuous gait cycle during overground walking outside the laboratory. In this approach, we instrument standard footwear with a sensorized insole and an inertial measurement unit. Artificial neural networks are used on the raw data obtained from the insoles and IMUs to compute the continuous percentage of the gait cycle for the entire walking session. We show in this paper that when tested with novel subjects, we can predict the gait cycle with a Root Mean Square Error (RMSE) of 7.2%. The onset of each cycle can be detected within an RMSE time of 41.5 ms with a 99% detection rate. The algorithm was tested with 18840 strides collected from 24 adults. In this paper, we tested a combination of fully-connected layers, an Encoder-Decoder using convolutional layers, and recurrent layers to identify an architecture that provided the best performance.
ER  - 

TY  - CONF
TI  - Flow Compensation for Hydraulic Direct-Drive System with a Single-rod Cylinder Applied to Biped Humanoid Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2857
EP  - 2863
AU  - J. Shimizu
AU  - T. Otani
AU  - H. Mizukami
AU  - K. Hashimoto
AU  - A. Takanishi
PY  - 2020
KW  - flow control
KW  - humanoid robots
KW  - legged locomotion
KW  - pressure control
KW  - robot dynamics
KW  - valves
KW  - hydraulic direct-drive system
KW  - biped humanoid robot
KW  - hydraulic direct drive system
KW  - simple equipment configuration
KW  - single-rod cylinder
KW  - flow rate
KW  - passive flow compensation valve
KW  - valve state
KW  - flow control
KW  - pressure control
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196956
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Biped robots require massive power on each leg while walking, hopping, and running. We have developed a flow-based control system-called hydraulic direct drive system- that can achieve high output while avoiding spatial limitations. To implement the proposed system with simple equipment configuration, a pump and single-rod cylinder are connected in a closed loop. However, because compensation for flow rate is impossible in a completely closed loop, owing to the difference in the pressure receiving area caused by the rod, a passive flow compensation valve is employed. This valve has a simple structure and is easy to implement. Further, an additional sensor is required to detect the open/close state because the valve state will cause an error in flow control. Therefore, we implemented a model in the controller to predict the state of the flow compensation valve and formulated a method of switching from flow control to pressure control according to the predicted state. Experimental results indicate that the error of the joint angle is reduced to less than 1.6 degrees for walking patterns, and stable walking is realized when the system is installed in biped humanoid robots.
ER  - 


TY  - CONF
TI  - Mechanically Programmed Miniature Origami Grippers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2872
EP  - 2878
AU  - A. Orlofsky
AU  - C. Liu
AU  - S. Kamrava
AU  - A. Vaziri
AU  - S. M. Felton
PY  - 2020
KW  - actuators
KW  - bending
KW  - grippers
KW  - miniature origami grippers
KW  - robotic gripper design
KW  - customizable grasping tasks
KW  - miniature fingers
KW  - single actuator input
KW  - grasping tasks
KW  - Grippers
KW  - Kinematics
KW  - Grasping
KW  - Actuators
KW  - Springs
KW  - Steel
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196545
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robotic gripper design that can perform customizable grasping tasks at the millimeter scale. The design is based on the origami string, a mechanism with a single degree of freedom that can be mechanically programmed to approximate arbitrary paths in space. By using this concept, we create miniature fingers that bend at multiple joints with a single actuator input. The shape and stiffness of these fingers can be varied to fit different grasping tasks by changing the crease pattern of the string. We show that the experimental behavior of these strings follows their analytical models and that they can perform a variety of tasks including pinching, wrapping, and twisting common objects such as pencils, bottle caps, and blueberries.
ER  - 

TY  - CONF
TI  - Bio-inspired Tensegrity Fish Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2887
EP  - 2892
AU  - J. Shintake
AU  - D. Zappetti
AU  - T. Peter
AU  - Y. Ikemoto
AU  - D. Floreano
PY  - 2020
KW  - autonomous underwater vehicles
KW  - biomechanics
KW  - bone
KW  - elasticity
KW  - mobile robots
KW  - muscle
KW  - robot dynamics
KW  - servomotors
KW  - underwater vehicles
KW  - fish robots
KW  - body stiffness
KW  - fish swimming
KW  - mechanical stiffness
KW  - tensegrity-based underwater robots
KW  - tensegrity class
KW  - elastic cables
KW  - rigid body segments
KW  - body shape
KW  - tensegrity systems
KW  - fish-like robots
KW  - bio-inspired tensegrity fish robot
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196675
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to create fish-like robots with tensegrity systems and describes a prototype modeled on the body shape of the rainbow trout with a length of 400 mm and a mass of 102 g that is driven by a waterproof servomotor. The structure of the tensegrity robot consists of rigid body segments and elastic cables that represent bone/tissue and muscles of fish, respectively. This structural configuration employing the tensegrity class 2 is much simpler than other tensegrity-based underwater robots. It also allows the tuning of the mechanical stiffness, which is often said to be an important factor in fish swimming. In our robot, the body stiffness can be tuned by changing the cross-section of the cables and their pre-stretch ratio. We characterize the robot in terms of body stiffness, swimming speed, and thrust force while varying the body stiffness i.e., the cross-section of the elastic cables. The results show that the body stiffness of the robot can be designed to approximate that of the real fish and modulate its performance characteristics. The measured swimming speed of the robot is 0.23 m/s (0.58 BL/s), which is comparable to other fish robots of the same type. Strouhal number of the robot 0.54 is close to that of the natural counterpart, suggesting that the presented method is an effective engineering approach to realize the swimming characteristics of real fish.
ER  - 

TY  - CONF
TI  - Gaussian-Dirichlet Random Fields for Inference over High Dimensional Categorical Observations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2924
EP  - 2931
AU  - J. E. S. Soucie
AU  - H. M. Sosik
AU  - Y. Girdhar
PY  - 2020
KW  - Bayes methods
KW  - Gaussian processes
KW  - image classification
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - vectors
KW  - low dimensional vector observations
KW  - Gaussian-Dirichlet random fields
KW  - high dimensional categorical observations
KW  - spatio-temporal distribution
KW  - imaging sensor
KW  - image classifier
KW  - Dirichlet distributions
KW  - Gaussian processes
KW  - high dimensional categorical measurements
KW  - taxonomic observations
KW  - informative path planning techniques
KW  - Gaussian processes
KW  - Biological system modeling
KW  - Spatiotemporal phenomena
KW  - Graphical models
KW  - Data models
KW  - Robots
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9196713
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a generative model for the spatio-temporal distribution of high dimensional categorical observations. These are commonly produced by robots equipped with an imaging sensor such as a camera, paired with an image classifier, potentially producing observations over thousands of categories. The proposed approach combines the use of Dirichlet distributions to model sparse co-occurrence relations between the observed categories using a latent variable, and Gaussian processes to model the latent variable's spatio-temporal distribution. Experiments in this paper show that the resulting model is able to efficiently and accurately approximate the temporal distribution of high dimensional categorical measurements such as taxonomic observations of microscopic organisms in the ocean, even in unobserved (held out) locations, far from other samples. This work's primary motivation is to enable deployment of informative path planning techniques over high dimensional categorical fields, which until now have been limited to scalar or low dimensional vector observations.
ER  - 

TY  - CONF
TI  - Investigation of a Multistable Tensegrity Robot applied as Tilting Locomotion System*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2932
EP  - 2938
AU  - P. Schorr
AU  - F. Schale
AU  - J. M. Otterbach
AU  - L. Zentner
AU  - K. Zimmermann
AU  - V. B√∂hm
PY  - 2020
KW  - actuators
KW  - bifurcation
KW  - mechanical stability
KW  - mobile robots
KW  - motion control
KW  - numerical analysis
KW  - robot dynamics
KW  - robot kinematics
KW  - vibration control
KW  - locomotion characteristics
KW  - actuation strategy
KW  - compliant tensegrity structure
KW  - multistable tensegrity robot
KW  - multiple stable equilibrium configurations
KW  - tilting locomotion system
KW  - Prototypes
KW  - Bifurcation
KW  - Robots
KW  - Mathematical model
KW  - Shape
KW  - Reliability
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196706
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes the development of a tilting locomotion system based on a compliant tensegrity structure with multiple stable equilibrium configurations. A tensegrity structure featuring 4 stable equilibrium states is considered. The mechanical model of the structure is presented and the according equations of motion are derived. The variation of the length of selected structural members allows to influence the prestress state and the corresponding shape of the tensegrity structure. Based on bifurcation analyses a reliable actuation strategy to control the current equilibrium state is designed. In this work, the tensegrity structure is assumed to be in contact with a horizontal plane due to gravity. The derived actuation strategy is utilized to generate tilting locomotion by successively changing the equilibrium state. Numerical simulations are evaluated considering the locomotion characteristics. In order to validate this theoretical approach a prototype is developed. Experiments regarding to the equilibrium configurations, the actuation strategy and the locomotion characteristics are evaluated using image processing tools and motion capturing. The results verify the theoretical data and confirm the working principle of the investigated tilting locomotion system. This approach represents a feasible actuation strategy to realize a reliable tilting locomotion utilizing the multistability of compliant tensegrity structures.
ER  - 

TY  - CONF
TI  - A Novel Articulated Soft Robot Capable of Variable Stiffness through Bistable Structure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2939
EP  - 2945
AU  - Y. Zhong
AU  - R. Du
AU  - L. Wu
AU  - H. Yu
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - dexterous manipulators
KW  - elasticity
KW  - manipulator dynamics
KW  - position control
KW  - servomotors
KW  - variable stiffness
KW  - bistable structure
KW  - unstructured environments
KW  - dynamic environments
KW  - highly dissipative nature
KW  - elastic materials results
KW  - load capability
KW  - rigid joints
KW  - compliant bistable structures
KW  - bending stiffness
KW  - articulated soft robot
KW  - force transmission
KW  - position accuracy
KW  - mechanical constrain
KW  - construction method
KW  - servomotor
KW  - variable workspace
KW  - dexterous manipulation
KW  - tip load
KW  - Soft robotics
KW  - Manipulators
KW  - Force
KW  - Springs
KW  - Kinematics
KW  - Shape
KW  - Articulated Soft Robot
KW  - Variable Stiffness
KW  - Bistable Structure
KW  - Locking Function
DO  - 10.1109/ICRA40945.2020.9197479
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robot has demonstrated promise in unstructured and dynamic environments due to unique advantages, such as safe interaction, adaptiveness, easy to actuate, and easy fabrication. However, the highly dissipative nature of elastic materials results in small stiffness of soft robot which limits certain functions, such as force transmission, position accuracy, and load capability. In this paper, we present a novel articulated soft robot with variable stiffness. The robot is constructed by rigid joints and compliant bistable structures in series. Each joint can be independently locked through triggering the bistable structure to touch the mechanical constrain. Thus, the bending stiffness of the joint can be magnified which increases the stiffness of the articulated soft robot. Through this construction method, even driven by only one servomotor, the robot demonstrates variable workspace and stiffness which have the potential of dexterous manipulation and maintaining shape under tip load.
ER  - 

TY  - CONF
TI  - Modeling and Experiments on the Swallowing and Disgorging Characteristics of an Underwater Continuum Manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2946
EP  - 2952
AU  - H. Wang
AU  - H. Xu
AU  - F. Yu
AU  - X. Li
AU  - C. Yang
AU  - S. Chen
AU  - J. Chen
AU  - Y. Zhang
AU  - X. Zhou
PY  - 2020
KW  - bending
KW  - elasticity
KW  - grippers
KW  - hydraulic systems
KW  - manipulator dynamics
KW  - muscle
KW  - pneumatic actuators
KW  - underwater continuum manipulator
KW  - compliant materials
KW  - McKibben water hydraulic artificial muscle
KW  - WHAM
KW  - mechanical properties
KW  - kinematics model
KW  - soft grippers
KW  - bending procedure
KW  - disgorging procedure
KW  - mouth-tongue collaborative soft robot
KW  - single-segment soft robot arm
KW  - swallowing procedure
KW  - Manipulators
KW  - Muscles
KW  - Grippers
KW  - Kinematics
KW  - Hydraulic systems
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196780
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots apply compliant materials to perform motions and behaviors not typically achievable by rigid robots. An underwater, compliant, multi-segment continuum manipulator that can bend, swallow, disgorge is developed in this study. The manipulator is driven by McKibben water hydraulic artificial muscle (WHAM). The mechanical properties of the WHAM are tested and analyzed experimentally. The kinematics model, which concerns about the variable diameter structure of the soft grippers, are established to simulate the behaviors of the manipulator among the bending, swallowing and disgorging procedure. A mouth-tongue collaborative soft robot assembled with another single-segment soft robot arm is presented. And its functions are experimentally testified. The distinctive functions were verified according to the experimental results.
ER  - 

TY  - CONF
TI  - Salamanderbot: A soft-rigid composite continuum mobile robot to traverse complex environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2953
EP  - 2959
AU  - Y. Sun
AU  - Y. Jiang
AU  - H. Yang
AU  - L. -C. Walter
AU  - J. Santoso
AU  - E. H. Skorina
AU  - C. Onal
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - continuously deformable slender body structure
KW  - salamanderbot
KW  - cable-driven bellows-like origami module
KW  - powered wheels
KW  - origami structure
KW  - soft-rigid composite continuum mobile robot
KW  - exploration applications
KW  - mobile soft robots
KW  - Yoshimura crease pattern
KW  - velocity 303.1 mm/s
KW  - radius 79.9 mm
KW  - Mobile robots
KW  - Wheels
KW  - Gears
KW  - Manipulators
KW  - DC motors
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196790
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots are theoretically well-suited to rescue and exploration applications where their flexibility allows for the traversal of highly cluttered environments. However, most existing mobile soft robots are not fast or powerful enough to effectively traverse three dimensional environments. In this paper, we introduce a new mobile robot with a continuously deformable slender body structure, the SalamanderBot, which combines the flexibility and maneuverability of soft robots, with the speed and power of traditional mobile robots. It consists of a cable-driven bellows-like origami module based on the Yoshimura crease pattern mounted between sets of powered wheels. The origami structure allows the body to deform as necessary to adapt to complex environments and terrains, while the wheels allow the robot to reach speeds of up to 303.1 mm/s (2.05 body-length/s). Salamanderbot can climb up to 60-degree slopes and perform sharp turns with a minimum turning radius of 79.9 mm (0.54 body-length).
ER  - 

TY  - CONF
TI  - Flexure Hinge-based Biomimetic Thumb with a Rolling-Surface Metacarpal Joint
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2960
EP  - 2966
AU  - S. Pulleyking
AU  - J. Schultz
PY  - 2020
KW  - biomimetics
KW  - bone
KW  - control system synthesis
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - motion control
KW  - position control
KW  - surgery
KW  - flexure hinge-based biomimetic thumb
KW  - rolling-surface metacarpal joint
KW  - grasping
KW  - dexterous manipulation
KW  - kinematic multiplicity
KW  - robotic hand
KW  - kinematic model
KW  - surgical techniques
KW  - motion capture data
KW  - end effector
KW  - task-space velocities
KW  - tendon excursion velocity
KW  - human thumb state contribution
KW  - data representation
KW  - effector velocity
KW  - Joints
KW  - Fasteners
KW  - Thumb
KW  - Ellipsoids
KW  - Robots
KW  - Prototypes
KW  - Ceramics
DO  - 10.1109/ICRA40945.2020.9196578
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The human thumb's state contribution to grasping and dexterous manipulation of objects is a function of the kinematic multiplicity of joints and structure of the bones, joints, and ligaments. This paper looks at the design and evaluation of a human-like thumb for use in a robotic hand, where the thumb's state contribution to grasping and dexterous manipulation is a function of a simplified kinematic model based on that of the human thumb, but also on empirical trials of surgical techniques to retain functionality while reducing the number of joints in the thumb. Motion Capture Data of the End Effector is analyzed with the measured excursion of the tendons to determine the relationship between tendon velocities and task-space velocities. After validating the procedure experimentally, a simplified metric is proposed to represent this data, and shows that our prototype is predicted to have a relatively smooth mapping between tendon excursion velocity and end effector velocity.
ER  - 

TY  - CONF
TI  - Ibex: A reconfigurable ground vehicle with adaptive terrain navigation capability
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2975
EP  - 2980
AU  - S. Raj
AU  - R. P. Manu Aatitya
AU  - S. Jack Samuel
AU  - J. V. Karthik
AU  - D. Ezhilarasi
PY  - 2020
KW  - friction
KW  - mobile robots
KW  - off-road vehicles
KW  - optimisation
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - stability
KW  - vehicle dynamics
KW  - reconfigurable ground vehicle
KW  - adaptive terrain navigation capability
KW  - unmanned ground vehicle
KW  - dynamic wheelbase
KW  - adaptive thrust
KW  - friction optimization
KW  - steep slopes
KW  - slippery surfaces
KW  - surface topography
KW  - impedance-based stabilization module
KW  - mechanical oscillatory transients
KW  - Ibex
KW  - Force
KW  - Wheels
KW  - Surface topography
KW  - Surface impedance
KW  - Land vehicles
KW  - Kinematics
KW  - Drag
DO  - 10.1109/ICRA40945.2020.9196571
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a unique unmanned ground vehicle with a dynamic wheelbase and an adaptive thrust based friction optimization scheme that aids in the traversal of steep slopes and slippery surfaces. The vehicle is capable of adapting itself to the surface topography using an impedance-based stabilization module to minimize the mechanical oscillatory transients induced during its motion. A detailed analysis of its modules has been elucidated in this paper based on the vehicle parameters. The proposed methodologies have been integrated and tested on a customized prototype. Experimental validation and simulation for the proposed modules at various terrain conditions have been carried out to authenticate its performance.
ER  - 

TY  - CONF
TI  - Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2981
EP  - 2987
AU  - Y. Yue
AU  - C. Yang
AU  - J. Zhang
AU  - M. Wen
AU  - Z. Wu
AU  - H. Zhang
AU  - D. Wang
PY  - 2020
KW  - groupware
KW  - image fusion
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - sensor fusion
KW  - SLAM (robots)
KW  - dynamic collaborative mapping
KW  - multimodal environmental perception
KW  - heterogeneous sensor fusion model
KW  - local 3D maps
KW  - night rainforest
KW  - 3D map fusion missions
KW  - multimodal sensors
KW  - long-term operation
KW  - collaborative robots
KW  - dynamic environment
KW  - dynamic objects
KW  - Collaboration
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197072
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.
ER  - 

TY  - CONF
TI  - Generating Locomotion with Effective Wheel Radius Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2988
EP  - 2994
AU  - T. Hojnik
AU  - L. Pond
AU  - R. Dungavell
AU  - P. Flick
AU  - J. Roberts
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - motor drives
KW  - road vehicles
KW  - robot dynamics
KW  - vehicle dynamics
KW  - wheels
KW  - motor drives
KW  - sloped terrain
KW  - wheel rotation
KW  - plain centre hub drive
KW  - active ride height selection
KW  - wheel radius manipulation
KW  - locomotion generation
KW  - slope traversability
KW  - wheel pose control
KW  - centre of gravity manipulation
KW  - Wheels
KW  - Gravity
KW  - Acceleration
KW  - Mathematical model
KW  - Torque
KW  - Axles
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196825
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Travel over sloped terrain is difficult as an incline changes the interaction between each wheel and the ground resulting in an unbalanced load distribution which can lead to loss of traction and instability. This paper presents a novel approach to generating wheel rotation for primary locomotion by only changing its centre of rotation, or as a complimentary locomotion source to increase versatility of a plain centre hub drive. This is done using linear actuators within a wheel to control the position of the centre hub and induce a moment on the wheel from gravity. In doing so our platform allows for active ride height selection and individual wheel pose control. We present the system with calculations outlining the theoretical properties and perform experiments to validate the concept under loading via multiple gaits to show motion on slopes, and sustained motion over extended distance. We envision applications in conjunction to assist current motor drives and increasing slope traversability by allowing body pose and centre of gravity manipulation, or as a primary locomotion system.
ER  - 

TY  - CONF
TI  - A GNC Architecture for Planetary Rovers with Autonomous Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3003
EP  - 3009
AU  - M. Azkarate
AU  - L. Gerdes
AU  - L. Joudrier
AU  - C. J. P√©rez-del-Pulgar
PY  - 2020
KW  - aerospace navigation
KW  - aerospace robotics
KW  - distance measurement
KW  - Mars
KW  - mobile robots
KW  - optimal control
KW  - path planning
KW  - planetary rovers
KW  - robot vision
KW  - SLAM (robots)
KW  - trajectory control
KW  - GNC architecture
KW  - planetary rovers
KW  - autonomous navigation
KW  - Mars exploration missions
KW  - sample fetching rover
KW  - autonomous capabilities
KW  - two-level architecture
KW  - terrain
KW  - local path replanning
KW  - trajectory control
KW  - global localization
KW  - planetary exploration
KW  - planetary analog field test campaigns
KW  - guidance navigation and control architecture
KW  - hazard detection
KW  - visual odometry
KW  - adaptive SLAM algorithm
KW  - optimal path planning
KW  - Hazards
KW  - Navigation
KW  - Space vehicles
KW  - Autonomous robots
KW  - Computer architecture
KW  - Cameras
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197122
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (lower) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long-range, low supervision, and fast planetary exploration. Both navigation levels have been validated on planetary analog field test campaigns.
ER  - 

TY  - CONF
TI  - Learning Face Recognition Unsupervisedly by Disentanglement and Self-Augmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3018
EP  - 3024
AU  - Y. -L. Lee
AU  - M. -Y. Tseng
AU  - Y. -C. Luo
AU  - D. -R. Yu
AU  - W. -C. Chiu
PY  - 2020
KW  - face recognition
KW  - feature extraction
KW  - home automation
KW  - unsupervised learning
KW  - video surveillance
KW  - triplet network
KW  - augmentation network
KW  - identity-aware features
KW  - face samples
KW  - identity-irrelevant features
KW  - home robot applications
KW  - face recognition system
KW  - smart home environment
KW  - environment-specific face recognition model
KW  - unsupervised learning
KW  - self-augmentation
KW  - healthcare application
KW  - camera position
KW  - surveillance video
KW  - identity-aware feature extraction
KW  - spatiotemporal characteristic
KW  - face image disentanglement
DO  - 10.1109/ICRA40945.2020.9197348
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As the growth of smart home, healthcare, and home robot applications, learning a face recognition system which is specific for a particular environment and capable of self-adapting to the temporal changes in appearance (e.g., caused by illumination or camera position) is nowadays an important topic. In this paper, given a video of a group of people, which simulates the surveillance video in a smart home environment, we propose a novel approach which unsuper- visedly learns a face recognition model based on two main components: (1) a triplet network that extracts identity-aware feature from face images for performing face recognition by clustering, and (2) an augmentation network that is conditioned on the identity-aware features and aims at synthesizing more face samples. Particularly, the training data for the triplet network is obtained by using the spatiotemporal characteristic of face samples within a video, while the augmentation network learns to disentangle a face image into identity-aware and identity-irrelevant features thus is able to generate new faces of the same identity but with variance in appearance. With taking the richer training data produced by augmentation network, the triplet network is further fine-tuned and achieves better performance in face recognition. Extensive experiments not only show the efficacy of our model in learning an environment- specific face recognition model unsupervisedly, but also verify its adaptability to various appearance changes.
ER  - 

TY  - CONF
TI  - PARC: A Plan and Activity Recognition Component for Assistive Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3025
EP  - 3031
AU  - J. Massardi
AU  - M. Gravel
AU  - √â. Beaudry
PY  - 2020
KW  - assisted living
KW  - mobile robots
KW  - robot vision
KW  - activity recognition solutions
KW  - human-object interaction
KW  - low-level actions
KW  - goal recognition algorithm
KW  - low-cost robotics platform
KW  - assistive robots
KW  - mobile robot assistants
KW  - daily living activities
KW  - RGB-D camera
KW  - Activity recognition
KW  - Hidden Markov models
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Clustering algorithms
KW  - Activity recognition
KW  - Plan recognition
KW  - Computer vision
KW  - Robotic assistant
KW  - Activities For Daily Living
KW  - Object affordance
KW  - Particle filter
DO  - 10.1109/ICRA40945.2020.9196856
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robot assistants have many applications, such as helping people in their daily living activities. These robots have to detect and recognize the actions and goals of the humans they are assisting. While there are several wide-spread plan and activity recognition solutions for controlled environments with many built-in sensors, like smart-homes, there is a lack of such systems for mobile robots operating in open settings, such as an apartment. We propose a module for the recognition of activities and goals for daily living by mobile robots, in real time and for complex activities. Our approach recognizes human-object interaction using an RGB-D camera to infer low-level actions which are sent to a goal recognition algorithm. Results show that our approach is both in real time and requires little computational resources, which facilitates its deployment on a mobile and low-cost robotics platform.
ER  - 

TY  - CONF
TI  - Image-Based Place Recognition on Bucolic Environment Across Seasons From Semantic Edge Description
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3032
EP  - 3038
AU  - A. Benbihi
AU  - S. Arravechia
AU  - M. Geist
AU  - C. Pradalier
PY  - 2020
KW  - edge detection
KW  - feature extraction
KW  - image matching
KW  - image retrieval
KW  - image texture
KW  - object recognition
KW  - wavelet transforms
KW  - multiseason environment-monitoring datasets
KW  - urban scenes
KW  - image-based place recognition
KW  - bucolic environment
KW  - semantic edge description
KW  - urban environments
KW  - natural scenes
KW  - semantic content
KW  - vegetation state
KW  - bucolic scene
KW  - global image description
KW  - semantic information
KW  - topological information
KW  - matching two images
KW  - semantic edge transforms
KW  - state-of-the-art image retrieval performance
KW  - Image edge detection
KW  - Semantics
KW  - Image segmentation
KW  - Image retrieval
KW  - Feature extraction
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197529
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Most of the research effort on image-based place recognition is designed for urban environments. In bucolic environments such as natural scenes with low texture and little semantic content, the main challenge is to handle the variations in visual appearance across time such as illumination, weather, vegetation state or viewpoints. The nature of the variations is different and this leads to a different approach to describing a bucolic scene. We introduce a global image description computed from its semantic and topological information. It is built from the wavelet transforms of the image's semantic edges. Matching two images is then equivalent to matching their semantic edge transforms. This method reaches state-of-the-art image retrieval performance on two multi-season environment-monitoring datasets: the CMU-Seasons and the Symphony Lake dataset. It also generalizes to urban scenes on which it is on par with the current baselines NetVLAD and DELF.
ER  - 

TY  - CONF
TI  - A Multilayer-Multimodal Fusion Architecture for Pattern Recognition of Natural Manipulations in Percutaneous Coronary Interventions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3039
EP  - 3045
AU  - X. -H. Zhou
AU  - X. -L. Xie
AU  - Z. -Q. Feng
AU  - Z. -G. Hou
AU  - G. -B. Bian
AU  - R. -Q. Li
AU  - Z. -L. Ni
AU  - S. -Q. Liu
AU  - Y. -J. Zhou
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - manipulators
KW  - medical computing
KW  - medical robotics
KW  - sensor fusion
KW  - multilayer-multimodal fusion architecture
KW  - pattern recognition
KW  - natural manipulations
KW  - percutaneous coronary interventions
KW  - robotic systems
KW  - robot-assisted procedures
KW  - human-robot interfaces
KW  - guidewire manipulations
KW  - multimodal behaviors
KW  - rule-based fusion algorithms
KW  - singlelayer recognition architecture
KW  - robot-assisted PCI
KW  - HRI
KW  - X-ray radiation reduction
KW  - medical staff
KW  - Sensors
KW  - Robots
KW  - Muscles
KW  - Feature extraction
KW  - Force
KW  - Catheters
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9197111
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The increasingly-used robotic systems can provide precise delivery and reduce X-ray radiation to medical staff in percutaneous coronary interventions (PCI), but natural manipulations of interventionalists are forgone in most robot-assisted procedures. Therefore, it is necessary to explore natural manipulations to design more advanced human-robot interfaces (HRI). In this study, a multilayer-multimodal fusion architecture is proposed to recognize six typical subpatterns of guidewire manipulations in conventional PCI. The synchronously acquired multimodal behaviors from ten subjects are used as the inputs of the fusion architecture. Six classification-based and two rule-based fusion algorithms are evaluated for performance comparisons. Experimental results indicate that the multimodal fusion brings significant accuracy improvement in comparison with single-modal schemes. Furthermore, the proposed architecture can achieve the overall accuracy of 96.90%, much higher than that of a singlelayer recognition architecture (92.56%). These results have indicated the potential of the proposed method for facilitating the development of HRI for robot-assisted PCI.
ER  - 

TY  - CONF
TI  - Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3106
EP  - 3111
AU  - C. Schulz
AU  - A. Zell
PY  - 2020
KW  - graph theory
KW  - least squares approximations
KW  - mobile robots
KW  - normal distribution
KW  - robot vision
KW  - SLAM (robots)
KW  - occupancy grid map
KW  - graph-based SLAM
KW  - occupancy normal distribution transforms
KW  - normal distributions transforms
KW  - simultaneous localization and mapping
KW  - mobile robotics
KW  - least squares problem
KW  - nonlinear optimizers
KW  - global NDT scan matcher
KW  - Simultaneous localization and mapping
KW  - Cost function
KW  - Google
KW  - Jacobian matrices
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9197325
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simultaneous Localization and Mapping (SLAM) is one of the basic problems in mobile robotics. While most approaches are based on occupancy grid maps, Normal Distributions Transforms (NDT) and mixtures like Occupancy Normal Distribution Transforms (ONDT) have been shown to represent sensor measurements more accurately. In this work, we slightly re-formulate the (O)NDT matching function such that it becomes a least squares problem that can be solved with various robust numerical and analytical non-linear optimizers. Further, we propose a novel global (O)NDT scan matcher for loop closure. In our evaluation, our NDT and ONDT methods are able to outperform the occupancy grid map based ones we adopted from Google's Cartographer implementation.
ER  - 

TY  - CONF
TI  - Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3112
EP  - 3118
AU  - N. Chebrolu
AU  - T. L√§be
AU  - C. Stachniss
PY  - 2020
KW  - agriculture
KW  - cameras
KW  - image registration
KW  - robot vision
KW  - spatio-temporal nonrigid registration
KW  - 3D point clouds
KW  - sensor data
KW  - agricultural robotics
KW  - plant science
KW  - agricultural tasks
KW  - automated temporal plant-trait analysis
KW  - plant performance monitoring
KW  - plant registration
KW  - Three-dimensional displays
KW  - Skeleton
KW  - Hidden Markov models
KW  - Strain
KW  - Topology
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197569
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Analyzing sensor data of plants and monitoring plant performance is a central element in different agricultural robotics applications. In plant science, phenotyping refers to analyzing plant traits for monitoring growth, for describing plant properties, or characterizing the plant's overall performance. It plays a critical role in the agricultural tasks and in plant breeding. Recently, there is a rising interest in using 3D data obtained from laser scanners and 3D cameras to develop automated non-intrusive techniques for estimating plant traits. In this paper, we address the problem of registering 3D point clouds of the plants over time, which is a backbone of applications interested in tracking spatio-temporal traits of individual plants. Registering plants over time is challenging due to its changing topology, anisotropic growth, and non-rigid motion in between scans. We propose a novel approach that exploits the skeletal structure of the plant and determines correspondences over time and drives the registration process. Our approach explicitly accounts for the non-rigidity and the growth of the plant over time in the registration. We tested our approach on a challenging dataset acquired over the course of two weeks and successfully registered the 3D plant point clouds recorded with a laser scanner forming a basis for developing systems for automated temporal plant-trait analysis.
ER  - 

TY  - CONF
TI  - Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3119
EP  - 3125
AU  - R. Nakashima
AU  - A. Seki
PY  - 2020
KW  - distance measurement
KW  - image fusion
KW  - Jacobian matrices
KW  - motion estimation
KW  - visual-inertial odometry
KW  - inertial measurement units
KW  - scale-aware estimation
KW  - sensor states
KW  - sensor motion
KW  - noninformative inertial measurements
KW  - estimation modes
KW  - motion characteristics
KW  - uncertainty-based adaptive sensor fusion
KW  - uncertainty-based sensor fusion
KW  - relative motion estimation
KW  - observability
KW  - Jacobian matrices
KW  - Estimation
KW  - Bundle adjustment
KW  - Motion measurement
KW  - Velocity measurement
KW  - Uncertainty
KW  - Sensor fusion
DO  - 10.1109/ICRA40945.2020.9197397
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose an uncertainty-based sensor fusion framework for visual-inertial odometry, which is the task of estimating relative motion using images and measurements from inertial measurement units. Visual-inertial odometry enables robust and scale-aware estimation of motion by incorporating sensor states, such as metric scale, velocity, and the direction of gravity, into the estimation. However, the observability of the states depends on sensor motion. For example, if the sensor moves in a constant velocity, scale and velocity cannot be observed from inertial measurements. Under these degenerate motions, existing methods may produce inaccurate results because they incorporate erroneous states estimated from non-informative inertial measurements. Our proposed framework is able to avoid this situation by adaptively switching estimation modes, which represents the states that should be incorporated, based on their uncertainties. These uncertainties can be obtained at a small computational cost by reusing the Jacobian matrices computed in bundle adjustment. Our approach consistently outperformed conventional sensor fusion in datasets with different motion characteristics, namely, the KITTI odometry dataset recorded by a ground vehicle and the EuRoC MAV dataset captured from a micro aerial vehicle.
ER  - 

TY  - CONF
TI  - Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3126
EP  - 3131
AU  - J. Lin
AU  - F. Zhang
PY  - 2020
KW  - distance measurement
KW  - mobile robots
KW  - optical radar
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - FoV
KW  - autonomous vehicles
KW  - autonomous navigation
KW  - path planning
KW  - LOAM algorithm
KW  - LiDAR odometry and mapping
KW  - robot pose localization
KW  - Laser radar
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Measurement by laser beam
KW  - Laser noise
KW  - Real-time systems
KW  - Spinning
DO  - 10.1109/ICRA40945.2020.9197440
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.
ER  - 

TY  - CONF
TI  - Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3132
EP  - 3138
AU  - S. Suresh
AU  - P. Sodhi
AU  - J. G. Mangelson
AU  - D. Wettergreen
AU  - M. Kaess
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - underwater volumetric exploration
KW  - active SLAM framework
KW  - 3D underwater environments
KW  - multibeam sonar
KW  - integrated SLAM
KW  - volumetric free-space information
KW  - informative loop closures
KW  - navigation policy
KW  - 3D visual dictionary
KW  - submap saliency
KW  - sensor information
KW  - pose-graph SLAM formulation
KW  - global occupancy grid map
KW  - uncertainty-agnostic framework
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Uncertainty
KW  - Conferences
KW  - Automation
KW  - Sonar
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196939
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.
ER  - 

TY  - CONF
TI  - Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3139
EP  - 3145
AU  - X. Shi
AU  - D. Li
AU  - P. Zhao
AU  - Q. Tian
AU  - Y. Tian
AU  - Q. Long
AU  - C. Zhu
AU  - J. Song
AU  - F. Qiao
AU  - L. Song
AU  - Y. Guo
AU  - Z. Wang
AU  - Y. Zhang
AU  - B. Qin
AU  - W. Yang
AU  - F. Wang
AU  - R. H. M. Chan
AU  - Q. She
PY  - 2020
KW  - mobile robots
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - service robots
KW  - SLAM (robots)
KW  - simultaneous localization and mapping
KW  - data sequences
KW  - robotic autonomy
KW  - service robots
KW  - real-world indoor scenes
KW  - OpenLORIS-Scene datasets
KW  - SLAM problems
KW  - pose estimation
KW  - Simultaneous localization and mapping
KW  - Robot kinematics
KW  - Cameras
KW  - Synchronization
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196638
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.
ER  - 

TY  - CONF
TI  - RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3146
EP  - 3152
AU  - S. Herath
AU  - H. Yan
AU  - Y. Furukawa
PY  - 2020
KW  - computer vision
KW  - image motion analysis
KW  - inertial navigation
KW  - neural nets
KW  - robust neural inertial navigation
KW  - data-driven inertial navigation research
KW  - horizontal positions
KW  - moving subject
KW  - IMU sensor data
KW  - ground-truth 3D trajectories
KW  - natural human motions
KW  - RoNIN
KW  - Inertial navigation
KW  - Three-dimensional displays
KW  - Robustness
KW  - Estimation
KW  - Trajectory
KW  - Task analysis
KW  - Gravity
DO  - 10.1109/ICRA40945.2020.9196860
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http://ronin.cs.sfu.ca).
ER  - 

TY  - CONF
TI  - Segmenting 2K-Videos at 36.5 FPS with 24.3 GFLOPs: Accurate and Lightweight Realtime Semantic Segmentation Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3153
EP  - 3160
AU  - D. Oh
AU  - D. Ji
AU  - C. Jang
AU  - Y. Hyun
AU  - H. S. Bae
AU  - S. Hwang
PY  - 2020
KW  - convolutional neural nets
KW  - image classification
KW  - image segmentation
KW  - object detection
KW  - video signal processing
KW  - lightweight segementation models
KW  - 2K-Videos
KW  - high resolution videos
KW  - NfS-SegNet
KW  - computation-efficiency
KW  - encoder network
KW  - NfS-Net
KW  - simple building blocks
KW  - memory-heavy operations
KW  - depthwise convolutions
KW  - CNN architectures
KW  - asymmetric architecture
KW  - deeper encoder
KW  - uncertainty-aware knowledge distillation method
KW  - realtime semantic segmentation network
KW  - CITYSCAPE benchmark
KW  - computer speed 24.3 GFLOPS
KW  - Knowledge engineering
KW  - Computational modeling
KW  - Real-time systems
KW  - Computer architecture
KW  - Semantics
KW  - Uncertainty
KW  - Videos
DO  - 10.1109/ICRA40945.2020.9196510
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a fast and lightweight end-to-end convolutional network architecture for real-time segmentation of high resolution videos, NfS-SegNet, that can segement 2K-videos at 36.5 FPS with 24.3 GFLOPS. This speed and computation-efficiency is due to following reasons: 1) The encoder network, NfS-Net, is optimized for speed with simple building blocks without memory-heavy operations such as depthwise convolutions, and outperforms state-of-the-art lightweight CNN architectures such as SqueezeNet [2], Mo- bileNet v1 [3] & v2 [4] and ShuffleNet v1 [5] & v2 [6] on image classification with significantly higher speed. 2) The NfS- SegNet has an asymmetric architecture with deeper encoder and shallow decoder, whose design is based on our empirical finding that the decoder is the main bottleneck in computation with relatively small contribution to the final performance. 3) Our novel uncertainty-aware knowledge distillation method guides the teacher model to focus its knowledge transfer on the most difficult image regions. We validate the performance of NfS-SegNet with the CITYSCAPE [1] benchmark, on which it achieves state-of-the-art performance among lightweight segementation models in terms of both accuracy and speed.
ER  - 

TY  - CONF
TI  - Temporally Consistent Horizon Lines
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3161
EP  - 3167
AU  - F. Kluger
AU  - H. Ackermann
AU  - M. Y. Yang
AU  - B. Rosenhahn
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - image reconstruction
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - recurrent neural nets
KW  - stereo image processing
KW  - video signal processing
KW  - geometric feature
KW  - image processing
KW  - scene understanding
KW  - computer vision
KW  - autonomous vehicle navigation
KW  - driver assistance
KW  - semantic interpretation
KW  - dynamic environments
KW  - convolutional neural networks
KW  - residual convolutional LSTM
KW  - temporally consistent horizon line estimation
KW  - video sequences
KW  - 3D reconstruction
KW  - CNN architecture
KW  - adaptive loss function
KW  - Video sequences
KW  - Cameras
KW  - Three-dimensional displays
KW  - Observers
KW  - Task analysis
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197170
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The horizon line is an important geometric feature for many image processing and scene understanding tasks in computer vision. For instance, in navigation of autonomous vehicles or driver assistance, it can be used to improve 3D reconstruction as well as for semantic interpretation of dynamic environments. While both algorithms and datasets exist for single images, the problem of horizon line estimation from video sequences has not gained attention. In this paper, we show how convolutional neural networks are able to utilise the temporal consistency imposed by video sequences in order to increase the accuracy and reduce the variance of horizon line estimates. A novel CNN architecture with an improved residual convolutional LSTM is presented for temporally consistent horizon line estimation. We propose an adaptive loss function that ensures stable training as well as accurate results. Furthermore, we introduce an extension of the KITTI dataset which contains precise horizon line labels for 43699 images across 72 video sequences. A comprehensive evaluation shows that the proposed approach consistently achieves superior performance compared with existing methods.
ER  - 

TY  - CONF
TI  - Full-Scale Continuous Synthetic Sonar Data Generation with Markov Conditional Generative Adversarial Networks*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3168
EP  - 3174
AU  - M. Jegorova
AU  - A. I. Karjalainen
AU  - J. Vazquez
AU  - T. Hospedales
PY  - 2020
KW  - acoustic signal processing
KW  - autonomous underwater vehicles
KW  - data analysis
KW  - environmental factors
KW  - Markov processes
KW  - naval engineering computing
KW  - neural nets
KW  - object recognition
KW  - realistic images
KW  - sonar imaging
KW  - statistical analysis
KW  - bootstrapping ATR systems
KW  - autonomous underwater vehicles
KW  - autonomous target recognition systems
KW  - realistic synthetic sonar imagery
KW  - Markov conditional generative adversarial networks
KW  - continuous synthetic sonar data generation
KW  - Markov conditional pix2pix
KW  - environmental factors
KW  - acoustic sensors
KW  - Sonar
KW  - Training
KW  - Semantics
KW  - Data models
KW  - Gallium nitride
KW  - Training data
KW  - Markov processes
DO  - 10.1109/ICRA40945.2020.9197353
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deployment and operation of autonomous underwater vehicles is expensive and time-consuming. High-quality realistic sonar data simulation could be of benefit to multiple applications, including training of human operators for post-mission analysis, as well as tuning and validation of autonomous target recognition (ATR) systems for underwater vehicles. Producing realistic synthetic sonar imagery is a challenging problem as the model has to account for specific artefacts of real acoustic sensors, vehicle attitude, and a variety of environmental factors. We propose a novel method for generating realistic-looking sonar side-scans of full-length missions, called Markov Conditional pix2pix (MC-pix2pix). Quantitative assessment results confirm that the quality of the produced data is almost indistinguishable from real. Furthermore, we show that bootstrapping ATR systems with MC-pix2pix data can improve the performance. Synthetic data is generated 18 times faster than real acquisition speed, with full user control over the topography of the generated data.
ER  - 

TY  - CONF
TI  - Adaptively Informed Trees (AIT*): Fast Asymptotically Optimal Path Planning through Adaptive Heuristics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3191
EP  - 3198
AU  - M. P. Strub
AU  - J. D. Gammell
PY  - 2020
KW  - path planning
KW  - sampling methods
KW  - search problems
KW  - trees (mathematics)
KW  - heuristic estimates
KW  - AIT*
KW  - asymmetric bidirectional search
KW  - optimal path planning
KW  - informed sampling-based planning algorithm
KW  - adaptively informed trees
KW  - Search problems
KW  - Image edge detection
KW  - Approximation algorithms
KW  - Planning
KW  - Heuristic algorithms
KW  - Databases
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197338
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Informed sampling-based planning algorithms exploit problem knowledge for better search performance. This knowledge is often expressed as heuristic estimates of solution cost and used to order the search. The practical improvement of this informed search depends on the accuracy of the heuristic. Selecting an appropriate heuristic is difficult. Heuristics applicable to an entire problem domain are often simple to define and inexpensive to evaluate but may not be beneficial for a specific problem instance. Heuristics specific to a problem instance are often difficult to define or expensive to evaluate but can make the search itself trivial. This paper presents Adaptively Informed Trees (AIT*), an almost-surely asymptotically optimal sampling-based planner based on BIT*. AIT* adapts its search to each problem instance by using an asymmetric bidirectional search to simultaneously estimate and exploit a problem-specific heuristic. This allows it to quickly find initial solutions and converge towards the optimum. AIT* solves the tested problems as fast as RRT-Connect while also converging towards the optimum.
ER  - 

TY  - CONF
TI  - Informing Multi-Modal Planning with Synergistic Discrete Leads
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3199
EP  - 3205
AU  - Z. Kingston
AU  - A. M. Wells
AU  - M. Moll
AU  - L. E. Kavraki
PY  - 2020
KW  - continuous systems
KW  - discrete systems
KW  - manipulators
KW  - multimodal planning
KW  - robotic manipulation problems
KW  - continuous infinity
KW  - object grasping
KW  - manipulation plan
KW  - single-mode motions
KW  - valid transitions
KW  - manipulation planners
KW  - multimodal structure
KW  - mode-specific planners
KW  - general layered planning approach
KW  - pick-and-place manipulation domain
KW  - synergistic discrete leads
KW  - specific mode transitions
KW  - useful mode transitions
KW  - bias search
KW  - Planning
KW  - Manifolds
KW  - Task analysis
KW  - Lead
KW  - Probabilistic logic
KW  - Robot motion
DO  - 10.1109/ICRA40945.2020.9197545
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic manipulation problems are inherently continuous, but typically have underlying discrete structure, e.g., whether or not an object is grasped. This means many problems are multi-modal and in particular have a continuous infinity of modes. For example, in a pick-and-place manipulation domain, every grasp and placement of an object is a mode. Usually manipulation problems require the robot to transition into different modes, e.g., going from a mode with an object placed to another mode with the object grasped. To successfully find a manipulation plan, a planner must find a sequence of valid single-mode motions as well as valid transitions between these modes. Many manipulation planners have been proposed to solve tasks with multi-modal structure. However, these methods require mode-specific planners and fail to scale to very cluttered environments or to tasks that require long sequences of transitions. This paper presents a general layered planning approach to multi-modal planning that uses a discrete "lead" to bias search towards useful mode transitions. The difficulty of achieving specific mode transitions is captured online and used to bias search towards more promising sequences of modes. We demonstrate our planner on complex scenes and show that significant performance improvements are tied to both our discrete "lead" and our continuous representation.
ER  - 

TY  - CONF
TI  - Hierarchical Coverage Path Planning in Complex 3D Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3206
EP  - 3212
AU  - C. Cao
AU  - J. Zhang
AU  - M. Travers
AU  - H. Choset
PY  - 2020
KW  - autonomous aerial vehicles
KW  - hierarchical systems
KW  - image resolution
KW  - image sampling
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - complex three-dimensional environment
KW  - nooks
KW  - crannies
KW  - coverage planning
KW  - multiresolution hierarchical framework
KW  - three-dimensional scenes
KW  - hierarchical coverage path planning
KW  - lightweight UAV
KW  - low-level sampling
KW  - complex 3D environments
KW  - Planning
KW  - Robot sensing systems
KW  - Cameras
KW  - Octrees
KW  - Three-dimensional displays
KW  - Surface treatment
DO  - 10.1109/ICRA40945.2020.9196575
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State-of-the-art coverage planning methods perform well in simple environments but take an ineffectively long time to converge to an optimal solution in complex three-dimensional (3D) environments. As more structures are present in the same volume of workspace, these methods slow down as they spend more time searching for all of the nooks and crannies concealed in three-dimensional spaces. This work presents a method for coverage planning that employs a multi-resolution hierarchical framework to solve the problem at two different levels, producing much higher efficiency than the state-of-the-art. First, a high-level algorithm separates the environment into multiple subspaces at different resolutions and computes an order of the subspaces for traversal. Second, a low-level sampling-based algorithm solves for paths within the subspaces for detailed coverage. In experiments, we evaluate our method using real-world datasets from complex three-dimensional scenes. Our method finds paths that are constantly shorter and converges at least ten times faster than the state-of-the-art. Further, we show results of a physical experiment where a lightweight UAV follows the paths to realize the coverage.
ER  - 

TY  - CONF
TI  - Perception-aware time optimal path parameterization for quadrotors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3213
EP  - 3219
AU  - I. Spasojevic
AU  - V. Murali
AU  - S. Karaman
PY  - 2020
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - perception-aware time optimal path parameterization
KW  - quadrotors
KW  - perception-aware time optimal path parametrization
KW  - quadrotor systems
KW  - on-board navigation
KW  - estimation algorithms
KW  - planning
KW  - efficient time optimal path parametrization algorithm
KW  - quadrotor platform
KW  - vision-driven vehicles
KW  - Trajectory
KW  - Cameras
KW  - Planning
KW  - Task analysis
KW  - Aerodynamics
KW  - Heuristic algorithms
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197157
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The increasing popularity of quadrotors has given rise to a class of predominantly vision-driven vehicles. This paper addresses the problem of perception-aware time optimal path parametrization for quadrotors. Although many different choices of perceptual modalities are available, the low weight and power budgets of quadrotor systems makes a camera ideal for on-board navigation and estimation algorithms. However, this does come with a set of challenges. The limited field of view of the camera can restrict the visibility of salient regions in the environment, which dictates the necessity to consider perception and planning jointly. The main contribution of this paper is an efficient time optimal path parametrization algorithm for quadrotors with limited field of view constraints. We show in a simulation study that a state-of-the-art controller can track planned trajectories, and we validate the proposed algorithm on a quadrotor platform in experiments.
ER  - 

TY  - CONF
TI  - Generating Visibility-Aware Trajectories for Cooperative and Proactive Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3220
EP  - 3226
AU  - N. Buckman
AU  - A. Pierson
AU  - S. Karaman
AU  - D. Rus
PY  - 2020
KW  - mobile robots
KW  - motion estimation
KW  - object detection
KW  - path planning
KW  - road traffic
KW  - road vehicles
KW  - vehicles
KW  - autonomous vehicle
KW  - ego vehicle
KW  - visibility-aware planning
KW  - visibility-aware trajectories
KW  - proactive motion planning
KW  - cooperative motion planning
KW  - partially-occluded intersection
KW  - emergent behavior
KW  - Trajectory
KW  - Uncertainty
KW  - Planning
KW  - Safety
KW  - Autonomous vehicles
KW  - Splines (mathematics)
DO  - 10.1109/ICRA40945.2020.9196809
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The safety of an autonomous vehicle not only depends on its own perception of the world around it, but also on the perception and recognition from other vehicles. If an ego vehicle considers the uncertainty other vehicles have about itself, then by reducing the estimated uncertainty it can increase its safety. In this paper, we focus on how an ego vehicle plans its trajectories through the blind spots of other vehicles. We create visibility-aware planning, where the ego vehicle chooses its trajectories such that it reduces the perceived uncertainty other vehicles may have about the state of the ego vehicle. We present simulations of traffic and highway environments, where an ego vehicle must pass another vehicle, make a lane change, or traverse a partially-occluded intersection. Emergent behavior shows that when using visibility-aware planning, the ego vehicle spends less time in a blind spot, and may slow down before entering the blind spot so as to increase the likelihood other vehicles perceive the ego vehicle.
ER  - 

TY  - CONF
TI  - An obstacle-interaction planning method for navigation of actuated vine robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3227
EP  - 3233
AU  - M. Selvaggio
AU  - L. A. Ramirez
AU  - N. D. Naclerio
AU  - B. Siciliano
AU  - E. W. Hawkes
PY  - 2020
KW  - bending
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - soft robotics
KW  - reliable robot-environment interaction models
KW  - obstacle-interaction model
KW  - robot tip
KW  - obstacle-interaction planning method
KW  - actuated vine robot navigation
KW  - wrinkling deformation
KW  - bending deformation
KW  - Soft robotics
KW  - Strain
KW  - Deformable models
KW  - Planning
KW  - Kinematics
KW  - Pneumatic systems
DO  - 10.1109/ICRA40945.2020.9196587
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The field of soft robotics is grounded on the idea that, due to their inherent compliance, soft robots can safely interact with the environment. Thus, the development of effective planning and control pipelines for soft robots should incorporate reliable robot-environment interaction models. This strategy enables soft robots to effectively exploit contacts to autonomously navigate and accomplish tasks in the environment. However, for a class of soft robots, namely vine-inspired, tip-extending or "vine" robots, such interaction models and the resulting planning and control strategies do not exist. In this paper, we analyze the behavior of vine robots interacting with their environment and propose an obstacle-interaction model that characterizes the bending and wrinkling deformation induced by the environment. Starting from this, we devise a novel obstacle-interaction planning method for these robots. We show how obstacle interactions can be effectively leveraged to enlarge the set of reachable workspace for the robot tip, and verify our findings with both simulated and real experiments. Our work improves the capabilities of this new class of soft robot, helping to advance the field of soft robotics.
ER  - 

TY  - CONF
TI  - Distributed Consensus Control of Multiple UAVs in a Constrained Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3234
EP  - 3240
AU  - G. Wang
AU  - W. Yang
AU  - N. Zhao
AU  - Y. Ji
AU  - Y. Shen
AU  - H. Xu
AU  - P. Li
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - decentralised control
KW  - distributed control
KW  - multi-robot systems
KW  - position control
KW  - tracking
KW  - trees (mathematics)
KW  - multiple UAVs
KW  - constrained environment
KW  - consensus problem
KW  - multiple unmanned aerial vehicles
KW  - environmental constraints
KW  - general communication topology
KW  - directed spanning tree
KW  - position transformation function
KW  - dynamic reference position
KW  - yaw angle
KW  - asymmetric topology
KW  - local tracking controller
KW  - distributed consensus control
KW  - Topology
KW  - Decentralized control
KW  - Protocols
KW  - Unmanned aerial vehicles
KW  - Tracking loops
KW  - Heuristic algorithms
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9196926
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we investigate the consensus problem of multiple unmanned aerial vehicles (UAVs) in the presence of environmental constraints under a general communication topology containing a directed spanning tree. First, based on a position transformation function, we propose a novel dynamic reference position and yaw angle for each UAV to cope with both the asymmetric topology and the constraints. Then, the backstepping-like design methodology is presented to derive a local tracking controller for each UAV such that its position and yaw angle can converge to the reference ones. The proposed protocol is distributed in the sense that, the input update of each UAV dynamically relies only on local state information from its neighborhood set and the constraints, and it does not require any additional centralized information. It is demonstrated that under the proposed protocol, all UAVs reach consensus without violation of the environmental constraints. Finally, simulation and experimental results are provided to demonstrate the performance of the protocol.
ER  - 

TY  - CONF
TI  - Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3241
EP  - 3247
AU  - G. Shi
AU  - W. H√∂nig
AU  - Y. Yue
AU  - S. -J. Chung
PY  - 2020
KW  - aerodynamics
KW  - aerospace robotics
KW  - control system synthesis
KW  - decentralised control
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - neurocontrollers
KW  - nonlinear control systems
KW  - particle swarm optimisation
KW  - stability
KW  - close-proximity multirotor control
KW  - learned interactions
KW  - Neural-Swarm
KW  - nonlinear decentralized stable controller
KW  - close-proximity flight
KW  - multirotor swarms
KW  - close-proximity control
KW  - complex aerodynamic interaction effects
KW  - safety distances
KW  - nominal dynamics model
KW  - regularized permutation-invariant Deep Neural Network
KW  - high-order multivehicle interactions
KW  - larger swarm sizes
KW  - baseline nonlinear
KW  - stable nonlinear tracking controller
KW  - Vehicle dynamics
KW  - Aerodynamics
KW  - Neural networks
KW  - Rotors
KW  - Stability analysis
KW  - Training
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196800
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present Neural-Swarm, a nonlinear decentralized stable controller for close-proximity flight of multirotor swarms. Close-proximity control is challenging due to the complex aerodynamic interaction effects between multirotors, such as downwash from higher vehicles to lower ones. Conventional methods often fail to properly capture these interaction effects, resulting in controllers that must maintain large safety distances between vehicles, and thus are not capable of close-proximity flight. Our approach combines a nominal dynamics model with a regularized permutation-invariant Deep Neural Network (DNN) that accurately learns the high-order multi-vehicle interactions. We design a stable nonlinear tracking controller using the learned model. Experimental results demonstrate that the proposed controller significantly outperforms a baseline nonlinear tracking controller with up to four times smaller worst-case height tracking errors. We also empirically demonstrate the ability of our learned model to generalize to larger swarm sizes.
ER  - 

TY  - CONF
TI  - Line Coverage with Multiple Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3248
EP  - 3254
AU  - S. Agarwal
AU  - S. Akella
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - integer programming
KW  - linear programming
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - robot tour generation
KW  - multiple robots
KW  - line coverage problem
KW  - mixed integer linear program
KW  - NP-hard
KW  - merge-embed-merge
KW  - MEM algorithm
KW  - graph simplification
KW  - graph partitioning
KW  - Roads
KW  - Task analysis
KW  - Robot sensing systems
KW  - Routing
KW  - Heuristic algorithms
KW  - Partitioning algorithms
DO  - 10.1109/ICRA40945.2020.9197292
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The line coverage problem is the coverage of linear environment features (e.g., road networks, power lines), modeled as 1D segments, by one or more robots while respecting resource constraints (e.g., battery capacity, flight time) for each of the robots. The robots incur direction dependent costs and resource demands as they traverse the edges. We treat the line coverage problem as an optimization problem, with the total cost of the tours as the objective, by formulating it as a mixed integer linear program (MILP). The line coverage problem is NP-hard and hence we develop a heuristic algorithm, Merge-Embed-Merge (MEM). We compare it against the optimal MILP approach and a baseline heuristic algorithm, Extended Path Scanning. We show the MEM algorithm is fast and suitable for real-time applications. To tackle large-scale problems, our approach performs graph simplification and graph partitioning, followed by robot tour generation for each of the partitioned subgraphs. We demonstrate our approach on a large graph with 4,658 edges and 4,504 vertices that represents an urban region of about 16 sq. km. We compare the performance of the algorithms on several small road networks and experimentally demonstrate the approach using UAVs on the UNC Charlotte campus road network.
ER  - 

TY  - CONF
TI  - Visual Coverage Maintenance for Quadcopters Using Nonsmooth Barrier Functions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3255
EP  - 3261
AU  - R. Funada
AU  - M. Santos
AU  - T. Gencho
AU  - J. Yamauchi
AU  - M. Fujita
AU  - M. Egerstedt
PY  - 2020
KW  - aircraft control
KW  - helicopters
KW  - image sensors
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - quadcopters
KW  - visual sensors
KW  - coverage holes
KW  - coverage quality
KW  - sufficient conditions
KW  - nonsmooth barrier functions
KW  - visual coverage maintenance
KW  - coverage control
KW  - necessary conditions
KW  - Visualization
KW  - Monitoring
KW  - Robot sensing systems
KW  - Switches
KW  - Space missions
DO  - 10.1109/ICRA40945.2020.9196650
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a coverage control algorithm for teams of quadcopters with downward facing visual sensors that prevents the appearance of coverage holes in-between the monitored areas while maximizing the coverage quality as much as possible. We derive necessary and sufficient conditions for preventing the appearance of holes in-between the fields of views among trios of robots. Because this condition can be expressed as logically combined constraints, control nonsmooth barrier functions are implemented to enforce it. An algorithm which extends control nonsmooth barrier functions to hybrid systems is implemented to manage the switching among barrier functions caused by the changes of the robots composing trio. The performance and validity of the proposed algorithm are evaluated in simulation as well as on a team of quadcopters.
ER  - 

TY  - CONF
TI  - Goal-Directed Occupancy Prediction for Lane-Following Actors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3270
EP  - 3276
AU  - P. Kaniarasu
AU  - G. C. Haynes
AU  - M. Marchetti-Bowick
PY  - 2020
KW  - mobile robots
KW  - motion estimation
KW  - prediction theory
KW  - road safety
KW  - road traffic
KW  - road vehicles
KW  - roads
KW  - robot vision
KW  - traffic engineering computing
KW  - complex road networks
KW  - mapped road topology
KW  - dynamic road actors
KW  - mapped lane geometry
KW  - mode collapse problem
KW  - goal-directed occupancy prediction
KW  - lane-following actors
KW  - shared roads
KW  - safe autonomous driving
KW  - possible vehicle behaviors
KW  - possible goal reasoning
KW  - local scene context multimodality
KW  - high-level action set
KW  - future spatial occupancy prediction
KW  - Roads
KW  - Predictive models
KW  - Trajectory
KW  - Topology
KW  - Geometry
KW  - Task analysis
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197495
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Predicting the possible future behaviors of vehicles that drive on shared roads is a crucial task for safe autonomous driving. Many existing approaches to this problem strive to distill all possible vehicle behaviors into a simplified set of high-level actions. However, these action categories do not suffice to describe the full range of maneuvers possible in the complex road networks we encounter in the real world. To combat this deficiency, we propose a new method that leverages the mapped road topology to reason over possible goals and predict the future spatial occupancy of dynamic road actors. We show that our approach is able to accurately predict future occupancy that remains consistent with the mapped lane geometry and naturally captures multi-modality based on the local scene context while also not suffering from the mode collapse problem observed in prior work.
ER  - 

TY  - CONF
TI  - Intent-Aware Pedestrian Prediction for Adaptive Crowd Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3277
EP  - 3283
AU  - K. D. Katyal
AU  - G. D. Hager
AU  - C. -M. Huang
PY  - 2020
KW  - adaptive control
KW  - collision avoidance
KW  - control engineering computing
KW  - mobile robots
KW  - navigation
KW  - pedestrians
KW  - road traffic control
KW  - traffic engineering computing
KW  - personal space violation
KW  - adaptive navigation policy
KW  - pedestrian motion
KW  - intent-aware pedestrian prediction
KW  - adaptive crowd navigation
KW  - mobile robots
KW  - pedestrian rich environments
KW  - robotic assistance
KW  - pedestrian navigation
KW  - real-world pedestrian datasets
KW  - Navigation
KW  - Trajectory
KW  - Robots
KW  - Prediction algorithms
KW  - Training
KW  - Collision avoidance
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197434
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robots capable of navigating seamlessly and safely in pedestrian rich environments promise to bring robotic assistance closer to our daily lives. In this paper we draw on insights of how humans move in crowded spaces to explore how to recognize pedestrian navigation intent, how to predict pedestrian motion and how a robot may adapt its navigation policy dynamically when facing unexpected human movements. Our approach is to develop algorithms that replicate this behavior. We experimentally demonstrate the effectiveness of our prediction algorithm using real-world pedestrian datasets and achieve comparable or better prediction accuracy compared to several state-of-the-art approaches. Moreover, we show that confidence of pedestrian prediction can be used to adjust the risk of a navigation policy adaptively to afford the most comfortable level as measured by the frequency of personal space violation in comparison with baselines. Furthermore, our adaptive navigation policy is able to reduce the number of collisions by 43% in the presence of novel pedestrian motion not seen during training.
ER  - 

TY  - CONF
TI  - Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3284
EP  - 3290
AU  - A. Ligocki
AU  - A. Jelinek
AU  - L. Zalud
PY  - 2020
KW  - cameras
KW  - Global Positioning System
KW  - infrared detectors
KW  - mobile robots
KW  - optical radar
KW  - SLAM (robots)
KW  - WUXGA cameras
KW  - 3D LiDAR
KW  - inertial measurement unit
KW  - infrared camera
KW  - differential RTK GNSS receiver
KW  - centimetre accuracy
KW  - public dataset
KW  - submillisecond precision
KW  - autonomous driving
KW  - Brno Urban dataset
KW  - self-driving agents
KW  - mapping tasks
KW  - Brno-Czech Republic
KW  - https://github.com/RoboticsBUT/Brno-Urban-Dataset
KW  - Cameras
KW  - Sensors
KW  - Global Positioning System
KW  - Global navigation satellite system
KW  - Receivers
KW  - Laser radar
KW  - Synchronization
DO  - 10.1109/ICRA40945.2020.9197277
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.
ER  - 

TY  - CONF
TI  - Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3291
EP  - 3297
AU  - L. Zhang
AU  - W. Ding
AU  - J. Chen
AU  - S. Shen
PY  - 2020
KW  - decision making
KW  - decision theory
KW  - Markov processes
KW  - multi-agent systems
KW  - road traffic
KW  - trees (mathematics)
KW  - dense traffic scenarios
KW  - automated vehicles
KW  - stochastic behaviors
KW  - traffic participants
KW  - perception uncertainties
KW  - partially observable Markov decision process
KW  - efficient uncertainty-aware decision-making
KW  - longitudinal behaviors
KW  - complex driving environments
KW  - automated driving
KW  - guided branching
KW  - domain-specific closed-loop policy tree structure
KW  - DCP-Tree
KW  - conditional focused branching mechanism
KW  - CFB
KW  - domain-specific expert knowledge
KW  - Planning
KW  - Decision making
KW  - Uncertainty
KW  - Semantics
KW  - Vegetation
KW  - Aerospace electronics
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197302
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Decision-making in dense traffic scenarios is challenging for automated vehicles (AVs) due to potentially stochastic behaviors of other traffic participants and perception uncertainties (e.g., tracking noise and prediction errors, etc.). Although the partially observable Markov decision process (POMDP) provides a systematic way to incorporate these uncertainties, it quickly becomes computationally intractable when scaled to the real-world large-size problem. In this paper, we present an efficient uncertainty-aware decision-making (EUDM) framework, which generates long-term lateral and longitudinal behaviors in complex driving environments in real-time. The computation complexity is controlled to an appropriate level by two novel techniques, namely, the domain-specific closed-loop policy tree (DCP-Tree) structure and conditional focused branching (CFB) mechanism. The key idea is utilizing domain-specific expert knowledge to guide the branching in both action and intention space. The proposed framework is validated using both onboard sensing data captured by a real vehicle and an interactive multi-agent simulation platform. We also release the code of our framework to accommodate benchmarking.
ER  - 

TY  - CONF
TI  - Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3298
EP  - 3304
AU  - M. Peng
AU  - Z. Gong
AU  - C. Sun
AU  - L. Chen
AU  - D. Cao
PY  - 2020
KW  - generalisation (artificial intelligence)
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - road vehicles
KW  - sensor fusion
KW  - steering systems
KW  - traffic engineering computing
KW  - pretrained IPP model
KW  - CARLA driving benchmark
KW  - generalization capability
KW  - pure pursuit
KW  - autonomous urban driving navigation
KW  - two-stage framework
KW  - visual information
KW  - pure-pursuit method
KW  - steering angle
KW  - imitation learning performance
KW  - driving data
KW  - reinforcement learning method
KW  - deep deterministic policy gradient
KW  - IPP-RL framework
KW  - Learning (artificial intelligence)
KW  - Meteorology
KW  - Robustness
KW  - Task analysis
KW  - Navigation
KW  - Training
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197027
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous urban driving navigation is still an open problem and has ample room for improvement in unknown complex environments and terrible weather conditions. In this paper, we propose a two-stage framework, called IPP-RL, to handle these problems. IPP means an Imitation learning method fusing visual information with the additional steering angle calculated by Pure-Pursuit (PP) method, and RL means using Reinforcement Learning for further training. In our IPP model, the visual information captured by camera can be compensated by the calculated steering angle, thus it could perform well under bad weather conditions. However, imitation learning performance is limited by the driving data severely. Thus we use a reinforcement learning method-Deep Deterministic Policy Gradient (DDPG)-in the second stage training, which shares the learned weights from pretrained IPP model. In this way, our IPP-RL can lower the dependency of imitation learning on demonstration data and solve the problem of low exploration efficiency caused by randomly initialized weights in reinforcement learning. Moreover, we design a more reasonable reward function and use the n-step return to update the critic-network in DDPG. Our experiments on CARLA driving benchmark demonstrate that our IPP-RL is robust to lousy weather conditions and shows remarkable generalization capability in unknown environments on navigation task.
ER  - 

TY  - CONF
TI  - Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3305
EP  - 3311
AU  - A. Savkin
AU  - T. Lapotre
AU  - K. Strauss
AU  - U. Akbar
AU  - F. Tombari
PY  - 2020
KW  - augmented reality
KW  - driver information systems
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object detection
KW  - pedestrians
KW  - road vehicles
KW  - traffic engineering computing
KW  - adversarial appearance learning
KW  - augmented Cityscapes
KW  - pedestrian recognition
KW  - autonomous driving area synthetic data
KW  - traffic scenarios
KW  - autonomous vehicle
KW  - data augmentation
KW  - Cityscapes dataset
KW  - virtual pedestrians
KW  - augmentation realism
KW  - generative network architecture
KW  - data-set lighting conditions
KW  - VRU
KW  - Solid modeling
KW  - Semantics
KW  - Three-dimensional displays
KW  - Autonomous vehicles
KW  - Training
KW  - Cameras
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197024
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.
ER  - 

TY  - CONF
TI  - ROI-cloud: A Key Region Extraction Method for LiDAR Odometry and Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3312
EP  - 3318
AU  - Z. Zhou
AU  - M. Yang
AU  - C. Wang
AU  - B. Wang
PY  - 2020
KW  - Bayes methods
KW  - distance measurement
KW  - feature extraction
KW  - image filtering
KW  - image matching
KW  - image registration
KW  - image sampling
KW  - Monte Carlo methods
KW  - optical radar
KW  - pose estimation
KW  - robot vision
KW  - ROI-cloud
KW  - key region extraction method
KW  - LiDAR odometry
KW  - LiDAR scan
KW  - on-board IMU/odometry data
KW  - Bayes filtering
KW  - Monte Carlo sampling
KW  - autonomous robot
KW  - LiDAR localization
KW  - voxelized cube set
KW  - pose estimation
KW  - point set registration
KW  - massive point cloud data
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Heuristic algorithms
KW  - Vehicle dynamics
KW  - Robots
KW  - Urban areas
DO  - 10.1109/ICRA40945.2020.9197059
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel key region extraction method of point cloud, ROI-cloud, for LiDAR odometry and localization with autonomous robots. Traditional methods process massive point cloud data in every region within the field of view. In dense urban environments, however, processing redundant and dynamic regions of point cloud is time-consuming and harmful to the results of matching algorithms. In this paper, a voxelized cube set, ROI-cloud, is proposed to solve this problem by exclusively reserving the regions of interest for better point set registration and pose estimation. 3D space is firstly voxelized into weighted cubes. The key idea is to update their weights continually and extract cubes with high importance as key regions. By extracting geometrical features of a LiDAR scan, the importance of each cube is evaluated as a new measurement. With the help of on-board IMU/odometry data as well as new measurements, the weights of cubes are updated recursively through Bayes filtering. Thus, dynamic and redundant point cloud inside cubes with low importance are discarded by means of Monte Carlo sampling. Our method is validated on various datasets, and results indicate that the ROI-cloud improves the existing method in both accuracy and speed.
ER  - 

TY  - CONF
TI  - To Learn or Not to Learn: Visual Localization from Essential Matrices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3319
EP  - 3326
AU  - Q. Zhou
AU  - T. Sattler
AU  - M. Pollefeys
AU  - L. Leal-Taix√©
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image representation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - visual localization
KW  - scene-specific representations
KW  - relative pose estimation
KW  - feature-based approach
KW  - deep learning
KW  - camera
KW  - autonomous robots
KW  - Cameras
KW  - Visualization
KW  - Three-dimensional displays
KW  - Pipelines
KW  - Pose estimation
KW  - Image retrieval
DO  - 10.1109/ICRA40945.2020.9196607
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual localization is the problem of estimating a camera within a scene and a key technology for autonomous robots. State-of-the-art approaches for accurate visual localization use scene-specific representations, resulting in the overhead of constructing these models when applying the techniques to new scenes. Recently, learned approaches based on relative pose estimation have been proposed, carrying the promise of easily adapting to new scenes. However, they are currently significantly less accurate than state-of-the-art approaches. In this paper, we are interested in analyzing this behavior. To this end, we propose a novel framework for visual localization from relative poses. Using a classical feature-based approach within this framework, we show state-of-the-art performance. Replacing the classical approach with learned alternatives at various levels, we then identify the reasons for why deep learned approaches do not perform well. Based on our analysis, we make recommendations for future work.
ER  - 

TY  - CONF
TI  - Hierarchical Multi-Process Fusion for Visual Place Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3327
EP  - 3333
AU  - S. Hausler
AU  - M. Milford
PY  - 2020
KW  - image fusion
KW  - mobile robots
KW  - object recognition
KW  - robot vision
KW  - hierarchical multiprocess fusion
KW  - multiple complementary techniques
KW  - visual localization
KW  - multisensor fusion
KW  - varying performance characteristics
KW  - hierarchical localization system
KW  - localization hypotheses
KW  - localization performance
KW  - final localization stage
KW  - parallel fusion
KW  - visual place recognition
KW  - Databases
KW  - Visualization
KW  - Feature extraction
KW  - Robots
KW  - Pipelines
KW  - Histograms
DO  - 10.1109/ICRA40945.2020.9197360
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpointinvariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques.
ER  - 

TY  - CONF
TI  - Camera Tracking in Lighting Adaptable Maps of Indoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3334
EP  - 3340
AU  - T. Caselitz
AU  - M. Krawez
AU  - J. Sundram
AU  - M. Van Loock
AU  - W. Burgard
PY  - 2020
KW  - feature extraction
KW  - image colour analysis
KW  - image sequences
KW  - indoor environment
KW  - lighting
KW  - object tracking
KW  - rendering (computer graphics)
KW  - direct dense camera tracking approach
KW  - lighting adaptable map representation
KW  - lighting invariant map representations
KW  - lighting conditions
KW  - visual localization methods
KW  - indoor environments
KW  - Lighting
KW  - Cameras
KW  - Visualization
KW  - Indoor environments
KW  - Estimation
KW  - Mathematical model
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197471
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tracking the pose of a camera is at the core of visual localization methods used in many applications. As the observations of a camera are inherently affected by lighting, it has always been a challenge for these methods to cope with varying lighting conditions. Thus far, this issue has mainly been approached with the intent to increase robustness by choosing lighting invariant map representations. In contrast, our work aims at explicitly exploiting lighting effects for camera tracking. To achieve this, we propose a lighting adaptable map representation for indoor environments that allows real-time rendering of the scene illuminated by an arbitrary subset of the lamps contained in the model. Our method for estimating the light setting from the current camera observation enables us to adapt the model according to the lighting conditions present in the scene. As a result, lighting effects like cast shadows do no longer act as disturbances that demand robustness but rather as beneficial features when matching observations against the map. We leverage these capabilities in a direct dense camera tracking approach and demonstrate its performance in realworld experiments in scenes with varying lighting conditions.
ER  - 

TY  - CONF
TI  - Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3341
EP  - 3348
AU  - S. Garg
AU  - M. Milford
PY  - 2020
KW  - image matching
KW  - image representation
KW  - image sequences
KW  - mobile robots
KW  - quantisation (signal)
KW  - robot vision
KW  - storage management
KW  - hashing overload approach
KW  - compact place recognition
KW  - overloaded representations
KW  - storage footprint
KW  - place recognition system
KW  - ultra-compact place representations
KW  - sublinear storage scaling
KW  - sublinear computational scaling
KW  - visual place recognition
KW  - match selections
KW  - sequence-based matching
KW  - scalar quantization-based hashing
KW  - mobile robot
KW  - Quantization (signal)
KW  - Visualization
KW  - Indexes
KW  - Robots
KW  - Principal component analysis
KW  - Benchmark testing
KW  - Image recognition
DO  - 10.1109/ICRA40945.2020.9196827
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual place recognition algorithms trade off three key characteristics: their storage footprint, their computational requirements, and their resultant performance, often expressed in terms of recall rate. Significant prior work has investigated highly compact place representations, sub-linear computational scaling and sub-linear storage scaling techniques, but have always involved a significant compromise in one or more of these regards, and have only been demonstrated on relatively small datasets. In this paper we present a novel place recognition system which enables for the first time the combination of ultra-compact place representations, near sub-linear storage scaling and extremely lightweight compute requirements. Our approach exploits the inherently sequential nature of much spatial data in the robotics domain and inverts the typical target criteria, through intentionally coarse scalar quantization-based hashing that leads to more collisions but is resolved by sequence-based matching. For the first time, we show how effective place recognition rates can be achieved on a new very large 10 million place dataset, requiring only 8 bytes of storage per place and 37K unitary operations to achieve over 50% recall for matching a sequence of 100 frames, where a conventional stateof-the-art approach both consumes 1300 times more compute and fails catastrophically. We present analysis investigating the effectiveness of our hashing overload approach under varying sizes of quantized vector length, comparison of near miss matches with the actual match selections and characterise the effect of variance re-scaling of data on quantization. Resource link: https://github.com/oravus/CoarseHash.
ER  - 

TY  - CONF
TI  - Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3349
EP  - 3355
AU  - T. Nguyen
AU  - K. Mohta
AU  - C. J. Taylor
AU  - V. Kumar
PY  - 2020
KW  - aerospace robotics
KW  - microrobots
KW  - mobile robots
KW  - multi-robot systems
KW  - pose estimation
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - target tracking
KW  - multiMAV system
KW  - robot team
KW  - vision based detection
KW  - distance measurements
KW  - coupled probabilistic data association filter
KW  - nonlinear measurements
KW  - visual based robot to robot detection
KW  - vision based multiMAV localization
KW  - robot localization
KW  - robot pose estimation
KW  - multiple microaerial vehicles
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Noise measurement
KW  - Probabilistic logic
KW  - Task analysis
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196793
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the localization of robots in a multi-MAV system where external infrastructure like GPS or motion capture systems may not be available. Our approach lends itself to implementation on platforms with several constraints on size, weight, and power (SWaP). Particularly, our framework fuses the onboard VIO with the anonymous, visual-based robot-to-robot detection to estimate all robot poses in one common frame, addressing three main challenges: 1) the initial configuration of the robot team is unknown, 2) the data association between each vision-based detection and robot targets is unknown, and 3) the vision-based detection yields false negatives, false positives, inaccurate, and provides noisy bearing, distance measurements of other robots. Our approach extends the Coupled Probabilistic Data Association Filter [1] to cope with nonlinear measurements. We demonstrate the superior performance of our approach over a simple VIO-based method in a simulation with the measurement models statistically modeled using the real experimental data. We also show how onboard sensing, estimation, and control can be used for formation flight.
ER  - 

TY  - CONF
TI  - MANGA: Method Agnostic Neural-policy Generalization and Adaptation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3356
EP  - 3362
AU  - H. Bharadhwaj
AU  - S. Yamaguchi
AU  - S. -i. Maeda
PY  - 2020
KW  - learning (artificial intelligence)
KW  - robot dynamics
KW  - MANGA
KW  - multiple environments
KW  - dynamics parameters
KW  - motor noise variations
KW  - policy learning
KW  - system identification
KW  - unknown environment
KW  - dynamics configurations
KW  - dynamics conditioned policies
KW  - off-policy state-transition rollouts
KW  - training method
KW  - method agnostic neural-policy generalization and adaptation
KW  - transferring policies
KW  - Training
KW  - Robots
KW  - Task analysis
KW  - Encoding
KW  - Decoding
KW  - Heuristic algorithms
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197398
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we target the problem of transferring policies across multiple environments with different dynamics parameters and motor noise variations, by introducing a framework that decouples the processes of policy learning and system identification. Efficiently transferring learned policies to an unknown environment with changes in dynamics configurations in the presence of motor noise is very important for operating robots in the real world, and our work is a novel attempt in that direction. We introduce MANGA: Method Agnostic Neural-policy Generalization and Adaptation, that trains dynamics conditioned policies and efficiently learns to estimate the dynamics parameters of the environment given off-policy state-transition rollouts in the environment. Our scheme is agnostic to the type of training method used - both reinforcement learning (RL) and imitation learning (IL) strategies can be used. We demonstrate the effectiveness of our approach by experimenting with four different MuJoCo agents and comparing against previously proposed transfer baselines.
ER  - 

TY  - CONF
TI  - Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3363
EP  - 3370
AU  - J. Choi
AU  - C. Dance
AU  - J. -e. Kim
AU  - K. -s. Park
AU  - J. Han
AU  - J. Seo
AU  - M. Kim
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - robust control
KW  - fast adaptation
KW  - deep reinforcement learning-based navigation skills
KW  - human preference
KW  - robot navigation
KW  - robustness
KW  - maximum velocities
KW  - reward components
KW  - optimal choice
KW  - real-world service scenarios
KW  - deep RL navigation method
KW  - reward functions
KW  - Bayesian deep learning method
KW  - preference data
KW  - diverse navigation skills
KW  - deep RL navigation agents
KW  - Navigation
KW  - Collision avoidance
KW  - Bayes methods
KW  - Training
KW  - Robot kinematics
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197159
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.
ER  - 

TY  - CONF
TI  - Variational Inference with Mixture Model Approximation for Applications in Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3395
EP  - 3401
AU  - E. Pignat
AU  - T. Lembono
AU  - S. Calinon
PY  - 2020
KW  - approximation theory
KW  - Bayes methods
KW  - control engineering computing
KW  - inference mechanisms
KW  - mixture models
KW  - robots
KW  - statistical distributions
KW  - variational inference
KW  - mixture model approximation
KW  - robot configurations
KW  - Bayesian computation
KW  - Robots
KW  - Mixture models
KW  - Kinematics
KW  - Bayes methods
KW  - Optimization
KW  - Task analysis
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197166
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose to formulate the problem of representing a distribution of robot configurations (e.g. joint angles) as that of approximating a product of experts. Our approach uses variational inference, a popular method in Bayesian computation, which has several practical advantages over sampling-based techniques. To be able to represent complex and multimodal distributions of configurations, mixture models are used as approximate distribution. We show that the problem of approximating a distribution of robot configurations while satisfying multiple objectives arises in a wide range of problems in robotics, for which the properties of the proposed approach have relevant consequences. Several applications are discussed, including learning objectives from demonstration, planning, and warm-starting inverse kinematics problems. Simulated experiments are presented with a 7-DoF Panda arm and a 28-DoF Talos humanoid.
ER  - 

TY  - CONF
TI  - Injection of a Fluorescent Microsensor into a Specific Cell by Laser Manipulation and Heating with Multiple Wavelengths of Light
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3437
EP  - 3442
AU  - H. Maruyama
AU  - H. Hashim
AU  - R. Yanagawa
AU  - F. Arai
PY  - 2020
KW  - biomedical optical imaging
KW  - cellular biophysics
KW  - dyes
KW  - fluorescence
KW  - infrared spectra
KW  - kidney
KW  - microsensors
KW  - refractive index
KW  - Rhodamine B
KW  - temperature-sensitive fluorescent dye
KW  - refractive index
KW  - polystyrene particle
KW  - cell injection
KW  - laser manipulation
KW  - multiple wavelengths
KW  - fluorescent microsensor
KW  - Microsensors
KW  - Fluorescence
KW  - Semiconductor lasers
KW  - Heating systems
KW  - Measurement by laser beam
KW  - Laser excitation
KW  - Temperature measurement
DO  - 10.1109/ICRA40945.2020.9197234
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, we propose the manipulation and cell injection of a fluorescent microsensor using multiple wavelengths of light. The fluorescent microsensor is made of a 1-Œºm polystyrene particle containing infrared (IR: 808 nm) absorbing dye and Rhodamine B. The polystyrene particle can be manipulated in water using a 1064-nm laser because the refractive index of the polystyrene is 1.6 (refractive index of water: 1.3). The IR absorbing dye absorbs 808-nm light but does not absorb the 1064-nm laser. Rhodamine B is a temperature-sensitive fluorescent dye (excitation wavelength: 488 nm, emission wavelength: 560 nm). The functions of manipulation, heating for injection, and temperature measurement are achieved by different wavelengths of 1064 nm, 808 nm, and 488 nm, respectively. The temperature increase of fluorescent microsensor with 808-nm (40 mW, 10 s) laser was approximately 15¬∞C, and enough for injection of fluorescent microsensor. We demonstrated manipulation and injection of the microsensor into Madin-Darby canine kidney cell using 1064-nm and 808-nm lasers. These results confirmed the effectiveness of our proposed cell injection of a fluorescent microsensor using multiple wavelengths of light.
ER  - 

TY  - CONF
TI  - Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3451
EP  - 3457
AU  - D. J. Gonzalez
AU  - H. H. Asada
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - springs (mechanical)
KW  - synchronisation
KW  - wheels
KW  - extra robotic legs system
KW  - robotic augmentation
KW  - human operator
KW  - articulated robot legs
KW  - human-XRL quadruped system
KW  - rear legs
KW  - quadrupedal robots
KW  - quadrupedal locomotion
KW  - coupler design parameters
KW  - passive quadrupedal gait synchronization
KW  - dynamically coupled double rimless wheel system
KW  - Poincar√© return map
KW  - numerical simulation
KW  - Legged locomotion
KW  - Wheels
KW  - Synchronization
KW  - Couplers
KW  - Robot kinematics
KW  - Human Augmentation
KW  - Supernumerary Robotic Limbs
KW  - Exoskeletons
KW  - Locomotion
KW  - Nonlinear Dynamics
DO  - 10.1109/ICRA40945.2020.9196773
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The Extra Robotic Legs (XRL) system is a robotic augmentation worn by a human operator consisting of two articulated robot legs that walk with the operator and help bear a heavy backpack payload. It is desirable for the Human-XRL quadruped system to walk with the rear legs lead the front by 25% of the gait period, minimizing the energy lost from foot impacts while maximizing balance stability. Unlike quadrupedal robots, the XRL cannot command the human's limbs to coordinate quadrupedal locomotion. Using a pair of Rimless Wheel models, it is shown that the systems coupled with a spring and damper converge to the desired 25% phase difference. A Poincar√© return map was generated using numerical simulation to examine the convergence properties to different coupler design parameters, and initial conditions. The Dynamically Coupled Double Rimless Wheel system was physically realized with a spring and dashpot chosen from the theoretical results, and initial experiments indicate that the desired synchronization properties may be achieved within several steps using this set of passive components alone.
ER  - 

TY  - CONF
TI  - Optimal Fast Entrainment Waveform for Indirectly Controlled Limit Cycle Walker Against External Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3458
EP  - 3463
AU  - L. Li
AU  - I. Tokuda
AU  - F. Asano
PY  - 2020
KW  - legged locomotion
KW  - limit cycles
KW  - numerical analysis
KW  - oscillators
KW  - robot dynamics
KW  - stability
KW  - optimal fast entrainment waveform
KW  - phase recovery
KW  - phase reduction theory
KW  - entrainment effect
KW  - limit cycle walking
KW  - indirectly controlled limit cycle walker
KW  - external disturbances
KW  - occasional perturbation
KW  - closed orbit
KW  - phase space
KW  - successive perturbation
KW  - accumulated deviation
KW  - control law
KW  - disturbed phase
KW  - wobbling mass
KW  - wobbling motion
KW  - Limit-cycles
KW  - Legged locomotion
KW  - Perturbation methods
KW  - Mathematical model
KW  - Oscillators
KW  - Convergence
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196525
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - After occasional perturbation, it is crucial to spontaneously control the limit cycle walking so that it quickly returns to its closed orbit in phase space. Otherwise, its stability can not be sufficiently guaranteed if the speed of recovery is slow while successive perturbation is applied. The accumulated deviation may eventually drive the phase outside the basin of attraction, leading to failure of the walking. In this sense, a control law that quickly recovers the disturbed phase before encountering the following perturbations is indispensable. With this consideration, here we analytically derive an optimal fast entrainment waveform that maximizes the speed of phase recovery based on phase reduction theory. Our theoretical method is numerically evaluated using a limit cycle walker, which is indirectly controlled by the oscillation of a wobbling mass via entrainment effect. The obtained waveform is used as the desired trajectory of the wobbling motion. The simulation results show that the waveform we derived achieves the best performance among all candidates. Our method helps to enhance the stability of limit cycle walking.
ER  - 

TY  - CONF
TI  - Correspondence Identification in Collaborative Robot Perception through Maximin Hypergraph Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3488
EP  - 3494
AU  - P. Gao
AU  - Z. Zhang
AU  - R. Guo
AU  - H. Lu
AU  - H. Zhang
PY  - 2020
KW  - concave programming
KW  - graph theory
KW  - image matching
KW  - image representation
KW  - multi-robot systems
KW  - object detection
KW  - robot vision
KW  - collaborative multirobot perception
KW  - nonconvex noncontinuous optimization problem
KW  - multirobot coordination
KW  - collaborative robot perception
KW  - hypergraph matching approach
KW  - Collaboration
KW  - Robot kinematics
KW  - Object recognition
KW  - Optimization
KW  - Robot sensing systems
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196594
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correspondence identification is an essential problem for collaborative multi-robot perception, with the objective of deciding the correspondence of objects that are observed in the field of view of each robot. In this paper, we introduce a novel maximin hypergraph matching approach that formulates correspondence identification as a hypergraph matching problem. The proposed approach incorporates both spatial relationships and appearance features of objects to improve representation capabilities. It also integrates the maximin theorem to optimize the worst-case scenario in order to address distractions caused by non-covisible objects. In addition, we design an optimization algorithm to address the formulated non-convex non-continuous optimization problem. We evaluate our approach and compare it with seven previous techniques in two application scenarios, including multi-robot coordination on real robots and connected autonomous driving in simulations. Experimental results have validated the effectiveness of our approach in identifying object correspondence from partially overlapped views in collaborative perception, and have shown that the proposed maximin hypergraph matching approach outperforms previous techniques and obtains state-of-the-art performance.
ER  - 

TY  - CONF
TI  - Distributed Multi-Target Tracking for Autonomous Vehicle Fleets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3495
EP  - 3501
AU  - O. Shorinwa
AU  - J. Yu
AU  - T. Halsted
AU  - A. Koufos
AU  - M. Schwager
PY  - 2020
KW  - cameras
KW  - Kalman filters
KW  - maximum likelihood estimation
KW  - target tracking
KW  - vehicular ad hoc networks
KW  - wireless sensor networks
KW  - Consensus Kalman Filter
KW  - fixed communication bandwidth
KW  - high fidelity urban driving simulator
KW  - autonomous cars
KW  - time-varying communication network
KW  - distributed multitarget tracking
KW  - autonomous vehicle fleets
KW  - scalable distributed target tracking algorithm
KW  - alternating direction method of multipliers
KW  - vehicle-to-vehicle network
KW  - sensing vehicle
KW  - Kalman filter-like update
KW  - centralized maximum a posteriori estimate
KW  - CARLA
KW  - on-board cameras
KW  - Sensors
KW  - Target tracking
KW  - Kalman filters
KW  - Microsoft Windows
KW  - Estimation
KW  - Trajectory
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197241
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a scalable distributed target tracking algorithm based on the alternating direction method of multipliers that is well-suited for a fleet of autonomous cars communicating over a vehicle-to-vehicle network. Each sensing vehicle communicates with its neighbors to execute iterations of a Kalman filter-like update such that each agent's estimate approximates the centralized maximum a posteriori estimate without requiring the communication of measurements. We show that our method outperforms the Consensus Kalman Filter in recovering the centralized estimate given a fixed communication bandwidth. We also demonstrate the algorithm in a high fidelity urban driving simulator (CARLA), in which 50 autonomous cars connected on a time-varying communication network track the positions and velocities of 50 target vehicles using on-board cameras.
ER  - 

TY  - CONF
TI  - Flying batteries: In-flight battery switching to increase multirotor flight time
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3510
EP  - 3516
AU  - K. P. Jain
AU  - M. W. Mueller
PY  - 2020
KW  - aerospace control
KW  - helicopters
KW  - mobile robots
KW  - secondary cells
KW  - in-flight battery switching
KW  - multirotor flight time
KW  - mid-air docking
KW  - primary battery
KW  - quadcopter flight
KW  - docking platform
KW  - flying battery
KW  - secondary battery
KW  - arbitrary switching
KW  - Batteries
KW  - Switches
KW  - Legged locomotion
KW  - Switching circuits
KW  - Aerodynamics
KW  - Connectors
KW  - Propellers
DO  - 10.1109/ICRA40945.2020.9197580
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel approach to increase the flight time of a multirotor via mid-air docking and in-flight battery switching. A main quadcopter flying using a primary battery has a docking platform attached to it. A `flying battery' - a small quadcopter carrying a secondary battery - is equipped with docking legs that can mate with the main quadcopter's platform. Connectors between the legs and the platform establish electrical contact on docking, and enable power transfer from the secondary battery to the main quadcopter. A custom-designed circuit allows arbitrary switching between the primary battery and secondary battery. We demonstrate the concept in a flight experiment involving repeated docking, battery switching, and undocking. This is shown in the video attachment. The experiment increases the flight time of the main quadcopter by a factor of 4.7√ó compared to solo flight, and 2.2√ó a theoretical limit for that given multirotor. Importantly, this increase in flight time is not associated with a large increase in overall vehicle mass or size, leaving the main quadcopter in fundamentally the same safety class.
ER  - 

TY  - CONF
TI  - Optimal Control of an Energy-Recycling Actuator for Mobile Robotics Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3559
EP  - 3565
AU  - E. Krimsky
AU  - S. H. Collins
PY  - 2020
KW  - actuators
KW  - clutches
KW  - electric motors
KW  - energy consumption
KW  - gears
KW  - integer programming
KW  - mobile robots
KW  - optimal control
KW  - power consumption
KW  - quadratic programming
KW  - springs (mechanical)
KW  - torque
KW  - actuator power consumption
KW  - mobile robot design
KW  - elastic energy
KW  - electrical energy consumption
KW  - optimal control
KW  - given actuator design
KW  - optimized actuator energy consumption
KW  - optimized gear motor
KW  - simulated energy-recycling actuator
KW  - mobile robotics applications
KW  - Springs
KW  - Actuators
KW  - Torque
KW  - Power demand
KW  - Force
KW  - Robots
KW  - Gears
KW  - Optimization and optimal control
KW  - force control
KW  - prosthetics and exoskeletons
DO  - 10.1109/ICRA40945.2020.9196870
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Actuator power consumption is a limiting factor in mobile robot design. In this paper we introduce the concept of an energy-recycling actuator, which uses an array of springs and clutches to capture and return elastic energy in parallel with an electric motor. Engaging and disengaging clutches appropriately could reduce electrical energy consumption without sacrificing controllability, but presents a challenging control problem. We formulated the optimal control objective of minimizing actuator power consumption as a mixed-integer quadratic program (MIQP) and solved for the global minimum. For a given actuator design and a wide range of simulated torque and rotation patterns, all corresponding to zero net work over one cycle, we compared optimized actuator energy consumption to that of an optimized gear motor with simple parallel elasticity. The simulated energy-recycling actuator consumed less electrical energy: 57% less on average and 80% less in the best case. These results demonstrate an effective approach to optimal control of this type of system, and suggest that energy-recycling actuators could substantially reduce power consumption in some robotics applications.
ER  - 

TY  - CONF
TI  - An NMPC Approach using Convex Inner Approximations for Online Motion Planning with Guaranteed Collision Avoidance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3574
EP  - 3580
AU  - T. Schoels
AU  - L. Palmieri
AU  - K. O. Arras
AU  - M. Diehl
PY  - 2020
KW  - collision avoidance
KW  - continuous time systems
KW  - convex programming
KW  - mobile robots
KW  - nonlinear control systems
KW  - predictive control
KW  - trajectory control
KW  - trajectory optimization
KW  - NMPC approach
KW  - mobile robots
KW  - continuous time collision avoidance
KW  - kinodynamic feasibility
KW  - nonlinear model predictive control
KW  - convex inner approximation
KW  - online motion planning
KW  - continuous time collision free trajectories
KW  - Collision avoidance
KW  - Robots
KW  - Trajectory optimization
KW  - Planning
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197206
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Even though mobile robots have been around for decades, trajectory optimization and continuous time collision avoidance remain subject of active research. Existing methods trade off between path quality, computational complexity, and kinodynamic feasibility. This work approaches the problem using a nonlinear model predictive control (NMPC) framework, that is based on a novel convex inner approximation of the collision avoidance constraint. The proposed Convex Inner ApprOximation (CIAO) method finds kinodynamically feasible and continuous time collision free trajectories, in few iterations, typically one. For a feasible initialization, the approach is guaranteed to find a feasible solution, i.e. it preserves feasibility. Our experimental evaluation shows that CIAO outperforms state of the art baselines in terms of planning efficiency and path quality. Experiments show that it also efficiently scales to high-dimensional systems. Furthermore real-world experiments demonstrate its capability of unifying trajectory optimization and tracking for safe motion planning in dynamic environments.
ER  - 

TY  - CONF
TI  - Action Image Representation: Learning Scalable Deep Grasping Policies with Zero Real World Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3597
EP  - 3603
AU  - M. Khansari
AU  - D. Kappler
AU  - J. Luo
AU  - J. Bingham
AU  - M. Kalakrishnan
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - grippers
KW  - image colour analysis
KW  - image representation
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - vectors
KW  - Action image representation
KW  - zero real world data
KW  - end-to-end deep-grasping policy
KW  - grasp quality
KW  - object-gripper relationship
KW  - deep convolutional network
KW  - Action Image representation
KW  - color images
KW  - depth images
KW  - combined color-depth
KW  - scalable deep grasping policy learning
KW  - salient feature extraction
KW  - Grasping
KW  - Robot sensing systems
KW  - Image representation
KW  - Proposals
KW  - Grippers
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197415
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces Action Image, a new grasp proposal representation that allows learning an end-to-end deep-grasping policy. Our model achieves 84% grasp success on 172 real world objects while being trained only in simulation on 48 objects with just naive domain randomization. Similar to computer vision problems, such as object detection, Action Image builds on the idea that object features are invariant to translation in image space. Therefore, grasp quality is invariant when evaluating the object-gripper relationship; a successful grasp for an object depends on its local context, but is independent of the surrounding environment. Action Image represents a grasp proposal as an image and uses a deep convolutional network to infer grasp quality. We show that by using an Action Image representation, trained networks are able to extract local, salient features of grasping tasks that generalize across different objects and environments. We show that this representation works on a variety of inputs, including color images (RGB), depth images (D), and combined color-depth (RGB-D). Our experimental results demonstrate that networks utilizing an Action Image representation exhibit strong domain transfer between training on simulated data and inference on real-world sensor streams. Finally, our experiments show that a network trained with Action Image improves grasp success (84% vs. 53%) over a baseline model with the same structure, but using actions encoded as vectors.
ER  - 

TY  - CONF
TI  - High Accuracy and Efficiency Grasp Pose Detection Scheme with Dense Predictions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3604
EP  - 3610
AU  - H. Cheng
AU  - D. Ho
AU  - M. Q. . -H. Meng
PY  - 2020
KW  - grippers
KW  - image colour analysis
KW  - image resolution
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - parallel plate gripper
KW  - neural network
KW  - pixel location
KW  - nonmaximum suppression
KW  - sampling grasps
KW  - learning-based grasp pose detection scheme
KW  - channels images
KW  - resolution RGB image
KW  - robot grasping success rate
KW  - Cornell Grasp Dataset
KW  - predicted grasps
KW  - dense predictions
KW  - dense grasp predictions
KW  - ranking procedures
KW  - clustering procedures
KW  - NMS
KW  - nonmaximum suppression strategy
KW  - detection model
KW  - generating grasp proposals
KW  - intermediate procedures
KW  - detection accuracy
KW  - fine-tuning steps
KW  - detection algorithms
KW  - Predictive models
KW  - Solid modeling
KW  - Grippers
KW  - Robots
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Image edge detection
DO  - 10.1109/ICRA40945.2020.9197333
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based grasp pose detection algorithms have boosted the performance of robot grasping, but they usually need manually fine-tuning steps to find the balance between detection accuracy and efficient. In this paper, we discard these intermediate procedures, like sampling grasps and generating grasp proposals, and propose an end-to-end grasp pose detection model. Our model uses the RGB image as the input and predicts the single grasp pose in each small grid of the image. Furthermore, the best grasps are found by non-maximum suppression (NMS) strategy. The clustering and ranking procedures are left for NMS while the network only generates dense grasp predictions, which keeps the network simple and efficient. To achieve dense predictions, the predicted grasps of our detection model are represented by the 6 channels images with each pixel location representing a rated grasp. To the best of our knowledge, our model is the first neural network that attaches a grasp pose in pixel level. The model achieves 96.5% accuracy which costs 14ms for prediction of a 480√ó360 resolution RGB image in Cornell Grasp Dataset, and 90.4% robot grasping success rate for unknown objects with a parallel plate gripper in the real environment.
ER  - 

TY  - CONF
TI  - Transferable Active Grasping and Real Embodied Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3611
EP  - 3618
AU  - X. Chen
AU  - Z. Ye
AU  - J. Sun
AU  - Y. Fan
AU  - F. Hu
AU  - C. Wang
AU  - C. Lu
PY  - 2020
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - stereo image processing
KW  - cluttered scenes
KW  - robot vision systems
KW  - reinforcement learning framework
KW  - 3D vision architectures
KW  - RGB-D cameras
KW  - 3-stage transferable active grasping pipeline
KW  - real embodied dataset
KW  - RED
KW  - Grasping
KW  - Clutter
KW  - Cameras
KW  - Image segmentation
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197185
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping in cluttered scenes is challenging for robot vision systems, as detection accuracy can be hindered by partial occlusion of objects. We adopt a reinforcement learning (RL) framework and 3D vision architectures to search for feasible viewpoints for grasping by the use of hand-mounted RGB-D cameras. To overcome the disadvantages of photo-realistic environment simulation, we propose a large-scale dataset called Real Embodied Dataset (RED), which includes full-viewpoint real samples on the upper hemisphere with amodal annotation and enables a simulator that has real visual feedback. Based on this dataset, a practical 3-stage transferable active grasping pipeline is developed, that is adaptive to unseen clutter scenes. In our pipeline, we propose a novel mask-guided reward to overcome the sparse reward issue in grasping and ensure category-irrelevant behavior. The grasping pipeline and its possible variants are evaluated with extensive experiments both in simulation and on a real-world UR-5 robotic arm.
ER  - 

TY  - CONF
TI  - PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3619
EP  - 3625
AU  - P. Ni
AU  - W. Zhang
AU  - X. Zhu
AU  - Q. Cao
PY  - 2020
KW  - edge detection
KW  - feature extraction
KW  - image annotation
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - PointNet++ grasping
KW  - sparse point clouds
KW  - robot manipulation
KW  - local feature extractor
KW  - deep learning
KW  - multiobject scene
KW  - multiobject dataset
KW  - grasp generation algorithm
KW  - multiobject grasp detection algorithm
KW  - Ferrari-Canny metrics
KW  - PointNet++ based network
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Grasping
KW  - Measurement
KW  - Training
KW  - Cameras
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9196740
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping for novel objects is important for robot manipulation in unstructured environments. Most of current works require a grasp sampling process to obtain grasp candidates, combined with local feature extractor using deep learning. This pipeline is time-costly, expecially when grasp points are sparse such as at the edge of a bowl.In this paper, we propose an end-to-end approach to directly predict the poses, categories and scores (qualities) of all the grasps. It takes the whole sparse point clouds as the input and requires no sampling or search process. Moreover, to generate training data of multi-object scene, we propose a fast multi-object grasp detection algorithm based on Ferrari Canny metrics. A single-object dataset (79 objects from YCB object set, 23.7k grasps) and a multi-object dataset (20k point clouds with annotations and masks) are generated. A PointNet++ based network combined with multi-mask loss is introduced to deal with different training points. The whole weight size of our network is only about 11.6M, which takes about 102ms for a whole prediction process using a GeForce 840M GPU. Our experiment shows our work get 71.43% success rate and 91.60% completion rate, which performs better than current state-of-art works.
ER  - 

TY  - CONF
TI  - Clear Grasp: 3D Shape Estimation of Transparent Objects for Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3634
EP  - 3642
AU  - S. Sajjan
AU  - M. Moore
AU  - M. Pan
AU  - G. Nagaraja
AU  - J. Lee
AU  - A. Zeng
AU  - S. Song
PY  - 2020
KW  - control engineering computing
KW  - convolutional neural nets
KW  - image colour analysis
KW  - image reconstruction
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - stereo image processing
KW  - ClearGrasp
KW  - monocular depth estimation baselines
KW  - transparent objects
KW  - 3D shape estimation
KW  - 3D geometry
KW  - RGB-D image
KW  - transparent surfaces
KW  - occlusion boundaries
KW  - robotic manipulation
KW  - Three-dimensional displays
KW  - Geometry
KW  - Estimation
KW  - Solid modeling
KW  - Cameras
KW  - Image edge detection
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197518
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp - a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials: https://sites.google.com/view/cleargrasp.
ER  - 

TY  - CONF
TI  - 6D Object Pose Regression via Supervised Learning on Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3643
EP  - 3649
AU  - G. Gao
AU  - M. Lauri
AU  - Y. Wang
AU  - X. Hu
AU  - J. Zhang
AU  - S. Frintrop
PY  - 2020
KW  - convolutional neural nets
KW  - image colour analysis
KW  - image representation
KW  - object detection
KW  - pose estimation
KW  - regression analysis
KW  - supervised learning
KW  - convolutional neural networks
KW  - color information
KW  - translation regression
KW  - deep learning
KW  - rotation regression
KW  - 6D object pose regression
KW  - supervised learning
KW  - geometry-based pose refinement
KW  - axis-angle representation
KW  - geodesic loss function
KW  - quaternion representation
KW  - YCB-video dataset
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Feature extraction
KW  - Image color analysis
KW  - Supervised learning
KW  - Rotation measurement
KW  - Quaternions
DO  - 10.1109/ICRA40945.2020.9197461
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the task of estimating the 6 degrees of freedom pose of a known 3D object from depth information represented by a point cloud. Deep features learned by convolutional neural networks from color information have been the dominant features to be used for inferring object poses, while depth information receives much less attention. However, depth information contains rich geometric information of the object shape, which is important for inferring the object pose. We use depth information represented by point clouds as the input to both deep networks and geometry-based pose refinement and use separate networks for rotation and translation regression. We argue that the axis-angle representation is a suitable rotation representation for deep learning, and use a geodesic loss function for rotation regression. Ablation studies show that these design choices outperform alternatives such as the quaternion representation and L2 loss, or regressing translation and rotation with the same network. Our simple yet effective approach clearly outperforms state-of-the-art methods on the YCB-video dataset.
ER  - 

TY  - CONF
TI  - YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3650
EP  - 3656
AU  - T. Grenzd√∂rffer
AU  - M. G√ºnther
AU  - J. Hertzberg
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image colour analysis
KW  - image segmentation
KW  - object detection
KW  - object recognition
KW  - pose estimation
KW  - stereo image processing
KW  - 2D bounding boxes
KW  - 3D cameras
KW  - YCB-M
KW  - multicamera RGB-D dataset
KW  - estimation system
KW  - object recognition
KW  - 3D bounding boxes
KW  - ground truth 6DoF poses
KW  - YCB object
KW  - camera model
KW  - robust algorithms
KW  - estimation algorithms
KW  - 6DoF pose estimation
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197426
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While a great variety of 3D cameras have been introduced in recent years, most publicly available datasets for object recognition and pose estimation focus on one single camera. In this work, we present a dataset of 32 scenes that have been captured by 7 different 3D cameras, totaling 49,294 frames. This allows evaluating the sensitivity of pose estimation algorithms to the specifics of the used camera and the development of more robust algorithms that are more independent of the camera model. Vice versa, our dataset enables researchers to perform a quantitative comparison of the data from several different cameras and depth sensing technologies and evaluate their algorithms before selecting a camera for their specific task. The scenes in our dataset contain 20 different objects from the common benchmark YCB object and model set [1], [2]. We provide full ground truth 6DoF poses for each object, per-pixel segmentation, 2D and 3D bounding boxes and a measure of the amount of occlusion of each object. We have also performed an initial evaluation of the cameras using our dataset on a state-of-the-art object recognition and pose estimation system [3].
ER  - 

TY  - CONF
TI  - Self-supervised 6D Object Pose Estimation for Robot Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3665
EP  - 3671
AU  - X. Deng
AU  - Y. Xiang
AU  - A. Mousavian
AU  - C. Eppner
AU  - T. Bretl
AU  - D. Fox
PY  - 2020
KW  - image segmentation
KW  - intelligent robots
KW  - manipulators
KW  - pose estimation
KW  - robot vision
KW  - supervised learning
KW  - robot manipulation
KW  - self-supervised 6D object pose estimation
KW  - self-supervised learning
KW  - object configuration
KW  - pose estimation modules
KW  - object segmentation
KW  - 6D pose estimation performance
KW  - robots skill teaching
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Cameras
KW  - Training
KW  - Manipulators
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196714
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is time-consuming and expensive, enabling robots to learn in a self- supervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object configuration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably. A video showing the experiments can be found at https://youtu.be/W1Y0Mmh1Gd8.
ER  - 

TY  - CONF
TI  - Low-cost GelSight with UV Markings: Feature Extraction of Objects Using AlexNet and Optical Flow without 3D Image Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3680
EP  - 3685
AU  - A. C. Abad
AU  - A. Ranasinghe
PY  - 2020
KW  - cameras
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - image recognition
KW  - image reconstruction
KW  - image sequences
KW  - object recognition
KW  - UV markings
KW  - UV ink markers
KW  - low-cost GelSight sensor
KW  - nonGelsight captured images
KW  - optical flow algorithm
KW  - feature extraction
KW  - convolutional neural networks
KW  - CNN
KW  - 2D image conversion
KW  - feature recognition
KW  - Prototypes
KW  - Light emitting diodes
KW  - Feature extraction
KW  - Image recognition
KW  - Three-dimensional displays
KW  - Lighting
KW  - Coatings
DO  - 10.1109/ICRA40945.2020.9197264
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - GelSight sensor has been used to study microgeometry of objects since 2009 in tactile sensing applications. Elastomer, reflective coating, lighting, and camera were the main challenges of making a GelSight sensor within a short period. The recent addition of permanent markers to the GelSight was a new era in shear/slip studies. In our previous studies, we introduced Ultraviolet (UV) ink and UV LEDs as a new form of marker and lighting respectively. UV ink markers are invisible using ordinary LED but can be made visible using UV LED. Currently, recognition of objects or surface textures using GelSight sensor is done using fusion of camera-only images and GelSight captured images with permanent markings. Those images are fed to Convolutional Neural Networks (CNN) to classify objects. However, our novel approach in using low-cost GelSight sensor with UV markings, the 3D height map to 2D image conversion, and the additional non-Gelsight captured images for training the CNN can be eliminated. AlexNet and optical flow algorithm have been used for feature recognition of five coins without UV markings and shear/slip of the coin in GelSight with UV markings respectively. Our results on confusion matrix show that, on average coin recognition can reach 93.4% without UV markings using AlexNet. Therefore, our novel method of using GelSight with UV markings would be useful to recognize full/partial object, shear/slip, and force applied to the objects without any 3D image reconstruction.
ER  - 

TY  - CONF
TI  - Evaluation of Non-collocated Force Feedback Driven by Signal-independent Noise
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3686
EP  - 3692
AU  - Z. Chua
AU  - A. M. Okamura
AU  - D. R. Deo
PY  - 2020
KW  - brain-computer interfaces
KW  - feedback
KW  - force feedback
KW  - haptic interfaces
KW  - medical computing
KW  - neurophysiology
KW  - prosthetics
KW  - exploratory action
KW  - conventional haptic interface
KW  - iBCI-based prostheses control strategies
KW  - neural prostheses
KW  - intracortical brain computer interface
KW  - input signal
KW  - robotic prostheses
KW  - paralysis
KW  - signal-independent noise
KW  - noncollocated force feedback
KW  - signal-to-noise ratio
KW  - virtual environment
KW  - noncollocated haptic feedback
KW  - Force
KW  - Haptic interfaces
KW  - Task analysis
KW  - Noise measurement
KW  - Virtual environments
KW  - Signal to noise ratio
KW  - Probes
DO  - 10.1109/ICRA40945.2020.9197112
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Individuals living with paralysis or amputation can operate robotic prostheses using input signals based on their intent or attempt to move. Because sensory function is lost or diminished in these individuals, haptic feedback must be non-collocated. The intracortical brain computer interface (iBCI) has enabled a variety of neural prostheses for people with paralysis. An important attribute of the iBCI is that its input signal contains signal-independent noise. To understand the effects of signal-independent noise on a system with non-collocated haptic feedback and inform iBCI-based prostheses control strategies, we conducted an experiment with a conventional haptic interface as a proxy for the iBCI. Ablebodied users were tasked with locating an indentation within a virtual environment using input from their right hand. Non-collocated haptic feedback of the interaction forces in the virtual environment was augmented with noise of three different magnitudes and simultaneously rendered on users' left hands. We found increases in distance error of the guess of the indentation location, mean time per trial, mean peak absolute displacement and speed of tool movements during localization for the highest noise level compared to the other two levels. The findings suggest that users have a threshold of disturbance rejection and that they attempt to increase their signal-to-noise ratio through their exploratory actions.
ER  - 

TY  - CONF
TI  - Tactile sensing based on fingertip suction flow for submerged dexterous manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3701
EP  - 3707
AU  - P. Nadeau
AU  - M. Abbott
AU  - D. Melville
AU  - H. S. Stuart
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - motion control
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - tactile sensors
KW  - saltwater corrosion
KW  - low-light conditions
KW  - robust electrical parts
KW  - mechanical parts
KW  - underwater robots
KW  - mobile manipulation tasks
KW  - suction flow mechanism
KW  - orifice occlusion
KW  - ambient pressure
KW  - tactile sensing modality
KW  - automated robotic behaviors
KW  - fingertip suction flow
KW  - submerged dexterous manipulation
KW  - robotic systems
KW  - Electron tubes
KW  - Sea measurements
KW  - Tactile sensors
KW  - Oceans
DO  - 10.1109/ICRA40945.2020.9197582
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ocean is a harsh and unstructured environment for robotic systems; high ambient pressures, saltwater corrosion and low-light conditions demand machines with robust electrical and mechanical parts that are able to sense and respond to the environment. Prior work shows that the addition of gentle suction flow to the hands of underwater robots can aid in the handling of objects during mobile manipulation tasks. The current paper explores using this suction flow mechanism as a new modality for tactile sensing; by monitoring orifice occlusion we can get a sense of how objects make contact in the hand. The electronics required for this sensor can be located remotely from the hand and the signal is insensitive to large changes in ambient pressure associated with diving depth. In this study, suction is applied to the fingertips of a two-fingered compliant gripper and suction-based tactile sensing is monitored while an object is pulled out of a pinch grasp. As a proof of concept, a recurrent neural network model was trained to predict external force trends using only the suction signals. This tactile sensing modality holds the potential to enable automated robotic behaviors or to provide operators of remotely operated vehicles with additional feedback in a robust fashion suitable for ocean deployment.
ER  - 

TY  - CONF
TI  - Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3748
EP  - 3755
AU  - L. G. Camara
AU  - C. G√§bert
AU  - L. P≈ôeuƒçil
PY  - 2020
KW  - convolutional neural nets
KW  - image coding
KW  - image matching
KW  - image resolution
KW  - visual databases
KW  - image resolution
KW  - VGG16 CNN architecture
KW  - matching CNN features
KW  - query image
KW  - VGG16 Convolutional Neural Network architecture
KW  - Spatial Matching Visual Place Recognition
KW  - SSM-VPR
KW  - optimal image resolutions
KW  - Visualization
KW  - Robustness
KW  - Semantics
KW  - Task analysis
KW  - Histograms
KW  - Correlation
KW  - Simultaneous localization and mapping
KW  - Visual Place Recognition
KW  - Convolutional Neural Networks
KW  - SLAM
KW  - Loop Closure
KW  - Life-long Navigation
DO  - 10.1109/ICRA40945.2020.9196967
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We revise, improve and extend the system previously introduced by us and named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition), largely boosting its performance above the current state of the art. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. It consists of two stages: given a query image of a place, (1) a list of candidates is selected from a database of places and (2) the candidates are geometrically compared with the query. The comparison is made by matching CNN features and, equally important, their spatial locations, selecting the best candidate as the recognized place. The performance of the system is maximized by finding optimal image resolutions during the second stage and by exploiting temporal correlation between consecutive frames in the employed datasets.
ER  - 

TY  - CONF
TI  - Online Trajectory Planning Through Combined Trajectory Optimization and Function Approximation: Application to the Exoskeleton Atalante
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3756
EP  - 3762
AU  - A. Duburcq
AU  - Y. Chevaleyre
AU  - N. Bredeche
AU  - G. Bo√©ris
PY  - 2020
KW  - control engineering computing
KW  - function approximation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robust control
KW  - trajectory control
KW  - wearable robots
KW  - trajectory optimization
KW  - function approximation
KW  - autonomous robots
KW  - online trajectory planning
KW  - guided trajectory learning
KW  - self-balanced exoskeleton
KW  - Atalante
KW  - robust control strategies
KW  - Trajectory optimization
KW  - Function approximation
KW  - Task analysis
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196633
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous robots require online trajectory planning capability to operate in the real world. Efficient offline trajectory planning methods already exist, but are computationally demanding, preventing their use online. In this paper, we present a novel algorithm called Guided Trajectory Learning that learns a function approximation of solutions computed through trajectory optimization while ensuring accurate and reliable predictions. This function approximation is then used online to generate trajectories. This algorithm is designed to be easy to implement, and practical since it does not require massive computing power. It is readily applicable to any robotics systems and effortless to set up on real hardware since robust control strategies are usually already available. We demonstrate the computational performance of our algorithm on flat-foot walking with the self-balanced exoskeleton Atalante.
ER  - 

TY  - CONF
TI  - Act, Perceive, and Plan in Belief Space for Robot Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3763
EP  - 3769
AU  - M. Colledanchise
AU  - D. Malafronte
AU  - L. Natale
PY  - 2020
KW  - mobile robots
KW  - object recognition
KW  - path planning
KW  - belief space
KW  - robot localization
KW  - interleaved acting
KW  - planning technique
KW  - low-level geometric features
KW  - task planner
KW  - perception tasks
KW  - state spaces
KW  - Uncertainty
KW  - Planning
KW  - Task analysis
KW  - Object detection
KW  - Robot sensing systems
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9197097
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we outline an interleaved acting and planning technique to rapidly reduce the uncertainty of the estimated robot's pose by perceiving relevant information from the environment, as recognizing an object or asking someone for a direction. Generally, existing localization approaches rely on low-level geometric features such as points, lines, and planes. While these approaches provide the desired accuracy, they may require time to converge, especially with incorrect initial guesses. In our approach, a task planner computes a sequence of action and perception tasks to actively obtain relevant information from the robot's perception system. We validate our approach in large state spaces, to show how the approach scales, and in real environments, to show the applicability of our method on real robots. We prove that our approach is sound, probabilistically complete, and tractable in practical cases.
ER  - 

TY  - CONF
TI  - Decentralized Task Allocation in Multi-Agent Systems Using a Decentralized Genetic Algorithm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3770
EP  - 3776
AU  - R. Patel
AU  - E. Rudnick-Cohen
AU  - S. Azarm
AU  - M. Otte
AU  - H. Xu
AU  - J. W. Herrmann
PY  - 2020
KW  - genetic algorithms
KW  - multi-agent systems
KW  - min-time objective
KW  - decentralized GA approach
KW  - task execution
KW  - multiagent collaborative search missions
KW  - decentralized genetic algorithm
KW  - multiagent systems
KW  - decentralized task allocation problem
KW  - decentralized evolutionary approaches
KW  - min-time performance
KW  - min-sum objective
KW  - Task analysis
KW  - Resource management
KW  - Genetic algorithms
KW  - Sociology
KW  - Statistics
KW  - Cost function
KW  - Message systems
DO  - 10.1109/ICRA40945.2020.9197314
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In multi-agent collaborative search missions, task allocation is required to determine which agents will perform which tasks. We propose a new approach for decentralized task allocation based on a decentralized genetic algorithm (GA). The approach parallelizes a genetic algorithm across the team of agents, making efficient use of their computational resources. In the proposed approach, the agents continuously search for and share better solutions during task execution. We conducted simulation experiments to compare the decentralized GA approach and several existing approaches. Two objectives were considered: a min-sum objective (minimizing the total distance traveled by all agents) and a min-time objective (minimizing the time to visit all locations of interest). The results showed that the decentralized GA approach yielded task allocations that were better on the min-time objective than those created by existing approaches and solutions that were reasonable on the min-sum objective. The decentralized GA improved min-time performance by an average of 5.6% on the larger instances. The results indicate that decentralized evolutionary approaches have a strong potential for solving the decentralized task allocation problem.
ER  - 

TY  - CONF
TI  - Fast and resilient manipulation planning for target retrieval in clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3777
EP  - 3783
AU  - C. Nam
AU  - J. Lee
AU  - S. Hun Cheong
AU  - B. Y. Cho
AU  - C. Kim
PY  - 2020
KW  - collision avoidance
KW  - image retrieval
KW  - manipulators
KW  - mobile robots
KW  - object detection
KW  - resilient manipulation planning
KW  - clutter
KW  - task and motion planning
KW  - robotic manipulator
KW  - target object retrieval
KW  - collision-free path
KW  - object rearrangement
KW  - TAMP framework
KW  - static environments
KW  - pick-and-place actions
KW  - baseline methods
KW  - Planning
KW  - Task analysis
KW  - Clutter
KW  - Collision avoidance
KW  - Manipulators
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9196652
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a task and motion planning (TAMP) framework for a robotic manipulator in order to retrieve a target object from clutter. We consider a configuration of objects in a confined space with a high density so no collision-free path to the target exists. The robot must relocate some objects to retrieve the target without collisions. For fast completion of object rearrangement, the robot aims to optimize the number of pick-and-place actions which often determines the efficiency of a TAMP framework.We propose a task planner incorporating motion planning to generate executable plans which aims to minimize the number of pick-and-place actions. In addition to fully known and static environments, our method can deal with uncertain and dynamic situations incurred by occluded views. Our method is shown to reduce the number of pick-and-place actions compared to baseline methods (e.g., at least 28.0% of reduction in a known static environment with 20 objects).
ER  - 

TY  - CONF
TI  - Untethered Soft Millirobot with Magnetic Actuation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3792
EP  - 3798
AU  - A. Bhattacharjee
AU  - L. W. Rogowski
AU  - X. Zhang
AU  - M. J. Kim
PY  - 2020
KW  - bending
KW  - magnetic actuators
KW  - magnetic flux
KW  - microrobots
KW  - mobile robots
KW  - motion control
KW  - permanent magnets
KW  - polymers
KW  - robot kinematics
KW  - rods (structures)
KW  - untethered soft millirobot
KW  - magnetic actuation
KW  - scalable designs
KW  - moulding technique
KW  - acrylonitrile butadiene styrene filaments
KW  - embedded permanent magnets
KW  - soft-body
KW  - soft-robots
KW  - external uniform magnetic field control system
KW  - magnetic flux densities
KW  - soft-robotic body
KW  - magnetic field strength
KW  - motion modes
KW  - magnetic field inputs
KW  - hollow rod-like structures
KW  - polydimethylsiloxane
KW  - 3D printed polylactic acid rings
KW  - pivot walking
KW  - rolling motion
KW  - tumbling motion
KW  - wiggling motion
KW  - side-tapping motion
KW  - wavy motion
KW  - deflection curve
KW  - navigation
KW  - minimally invasive in vivo applications
KW  - bending angle
KW  - Robots
KW  - Permanent magnets
KW  - Magnetic resonance imaging
KW  - Programmable logic arrays
KW  - Fabrication
KW  - Plastics
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197202
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents scalable designs and fabrication, actuation, and manipulation techniques for soft millirobots under uniform magnetic field control. The millirobots were fabricated through an economic and robust moulding technique using polydimethylsiloxane (PDMS), acrylonitrile butadiene styrene (ABS) filaments, and 3D printed polylactic acid (PLA) rings. The soft millirobots were simple hollow rod-like structures with different configurations of embedded permanent magnets inside of their soft-body or at their ends. The soft-robots were actuated using six different motion modes including: pivot walking, rolling, tumbling, side-tapping, wiggling, and wavy-motion under an external uniform magnetic field control system. The velocities of the millirobots under different motion modes were analyzed under varying magnetic flux densities (B). Moreover, deformation of the soft-robotic body in response to the magnetic field strength was measured and a deflection curve showing bending angle (œÜ) was produced. Soft millirobots were navigated through a maze using a combination of the available motion modes. Different arrangements of the embedded permanent magnets enabled individual soft millirobots to respond heterogeneously under the same magnetic field inputs towards performing assembly and disassembly operation as modular subunits. Overall, this soft millirobot platform shows enormous potential for minimally invasive in vivo applications.
ER  - 

TY  - CONF
TI  - Accelerated Robot Learning via Human Brain Signals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3799
EP  - 3805
AU  - I. Akinola
AU  - Z. Wang
AU  - J. Shi
AU  - X. He
AU  - P. Lapborisuth
AU  - J. Xu
AU  - D. Watkins-Valls
AU  - P. Sajda
AU  - P. Allen
PY  - 2020
KW  - brain
KW  - electroencephalography
KW  - feedback
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - accelerated robot learning
KW  - human brain signals
KW  - reinforcement learning
KW  - RL algorithms struggle
KW  - learning signal
KW  - robot learning process
KW  - error-related signal
KW  - measurable using electroencephelography
KW  - robotic agents
KW  - sparse reward settings
KW  - human observer
KW  - robot attempts
KW  - noisy error feedback signal
KW  - supervised learning
KW  - RL learning process
KW  - robotic navigation task
KW  - Task analysis
KW  - Robots
KW  - Electroencephalography
KW  - Navigation
KW  - Hafnium
KW  - Learning (artificial intelligence)
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196566
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a simple robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.
ER  - 

TY  - CONF
TI  - Muscle and Brain Activations in Cylindrical Rotary Controller Manipulation with Index Finger and Thumb
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3806
EP  - 3811
AU  - R. Okatani
AU  - T. Tsumugiwa
AU  - R. Yokogawa
AU  - M. Narusue
AU  - H. Nishimura
AU  - Y. Takeda
AU  - T. Hara
PY  - 2020
KW  - biomechanics
KW  - brain
KW  - electromyography
KW  - medical signal processing
KW  - muscle
KW  - neurophysiology
KW  - position measurement
KW  - viscosity
KW  - cylindrical rotary controller manipulation
KW  - index finger
KW  - viscosity characteristics differences
KW  - rotational manipulation
KW  - brain activations
KW  - rotary manipulation
KW  - rotary motion
KW  - viscosity characteristics
KW  - brain activity
KW  - muscles activity
KW  - muscle activity
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196520
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study aim to confirm the effect of viscosity characteristics differences on the rotational manipulation of a cylindrical rotary controller with the index finger and thumb through a quantitative analysis and evaluation of muscle and brain activations. The target motion was a rotary manipulation with the index finger and thumb of a cylindrical rotary controller with a 50 mm diameter. The rotary motion of the controller produces a click sensation at every 12 degrees in the rotation. The experimental conditions were three conditions with different viscosity characteristics related to the rotary motion of the controller. The subjects were six right-handed healthy males with a mean age of 21.7 (S. D.: 1.03) years. We analyzed the brain activity from a near- infrared spectroscopy measurement system, the muscles activity using a surface myoelectric potential measurement device, the force data at the index finger and thumb tip using two independent six-axis force/torque sensors, and the position data using a 3D position measurement device. The experimental results showed that there was no significant difference in the questionnaire survey, muscle activity, and grasping force, respectively; however, a significant difference in brain activity was observed with increased controller viscosity. Therefore, it became clear that there was a change in the brain activity when rotating the cylindrical rotary controller with the viscosity characteristics related to the rotary motion.
ER  - 

TY  - CONF
TI  - Real-Time Robot Reach-To-Grasp Movements Control Via EOG and EMG Signals Decoding
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3812
EP  - 3817
AU  - B. Specht
AU  - Z. Tayeb
AU  - E. Dean
AU  - R. Soroushmojdehi
AU  - G. Cheng
PY  - 2020
KW  - biomechanics
KW  - brain-computer interfaces
KW  - electromyography
KW  - electro-oculography
KW  - grippers
KW  - human-robot interaction
KW  - man-machine systems
KW  - medical robotics
KW  - medical signal processing
KW  - neurophysiology
KW  - signal classification
KW  - time robot reach-to-grasp movement control
KW  - grasping task
KW  - industrial robot
KW  - eye movements
KW  - electromyography signals
KW  - real-time human-robot interface system
KW  - human-controlled assistive devices
KW  - dexterous devices
KW  - EOG signals
KW  - robot arms
KW  - real-time control
KW  - HRI systems
KW  - robot control
KW  - EMG signals
KW  - real-time decoding
KW  - EMG decoding
KW  - UR-10 robot arm
KW  - Electrooculography
KW  - Electromyography
KW  - Real-time systems
KW  - Decoding
KW  - Electrodes
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197550
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-to-grasp movements. For that, five different eye movements (up, down, left, right and rest) were classified in real-time and translated into commands to steer an industrial robot (UR-10) to one of the four approximate target directions. Thereafter, EMG signals were decoded to perform the grasping task using an attached gripper to the UR-10 robot arm. The proposed system was tested offline on three different healthy subjects, and mean validation accuracy of 93.62% and 99.50% were obtained across the three subjects for EOG and EMG decoding, respectively. Furthermore, the system was successfully tested in real-time with one subject, and mean online accuracy of 91.66% and 100% were achieved for EOG and EMG decoding, respectively. Our results obtained by combining real-time decoding of EOG and EMG signals for robot control show overall the potential of this approach to develop powerful and less complex HRI systems. Overall, this work provides a proof-of-concept for successful real-time control of robot arms using EMG and EOG signals, paving the way for the development of more dexterous and human-controlled assistive devices.
ER  - 

TY  - CONF
TI  - Simultaneous Estimations of Joint Angle and Torque in Interactions with Environments using EMG
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3818
EP  - 3824
AU  - D. Kim
AU  - K. Koh
AU  - G. Oppizzi
AU  - R. Baghi
AU  - L. -C. Lo
AU  - C. Zhang
AU  - L. -Q. Zhang
PY  - 2020
KW  - biomechanics
KW  - decoding
KW  - electromyography
KW  - medical signal processing
KW  - muscle
KW  - time series
KW  - decoding method
KW  - LSTM network
KW  - decoding approach
KW  - wrist joint
KW  - long-time span
KW  - time series
KW  - core processor
KW  - short-term memory network
KW  - electromyography
KW  - decoding technique
KW  - joint angle
KW  - learning EMG signals
KW  - Electromyography
KW  - Torque
KW  - Decoding
KW  - Wrist
KW  - Logic gates
KW  - Neural networks
KW  - Kinematics
KW  - Human-machine interaction
KW  - Electromyography (EMG)
KW  - Decoding
KW  - Machine learning
KW  - Prosthesis
DO  - 10.1109/ICRA40945.2020.9197441
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We develop a decoding technique that estimates both the position and torque of a joint of the limb in interaction with an environment based on activities of the agonist-antagonist pair of muscles using electromyography in real time. The long short-term memory (LSTM) network is employed as the core processor of the proposed technique that is capable of learning time series of a long-time span with varying time lags. A validation that is conducted on the wrist joint shows that the decoding approach provides an agreement of greater than 95% in kinetics (i.e. torque) estimation and an agreement of greater than 85% in kinematics (i.e. angle) estimation, between the actual and estimated variables, during interactions with an environment. Also demonstrated is the fact that the proposed decoding method inherits the strengths of the LSTM network in terms of the capability of learning EMG signals and the corresponding responses with time dependency.
ER  - 

TY  - CONF
TI  - High-Density Electromyography Based Control of Robotic Devices: On the Execution of Dexterous Manipulation Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3825
EP  - 3831
AU  - A. Dwivedi
AU  - J. Lara
AU  - L. K. Cheng
AU  - N. Paskaranandavadivel
AU  - M. Liarokapis
PY  - 2020
KW  - biomechanics
KW  - biomedical electrodes
KW  - dexterous manipulators
KW  - electromyography
KW  - manipulators
KW  - medical robotics
KW  - medical signal processing
KW  - patient rehabilitation
KW  - prosthetics
KW  - telerobotics
KW  - EMG signals
KW  - object motion decoding
KW  - muscle importances
KW  - muscle importance results
KW  - decoded motions
KW  - fingered robotic hand
KW  - robotic devices
KW  - dexterous manipulation tasks
KW  - electromyography-based interfaces
KW  - robotics studies
KW  - teleoperation
KW  - telemanipulation applications
KW  - EMG-based control
KW  - prosthetic rehabilitation devices
KW  - assistive rehabilitation devices
KW  - robotic rehabilitation devices
KW  - grasping tasks
KW  - learning scheme
KW  - High Density Electromyography sensors
KW  - in-hand manipulation motions
KW  - object space
KW  - myoelectric activations
KW  - human forearm
KW  - hand muscles
KW  - yaw motions
KW  - geometric center
KW  - myoelectric data
KW  - high-density electromyography-based control
KW  - HD-EMG electrode arrays
KW  - Muscles
KW  - Electromyography
KW  - Task analysis
KW  - Electrodes
KW  - Robots
KW  - Decoding
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196629
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electromyography (EMG) based interfaces have been used in various robotics studies ranging from teleoperation and telemanipulation applications to the EMG based control of prosthetic, assistive, or robotic rehabilitation devices. But most of these studies have focused on the decoding of user's motion or on the control of the robotic devices in the execution of simple tasks (e.g., grasping tasks). In this work, we present a learning scheme that employs High Density Electromyography (HD-EMG) sensors to decode a set of dexterous, in-hand manipulation motions (in the object space) based on the myoelectric activations of human forearm and hand muscles. To do that, the subjects were asked to perform roll, pitch, and yaw motions manipulating two different cubes. The first cube was designed to have a center of mass coinciding with the geometric center of the cube, while for the second cube the center of mass was shifted 14 mm to the right (off-centered design). Regarding the acquisition of the myoelectric data, custom HD-EMG electrode arrays were designed and fabricated. Using these arrays, a total of 89 EMG signals were extracted. The object motion decoding was formulated as a regression problem using the Random Forests (RF) technique and the muscle importances were studied using the inherent feature variables importance calculation procedure of the RF. The muscle importance results show that different subjects use different strategies to execute the same motions on same object when the weight is off-centered. Finally, the decoded motions were used to control a five fingered robotic hand in a proof-of-concept application.
ER  - 

TY  - CONF
TI  - Perception-Action Coupling in Usage of Telepresence Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3846
EP  - 3852
AU  - A. Valiton
AU  - Z. Li
PY  - 2020
KW  - cameras
KW  - control system synthesis
KW  - manipulators
KW  - robot vision
KW  - telecontrol
KW  - telerobotics
KW  - visual feedback
KW  - teleoperation assistance design
KW  - telepresence camera selection
KW  - standalone cameras
KW  - wearable cameras
KW  - manipulation motions
KW  - active perception control
KW  - human motor system
KW  - natural perception-action coupling
KW  - autonomous camera selection
KW  - active perception motions
KW  - coordinated manipulation
KW  - telepresence tele-action robots
KW  - telepresence cameras
KW  - robot teleoperation
KW  - telepresence system
KW  - Cameras
KW  - Task analysis
KW  - Robot vision systems
KW  - Robot kinematics
KW  - Telepresence
KW  - Teleoperators
DO  - 10.1109/ICRA40945.2020.9197578
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Telepresence tele-action robots enable human workers to reliably perform difficult tasks in remote, cluttered, and human environments. However, the effort to control coordinated manipulation and active perception motions may exhaust and intimidate novice workers. We hypothesize that such cognitive efforts would be effectively reduced if the teleoperators are provided with autonomous camera selection and control aligned with the natural perception-action coupling of the human motor system. Thus, we conducted a user study to investigate the coordination of active perception control and manipulation motions performed with visual feedback from various wearable and standalone cameras in a telepresence scenario. Our study discovered rich information about telepresence camera selection to inform telepresence system configuration and possible teleoperation assistance design for reduced cognitive effort in robot teleoperation.
ER  - 

TY  - CONF
TI  - A technical framework for human-like motion generation with autonomous anthropomorphic redundant manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3853
EP  - 3859
AU  - G. Averta
AU  - D. Caporale
AU  - C. D. Santina
AU  - A. Bicchi
AU  - M. Bianchi
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator kinematics
KW  - mechanical variables control
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - redundant manipulators
KW  - trajectory control
KW  - technical framework
KW  - motion generation
KW  - autonomous anthropomorphic redundant manipulators
KW  - co-bots
KW  - industrial settings
KW  - people assistance
KW  - robot motions
KW  - classic solutions
KW  - anthropomorphic movement generation
KW  - optimization procedures
KW  - neuroscientific literature
KW  - learning methods
KW  - motion variability
KW  - high dimensional datasets
KW  - human upper limb principal motion modes
KW  - functional analysis
KW  - robot trajectory optimization
KW  - redundant anthropomorphic kinematic architectures
KW  - human model
KW  - functional mode extraction
KW  - human trajectories
KW  - robotic manipulator
KW  - advanced human-robot interaction
KW  - industrial co-botics
KW  - human assistance
KW  - Kinematics
KW  - Manipulators
KW  - Trajectory
KW  - Computer architecture
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9196937
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The need for users' safety and technology accept-ability has incredibly increased with the deployment of co-bots physically interacting with humans in industrial settings, and for people assistance. A well-studied approach to meet these requirements is to ensure human-like robot motions. Classic solutions for anthropomorphic movement generation usually rely on optimization procedures, which build upon hypotheses devised from neuroscientific literature, or capitalize on learning methods. However, these approaches come with limitations, e.g. limited motion variability or the need for high dimensional datasets. In this work, we present a technique to directly embed human upper limb principal motion modes computed through functional analysis in the robot trajectory optimization. We report on the implementation with manipulators with redundant anthropomorphic kinematic architectures - although dissimilar with respect to the human model used for functional mode extraction - via Cartesian impedance control. In our experiments, we show how human trajectories mapped onto a robotic manipulator still exhibit the main characteristics of human-likeness, e.g. low jerk values. We discuss the results with respect to the state of the art, and their implications for advanced human-robot interaction in industrial co-botics and for human assistance.
ER  - 

TY  - CONF
TI  - Real-Time Adaptive Assembly Scheduling in Human-Multi-Robot Collaboration According to Human Capability*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3860
EP  - 3866
AU  - S. Zhang
AU  - Y. Chen
AU  - J. Zhang
AU  - Y. Jia
PY  - 2020
KW  - assembling
KW  - genetic algorithms
KW  - multi-agent systems
KW  - multi-robot systems
KW  - robotic assembly
KW  - scheduling
KW  - human-multirobot collaboration
KW  - human capability
KW  - optimal assembly scheduling
KW  - robot adaptation
KW  - human-single-robot interaction
KW  - human-multirobot interaction
KW  - multiagent interactions
KW  - real-time adaptive assembly scheduling approach
KW  - formulated adaptive assembly scheduling problem
KW  - human-multirobot assembly tasks
KW  - Robots
KW  - Job shop scheduling
KW  - Task analysis
KW  - Real-time systems
KW  - Schedules
KW  - Adaptive scheduling
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9196618
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human-multi-robot collaboration is becoming more and more common in intelligent manufacturing. Optimal assembly scheduling of such systems plays a critical role in their production efficiency. Existing approaches mostly consider humans as agents with assumed or known capabilities, which leads to suboptimal performance in realistic applications where human capabilities usually change. In addition, most robot adaptation focuses on human-single-robot interaction and the adaptation in human-multi-robot interaction with changing human capability still remains challenging due to the complexity of the heterogeneous multi-agent interactions. This paper proposes a real-time adaptive assembly scheduling approach for human-multi-robot collaboration by modeling and incorporating changing human capability. A genetic algorithm is also designed to derive implementable solutions for the formulated adaptive assembly scheduling problem. The proposed approaches are validated through different simulated human-multi-robot assembly tasks and the results demonstrate the effectiveness and advantages of the proposed approaches.
ER  - 

TY  - CONF
TI  - Microscope-Guided Autonomous Clear Corneal Incision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3867
EP  - 3873
AU  - J. Xia
AU  - S. J. Bergunder
AU  - D. Lin
AU  - Y. Yan
AU  - S. Lin
AU  - M. Ali Nasseri
AU  - M. Zhou
AU  - H. Lin
AU  - K. Huang
PY  - 2020
KW  - eye
KW  - medical computing
KW  - medical robotics
KW  - ophthalmic lenses
KW  - surgery
KW  - ex-vivo porcine eyes
KW  - microscope-guided autonomous clear corneal incision
KW  - ophthalmic microscope system
KW  - multiaxes robot
KW  - self-sealing incision
KW  - autonomous robotic system
KW  - cataract surgery
KW  - Robots
KW  - Surgery
KW  - Feature extraction
KW  - Iris
KW  - Cameras
KW  - Mirrors
KW  - Cataracts
DO  - 10.1109/ICRA40945.2020.9196645
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Clear Corneal Incision, a challenging step in cataract surgery, and important to the overall quality of the surgery. New surgeons usually spend one full year trying to perfect their incision, but even after such rigorous training deficient incisions can still occur. This paper proposes an autonomous robotic system for this self-sealing incision. A conventional ophthalmic microscope system with a monocular camera is utilized to capture the surgical scene, ascertain the robot's position, and estimate depth information. Kinematics with a remote centre of motion (RCM) is designed for a multi-axes robot to perform the incision route. The experimental results on ex-vivo porcine eyes show the autonomous Clear Corneal Incision has a stricter three-plane structure than a surgeon-made incision, which is closer to the ideal incision.
ER  - 

TY  - CONF
TI  - Asynchronous and decoupled control of the position and the stiffness of a spatial RCM tensegrity mechanism for needle manipulation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3882
EP  - 3888
AU  - J. R. J. Realpe
AU  - G. Aiche
AU  - S. Abdelaziz
AU  - P. Poignet
PY  - 2020
KW  - manipulator kinematics
KW  - mean square error methods
KW  - medical robotics
KW  - needles
KW  - position control
KW  - surgery
KW  - asynchronous control
KW  - decoupled control
KW  - spatial RCM tensegrity mechanism
KW  - needle manipulation
KW  - spatial remote center
KW  - double parallelogram system
KW  - percutaneous needle insertion
KW  - decoupled modulation
KW  - control methodology
KW  - position tracking
KW  - needle guide
KW  - RCM tensegrity mechanism
KW  - spatial remote center of motion
KW  - Needles
KW  - Kinematics
KW  - Mathematical model
KW  - Actuators
KW  - Modulation
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197507
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a 2-DOF spatial remote center of motion (RCM) tensegrity mechanism, based on a double parallelogram system, dedicated for percutaneous needle insertion. The originality of this mechanism is its ability to be reconfigured and its capacity to perform a decoupled modulation of its stiffness in an asynchronous way. To do so, an analytical stiffness model of the robot is established, and a control methodology is proposed. A prototype of the robot is developed and assessed experimentally. The position tracking is evaluated using a 6-DOF magnetic tracker sensor showing a root mean square error less than 0.8¬∞ in both directions of the needle guide.
ER  - 

TY  - CONF
TI  - Redundancy Resolution Integrated Model Predictive Control of CDPRs: Concept, Implementation and Experiments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3889
EP  - 3895
AU  - J. C. Santos
AU  - A. Chemori
AU  - M. Gouttefarde
PY  - 2020
KW  - cables (mechanical)
KW  - predictive control
KW  - robot kinematics
KW  - robust control
KW  - trajectory control
KW  - MPC scheme
KW  - fully-constrained cable-driven parallel robots
KW  - cable tension distribution
KW  - pick-and-place task
KW  - maximum tension
KW  - tracking error minimization
KW  - redundancy resolution integrated model predictive control
KW  - CDPRs
KW  - cable tension limits
KW  - robustness
KW  - payload mass
KW  - Power cables
KW  - Robots
KW  - Real-time systems
KW  - Kinematics
KW  - Payloads
KW  - Safety
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197271
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a Model Predictive Control (MPC) strategy for fully-constrained Cable-Driven Parallel Robots. The main advantage of the proposed scheme lies in its ability to explicitly handle cable tension limits. Indeed, the cable tension distribution is performed as an integral part of the main control architecture. This characteristic significantly improves the safety of the system. Experimental results demonstrate this advantage addressing a typical pick-and-place task with two different scenarios: nominal cable tension limits and reduced maximum tension. Satisfactory tracking errors were obtained in the first scenario. In the second scenario, the desired trajectory escapes from the workspace defined by the new set of tension limits. The proposed MPC scheme is able to minimize the tracking errors without violating the tension limits. Satisfying results were also obtained regarding robustness against uncertainties on the payload mass.
ER  - 

TY  - CONF
TI  - Mechanics for Tendon Actuated Multisection Continuum Arms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3896
EP  - 3902
AU  - P. S. Gonthina
AU  - M. B. Wooten
AU  - I. S. Godage
AU  - I. D. Walker
PY  - 2020
KW  - actuators
KW  - bending
KW  - biomechanics
KW  - inspection
KW  - medical robotics
KW  - robot kinematics
KW  - tendon actuated multisection continuum arms
KW  - bending deformations
KW  - high mechanical coupling
KW  - variable length-based kinematic models
KW  - continuum arm curve parameter kinematics
KW  - robot
KW  - Tendons
KW  - Robots
KW  - Computational modeling
KW  - Strain
KW  - Numerical models
KW  - Kinematics
KW  - Deformable models
DO  - 10.1109/ICRA40945.2020.9197006
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tendon actuated multisection continuum arms have high potential for inspection applications in highly constrained spaces. They generate motion by axial and bending deformations. However, because of the high mechanical coupling between continuum sections, variable length-based kinematic models produce poor results. A new mechanics model for tendon actuated multisection continuum arms is proposed in this paper. The model combines the continuum arm curve parameter kinematics and concentric tube kinematics to correctly account for the large axial and bending deformations observed in the robot. Also, the model is computationally efficient and utilizes tendon tensions as the joint space variables thus eliminating the actuator length related problems such as slack and backlash. A recursive generalization of the model is also presented. Despite the high coupling between continuum sections, numerical results show that the model can be used for generating correct forward and inverse kinematic results. The model is then tested on a thin and long multisection continuum arm. The results show that the model can be used to successfully model the deformation.
ER  - 

TY  - CONF
TI  - Trajectory Optimization for a Six-DOF Cable-Suspended Parallel Robot with Dynamic Motions Beyond the Static Workspace
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3903
EP  - 3908
AU  - S. Xiang
AU  - H. Gao
AU  - Z. Liu
AU  - C. Gosselin
PY  - 2020
KW  - cables (mechanical)
KW  - manipulator dynamics
KW  - optimisation
KW  - path planning
KW  - position control
KW  - trajectory control
KW  - cable-suspended parallel robot
KW  - static workspace
KW  - trajectory optimization formulation
KW  - dynamic trajectories
KW  - six-degree-of-freedom
KW  - low-dimensional dynamic models
KW  - narrow feasible state space
KW  - dynamic similarity
KW  - point-mass CSPR
KW  - feasible force polyhedra
KW  - transition trajectories
KW  - highly dynamic motions
KW  - periodic trajectories
KW  - Dynamics
KW  - Planning
KW  - Robots
KW  - Trajectory optimization
KW  - Chebyshev approximation
KW  - Dynamic trajectory planning
KW  - optimization and optimal control
KW  - cable-suspended parallel robots
DO  - 10.1109/ICRA40945.2020.9196803
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a trajectory optimization formulation for planning dynamic trajectories of a six-degree-of-freedom (six-DOF) cable-suspended parallel robot (CSPR) that extend beyond the static workspace. The optimization is guided by low-dimensional dynamic models to overcome the local minima and accelerate the exploration of the narrow feasible state space. The dynamic similarity between the six-DOF CSPR and the three-DOF point-mass CSPR is discussed with the analyses of their feasible force polyhedra. Finally, the transition trajectories of a three-DOF CSPR are used as the initial guess of the translational part of the six-DOF motion. With the proposed approach, highly dynamic motions for a six-DOF CSPR are efficiently generated with multiple oscillations. The feasibility is demonstrated by point-to-point and periodic trajectories in the physics simulation.
ER  - 

TY  - CONF
TI  - An Intelligent Spraying System with Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3923
EP  - 3929
AU  - J. Kim
AU  - J. Seol
AU  - S. Lee
AU  - S. -W. Hong
AU  - H. I. Son
PY  - 2020
KW  - agriculture
KW  - agrochemicals
KW  - cameras
KW  - crops
KW  - image capture
KW  - image classification
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - nozzles
KW  - sensor fusion
KW  - pear orchard
KW  - deep learning-based intelligent spraying system
KW  - fruit tree detection system
KW  - SegNet model
KW  - semantic segmentation structure
KW  - deep learning model
KW  - nozzle
KW  - data fusion
KW  - RGB-D camera
KW  - image capture
KW  - pesticides
KW  - environmental safety
KW  - image classification
KW  - Spraying
KW  - Cameras
KW  - Semantics
KW  - Image segmentation
KW  - Vegetation
KW  - Decoding
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197556
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study proposes an intelligent spraying system with semantic segmentation of fruit trees in a pear orchard. A fruit tree detection system was developed using the SegNet model, a semantic segmentation structure. The system is trained with images categorized into five distinct classes. The learned deep learning model performed with an accuracy of 83.79%. Further, we fusion depth data from an RGB-D camera to prevent the tree in the background from being detected. To operate the nozzles, each image captured from the camera is separated lengthwise into quarters and mapped to the nozzles. Then, the nozzle was opened when the area of fruit trees in each zone exceeded 20%. Two types of field experiments were performed in a pear orchard to verify the effectiveness of our system. From the results obtained, we can confirm the satisfactory performance of our deep learning-based intelligent spraying system. It is expected that the introduction of this system to actual farms will signicantly reduce the amount of pesticide used and will make the work environment safer for farmers.
ER  - 

TY  - CONF
TI  - An Efficient Planning and Control Framework for Pruning Fruit Trees
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3930
EP  - 3936
AU  - A. You
AU  - F. Sukkar
AU  - R. Fitch
AU  - M. Karkee
AU  - J. R. Davidson
PY  - 2020
KW  - agricultural products
KW  - image colour analysis
KW  - industrial manipulators
KW  - intelligent robots
KW  - robot vision
KW  - motion planning
KW  - sequence cut points
KW  - robotic pruning
KW  - control framework
KW  - pruning fruit trees
KW  - dormant pruning
KW  - fresh market tree fruit production
KW  - industrial manipulator
KW  - eye-in-hand RGB-D camera configuration
KW  - pneumatic cutter
KW  - Planning
KW  - Cameras
KW  - Three-dimensional displays
KW  - Manipulators
KW  - Vegetation
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197551
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dormant pruning is a major cost component of fresh market tree fruit production, nearly equal in scale to harvesting the fruit. However, relatively little focus has been given to the problem of pruning trees autonomously. In this paper, we introduce a robotic system consisting of an industrial manipulator, an eye-in-hand RGB-D camera configuration, and a custom pneumatic cutter. The system is capable of planning and executing a sequence of cuts while making minimal assumptions about the environment. We leverage a novel planning framework designed for high-throughput operation which builds upon previous work to reduce motion planning time and sequence cut points intelligently. In end-to-end experiments with a set of ten different branch configurations, the system achieved a high success rate in plan execution and a 1.5x speedup in throughput versus a baseline planner, representing a significant step towards the goal of practical implementation of robotic pruning.
ER  - 

TY  - CONF
TI  - Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3937
EP  - 3943
AU  - A. Binch
AU  - G. P. Das
AU  - J. Pulido Fentanes
AU  - M. Hanheide
PY  - 2020
KW  - genetic algorithms
KW  - iterative methods
KW  - mobile robots
KW  - motion control
KW  - navigation
KW  - path planning
KW  - simulated environments
KW  - genetic algorithm
KW  - resulting parameter sets
KW  - substantial performance improvements
KW  - agricultural robots
KW  - context dependant iterative parameter optimisation
KW  - robust robot navigation
KW  - autonomous mobile robotics
KW  - motion control
KW  - path planning
KW  - robust performance
KW  - robot model
KW  - parameter tuning
KW  - underlying algorithm
KW  - substantial combinatorial challenge
KW  - extensive manual tuning
KW  - navigation actions
KW  - spatial context
KW  - navigation task
KW  - respective navigation algorithms
KW  - iterative optimisation
KW  - performance metrics
KW  - Navigation
KW  - Robots
KW  - Optimization
KW  - Tuning
KW  - Robustness
KW  - Genetic algorithms
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196550
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Progress in autonomous mobile robotics has seen significant advances in the development of many algorithms for motion control and path planning. However, robust performance from these algorithms can often only be expected if the parameters controlling them are tuned specifically for the respective robot model, and optimised for specific scenarios in the environment the robot is working in. Such parameter tuning can, depending on the underlying algorithm, amount to a substantial combinatorial challenge, often rendering extensive manual tuning of these parameters intractable. In this paper, we present a framework that permits the use of different navigation actions and/or parameters depending on the spatial context of the navigation task. We consider the respective navigation algorithms themselves mostly as a "black box", and find suitable parameters by means of an iterative optimisation, improving for performance metrics in simulated environments. We present a genetic algorithm incorporated into the framework, and empirically show that the resulting parameter sets lead to substantial performance improvements in both simulated and real-world environments in the domain of agricultural robots.
ER  - 

TY  - CONF
TI  - Extending Riemmanian Motion Policies to a Class of Underactuated Wheeled-Inverted-Pendulum Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3967
EP  - 3973
AU  - B. Wingo
AU  - C. -A. Cheng
AU  - M. Murtaza
AU  - M. Zafar
AU  - S. Hutchinson
PY  - 2020
KW  - collision avoidance
KW  - humanoid robots
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - pendulums
KW  - robot dynamics
KW  - wheels
KW  - RMP formalism
KW  - underacutated systems
KW  - fully-actuated subsystem
KW  - residual dynamics
KW  - manipulation tasks
KW  - 7-DoF system
KW  - second-order motion policies
KW  - RMP-based approaches
KW  - operational space control
KW  - fully state-dependent
KW  - collision avoidance bahaviors
KW  - control input
KW  - Riemmanian motion policies
KW  - underactuated wheeled-inverted-pendulum humanoid robot
KW  - Task analysis
KW  - Manifolds
KW  - Manipulator dynamics
KW  - Aerospace electronics
KW  - Humanoid robots
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196866
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Riemannian Motion Policies (RMPs) have recently been introduced as a way to specify second-order motion policies defined on robot task spaces. RMP-based approaches have the advantage of being more general than traditional approaches based on operational space control; for example, the generalized task inertia in an RMP can be fully state-dependent, which is particularly effective in designing collision avoidance bahaviors. But until now RMPs have been applied only to fully actuated systems, i.e. systems for which each degree of freedom (DoF) can be directly actuated by a control input. In this paper, we present a method that extends the RMP formalism to a class of underacutated systems whose dynamics are amenable to a decomposition into a fully-actuated subsystem and a residual dynamics. We show the efficacy of the approach by constructing a suitable decomposition for a Wheeled-Inverted-Pendulum (WIP) humanoid robot and applying our method to derive motion policies for combined locomotion and manipulation tasks. Simulation results are presented for a 7-DoF system with one degree of underactuation.
ER  - 

TY  - CONF
TI  - Augmenting Self-Stability: Height Control of a Bernoulli Ball via Bang-Bang Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3974
EP  - 3980
AU  - T. Howison
AU  - F. Giardina
AU  - F. Iida
PY  - 2020
KW  - asymptotic stability
KW  - bang-bang control
KW  - feedback
KW  - feedforward
KW  - motion control
KW  - self-adjusting systems
KW  - uncertain systems
KW  - augmented controller
KW  - uncontrolled system
KW  - mechanical self-stability
KW  - uncertain environments
KW  - unstructured environments
KW  - explicit state observation
KW  - control law
KW  - performance metrics
KW  - minimalistic approach
KW  - feedforward bang-bang control
KW  - self-stabilizing dynamics
KW  - height control
KW  - global asymptotic stability
KW  - self-stabilising systems
KW  - Bernoulli ball
KW  - motion control
KW  - Atmospheric modeling
KW  - Dynamics
KW  - Mathematical model
KW  - Force
KW  - Asymptotic stability
KW  - Bang-bang control
DO  - 10.1109/ICRA40945.2020.9197391
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mechanical self-stability is often useful for controlling systems in uncertain and unstructured environments because it can regulate processes without explicit state observation or feedback computation. However, the performance of such systems is often not optimised, which begs the question how their dynamics can be naturally augmented by a control law to improve performance metrics. We propose a minimalistic approach to controlling mechanically self-stabilising systems by utilising model-based, feedforward bang-bang control at a global level and self-stabilizing dynamics at a local level. We demonstrate the approach in the height control problem of a sphere hovering in a vertical air jet - the so-called Bernoulli Ball. After developing a model to study the system and theoretically proving global asymptotic stability, we present the augmented controller and show how to enhance performance measures and plan behaviour. Our physical experiments show that the proposed control approach has a reduced time-to-target compared to the uncontrolled system without loss of stability (ranging from a 2.4 to 4.4 fold improvement) and that we can plan sequences of target positions at will.
ER  - 

TY  - CONF
TI  - Singularity-Free Inverse Dynamics for Underactuated Systems with a Rotating Mass
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3981
EP  - 3987
AU  - S. A. Tafrishi
AU  - M. Svinin
AU  - M. Yamamoto
PY  - 2020
KW  - manipulator dynamics
KW  - matrix algebra
KW  - motion control
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - singularity-free inverse dynamics
KW  - underactuated system
KW  - rotating mass
KW  - motion control
KW  - configuration singularities
KW  - configuration space
KW  - inertial coupling
KW  - small-amplitude sine wave
KW  - nonlinear dynamics
KW  - rolling system
KW  - singularity regions
KW  - coupling singularities
KW  - rolling carrier
KW  - Mathematical model
KW  - Couplings
KW  - Integrated circuits
KW  - Trajectory
KW  - Kinematics
KW  - Robots
KW  - Tensile stress
DO  - 10.1109/ICRA40945.2020.9197306
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion control of underactuated systems through the inverse dynamics contains configuration singularities. These limitations in configuration space mainly stem from the inertial coupling that passive joints/bodies create. In this study, we present a model that is free from singularity while the trajectory of the rotating mass has a small-amplitude sine wave around its circle. First, we derive the modified non-linear dynamics for a rolling system. Also, the singularity regions for this underactuated system is demonstrated. Then, the wave parameters are designed under certain conditions to remove the coupling singularities. We obtain these conditions from the positive definiteness of the inertia matrix in the inverse dynamics. Finally, the simulation results are confirmed by using a prescribed Beta function on the specified states of the rolling carrier. Because our algebraic method is integrated into the non-linear dynamics, the proposed solution has a great potential to be extended to the Lagrangian mechanics with multiple degrees-of-freedom.
ER  - 

TY  - CONF
TI  - Robust capture of unknown objects with a highly under-actuated gripper
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 3996
EP  - 4002
AU  - P. E. Glick
AU  - N. Van Crey
AU  - M. T. Tolley
AU  - D. Ruffatto
PY  - 2020
KW  - actuators
KW  - adhesion
KW  - adhesives
KW  - dexterous manipulators
KW  - friction
KW  - grippers
KW  - under-actuated gripper
KW  - robotic grippers
KW  - high-friction materials
KW  - scaling forces
KW  - adhesion-controlled friction
KW  - adhesion-based grippers
KW  - high-friction interfaces
KW  - robust capture
KW  - size 65.0 cm
KW  - Grippers
KW  - Friction
KW  - Couplings
KW  - Stability analysis
KW  - Force
KW  - Torque
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197100
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Capturing large objects of unknown shape and orientation remains a challenge for most robotic grippers. We present a highly under-actuated gripper well suited for this task. Prior work shows two primary limitations to these grippers: the grip force of each link tends to decrease as the number of links increases, and the stability of an under-actuated linkage depends on the configuration of the links so grippers with many links are unlikely to be stable for arbitrary surfaces. We address these concerns by implementing two complementary methods of stabilization: using high-friction materials and scaling forces into the surface. We show that gecko-inspired adhesives provide an adhesion-controlled friction that can stabilize the gripper and improve grasp performance without the need of large normal forces. The under-actuated linkages also conform around arbitrary shapes and provide capability beyond prior adhesion-based grippers. With these high-friction interfaces, we show highly under-actuated linkages successfully grasp in many configurations without strict stability. The gripper is capable of holding over 30 N and consists of two tendon driven linkages that are each 65 cm long. This type of gripper is well suited for tasks without a predefined target geometry or orientation such as satellite servicing.
ER  - 

TY  - CONF
TI  - SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4023
EP  - 4029
AU  - P. Cai
AU  - Y. Lee
AU  - Y. Luo
AU  - D. Hsu
PY  - 2020
KW  - control engineering computing
KW  - multi-agent systems
KW  - road traffic
KW  - road vehicles
KW  - telecommunication traffic
KW  - traffic engineering computing
KW  - SUMMIT
KW  - urban driving
KW  - massive mixed traffic
KW  - unregulated urban crowd
KW  - high-speed traffic participants
KW  - high-fidelity simulator
KW  - crowd-driving algorithms
KW  - open-source OpenStreetMap map database
KW  - multiagent motion prediction model
KW  - unregulated urban traffic
KW  - heterogeneous agents
KW  - autonomous driving simulation
KW  - realistic traffic behaviors
KW  - crowd-driving settings
KW  - Roads
KW  - Robot sensing systems
KW  - Context modeling
KW  - Planning
KW  - Automobiles
KW  - Geometry
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197228
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.
ER  - 

TY  - CONF
TI  - A Model-Based Reinforcement Learning and Correction Framework for Process Control of Robotic Wire Arc Additive Manufacturing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4030
EP  - 4036
AU  - A. G. Dharmawan
AU  - Y. Xiong
AU  - S. Foong
AU  - G. Song Soh
PY  - 2020
KW  - learning (artificial intelligence)
KW  - process control
KW  - rapid prototyping (industrial)
KW  - robotic welding
KW  - three-dimensional printing
KW  - welds
KW  - wires
KW  - integrated learning-correction framework
KW  - model-based reinforcement learning
KW  - process parameters
KW  - inter-layer geometric digression
KW  - process control
KW  - robot arm
KW  - 3D metallic objects
KW  - layer by layer fashion
KW  - multilayer multibead deposition control
KW  - robotic wire arc additive manufacturing
KW  - weld beads
KW  - error stacking
KW  - material wastage reduction
KW  - MLMB print
KW  - Training
KW  - Predictive models
KW  - Process control
KW  - Adaptation models
KW  - Printing
KW  - Robots
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197222
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic Wire Arc Additive Manufacturing (WAAM) utilizes a robot arm as a motion system to build 3D metallic objects by depositing weld beads one above the other in a layer by layer fashion. A key part of this approach is the process study and control of Multi-Layer Multi-Bead (MLMB) deposition, which is very sensitive to process parameters and prone to error stacking. Despite its importance, it has been receiving less attention than its single bead counterpart in literature, probably due to the higher experimental overhead and complexity of modeling. To address these challenges, this paper proposes an integrated learning-correction framework, adapted from Model-Based Reinforcement Learning, to iteratively learn the direct effect of process parameters on MLMB print while simultaneously correct for any inter-layer geometric digression such that the final output is still satisfactory. The advantage is that this learning architecture can be used in conjunction with actual parts printing (hence, in-situ study), thus minimizing the required training time and material wastage. The proposed learning framework is implemented on an actual robotic WAAM system and experimentally evaluated.
ER  - 

TY  - CONF
TI  - Toward Optimal FDM Toolpath Planning with Monte Carlo Tree Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4037
EP  - 4043
AU  - C. Yoo
AU  - S. Lensgraf
AU  - R. Fitch
AU  - L. M. Clemon
AU  - R. Mettu
PY  - 2020
KW  - machine tools
KW  - Monte Carlo methods
KW  - optimisation
KW  - planning
KW  - rapid prototyping (industrial)
KW  - search problems
KW  - tree searching
KW  - 3D printing slice
KW  - dependency graph
KW  - MCTS-based algorithm
KW  - local search
KW  - toolpath quality
KW  - Monte Carlo tree search
KW  - optimal FDM toolpath planning
KW  - Planning
KW  - Monte Carlo methods
KW  - Three-dimensional printing
KW  - Solid modeling
KW  - Clustering algorithms
KW  - Printers
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196945
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The most widely used methods for toolpath planning in 3D printing slice the input model into successive 2D layers to construct the toolpath. Unfortunately the methods can incur a substantial amount of wasted motion (i.e., the extruder is moving while not printing). In recent years we have introduced a new paradigm that characterizes the space of feasible toolpaths using a dependency graph on the input model, along with several algorithms that optimize objective functions (wasted motion or print time). A natural question that arises is, under what circumstances can we efficiently compute an optimal toolpath? In this paper, we give an algorithm for computing fused deposition modeling (FDM) toolpaths that utilizes Monte Carlo Tree Search (MCTS), a powerful generalpurpose method for navigating large search spaces that is guaranteed to converge to the optimal solution. Under reasonable assumptions on printer geometry that allow us to compress the dependency graph, our MCTS-based algorithm converges to find the optimal toolpath. We validate our algorithm on a dataset of 75 models and examine the performance on MCTS against our previous best local search-based algorithm in terms of toolpath quality. We show that a relatively short time budget for MCTS yields results on par with local search, while a larger time budget yields a 15% improvement in quality over local search. Additionally, we examine the properties of the models and MCTS executions that lead to better or worse results.
ER  - 

TY  - CONF
TI  - Optimizing performance in automation through modular robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4044
EP  - 4050
AU  - S. B. Liu
AU  - M. Althoff
PY  - 2020
KW  - flexible manufacturing systems
KW  - industrial robots
KW  - mobile robots
KW  - robot dynamics
KW  - robot kinematics
KW  - modular robots
KW  - flexible manufacturing
KW  - module composition
KW  - cycle time
KW  - energy efficiency
KW  - kinematic constraints
KW  - dynamic constraints
KW  - industrial robots
KW  - randomly generated tasks
KW  - performance metrics
KW  - modular robot
KW  - proModular.1
KW  - obstacle constraints
KW  - Cartesian space
KW  - Kinematics
KW  - Service robots
KW  - Collision avoidance
KW  - Trajectory
KW  - Task analysis
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196590
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Flexible manufacturing and automation require robots that can be adapted to changing tasks. We propose to use modular robots that are customized from given modules for a specific task. This work presents an algorithm for proposing a module composition that is optimal with respect to performance metrics such as cycle time and energy efficiency, while considering kinematic, dynamic, and obstacle constraints. Tasks are defined as trajectories in Cartesian space, as a list of poses for the robot to reach as fast as possible, or as dexterity in a desired workspace. In a simulated comparison with commercially available industrial robots, we demonstrate the superiority of our approach in randomly generated tasks with respect to the chosen performance metrics. We use our modular robot proModular.1 for the comparison.
ER  - 


TY  - CONF
TI  - Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4051
EP  - 4058
AU  - R. Li
AU  - A. Jabri
AU  - T. Darrell
AU  - P. Agrawal
PY  - 2020
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - multiobject manipulation
KW  - relational reinforcement learning
KW  - learning robotic manipulation tasks
KW  - outrageous data requirements
KW  - task curriculum
KW  - graph-based relational architectures
KW  - simulated block stacking task
KW  - step-wise sparse rewards
KW  - zero-shot generalization
KW  - Task analysis
KW  - Stacking
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Poles and towers
KW  - Training
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197468
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the state- of-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more data- efficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.
ER  - 

TY  - CONF
TI  - SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4059
EP  - 4065
AU  - N. Majcherczyk
AU  - C. Pinciroli
PY  - 2020
KW  - control engineering computing
KW  - data handling
KW  - data structures
KW  - distributed processing
KW  - mobile robots
KW  - multi-robot systems
KW  - storage management
KW  - topology
KW  - data item position
KW  - near-perfect data retention
KW  - distributed data structure
KW  - cooperative multirobot applications
KW  - distributed storage
KW  - mobile robots
KW  - shared global memory
KW  - external storage infrastructure
KW  - swarm topology
KW  - data storage
KW  - SwarmMesh
KW  - data type
KW  - Robots
KW  - Data structures
KW  - Peer-to-peer computing
KW  - Overlay networks
KW  - Routing
KW  - Distributed databases
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9197403
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach to the distributed storage of data across a swarm of mobile robots that forms a shared global memory. We assume that external storage infrastructure is absent, and that each robot is capable of devoting a quota of memory and bandwidth to distributed storage. Our approach is motivated by the insight that in many applications data is collected at the periphery of a swarm topology, but the periphery also happens to be the most dangerous location for storing data, especially in exploration missions. Our approach is designed to promote data storage in the locations in the swarm that best suit a specific feature of interest in the data, while accounting for the constantly changing topology due to individual motion. We analyze two possible features of interest: the data type and the data item position in the environment. We assess the performance of our approach in a large set of simulated experiments. The evaluation shows that our approach is capable of storing quantities of data that exceed the memory of individual robots, while maintaining near-perfect data retention in high-load conditions.
ER  - 

TY  - CONF
TI  - Avalanche victim search via robust observers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4066
EP  - 4072
AU  - N. Mimmo
AU  - P. Bernard
AU  - L. Marconi
PY  - 2020
KW  - adaptive control
KW  - autonomous aerial vehicles
KW  - emergency management
KW  - multi-robot systems
KW  - observers
KW  - rescue robots
KW  - robust control
KW  - sensor fusion
KW  - avalanche victim search
KW  - robust observers
KW  - victim localization
KW  - ARVA sensor
KW  - adaptive control
KW  - UAVs
KW  - least square identifier
KW  - Receivers
KW  - Transmitters
KW  - Drones
KW  - Observers
KW  - Trajectory
KW  - Electromagnetics
KW  - Adaptive control
KW  - Search and Rescue
KW  - Robust Control
DO  - 10.1109/ICRA40945.2020.9196646
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a new approach for victim localization in avalanches that will be exploited by UAVs using the ARVA sensor. We show that the nominal ARVA measurement can be linearly related to a quantity that is sufficient to reconstruct the victim position. We explicitly deal with a robust scenario in which the measurement is actually perturbed by a noise that grows with the distance to the victim and we propose an adaptive control scheme made of a least-square identifier and a trajectory generator whose role is both to guarantee the persistence of excitation for the identifier and to steer the ARVA receiver towards the victim. We show that the system succeeds in localizing the victim in a domain where the ARVA output is sufficiently informative and illustrate its performance in simulation. This new approach could significantly reduce the searching time by providing an exploitable estimate before having reached the victim. The work is framed within the EU project AirBorne whose goals is to develop at TRL8 a drone for quick localization of victims in avalanche scenarios.
ER  - 

TY  - CONF
TI  - Reactive Control and Metric-Topological Planning for Exploration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4073
EP  - 4079
AU  - M. T. Ohradzansky
AU  - A. B. Mills
AU  - E. R. Rush
AU  - D. G. Riley
AU  - E. W. Frew
AU  - J. Sean Humbert
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - image sequences
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - optic flow
KW  - insect visuomotor system
KW  - obstacle detection
KW  - topological graph
KW  - metric-topological planning
KW  - autonomous navigation
KW  - robotic platforms
KW  - bio-inspired reactive control
KW  - spatial decomposition
KW  - obstacle avoidance
KW  - Fourier residual analysis
KW  - image processing
KW  - continuous occupancy grid
KW  - graph edge
KW  - Optical feedback
KW  - Optical sensors
KW  - Optical imaging
KW  - Planning
KW  - Navigation
KW  - Harmonic analysis
KW  - Mathematical model
KW  - exploration
KW  - centering
KW  - control
KW  - mapping
DO  - 10.1109/ICRA40945.2020.9197381
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation in unknown environments with the intent of exploring all traversable areas is a significant challenge for robotic platforms. In this paper, a simple yet reliable method for exploring unknown environments is presented based on bio-inspired reactive control and metric-topological planning. The reactive control algorithm is modeled after the spatial decomposition of wide and small-field patterns of optic flow in the insect visuomotor system. Centering behaviour and small obstacle detection and avoidance are achieved through wide-field integration and Fourier residual analysis of instantaneous measured nearness respectively. A topological graph is estimated using image processing techniques on a continuous occupancy grid. Node paths are rapidly generated to navigate to the nearest unexplored edge in the graph. It is shown through rigorous field-testing that the proposed control and planning method is robust, reliable, and computationally efficient.
ER  - 

TY  - CONF
TI  - Information Theoretic Active Exploration in Signed Distance Fields
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4080
EP  - 4085
AU  - K. Saulnier
AU  - N. Atanasov
AU  - G. J. Pappas
AU  - V. Kumar
PY  - 2020
KW  - deterministic algorithms
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - sensors
KW  - tree searching
KW  - robot sensing trajectories
KW  - autonomous TSDF mapping
KW  - TSDF uncertainty
KW  - sensor measurements
KW  - efficient uncertainty prediction
KW  - long-horizon optimization
KW  - deterministic tree-search algorithm
KW  - information gain
KW  - TSDF distribution
KW  - efficient planning
KW  - uninformative sensing trajectories
KW  - active TSDF mapping approach
KW  - simulated environments
KW  - information theoretic active exploration
KW  - occupancy mapping
KW  - mobile robot
KW  - truncated signed distance field
KW  - robot motion primitive sequences
KW  - branch-and-bound pruning
KW  - complex visibility constraints
KW  - Robot sensing systems
KW  - Trajectory
KW  - Measurement uncertainty
KW  - Standards
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196882
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on exploration and occupancy mapping of unknown environments using a mobile robot. While a truncated signed distance field (TSDF) is a popular, efficient, and highly accurate representation of occupancy, few works have considered optimizing robot sensing trajectories for autonomous TSDF mapping. We propose an efficient approach for maintaining TSDF uncertainty and predicting its evolution from potential future sensor measurements without actually receiving them. Efficient uncertainty prediction is critical for long-horizon optimization of potential sensing trajectories. We develop a deterministic tree-search algorithm that evaluates the information gain between the TSDF distribution and potential observations along sequences of robot motion primitives. Efficient planning is achieved by branch-and-bound pruning of uninformative sensing trajectories. The effectiveness of our active TSDF mapping approach is evaluated in several simulated environments with complex visibility constraints.
ER  - 

TY  - CONF
TI  - Bayesian Learning-Based Adaptive Control for Safety Critical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4093
EP  - 4099
AU  - D. D. Fan
AU  - J. Nguyen
AU  - R. Thakker
AU  - N. Alatur
AU  - A. -a. Agha-mohammadi
AU  - E. A. Theodorou
PY  - 2020
KW  - adaptive control
KW  - Bayes methods
KW  - belief networks
KW  - control engineering computing
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - Lyapunov methods
KW  - Markov processes
KW  - neural nets
KW  - probability
KW  - safety-critical software
KW  - stability
KW  - real-time performance
KW  - deep neural networks
KW  - model uncertainties
KW  - Bayesian model learning
KW  - adaptive control framework
KW  - control Lyapunov functions
KW  - control barrier functions
KW  - tractable Bayesian model
KW  - safety-critical high-speed Mars rover missions
KW  - Bayesian learning-based adaptive control
KW  - deep learning
KW  - safety-critical systems
KW  - high-speed terrestrial mobility
KW  - Safety
KW  - Adaptation models
KW  - Bayes methods
KW  - Stochastic processes
KW  - Uncertainty
KW  - Computational modeling
KW  - Switches
KW  - Robust/Adaptive Control of Robotic Systems
KW  - Robot Safety
KW  - Probability and Statistical Methods
KW  - Bayesian Adaptive Control
KW  - Deep Learning
KW  - Mars Rover
DO  - 10.1109/ICRA40945.2020.9196709
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning has enjoyed much recent success, and applying state-of-the-art model learning methods to controls is an exciting prospect. However, there is a strong reluctance to use these methods on safety-critical systems, which have constraints on safety, stability, and real-time performance. We propose a framework which satisfies these constraints while allowing the use of deep neural networks for learning model uncertainties. Central to our method is the use of Bayesian model learning, which provides an avenue for maintaining appropriate degrees of caution in the face of the unknown. In the proposed approach, we develop an adaptive control framework leveraging the theory of stochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control Barrier Functions) along with tractable Bayesian model learning via Gaussian Processes or Bayesian neural networks. Under reasonable assumptions, we guarantee stability and safety while adapting to unknown dynamics with probability 1. We demonstrate this architecture for high-speed terrestrial mobility targeting potential applications in safety-critical high-speed Mars rover missions.
ER  - 

TY  - CONF
TI  - Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4158
EP  - 4164
AU  - M. Ramezani
AU  - G. Tinchev
AU  - E. Iuganov
AU  - M. Fallon
PY  - 2020
KW  - feature extraction
KW  - graph theory
KW  - image matching
KW  - image registration
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - neural nets
KW  - optical radar
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - online LiDAR-SLAM
KW  - legged robot
KW  - robust registration
KW  - deep-learned loop closure
KW  - 3D factor-graph LiDAR-SLAM system
KW  - industrial environments
KW  - point clouds
KW  - inertial-kinematic state estimator
KW  - ICP registration
KW  - loop proposal mechanism
KW  - deep learning method
KW  - odometry
KW  - loop closure factors
KW  - pose graph optimization
KW  - SLAM map
KW  - risk alignment prediction method
KW  - deeply learned feature-based loop closure detector
KW  - Laser radar
KW  - Legged locomotion
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Iterative closest point algorithm
DO  - 10.1109/ICRA40945.2020.9196769
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.
ER  - 

TY  - CONF
TI  - Voxel Map for Visual SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4181
EP  - 4187
AU  - M. Muglikar
AU  - Z. Zhang
AU  - D. Scaramuzza
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image representation
KW  - image retrieval
KW  - SLAM (robots)
KW  - visual SLAM systems
KW  - camera field-of-view
KW  - voxel map representation
KW  - keyframe map
KW  - map points retrieval
KW  - simultaneous localization and mapping
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Cameras
KW  - Visualization
KW  - Cognition
KW  - Feature extraction
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197357
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multi-camera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.
ER  - 

TY  - CONF
TI  - Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4188
EP  - 4194
AU  - O. Mees
AU  - M. Merklinger
AU  - G. Kalweit
AU  - W. Burgard
PY  - 2020
KW  - interactive video
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - video signal processing
KW  - adversarial skill-transfer loss
KW  - task domain
KW  - learned skill embeddings
KW  - entropy-regularized adversarial skill-transfer loss
KW  - temporal video coherence
KW  - metric learning loss
KW  - adversarial loss
KW  - task context
KW  - unlabeled multiview videos
KW  - task-agnostic skill embedding space
KW  - reinforcement learning agents
KW  - unsupervised robot skill learning
KW  - adversarial skill networks
KW  - learned embedding
KW  - Task analysis
KW  - Entropy
KW  - Measurement
KW  - Training
KW  - Robots
KW  - Interpolation
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9196582
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.
ER  - 

TY  - CONF
TI  - Event-Based Angular Velocity Regression with Spiking Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4195
EP  - 4202
AU  - M. Gehrig
AU  - S. B. Shrestha
AU  - D. Mouritzen
AU  - D. Scaramuzza
PY  - 2020
KW  - angular velocity
KW  - image resolution
KW  - image sensors
KW  - neural nets
KW  - regression analysis
KW  - temporal regression problem
KW  - rotating event-camera
KW  - SNN
KW  - irregular event-based input
KW  - asynchronous event-based input
KW  - high-temporal resolution
KW  - synthetic event-camera
KW  - event-based angular velocity regression
KW  - spiking networks
KW  - temporal spikes
KW  - bio-inspired networks
KW  - brightness change
KW  - spiking neural networks
KW  - spike-based computational model
KW  - asynchronous sensors
KW  - artificial neural networks
KW  - 3-DOF angular velocity
KW  - Neurons
KW  - Biological neural networks
KW  - Angular velocity
KW  - Kernel
KW  - Task analysis
KW  - Computer architecture
KW  - Neuromorphics
DO  - 10.1109/ICRA40945.2020.9197133
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Spiking Neural Networks (SNNs) are bio-inspired networks that process information conveyed as temporal spikes rather than numeric values. An example of a sensor providing such data is the event-camera. It only produces an event when a pixel reports a significant brightness change. Similarly, the spiking neuron of an SNN only produces a spike whenever a significant number of spikes occur within a short period of time. Due to their spike-based computational model, SNNs can process output from event-based, asynchronous sensors without any pre-processing at extremely lower power unlike standard artificial neural networks. This is possible due to specialized neuromorphic hardware that implements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have not enjoyed the same rise of popularity as artificial neural networks. This not only stems from the fact that their input format is rather unconventional but also due to the challenges in training spiking networks. Despite their temporal nature and recent algorithmic advances, they have been mostly evaluated on classification problems. We propose, for the first time, a temporal regression problem of numerical values given events from an event-camera. We specifically investigate the prediction of the 3- DOF angular velocity of a rotating event-camera with an SNN. The difficulty of this problem arises from the prediction of angular velocities continuously in time directly from irregular, asynchronous event-based input. Directly utilising the output of event-cameras without any pre-processing ensures that we inherit all the benefits that they provide over conventional cameras. That is high-temporal resolution, high-dynamic range and no motion blur. To assess the performance of SNNs on this task, we introduce a synthetic event-camera dataset generated from real-world panoramic images and show that we can successfully train an SNN to perform angular velocity regression.
ER  - 

TY  - CONF
TI  - Visual Odometry Revisited: What Should Be Learnt?
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4203
EP  - 4210
AU  - H. Zhan
AU  - C. S. Weerasekera
AU  - J. -W. Bian
AU  - I. Reid
PY  - 2020
KW  - convolutional neural nets
KW  - distance measurement
KW  - geometry
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - SLAM (robots)
KW  - monocular visual odometry algorithm
KW  - geometry-based methods
KW  - monocular systems
KW  - scale-drift issue
KW  - deep learning
KW  - epipolar geometry
KW  - perspective-n-point method
KW  - frame-to-frame VO algorithm
KW  - DF-VO
KW  - scale consistent single-view depth CNN
KW  - Cameras
KW  - Adaptive optics
KW  - Geometry
KW  - Optical imaging
KW  - Machine learning
KW  - Estimation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197374
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue. Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system. Code is available at here: DF-VO.
ER  - 

TY  - CONF
TI  - 3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4211
EP  - 4217
AU  - M. Tian
AU  - Q. Nie
AU  - H. Shen
PY  - 2020
KW  - cameras
KW  - convolutional neural nets
KW  - feature extraction
KW  - image classification
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - pose estimation
KW  - 3D scene geometry-aware constraint
KW  - camera localization
KW  - deep learning
KW  - fundamental component
KW  - autonomous driving vehicles
KW  - path planning
KW  - motion control
KW  - end-to-end approaches
KW  - convolutional neural network
KW  - 3D-geometry based traditional methods
KW  - compact network
KW  - absolute camera
KW  - image contents
KW  - image-level structural similarity loss
KW  - challenging scenes
KW  - Cameras
KW  - Three-dimensional displays
KW  - Training
KW  - Geometry
KW  - Robot vision systems
KW  - Transforms
DO  - 10.1109/ICRA40945.2020.9196940
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Camera localization is a fundamental and key component of autonomous driving vehicles and mobile robots to localize themselves globally for further environment perception, path planning and motion control. Recently end-to-end approaches based on convolutional neural network have been much studied to achieve or even exceed 3D-geometry based traditional methods. In this work, we propose a compact network for absolute camera pose regression. Inspired from those traditional methods, a 3D scene geometry-aware constraint is also introduced by exploiting all available information including motion, depth and image contents. We add this constraint as a regularization term to our proposed network by defining a pixel-level photometric loss and an image-level structural similarity loss. To benchmark our method, different challenging scenes including indoor and outdoor environment are tested with our proposed approach and state-of-the-arts. And the experimental results demonstrate significant performance improvement of our method on both prediction accuracy and convergence efficiency.
ER  - 

TY  - CONF
TI  - ACDER: Augmented Curiosity-Driven Experience Replay
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4218
EP  - 4224
AU  - B. Li
AU  - T. Lu
AU  - J. Li
AU  - N. Lu
AU  - Y. Cai
AU  - S. Wang
PY  - 2020
KW  - augmented reality
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - manipulators
KW  - augmented curiosity-driven experience replay
KW  - hindsight experience replay
KW  - sample-efficiency
KW  - automatic exploratory curriculum
KW  - dynamic initial states selection
KW  - task-relevant states
KW  - goal-oriented curiosity-driven exploration
KW  - action space
KW  - high dimensional continuous state
KW  - low exploration efficiency
KW  - RL agent
KW  - reinforcement learning
KW  - sparse feed-back
KW  - ACDER
KW  - multistep robotic task learning
KW  - basic tasks
KW  - challenging robotic manipulation tasks
KW  - valuable states
KW  - Task analysis
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Training
KW  - Incentive schemes
KW  - Buffer storage
KW  - Games
DO  - 10.1109/ICRA40945.2020.9197421
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration in environments with sparse feed-back remains a challenging research problem in reinforcement learning (RL). When the RL agent explores the environment randomly, it results in low exploration efficiency, especially in robotic manipulation tasks with high dimensional continuous state and action space. In this paper, we propose a novel method, called Augmented Curiosity-Driven Experience Replay (ACDER), which leverages (i) a new goal-oriented curiosity-driven exploration to encourage the agent to pursue novel and task-relevant states more purposefully and (ii) the dynamic initial states selection as an automatic exploratory curriculum to further improve the sample-efficiency. Our approach complements Hindsight Experience Replay (HER) by introducing a new way to pursue valuable states. Experiments conducted on four challenging robotic manipulation tasks with binary rewards, including Reach, Push, Pick&Place and Multi-step Push. The empirical results show that our proposed method significantly outperforms existing methods in the first three basic tasks and also achieves satisfactory performance in multi-step robotic task learning.
ER  - 

TY  - CONF
TI  - TrueRMA: Learning Fast and Smooth Robot Trajectories with Recursive Midpoint Adaptations in Cartesian Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4225
EP  - 4231
AU  - J. C. Kiemel
AU  - P. Mei√üner
AU  - T. Kr√∂ger
PY  - 2020
KW  - learning (artificial intelligence)
KW  - motion control
KW  - neurocontrollers
KW  - robot kinematics
KW  - time optimal control
KW  - trajectory control
KW  - TrueRMA
KW  - robot trajectory learning
KW  - recursive midpoint adaptations
KW  - Cartesian space
KW  - differentiable path
KW  - inverse kinematics
KW  - time optimal parameterization
KW  - reinforcement learning
KW  - robot movement
KW  - neural network training
KW  - KUKA iiwa robot
KW  - Trajectory
KW  - Robots
KW  - Kinematics
KW  - Task analysis
KW  - Training
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196711
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present TrueRMA, a data-efficient, model-free method to learn cost-optimized robot trajectories over a wide range of starting points and endpoints. The key idea is to calculate trajectory waypoints in Cartesian space by recursively predicting orthogonal adaptations relative to the midpoints of straight lines. We generate a differentiable path by adding circular blends around the waypoints, calculate the corresponding joint positions with an inverse kinematics solver and calculate a time-optimal parameterization considering velocity and acceleration limits. During training, the trajectory is executed in a physics simulator and costs are assigned according to a user-specified cost function which is not required to be differentiable. Given a starting point and an endpoint as input, a neural network is trained to predict midpoint adaptations that minimize the cost of the resulting trajectory via reinforcement learning. We successfully train a KUKA iiwa robot to keep a ball on a plate while moving between specified points and compare the performance of TrueRMA against two baselines. The results show that our method requires less training data to learn the task while generating shorter and faster trajectories.
ER  - 

TY  - CONF
TI  - Fog Robotics Algorithms for Distributed Motion Planning Using Lambda Serverless Computing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4232
EP  - 4238
AU  - J. Ichnowski
AU  - W. Lee
AU  - V. Murta
AU  - S. Paradis
AU  - R. Alterovitz
AU  - J. E. Gonzalez
AU  - I. Stoica
AU  - K. Goldberg
PY  - 2020
KW  - computational complexity
KW  - control engineering computing
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - parallel processing
KW  - path planning
KW  - distributed motion planning
KW  - Lambda serverless computing
KW  - motion planning algorithms
KW  - RRT
KW  - computational load
KW  - local environment changes
KW  - cloud-based serverless lambda computing
KW  - parallel computation
KW  - serverless computers
KW  - learned parallel allocation
KW  - Amazon Lambda
KW  - sporadically computationally intensive motion planning tasks
KW  - fog robotics algorithms
KW  - Planning
KW  - Parallel processing
KW  - FAA
KW  - Robot kinematics
KW  - Cloud computing
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196651
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For robots using motion planning algorithms such as RRT and RRT*, the computational load can vary by orders of magnitude as the complexity of the local environment changes. To adaptively provide such computation, we propose Fog Robotics algorithms in which cloud-based serverless lambda computing provides parallel computation on demand. To use this parallelism, we propose novel motion planning algorithms that scale effectively with an increasing number of serverless computers. However, given that the allocation of computing is typically bounded by both monetary and time constraints, we show how prior learning can be used to efficiently allocate resources at runtime. We demonstrate the algorithms and application of learned parallel allocation in both simulation and with the Fetch commercial mobile manipulator using Amazon Lambda to complete a sequence of sporadically computationally intensive motion planning tasks.
ER  - 

TY  - CONF
TI  - Exploration of 3D terrains using potential fields with elevation-based local distortions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4239
EP  - 4244
AU  - R. Maffei
AU  - M. P. Souza
AU  - M. Mantelli
AU  - D. Pittol
AU  - M. Kolberg
AU  - V. A. M. Jorge
PY  - 2020
KW  - mobile robots
KW  - optical radar
KW  - radar receivers
KW  - stereo image processing
KW  - terrain mapping
KW  - exploration approach
KW  - potential fields
KW  - uneven terrains
KW  - high declivity regions
KW  - ground robot
KW  - elevation-based local distortions
KW  - mobile robots
KW  - numerous outdoor tasks
KW  - military applications
KW  - 3D terrain exploration
KW  - patrolling application
KW  - delivery application
KW  - 2D LIDAR sensors
KW  - Distortion
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Boundary conditions
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197577
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mobile robots can be used in numerous outdoor tasks such as patrolling, delivery and military applications. In order to deploy mobile robots in this kind of environment, where there are different challenges like slopes, elevations, or even holes, they should be able to detect such challenges and determine the best path to accomplish their tasks. In this paper, we are proposing an exploration approach based on potential fields with local distortions, in which we define preferences in uneven terrains to avoid high declivity regions without compromising the best path. The approach was implemented and tested in simulated environments, considering a ground robot embedded with two 2D LIDAR sensors, and the experiments demonstrated the efficiency of our method.
ER  - 

TY  - CONF
TI  - R3T: Rapidly-exploring Random Reachable Set Tree for Optimal Kinodynamic Planning of Nonlinear Hybrid Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4245
EP  - 4251
AU  - A. Wu
AU  - S. Sadraddini
AU  - R. Tedrake
PY  - 2020
KW  - control engineering computing
KW  - formal verification
KW  - nonlinear control systems
KW  - reachability analysis
KW  - robot dynamics
KW  - sampling methods
KW  - trees (mathematics)
KW  - R3T
KW  - random reachable set tree
KW  - optimal kinodynamic planning
KW  - nonlinear hybrid systems
KW  - reachability-based variant
KW  - rapidly-exploring random tree
KW  - multiple polytopes
KW  - reachability analysis
KW  - nonlinear systems
KW  - contact-rich robotic systems
KW  - formal verification
KW  - Planning
KW  - Approximation algorithms
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Measurement
KW  - Robots
KW  - Silicon
DO  - 10.1109/ICRA40945.2020.9196802
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce R3T, a reachability-based variant of the rapidly-exploring random tree (RRT) algorithm that is suitable for (optimal) kinodynamic planning in nonlinear and hybrid systems. We developed tools to approximate reachable sets using polytopes and perform sampling-based planning with them. This method has a unique advantage in hybrid systems: different dynamic modes in the reachable set can be explicitly represented using multiple polytopes. We prove that under mild assumptions, R3T is probabilistically complete in kinodynamic systems, and asymptotically optimal through rewiring. Moreover, R3T provides a formal verification method for reachability analysis of nonlinear systems. The advantages of R3T are demonstrated with case studies on nonlinear, hybrid, and contact-rich robotic systems.
ER  - 

TY  - CONF
TI  - DeepSemanticHPPC: Hypothesis-based Planning over Uncertain Semantic Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4252
EP  - 4258
AU  - Y. Han
AU  - H. Lin
AU  - J. Banfi
AU  - K. Bala
AU  - M. Campbell
PY  - 2020
KW  - belief networks
KW  - computational geometry
KW  - image reconstruction
KW  - mobile robots
KW  - neural nets
KW  - path planning
KW  - robot vision
KW  - DeepSemanticHPPC
KW  - hypothesis-based planning
KW  - uncertain semantic point clouds
KW  - deep Bayesian neural network
KW  - flexible point cloud scene representation
KW  - sparse visual measurements
KW  - hypothesis-based path planner
KW  - uncertainty-aware hypothesis-based planner
KW  - Uncertainty
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Semantics
KW  - Robots
KW  - Planning
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196828
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning in unstructured environments is challenging - it relies on sensing, perception, scene reconstruction, and reasoning about various uncertainties. We propose DeepSemanticHPPC, a novel uncertainty-aware hypothesis-based planner for unstructured environments. Our algorithmic pipeline consists of: a deep Bayesian neural network which segments surfaces with uncertainty estimates; a flexible point cloud scene representation; a next-best-view planner which minimizes the uncertainty of scene semantics using sparse visual measurements; and a hypothesis-based path planner that proposes multiple kinematically feasible paths with evolving safety confidences given next-best-view measurements. Our pipeline iteratively decreases semantic uncertainty along planned paths, filtering out unsafe paths with high confidence. We show that our framework plans safe paths in real-world environments where existing path planners typically fail.
ER  - 

TY  - CONF
TI  - Balancing Actuation and Computing Energy in Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4259
EP  - 4265
AU  - S. Sudhakar
AU  - S. Karaman
AU  - V. Sze
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - balancing actuation
KW  - motion planning problems
KW  - low-energy robotic vehicles
KW  - high-endurance autonomous blimps
KW  - computing hardware
KW  - actuation hardware
KW  - CEIMP
KW  - anytime planning algorithm
KW  - actuation energy
KW  - asymptotic computational complexity
KW  - sampling-based motion planning algorithms
KW  - compute energy included motion planning algorithm
KW  - Planning
KW  - Robots
KW  - Meters
KW  - Task analysis
KW  - Hardware
KW  - Buildings
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197164
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study a novel class of motion planning problems, inspired by emerging low-energy robotic vehicles, such as insect-size flyers, chip-size satellites, and high-endurance autonomous blimps, for which the energy consumed by computing hardware during planning a path can be as large as the energy consumed by actuation hardware during the execution of the same path. We propose a new algorithm, called Compute Energy Included Motion Planning (CEIMP). CEIMP operates similarly to any other anytime planning algorithm, except it stops when it estimates further computing will require more computing energy than potential savings in actuation energy. We show that CEIMP has the same asymptotic computational complexity as existing sampling-based motion planning algorithms, such as PRM*. We also show that CEIMP outperforms the average baseline of using maximum computing resources in realistic computational experiments involving 10 floor plans from MIT buildings. In one representative experiment, CEIMP outperforms the average baseline 90.6% of the time when energy to compute one more second is equal to the energy to move one more meter, and 99.7% of the time when energy to compute one more second is equal to or greater than the energy to move 3 more meters.
ER  - 

TY  - CONF
TI  - Posterior Sampling for Anytime Motion Planning on Graphs with Expensive-to-Evaluate Edges
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4266
EP  - 4272
AU  - B. Hou
AU  - S. Choudhury
AU  - G. Lee
AU  - A. Mandalika
AU  - S. S. Srinivasa
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - minimisation
KW  - mobile robots
KW  - 7D planning problems
KW  - posterior sampling
KW  - anytime motion planning
KW  - collision checking
KW  - computational bottleneck
KW  - lazy algorithms
KW  - collision uncertainty
KW  - shortest path
KW  - real-time applications
KW  - anytime performance
KW  - anytime lazy motion planning algorithm
KW  - edge collisions
KW  - initial feasible path
KW  - shorter paths
KW  - Planning
KW  - Bayes methods
KW  - History
KW  - Uncertainty
KW  - Geometry
KW  - Learning (artificial intelligence)
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9197014
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collision checking is a computational bottleneck in motion planning, requiring lazy algorithms that explicitly reason about when to perform this computation. Optimism in the face of collision uncertainty minimizes the number of checks before finding the shortest path. However, this may take a prohibitively long time to compute, with no other feasible paths discovered during this period. For many real-time applications, we instead demand strong anytime performance, defined as minimizing the cumulative lengths of the feasible paths yielded over time. We introduce Posterior Sampling for Motion Planning (PSMP), an anytime lazy motion planning algorithm that leverages learned posteriors on edge collisions to quickly discover an initial feasible path and progressively yield shorter paths. PSMP obtains an expected regret bound of √ï(‚àö(SAT)) and outperforms comparative baselines on a set of 2D and 7D planning problems.
ER  - 

TY  - CONF
TI  - Upset Recovery Control for Quadrotors Subjected to a Complete Rotor Failure from Large Initial Disturbances
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4273
EP  - 4279
AU  - S. Sun
AU  - M. Baert
AU  - B. S. van Schijndel
AU  - C. de Visser
PY  - 2020
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - cascade control
KW  - control system synthesis
KW  - helicopters
KW  - Monte Carlo methods
KW  - quadratic programming
KW  - upset recovery control
KW  - fault-tolerant controller
KW  - arbitrary initial orientations
KW  - angular velocities
KW  - control method
KW  - post-failure quadrotor
KW  - Monte-Carlo simulation
KW  - control allocator
KW  - quadratic programming
KW  - control allocation method
KW  - almost-global convergence attitude controller
KW  - Rotors
KW  - Attitude control
KW  - Angular velocity
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Resource management
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9197239
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study has developed a fault-tolerant controller that is able to recover a quadrotor from arbitrary initial orientations and angular velocities, despite the complete failure of a rotor. This cascaded control method includes a position/altitude controller, an almost-global convergence attitude controller, and a control allocation method based on quadratic programming. As a major novelty, a constraint of undesirable angular velocity is derived and fused into the control allocator, which significantly improves the recovery performance. For validation, we have conducted a set of Monte-Carlo simulation to test the reliability of the proposed method of recovering the quadrotor from arbitrary initial attitude/rate conditions. In addition, real-life flight tests have been performed. The results demonstrate that the post-failure quadrotor can recover after being casually tossed into the air.
ER  - 

TY  - CONF
TI  - Identification and evaluation of a force model for multirotor UAVs*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4280
EP  - 4286
AU  - A. Letalenet
AU  - P. Morin
PY  - 2020
KW  - aerodynamics
KW  - autonomous aerial vehicles
KW  - blades
KW  - momentum
KW  - propellers
KW  - rotors (mechanical)
KW  - vehicle dynamics
KW  - force model
KW  - multirotor UAV
KW  - model identification method
KW  - propellers
KW  - blade element theories
KW  - aerodynamics
KW  - momentum theory
KW  - actuation dynamics
KW  - Propellers
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Predictive models
KW  - Blades
KW  - Acceleration
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197317
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a model identification method and evaluation of a force model for multirotor UAVs. The model incorporates propellers' aerodynamics derived from momentum and blade element theories, as well as aerodynamics of the UAV's structure and actuation dynamics. A two-steps identification approach of the model parameters is proposed. The model is identified and evaluated from outdoor experiments with flight speeds exceeding 10m/s.
ER  - 

TY  - CONF
TI  - Preliminary Study of an Aerial Manipulator with Elastic Suspension
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4287
EP  - 4293
AU  - A. Yiƒüit
AU  - G. Grappe
AU  - L. Cuvillon
AU  - S. Durand
AU  - J. Gangloff
PY  - 2020
KW  - actuators
KW  - aerospace robotics
KW  - feedback
KW  - manipulator dynamics
KW  - propellers
KW  - vibration control
KW  - elastic suspension
KW  - aerial manipulator
KW  - contra-rotating propellers
KW  - computed torque control strategy
KW  - active vibration canceling
KW  - feedback linearization control strategy
KW  - robotic carrier
KW  - Springs
KW  - Manipulators
KW  - Propellers
KW  - Task analysis
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9196942
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a preliminary study of an Aerial Manipulator suspended by a spring to a robotic carrier. The suspended aerial manipulator is actuated by six pairs of contra-rotating propellers generating a 6-DoF wrench. Simulations show path following results using a computed torque (feedback linearization) control strategy. Active vibration canceling is validated experimentally on a first prototype.
ER  - 

TY  - CONF
TI  - Towards Low-Latency High-Bandwidth Control of Quadrotors using Event Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4294
EP  - 4300
AU  - R. S. Dimitrova
AU  - M. Gehrig
AU  - D. Brescianini
AU  - D. Scaramuzza
PY  - 2020
KW  - aircraft control
KW  - angular velocity control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - cameras
KW  - closed loop systems
KW  - feedback
KW  - helicopters
KW  - Hough transforms
KW  - image resolution
KW  - image sensors
KW  - Kalman filters
KW  - PD control
KW  - robot vision
KW  - state estimation
KW  - attitude tracking
KW  - state estimator
KW  - rotor thrusts
KW  - black-and-white disk
KW  - angular velocity
KW  - roll angle
KW  - Kalman filter
KW  - Hough transform
KW  - dualcopter platform
KW  - one-dimensional attitude tracking
KW  - drones
KW  - quadrotors
KW  - low-latency high-bandwidth control
KW  - event-based feedback
KW  - event-camera-driven closed loop control
KW  - proportional-derivative attitude control law
KW  - event-based state estimation
KW  - temporal resolution
KW  - sensor latency
KW  - high speed vision-based control
KW  - frequency 1.0 kHz
KW  - time 12.0 ms
KW  - Cameras
KW  - Robot vision systems
KW  - Transforms
KW  - Angular velocity
KW  - State estimation
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9197530
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Event cameras are a promising candidate to enable high speed vision-based control due to their low sensor latency and high temporal resolution. However, purely event-based feedback has yet to be used in the control of drones. In this work, a first step towards implementing low-latency high-bandwidth control of quadrotors using event cameras is taken. In particular, this paper addresses the problem of one-dimensional attitude tracking using a dualcopter platform equipped with an event camera. The event-based state estimation consists of a modified Hough transform algorithm combined with a Kalman filter that outputs the roll angle and angular velocity of the dualcopter relative to a horizon marked by a black-and-white disk. The estimated state is processed by a proportional-derivative attitude control law that computes the rotor thrusts required to track the desired attitude. The proposed attitude tracking scheme shows promising results of event-camera-driven closed loop control: the state estimator performs with an update rate of 1 kHz and a latency determined to be 12 ms, enabling attitude tracking at speeds of over 1600¬∞/s.
ER  - 

TY  - CONF
TI  - Perception-constrained and Motor-level Nonlinear MPC for both Underactuated and Tilted-propeller UAVS
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4301
EP  - 4306
AU  - M. Jacquet
AU  - G. Corsini
AU  - D. Bicego
AU  - A. Franchi
PY  - 2020
KW  - actuators
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - motion control
KW  - nonlinear control systems
KW  - predictive control
KW  - propellers
KW  - rotors (mechanical)
KW  - vehicle dynamics
KW  - motor-level Nonlinear MPC
KW  - tilted-propeller
KW  - Perception-constrained Nonlinear Model Predictive Control framework
KW  - real-time control
KW  - multirotor aerial vehicles
KW  - perceptive sensor
KW  - realistic actuator limitations
KW  - rotor minimum
KW  - maximum speeds
KW  - multirotor platforms
KW  - underactuated quadrotors
KW  - tilted-propellers hexarotors
KW  - motor-torque level
KW  - Propellers
KW  - Task analysis
KW  - Robot sensing systems
KW  - Real-time systems
KW  - Actuators
KW  - Vehicle dynamics
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197281
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a Perception-constrained Nonlinear Model Predictive Control (NMPC) framework for the real-time control of multi-rotor aerial vehicles. Our formulation considers both constraints from a perceptive sensor and realistic actuator limitations that are the rotor minimum and maximum speeds and accelerations. The formulation is meant to be generic and considers a large range of multi-rotor platforms (such as underactuated quadrotors or tilted-propellers hexarotors) since it does not rely on differential flatness for the dynamical equations, and a broad range of sensors, such as cameras, lidars, etc.... The perceptive constraints are expressed to maintain visibility of a feature point in the sensor's field of view, while performing a reference maneuver. We demonstrate both in simulation and real experiments that our framework is able to exploit the full capabilities of the multi-rotor, to achieve the motion under the aforementioned constraints, and control in real-time the platform at a motor-torque level, avoiding the use of an intermediate unconstrained trajectory tracker.
ER  - 

TY  - CONF
TI  - Coordinate-Free Dynamics and Differential Flatness of a Class of 6DOF Aerial Manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4307
EP  - 4313
AU  - J. Welde
AU  - V. Kumar
PY  - 2020
KW  - control system synthesis
KW  - end effectors
KW  - manipulator dynamics
KW  - manipulator kinematics
KW  - path planning
KW  - position control
KW  - simulated aerial videography task
KW  - differential flatness
KW  - 6DOF aerial manipulators
KW  - coordinate-free formulation
KW  - coupled dynamics
KW  - 2DOF articulated manipulator
KW  - end effector frame
KW  - Manipulator dynamics
KW  - End effectors
KW  - Trajectory
KW  - Task analysis
KW  - Planning
KW  - Vehicle dynamics
DO  - 10.1109/ICRA40945.2020.9196705
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we derive a coordinate-free formulation of the coupled dynamics of a class of 6DOF aerial manipulators consisting of an underactuated quadrotor equipped with a 2DOF articulated manipulator, and demonstrate that the system is differentially flat with respect to the end effector pose. In particular, we require the center of mass of the entire system to be fixed in the end effector frame, suggesting a reasonable mechanical design criterion. We make use of an inertial decoupling transformation to demonstrate differential flatness, allowing us to plan dynamically feasible trajectories for the system in the space of the 6DOF pose of the end effector, which is ideal for achieving precise manipulator tasks. Simulation results validate the flatness-based planning methodology for our dynamic model, and its usefulness is demonstrated in a simulated aerial videography task.
ER  - 

TY  - CONF
TI  - CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4314
EP  - 4321
AU  - W. Ding
AU  - M. Xu
AU  - D. Zhao
PY  - 2020
KW  - Bayes methods
KW  - data analysis
KW  - mobile robots
KW  - road safety
KW  - road vehicles
KW  - safety-critical software
KW  - trajectory control
KW  - CMTS
KW  - conditional multiple trajectory synthesizer
KW  - safety-critical driving scenarios
KW  - naturalistic driving trajectory generation
KW  - autonomous driving algorithms
KW  - collision-free scenarios
KW  - safety-critical cases
KW  - near-miss scenarios
KW  - off-the-shelf datasets
KW  - generative model
KW  - conditional probability
KW  - trajectory predictions
KW  - autonomous vehicle safety
KW  - safety-critical data synthesizing framework
KW  - variational Bayesian methods
KW  - Trajectory
KW  - Interpolation
KW  - Roads
KW  - Training
KW  - Aerospace electronics
KW  - Data models
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9197145
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Naturalistic driving trajectory generation is crucial for the development of autonomous driving algorithms. However, most of the data is collected in collision-free scenarios leading to the sparsity of the safety-critical cases. When considering safety, testing algorithms in near-miss scenarios that rarely show up in off-the-shelf datasets and are costly to accumulate is a vital part of the evaluation. As a remedy, we propose a safety-critical data synthesizing framework based on variational Bayesian methods and term it as Conditional Multiple Trajectory Synthesizer (CMTS). We extend a generative model to connect safe and collision driving data by representing their distribution in the latent space and use conditional probability to adapt to different maps. Sampling from the mixed distribution enables us to synthesize the safety-critical data not shown in the safe or collision datasets. Experimental results demonstrate that the generated dataset covers many different realistic scenarios, especially the near-misses. We conclude that the use of data generated by CMTS can improve the accuracy of trajectory predictions and autonomous vehicle safety.
ER  - 

TY  - CONF
TI  - LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4322
EP  - 4328
AU  - W. Ding
AU  - S. Hou
AU  - H. Gao
AU  - G. Wan
AU  - S. Song
PY  - 2020
KW  - distance measurement
KW  - graph theory
KW  - inertial systems
KW  - maximum likelihood estimation
KW  - motion estimation
KW  - optical radar
KW  - localization estimation
KW  - inertial LiDAR intensity
KW  - matching estimation
KW  - LiDAR localization system
KW  - environmental change detection method
KW  - kinematic estimation
KW  - frame-to-frame motion estimation
KW  - multiresolution occupancy grid based LiDAR inertial odometry
KW  - pose graph fusion framework
KW  - Apollo-SouthBay dataset
KW  - MAP estimation problem
KW  - Laser radar
KW  - Estimation
KW  - Robustness
KW  - Roads
KW  - Windows
KW  - Optimization
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9196698
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Environmental fluctuations pose crucial challenges to a localization system in autonomous driving. We present a robust LiDAR localization system that maintains its kinematic estimation in changing urban scenarios by using a dead reckoning solution implemented through a LiDAR inertial odometry. Our localization framework jointly uses information from complementary modalities such as global matching and LiDAR inertial odometry to achieve accurate and smooth localization estimation. To improve the performance of the LiDAR odometry, we incorporate inertial and LiDAR intensity cues into an occupancy grid based LiDAR odometry to enhance frame-to-frame motion and matching estimation. Multi-resolution occupancy grid is implemented yielding a coarse-to-fine approach to balance the odometry's precision and computational requirement. To fuse both the odometry and global matching results, we formulate a MAP estimation problem in a pose graph fusion framework that can be efficiently solved. An effective environmental change detection method is proposed that allows us to know exactly when and what portion of the map requires an update. We comprehensively validate the effectiveness of the proposed approaches using both the Apollo-SouthBay dataset and our internal dataset. The results confirm that our efforts lead to a more robust and accurate localization system, especially in dynamically changing urban scenarios.
ER  - 

TY  - CONF
TI  - Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning in Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4329
EP  - 4335
AU  - M. Huegle
AU  - G. Kalweit
AU  - M. Werling
AU  - J. Boedecker
PY  - 2020
KW  - convolutional neural nets
KW  - decision making
KW  - graph theory
KW  - image representation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - traffic engineering computing
KW  - DeepScene-Q off-policy reinforcement learning algorithms
KW  - graph-Q
KW  - graph convolutional networks
KW  - multiple variable-length sequences
KW  - novel deep scene architecture
KW  - complex interaction-aware scene representations
KW  - traffic participants
KW  - traffic signs
KW  - object types
KW  - high-level decision making
KW  - deep reinforcement learning
KW  - high-level decision component
KW  - perception component
KW  - autonomous driving systems
KW  - dynamic interaction-aware scene understanding
KW  - traffic simulator SUMO
KW  - Computer architecture
KW  - Autonomous vehicles
KW  - Lenses
KW  - Learning (artificial intelligence)
KW  - Decision making
KW  - Predictive models
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197086
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-ofthe-art methods in evaluations with the publicly available traffic simulator SUMO.
ER  - 

TY  - CONF
TI  - Interacting Vehicle Trajectory Prediction with Convolutional Recurrent Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4336
EP  - 4342
AU  - S. Mukherjee
AU  - S. Wang
AU  - A. Wallace
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - feedback
KW  - learning (artificial intelligence)
KW  - path planning
KW  - recurrent neural nets
KW  - traffic engineering computing
KW  - car
KW  - convolutional recurrent neural networks
KW  - convolutional long short term memory
KW  - Conv-LSTM
KW  - interacting vehicle trajectory prediction
KW  - novel feedback scheme
KW  - path planning
KW  - interaction learning
KW  - temporal learning
KW  - motion learning
KW  - Automobiles
KW  - Trajectory
KW  - Feature extraction
KW  - Hidden Markov models
KW  - Predictive models
KW  - Computer architecture
KW  - Road transportation
DO  - 10.1109/ICRA40945.2020.9196807
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Anticipating the future trajectories of surrounding vehicles is a crucial and challenging task in path planning for autonomy. We propose a novel Convolutional Long Short Term Memory (Conv-LSTM) based neural network architecture to predict the future positions of cars using several seconds of historical driving observations. This consists of three modules: 1) Interaction Learning to capture the effect of surrounding cars, 2) Temporal Learning to identify the dependency on past movements and 3) Motion Learning to convert the extracted features from these two modules into future positions. To continuously achieve accurate prediction, we introduce a novel feedback scheme where the current predicted positions of each car are leveraged to update future motion, encapsulating the effect of the surrounding cars. Experiments on two public datasets demonstrate that the proposed methodology can match or outperform the state-of-the-art methods for long-term trajectory prediction.
ER  - 

TY  - CONF
TI  - Navigation Command Matching for Vision-based Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4343
EP  - 4349
AU  - Y. Pan
AU  - J. Xue
AU  - P. Zhang
AU  - W. Ouyang
AU  - J. Fang
AU  - X. Chen
PY  - 2020
KW  - control engineering computing
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - navigation
KW  - path planning
KW  - road traffic control
KW  - robot vision
KW  - robust control
KW  - traffic engineering computing
KW  - suboptimal policy
KW  - CARLA driving benchmark
KW  - vision-based autonomous driving
KW  - imitative reinforcement learning
KW  - robust driving policy
KW  - nonsmooth rewards
KW  - state-action pairs
KW  - smooth rewards
KW  - matching measurer
KW  - navigation rewards
KW  - navigation command matching
KW  - attention-guided agent
KW  - salient regions
KW  - RGB images
KW  - Navigation
KW  - Task analysis
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Autonomous vehicles
KW  - Smoothing methods
KW  - Current measurement
DO  - 10.1109/ICRA40945.2020.9196609
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning an optimal policy for autonomous driving task to confront with complex environment is a long- studied challenge. Imitative reinforcement learning is accepted as a promising approach to learn a robust driving policy through expert demonstrations and interactions with environments. However, this model utilizes non-smooth rewards, which have a negative impact on matching between navigation commands and trajectory (state-action pairs), and degrade the generalizability of an agent. Smooth rewards are crucial to discriminate actions generated from sub-optimal policy. In this paper, we propose a navigation command matching (NCM) model to address this issue. There are two key components in NCM, 1) a matching measurer produces smooth navigation rewards that measure matching between navigation commands and trajectory; 2) attention-guided agent performs actions given states where salient regions in RGB images (i.e. roadsides, lane markings and dynamic obstacles) are highlighted to amplify their influence on the final model. We obtain navigation rewards and store transitions to replay buffer after an episode, so NCM is able to discriminate actions generated from suboptimal policy. Experiments on CARLA driving benchmark show our proposed NCM outperforms previous state-of-the- art models on various tasks in terms of the percentage of successfully completed episodes. Moreover, our model improves generalizability of the agent and obtains good performance even in unseen scenarios.
ER  - 

TY  - CONF
TI  - GraphRQI: Classifying Driver Behaviors Using Graph Spectrums
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4350
EP  - 4357
AU  - R. Chandra
AU  - U. Bhattacharya
AU  - T. Mittal
AU  - X. Li
AU  - A. Bera
AU  - D. Manocha
PY  - 2020
KW  - computational complexity
KW  - driver information systems
KW  - eigenvalues and eigenfunctions
KW  - graph theory
KW  - multi-agent systems
KW  - pattern classification
KW  - supervised learning
KW  - GraphRQI
KW  - graph spectrums
KW  - road-agent trajectories
KW  - driving traits
KW  - aggressive driving
KW  - conservative driving
KW  - nearby road-agents
KW  - interagent interactions
KW  - unweighted traffic graphs
KW  - undirected traffic graphs
KW  - supervised learning algorithm
KW  - traffic graph
KW  - eigenvalue algorithm
KW  - autonomous driving datasets
KW  - prior driver behavior classification algorithms
KW  - Vehicles
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Eigenvalues and eigenfunctions
KW  - Laplace equations
KW  - Classification algorithms
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9196751
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel algorithm (GraphRQI) to identify driver behaviors from road-agent trajectories. Our approach assumes that the road-agents exhibit a range of driving traits, such as aggressive or conservative driving. Moreover, these traits affect the trajectories of nearby road-agents as well as the interactions between road-agents. We represent these inter-agent interactions using unweighted and undirected traffic graphs. Our algorithm classifies the driver behavior using a supervised learning algorithm by reducing the computation to the spectral analysis of the traffic graph. Moreover, we present a novel eigenvalue algorithm to compute the spectrum efficiently. We provide theoretical guarantees for the running time complexity of our eigenvalue algorithm and show that it is faster than previous methods by 2 times. We evaluate the classification accuracy of our approach on traffic videos and autonomous driving datasets corresponding to urban traffic. In practice, GraphRQI achieves an accuracy improvement of up to 25% over prior driver behavior classification algorithms. We also use our classification algorithm to predict the future trajectories of road-agents.
ER  - 

TY  - CONF
TI  - Kidnapped Radar: Topological Radar Localisation using Rotationally-Invariant Metric Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4358
EP  - 4364
AU  - ≈û. SƒÉftescu
AU  - M. Gadd
AU  - D. De Martini
AU  - D. Barnes
AU  - P. Newman
PY  - 2020
KW  - convolutional neural nets
KW  - CW radar
KW  - feature extraction
KW  - FM radar
KW  - learning (artificial intelligence)
KW  - nearest neighbour methods
KW  - radar imaging
KW  - radar tracking
KW  - polar nature
KW  - radar scan formation
KW  - cylindrical convolutions
KW  - anti-aliasing blurring
KW  - azimuth-wise max-pooling
KW  - rotational invariance
KW  - enforced metric space
KW  - topological localisation system
KW  - random rotational perturbation
KW  - kidnapped radar
KW  - topological radar localisation
KW  - rotationally-invariant metric learning
KW  - large-scale topological localisation
KW  - frequency-modulated continuous-wave scanning radar
KW  - efficient learning-based approach
KW  - radar data
KW  - polar radar scans
KW  - NetVLAD architectures
KW  - visual domain
KW  - feature extraction
KW  - radar-focused mobile autonomy dataset
KW  - CNN architectures
KW  - reference trajectory
KW  - nearest neighbour
KW  - place recognition
KW  - root architecture
KW  - convolutional neural network
KW  - distance 280.0 km
KW  - Measurement
KW  - Radar imaging
KW  - Robot sensing systems
KW  - Azimuth
KW  - Feature extraction
KW  - Trajectory
KW  - radar
KW  - localisation
KW  - place recognition
KW  - deep learning
KW  - metric learning
DO  - 10.1109/ICRA40945.2020.9196682
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a system for robust, large-scale topological localisation using Frequency-Modulated Continuous-Wave scanning radar which extends the state-of-the-art by an efficient, learning-based approach to handle radar data for localisation. We learn a metric space for embedding polar radar scans using CNN and NetVLAD architectures traditionally applied to the visual domain. However, we tailor the feature extraction for more suitability to the polar nature of radar scan formation using cylindrical convolutions, anti-aliasing blurring, and azimuth-wise max-pooling; all in order to bolster the rotational invariance. The enforced metric space is then used to encode a reference trajectory, serving as a map, which is queried for nearest neighbour for recognition of places at run-time. We demonstrate the performance of our topological localisation system over the course of many repeat forays using the largest radar-focused mobile autonomy dataset released to date, totalling 280 km of urban driving, a small portion of which we also use to learn the weights of the modified architecture. As this work represents a novel application for radar, we analyse the utility of the proposed method via a comprehensive set of metrics which provide insight into the efficacy when used in a realistic system, showing improved performance over the root architecture even in the face of random rotational perturbation.
ER  - 

TY  - CONF
TI  - Global visual localization in LiDAR-maps through shared 2D-3D embedding space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4365
EP  - 4371
AU  - D. Cattaneo
AU  - M. Vaghi
AU  - S. Fontana
AU  - A. L. Ballardini
AU  - D. G. Sorrenti
PY  - 2020
KW  - image recognition
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - optical radar
KW  - robot vision
KW  - SLAM (robots)
KW  - global visual localization
KW  - LiDAR-maps
KW  - place recognition
KW  - autonomous driving field
KW  - vision-based approaches
KW  - image database
KW  - high definition 3D maps
KW  - deep neural network
KW  - shared embedding space
KW  - 3D-LiDAR place recognition
KW  - 3D DNN
KW  - 2D-3D embedding space
KW  - Oxford Robotcar Dataset
KW  - image w.r.t.
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Laser radar
KW  - Feature extraction
KW  - Visualization
KW  - Robots
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9196859
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.
ER  - 

TY  - CONF
TI  - Unsupervised Learning Methods for Visual Place Recognition in Discretely and Continuously Changing Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4372
EP  - 4378
AU  - S. Schubert
AU  - P. Neubert
AU  - P. Protzel
PY  - 2020
KW  - convolutional neural nets
KW  - image matching
KW  - object recognition
KW  - principal component analysis
KW  - unsupervised learning
KW  - place recognition performance
KW  - in-sequence condition changes
KW  - unsupervised learning methods
KW  - visual place recognition
KW  - query set
KW  - reference set
KW  - single distinctive condition
KW  - query sequence
KW  - traversal daytime-dusk-night-dawn
KW  - CNN-based descriptors
KW  - in-sequence changes
KW  - continuous changes
KW  - statistical normalization
KW  - PCA
KW  - Standardization
KW  - Visualization
KW  - Lighting
KW  - Unsupervised learning
KW  - Principal component analysis
KW  - Dimensionality reduction
KW  - Clouds
DO  - 10.1109/ICRA40945.2020.9197044
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual place recognition in changing environments is the problem of finding matchings between two sets of observations, a query set and a reference set, despite severe appearance changes. Recently, image comparison using CNNbased descriptors showed very promising results. However, the experiments in the literature typically assume a single distinctive condition within each set (e.g., reference images are captured at daytime and the query sequence is at night). In this paper, we will demonstrate that as soon as the conditions change within one set (e.g., reference is daytime and now the query is a traversal daytime-dusk-night-dawn), different places under the same condition can suddenly look more similar than same places under different conditions. As a consequence, state-of-the-art approaches like CNN-based descriptors fail. This paper discusses this practically very important problem of in-sequence condition changes and defines a hierarchy of problem setups from (1) no in-sequence changes, (2) discrete in-sequence changes, to (3) continuous in-sequence changes. We will experimentally evaluate the effect of in-sequence condition changes on two state-of-the-art CNN-descriptors and investigate unsupervised methods to improve their performance. This includes an evaluation of the importance of statistical normalization (standardization) of descriptors, which is often omitted in existing approaches but can considerably improve results for problems up to discrete in-sequence changes. To address the practical most relevant setup of continuous changes, we investigate the application of unsupervised learning methods using two PCA-based approaches from the literature and propose a novel clustering-based extension of the statistical normalization. We experimentally demonstrate that these approaches can significantly improve place recognition performance in case of continuous in-sequence condition changes. Matlab implementations of the presented approaches are available online: www.tu-chemnitz.de/etit/proaut/cont_changing_envs.
ER  - 

TY  - CONF
TI  - LOL: Lidar-only Odometry and Localization in 3D point cloud maps*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4379
EP  - 4385
AU  - D. Rozenberszki
AU  - A. L. Majdik
PY  - 2020
KW  - distance measurement
KW  - image enhancement
KW  - image matching
KW  - image segmentation
KW  - object detection
KW  - object recognition
KW  - optical radar
KW  - LOL system
KW  - 3D point cloud maps
KW  - Lidar-equipped vehicles
KW  - 3D point segment matching method
KW  - Lidar-only odometry and localization algorithm
KW  - Kitti datasets
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Sensors
KW  - Iterative closest point algorithm
KW  - Image color analysis
KW  - Trajectory
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197450
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we deal with the problem of odometry and localization for Lidar-equipped vehicles driving in urban environments, where a premade target map exists to localize against. In our problem formulation, to correct the accumulated drift of the Lidar-only odometry we apply a place recognition method to detect geometrically similar locations between the online 3D point cloud and the a priori offline map. In the proposed system, we integrate a state-of-the-art Lidaronly odometry algorithm with a recently proposed 3D point segment matching method by complementing their advantages. Also, we propose additional enhancements in order to reduce the number of false matches between the online point cloud and the target map, and to refine the position estimation error whenever a good match is detected. We demonstrate the utility of the proposed LOL system on several Kitti datasets of different lengths and environments, where the relocalization accuracy and the precision of the vehicle's trajectory were significantly improved in every case, while still being able to maintain real-time performance.
ER  - 

TY  - CONF
TI  - Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4386
EP  - 4392
AU  - L. Sun
AU  - D. Adolfsson
AU  - M. Magnusson
AU  - H. Andreasson
AU  - I. Posner
AU  - T. Duckett
PY  - 2020
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - neural nets
KW  - optical radar
KW  - path planning
KW  - recursive estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - precise lidar-based robot localisation
KW  - large-scale environments
KW  - global localisation
KW  - mobile robots
KW  - Monte Carlo Localisation
KW  - MCL
KW  - fast localisation system
KW  - deep-probabilistic model
KW  - Gaussian process regression
KW  - deep kernel
KW  - precise recursive estimator
KW  - Gaussian method
KW  - deep probabilistic localisation
KW  - large-scale localisation
KW  - largescale environment
KW  - time 0.8 s
KW  - size 0.75 m
KW  - Robots
KW  - Neural networks
KW  - Three-dimensional displays
KW  - Gaussian processes
KW  - Laser radar
KW  - Monte Carlo methods
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196708
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deeplearned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and nonGaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a largescale environment of approximately 0.5 km2.
ER  - 

TY  - CONF
TI  - Set-membership state estimation by solving data association
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4393
EP  - 4399
AU  - S. Rohou
AU  - B. Desrochers
AU  - L. Jaulin
PY  - 2020
KW  - mobile robots
KW  - position control
KW  - robot vision
KW  - sensor fusion
KW  - SLAM (robots)
KW  - state estimation
KW  - deterministic approach
KW  - data association
KW  - underwater robot
KW  - sonar data
KW  - membership state estimation
KW  - localization problem
KW  - indistinguishable landmarks
KW  - diving phase
KW  - unknown initial position
KW  - Sonar
KW  - State estimation
KW  - Rocks
KW  - Trajectory
KW  - Robot sensing systems
KW  - Reliability
DO  - 10.1109/ICRA40945.2020.9197039
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper deals with the localization problem of a robot in an environment made of indistinguishable landmarks, and assuming the initial position of the vehicle is unknown. This scenario is typically encountered in underwater applications for which landmarks such as rocks all look alike. Furthermore, the position of the robot may be lost during a diving phase, which obliges us to consider unknown initial position. We propose a deterministic approach to solve simultaneously the problems of data association and state estimation, without combinatorial explosion. The efficiency of the method is shown on an actual experiment involving an underwater robot and sonar data.
ER  - 

TY  - CONF
TI  - A Linearly Constrained Nonparametric Framework for Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4400
EP  - 4406
AU  - Y. Huang
AU  - D. G. Caldwell
PY  - 2020
KW  - Bayes methods
KW  - end effectors
KW  - learning systems
KW  - predictive control
KW  - trajectory control
KW  - imitation learning
KW  - constrained skills
KW  - linearly constrained optimization problem
KW  - nonparametric solution
KW  - linearly constrained nonparametric framework
KW  - human skills learning
KW  - constrained motor skills learning
KW  - robotic systems
KW  - end-effector trajectory
KW  - linearly constrained kernelized movement primitives
KW  - LC-KMP
KW  - probabilistic properties
KW  - predictive control
KW  - locomotion tasks
KW  - grasping tasks
KW  - human robot collaborations
KW  - Trajectory
KW  - Probabilistic logic
KW  - Robots
KW  - Task analysis
KW  - Optimization
KW  - Kernel
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196821
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In recent years, a myriad of advanced results have been reported in the community of imitation learning, ranging from parametric to non-parametric, probabilistic to non-probabilistic and Bayesian to frequentist approaches. Meanwhile, ample applications (e.g., grasping tasks and humanrobot collaborations) further show the applicability of imitation learning in a wide range of domains. While numerous literature is dedicated to the learning of human skills in unconstrained environments, the problem of learning constrained motor skills, however, has not received equal attention. In fact, constrained skills exist widely in robotic systems. For instance, when a robot is demanded to write letters on a board, its end-effector trajectory must comply with the plane constraint from the board. In this paper, we propose linearly constrained kernelized movement primitives (LC-KMP) to tackle the problem of imitation learning with linear constraints. Specifically, we propose to exploit the probabilistic properties of multiple demonstrations, and subsequently incorporate them into a linearly constrained optimization problem, which finally leads to a non-parametric solution. In addition, a connection between our framework and the classical model predictive control is provided. Several examples including simulated writing and locomotion tasks are presented to show the effectiveness of our framework.
ER  - 

TY  - CONF
TI  - An Energy-based Approach to Ensure the Stability of Learned Dynamical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4407
EP  - 4413
AU  - M. Saveriano
PY  - 2020
KW  - learning (artificial intelligence)
KW  - motion control
KW  - nonlinear dynamical systems
KW  - regression analysis
KW  - stability
KW  - energy-based approach
KW  - learned dynamical systems
KW  - nonlinear dynamical systems
KW  - reactive motion generation
KW  - stable motions
KW  - accurate motions
KW  - learning problems
KW  - training time
KW  - single-step learning
KW  - regression technique
KW  - single-step approach
KW  - energy considerations
KW  - learned dynamics
KW  - demonstrated motion
KW  - Lyapunov methods
KW  - Training data
KW  - Robots
KW  - Electrostatic discharges
KW  - Stability analysis
KW  - Trajectory
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196978
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Non-linear dynamical systems represent a compact, flexible, and robust tool for reactive motion generation. The effectiveness of dynamical systems relies on their ability to accurately represent stable motions. Several approaches have been proposed to learn stable and accurate motions from demonstration. Some approaches work by separating accuracy and stability into two learning problems, which increases the number of open parameters and the overall training time. Alternative solutions exploit single-step learning but restrict the applicability to one regression technique. This paper presents a single-step approach to learn stable and accurate motions that work with any regression technique. The approach makes energy considerations on the learned dynamics to stabilize the system at run-time while introducing small deviations from the demonstrated motion. Since the initial value of the energy injected into the system affects the reproduction accuracy, it is estimated from training data using an efficient procedure. Experiments on a real robot and a comparison on a public benchmark shows the effectiveness of the proposed approach.
ER  - 

TY  - CONF
TI  - IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4414
EP  - 4420
AU  - A. Mandlekar
AU  - F. Ramos
AU  - B. Boots
AU  - S. Savarese
AU  - L. Fei-Fei
AU  - A. Garg
AU  - D. Fox
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - RoboTurk Cans dataset
KW  - offline learning
KW  - IRIS
KW  - offline robot manipulation data
KW  - offline task demonstrations
KW  - robotics
KW  - goal-conditioned low-level controller
KW  - high-level goal selection mechanism
KW  - learning control
KW  - learning from large-scale demonstration datasets
KW  - Implicit Reinforcement without Interaction at Scale
KW  - crowdsourcing
KW  - Task analysis
KW  - Robots
KW  - Trajectory
KW  - Iris
KW  - Learning (artificial intelligence)
KW  - Iris recognition
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196935
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline learning from a small set of demonstrations can produce controllers that successfully solve the task. However, leveraging a fixed batch of data can be problematic for larger datasets and longer-horizon tasks with greater variations. The data can exhibit substantial diversity and consist of suboptimal solution approaches. In this paper, we propose Implicit Reinforcement without Interaction at Scale (IRIS), a novel framework for learning from large-scale demonstration datasets. IRIS factorizes the control problem into a goal-conditioned low-level controller that imitates short demonstration sequences and a high-level goal selection mechanism that sets goals for the low-level and selectively combines parts of suboptimal solutions leading to more successful task completions. We evaluate IRIS across three datasets, including the RoboTurk Cans dataset collected by humans via crowdsourcing, and show that performant policies can be learned from purely offline learning. Additional results at https://sites.google.com/stanford.edu/iris/.
ER  - 

TY  - CONF
TI  - Geometry-aware Dynamic Movement Primitives
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4421
EP  - 4426
AU  - F. J. Abu-Dakka
AU  - V. Kyrki
PY  - 2020
KW  - differential geometry
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - matrix algebra
KW  - geometry-aware dynamic movement primitives
KW  - robot control problems
KW  - manipulability ellipsoids
KW  - symmetric positive definite matrices
KW  - DMPs
KW  - SPD matrices
KW  - Euclidean space
KW  - mathematically principled framework
KW  - SPD manifold
KW  - Riemannian metrics
KW  - Manifolds
KW  - Robots
KW  - Symmetric matrices
KW  - Standards
KW  - Ellipsoids
KW  - Switches
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9196952
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many robot control problems, factors such as stiffness and damping matrices and manipulability ellipsoids are naturally represented as symmetric positive definite (SPD) matrices, which capture the specific geometric characteristics of those factors. Typical learned skill models such as dynamic movement primitives (DMPs) can not, however, be directly employed with quantities expressed as SPD matrices as they are limited to data in Euclidean space. In this paper, we propose a novel and mathematically principled framework that uses Riemannian metrics to reformulate DMPs such that the resulting formulation can operate with SPD data in the SPD manifold. Evaluation of the approach demonstrates that beneficial properties of DMPs such as change of the goal during operation apply also to the proposed formulation.
ER  - 

TY  - CONF
TI  - Learning a Pile Loading Controller from Demonstrations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4427
EP  - 4433
AU  - W. Yang
AU  - N. Strokina
AU  - N. Serbenyuk
AU  - R. Ghabcheloo
AU  - J. K√§m√§r√§inen
PY  - 2020
KW  - control engineering computing
KW  - earthmoving equipment
KW  - foundations
KW  - image representation
KW  - mobile robots
KW  - neural net architecture
KW  - random forests
KW  - video signal processing
KW  - hydrostatic driving pressure
KW  - control signals
KW  - application specific deep visual features
KW  - Siamese network architecture
KW  - random forest regressor
KW  - loading distance
KW  - autonomous robotic wheel loader
KW  - learning-based pile loading controller
KW  - controller parameters
KW  - low level sensor
KW  - boom angle
KW  - bucket angle
KW  - cross-entropy
KW  - contrastive loss
KW  - soil type
KW  - Visualization
KW  - Loading
KW  - Robot sensing systems
KW  - Task analysis
KW  - Feature extraction
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196907
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work introduces a learning-based pile loading controller for autonomous robotic wheel loaders. Controller parameters are learnt from a small number of demonstrations for which low level sensor (boom angle, bucket angle and hydrostatic driving pressure), egocentric video frames and control signals are recorded. Application specific deep visual features are learnt from demonstrations using a Siamese network architecture and a combination of cross-entropy and contrastive loss. The controller is based on a Random Forest (RF) regressor that provides robustness against changes in field conditions (loading distance, soil type, weather and illumination). The controller is deployed to a real autonomous robotic wheel loader and it outperforms prior art with a clear margin.
ER  - 

TY  - CONF
TI  - Learning Navigation Costs from Demonstration in Partially Observable Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4434
EP  - 4440
AU  - T. Wang
AU  - V. Dhiman
AU  - N. Atanasov
PY  - 2020
KW  - cost optimal control
KW  - dynamic programming
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - navigation
KW  - observability
KW  - path planning
KW  - probability
KW  - state-space methods
KW  - trajectory control
KW  - inverse reinforcement learning
KW  - safe navigation
KW  - autonomous navigation
KW  - unknown partially observable environments
KW  - navigation behavior
KW  - state-control trajectory
KW  - cost function representation
KW  - probabilistic occupancy encoder
KW  - observation sequence
KW  - cost encoder
KW  - occupancy features
KW  - representation parameters
KW  - control policy
KW  - value function
KW  - state space
KW  - motion-planning algorithm
KW  - robot navigation
KW  - navigation cost learning
KW  - dynamic
KW  - Robots
KW  - Cost function
KW  - Navigation
KW  - Planning
KW  - Feature extraction
KW  - Heuristic algorithms
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197199
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on inverse reinforcement learning (IRL) to enable safe and efficient autonomous navigation in unknown partially observable environments. The objective is to infer a cost function that explains expert-demonstrated navigation behavior while relying only on the observations and state-control trajectory used by the expert. We develop a cost function representation composed of two parts: a probabilistic occupancy encoder, with recurrent dependence on the observation sequence, and a cost encoder, defined over the occupancy features. The representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost encoder. Such differentiation is typically computed by dynamic programming through the value function over the whole state space. We observe that this is inefficient in large partially observable environments because most states are unexplored. Instead, we rely on a closed-form subgradient of the cost-to-go obtained only over a subset of promising states via an efficient motion-planning algorithm such as A* or RRT. Our experiments show that our model exceeds the accuracy of baseline IRL algorithms in robot navigation tasks, while substantially improving the efficiency of training and test-time inference.
ER  - 

TY  - CONF
TI  - Towards Bimanual Vein Cannulation: Preliminary Study of a Bimanual Robotic System With a Dual Force Constraint Controller
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4441
EP  - 4447
AU  - C. He
AU  - A. Ebrahimi
AU  - E. Yang
AU  - M. Urias
AU  - Y. Yang
AU  - P. Gehlbach
AU  - I. Iordachita
PY  - 2020
KW  - biological tissues
KW  - biomechanics
KW  - blood vessels
KW  - eye
KW  - manipulators
KW  - medical robotics
KW  - phantoms
KW  - surgery
KW  - surgical tools
KW  - dual force constraint controller
KW  - robot-assisted retinal surgery
KW  - tool-to-sclera forces
KW  - cannulation tool
KW  - dual force-sensing capability
KW  - force information
KW  - robot controller
KW  - retinal vein cannulation
KW  - target vessel
KW  - robotic manipulators
KW  - tool-to-tissue forces
KW  - retinal tissue injury
KW  - bimanual cannulation
KW  - bimanual robotic system
KW  - retinal vein occlusion
KW  - occluded vessel
KW  - interaction forces
KW  - bimanual vein cannulation
KW  - tool-to-sclera force
KW  - tool-to-vessel force
KW  - steady hand eye robot platforms
KW  - Tools
KW  - Force
KW  - Robot sensing systems
KW  - Retina
KW  - Surgery
KW  - Veins
DO  - 10.1109/ICRA40945.2020.9196889
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Retinal vein cannulation is a promising approach for treating retinal vein occlusion that involves injecting medicine into the occluded vessel to dissolve the clot. The approach remains largely unexploited clinically due to surgeon limitations in detecting interaction forces between surgical tools and retinal tissue. In this paper, a dual force constraint controller for robot-assisted retinal surgery was presented to keep the tool-to-vessel forces and tool-to-sclera forces below prescribed thresholds. A cannulation tool and forceps with dual force-sensing capability were developed and used to measure force information fed into the robot controller, which was implemented on existing Steady Hand Eye Robot platforms. The robotic system facilitates retinal vein cannulation by allowing a user to grasp the target vessel with the forceps and then enter the vessel with the cannula. The system was evaluated on an eye phantom. The results showed that, while the eyeball was subjected to rotational disturbances, the proposed controller actuates the robotic manipulators to maintain the average tool-to-vessel force at 10.9 mN and 13.1 mN and the average tool-to-sclera force at 38.1 mN and 41.2 mN for the cannula and the forcpes, respectively. Such small tool-to-tissue forces are acceptable to avoid retinal tissue injury. Additionally, two clinicians participated in a preliminary user study of the bimanual cannulation demonstrating that the operation time and tool-to-tissue forces are significantly decreased when using the bimanual robotic system as compared to freehand performance.
ER  - 

TY  - CONF
TI  - Evaluation of a combined grip of pinch and power grips in manipulating a master manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4448
EP  - 4454
AU  - S. Jeong
AU  - K. Tadano
PY  - 2020
KW  - biomechanics
KW  - dexterous manipulators
KW  - grippers
KW  - medical robotics
KW  - surgery
KW  - master manipulator
KW  - combined-grip-handle scheme
KW  - pinch grip motion
KW  - power grip motion
KW  - robotic surgery
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196547
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In conventional robotic surgery, the manipulating methods exhibit limitations that are strongly related to the advantages and disadvantages of a pinch grip and power grip. The context of this study is focused on the introduction of a combined grip to compensate for such restraints. In particular, this study proposed the combined-grip-handle scheme on a master manipulator. In this framework, the position of fingertips was designed to be adjustable in distance and direction to allow for a pinch grip motion around the holding axis of a power grip motion. A ring-bar experiment applying the master-slave scheme was conducted with the master manipulator under several manipulating conditions of the combined grip and the conventional gripping types. Results for using the combined grip demonstrated that the combined grip showed better performance on the positioning operation, compared with the conventional gripping types.
ER  - 

TY  - CONF
TI  - Contact Stability Analysis of Magnetically-Actuated Robotic Catheter Under Surface Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4455
EP  - 4462
AU  - R. Hao
AU  - T. Greigarn
AU  - M. C. √áavu≈üoƒülu
PY  - 2020
KW  - biological tissues
KW  - biomedical MRI
KW  - cardiology
KW  - catheters
KW  - medical image processing
KW  - medical robotics
KW  - contact stability analysis
KW  - magnetically-actuated robotic catheter
KW  - contact force quality
KW  - lesion formation
KW  - lesion size
KW  - gap-free lesion
KW  - tissue surface motion
KW  - contact model
KW  - contact force control schemes
KW  - heart surface motions
KW  - magnetic resonance imaging-actuated robotic catheter
KW  - Catheters
KW  - Force
KW  - Force control
KW  - Robots
KW  - Stability analysis
KW  - Magnetic resonance imaging
KW  - Friction
DO  - 10.1109/ICRA40945.2020.9196951
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Contact force quality is one of the most critical factors for safe and effective lesion formation during cardiac ablation. The contact force and contact stability plays important roles in determining the lesion size and creating a gap-free lesion. In this paper, the contact stability of a novel magnetic resonance imaging (MRI)-actuated robotic catheter under tissue surface motion is studied. The robotic catheter is modeled using a pseudo-rigid-body model, and the contact model under surface constraint is provided. Two contact force control schemes to improve the contact stability of the catheter under heart surface motions are proposed and their performance are evaluated in simulation.
ER  - 

TY  - CONF
TI  - Fast and accurate intracorporeal targeting through an anatomical orifice exhibiting unknown behavior.
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4463
EP  - 4469
AU  - R. Chalard
AU  - D. Reversat
AU  - G. Morel
AU  - M. -A. Vitrani
PY  - 2020
KW  - adaptive control
KW  - biomechanics
KW  - biomedical equipment
KW  - force control
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - position control
KW  - surgery
KW  - anatomical orifice
KW  - minimally invasive surgery
KW  - interaction forces
KW  - patient anatomy
KW  - orifice behavior
KW  - adaptive control scheme
KW  - wrist velocity
KW  - tip velocity
KW  - intracorporeal targeting
KW  - instrument tip positioning
KW  - 3DOF wrist center positioning problem
KW  - Robots
KW  - Instruments
KW  - Wrist
KW  - Surgery
KW  - Force
KW  - Adaptation models
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196950
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgery may involve precise instrument tip positioning in a minimally invasive way. During these operations, the instrument is inserted in the body through an orifice. The movements of the instrument are constrained by interaction forces arising at the orifice level. The physical constraints may drastically vary depending on the patient's anatomy. This introduces uncertainties that challenge the positioning task for a robot. Indeed, it raises an antagonism: On one side, the required precision appeals for a rigid behavior. On the other side, forces applied at the entry point should be limited, which requires softness. In this paper we choose to minimize forces at the orifice by using a passive ball joint wrist to manipulate the instrument. From a control perspective, this leads to consider the task as a 3 DOF wrist center positioning problem, whose softness can be achieved through conventional low impedance control. However, positioning the wrist center, even with a high static precision, does not allow to achieve a high precision of the instrument tip positioning when the orifice behavior is not known. To cope with this problem, we implement a controller that servos the tip position by commanding the wrist position. In order to deal with uncertainties, we exploit an adaptive control scheme that identifies in real-time the unknown mapping between the wrist velocity and the tip velocity. Both simulations and in vitro experimental results show the efficiency of the control law.
ER  - 

TY  - CONF
TI  - Robotic Swarm Control for Precise and On-Demand Embolization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4470
EP  - 4476
AU  - M. Luo
AU  - J. Law
AU  - X. Wang
AU  - L. Xin
AU  - G. Shan
AU  - M. Badiwala
AU  - X. Huang
AU  - Y. Sun
PY  - 2020
KW  - biomedical materials
KW  - bioMEMS
KW  - blood vessels
KW  - haemodynamics
KW  - magnetic particles
KW  - medical robotics
KW  - microfluidics
KW  - targeted embolization
KW  - swarm control technique
KW  - magnetic particles
KW  - mean absolute error
KW  - aggregation control approach
KW  - fluidic shear
KW  - magnetic forces
KW  - magnetic field
KW  - magnetic swarm control strategy
KW  - blood vessels
KW  - clinical embolization
KW  - swarm control capability
KW  - fluidic flow environment
KW  - magnetic aggregates
KW  - magnetic swarms
KW  - robotic control
KW  - robotic swarm control
KW  - Aggregates
KW  - Magnetic tunneling
KW  - Coils
KW  - Junctions
KW  - Magnetic particles
KW  - Blood vessels
KW  - Magnetic separation
DO  - 10.1109/ICRA40945.2020.9197009
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing approaches for robotic control of magnetic swarms are not capable of generating magnetic aggregates precisely in an arbitrarily specified target region in a fluidic flow environment. Such a swarm control capability is demanded by medical applications such as clinical embolization (i.e., localized clogging of blood vessels). This paper presents a new magnetic swarm control strategy to generate aggregates only in a specified target region under fluidic flow. Within the target region, the magnetic field generates sufficiently large magnetic forces among magnetic particles to maintain the aggregates' integrity at the junctions of blood vessels. In contrast, unintended aggregates outside the target region are disassembled by fluidic shear. The aggregation control approach achieved a mean absolute error of 0.15 mm in positioning a target region and a mean absolute error of 0.30 mm in controlling the target region's radius. With thrombin coating, 1 Œºm magnetic particles were controlled to perform embolization both in vitro (using microfluidic channel networks) and ex vivo (using porcine tissue). Experiments proved the effectiveness of the swarm control technique for on-demand, targeted embolization.
ER  - 

TY  - CONF
TI  - Bilateral Teleoperation Control of a Redundant Manipulator with an RCM Kinematic Constraint
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4477
EP  - 4482
AU  - H. Su
AU  - Y. Schmirander
AU  - Z. Li
AU  - X. Zhou
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - augmented reality
KW  - end effectors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - medical robotics
KW  - motion control
KW  - position control
KW  - redundant manipulators
KW  - stability
KW  - surgery
KW  - telerobotics
KW  - energy tank model
KW  - haptic feedback
KW  - KUKA LWR4+ serial robot
KW  - Sigma 7 haptic manipulator
KW  - redundant manipulator
KW  - RCM kinematic constraint
KW  - serial robot manipulator
KW  - remote center of motion constraint
KW  - decoupled cartesian admittance control
KW  - end effector
KW  - bilateral teleoperation control stability
KW  - augmented reality
KW  - teleoperated surgery
KW  - Manipulators
KW  - Surgery
KW  - Force
KW  - Frequency modulation
KW  - Kinematics
KW  - Haptic interfaces
KW  - Switches
DO  - 10.1109/ICRA40945.2020.9197267
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a bilateral teleoperation control of a serial robot manipulator, which guarantees a Remote Center of Motion (RCM) constraint in its kinematic level, is developed. A two-layered approach based on the energy tank model is proposed to achieve haptic feedback on the end effector with a pedal switch. The redundancy of the manipulator is exploited to maintain the RCM constraint using the decoupled Cartesian Admittance Control. Transparency and stability of the proposed bilateral teleoperation are demonstrated using a KUKA LWR4+ serial robot and a Sigma 7 haptic manipulator with an RCM constraint in augmented reality. The results prove that the control can achieve not only the bilateral teleoperation but also maintain the RCM constraint.
ER  - 

TY  - CONF
TI  - From Bipedal Walking to Quadrupedal Locomotion: Full-Body Dynamics Decomposition for Rapid Gait Generation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4491
EP  - 4497
AU  - W. -L. Ma
AU  - A. D. Ames
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - robot dynamics
KW  - trajectory control
KW  - full-order dynamics
KW  - rapid generation
KW  - stepping-in-place gaits
KW  - diagonally symmetric ambling gait
KW  - dynamic bipedal walking
KW  - quadrupedal locomotion
KW  - rapid gait generation
KW  - hybrid dynamics
KW  - three-dimensional quadrupedal robot
KW  - hybrid zero dynamics framework
KW  - bipedal robots
KW  - bipedal walking gaits
KW  - quadrupedal trajectory
KW  - Legged locomotion
KW  - Robot kinematics
KW  - Dynamics
KW  - Nonlinear dynamical systems
KW  - Jacobian matrices
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196841
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper systematically decomposes a quadrupedal robot into bipeds to rapidly generate walking gaits and then recomposes these gaits to obtain quadrupedal locomotion. We begin by decomposing the full-order, nonlinear and hybrid dynamics of a three-dimensional quadrupedal robot, including its continuous and discrete dynamics, into two bipedal systems that are subject to external forces. Using the hybrid zero dynamics (HZD) framework, gaits for these bipedal robots can be rapidly generated (on the order of seconds) along with corresponding controllers. The decomposition is achieved in such a way that the bipedal walking gaits and controllers can be composed to yield dynamic walking gaits for the original quadrupedal robot - the result is the rapid generation of dynamic quadruped gaits utilizing the full-order dynamics. This methodology is demonstrated through the rapid generation (3.96 seconds on average) of four stepping-in-place gaits and one diagonally symmetric ambling gait at 0.35 m/s on a quadrupedal robot - the Vision 60, with 36 state variables and 12 control inputs - both in simulation and through outdoor experiments. This suggested a new approach for fast quadrupedal trajectory planning using full-body dynamics, without the need for empirical model simplification, wherein methods from dynamic bipedal walking can be directly applied to quadrupeds.
ER  - 

TY  - CONF
TI  - Posture Control for a Low-Cost Commercially-Available Hexapod Robot*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4498
EP  - 4504
AU  - M. Tikam
AU  - D. Withey
AU  - N. J. Theron
PY  - 2020
KW  - compliance control
KW  - force control
KW  - gait analysis
KW  - image motion analysis
KW  - legged locomotion
KW  - position control
KW  - robot dynamics
KW  - torque control
KW  - direct force control
KW  - Vicon motion capture system
KW  - custom-designed platforms
KW  - low-cost commercially-available hexapod robot
KW  - legged robots
KW  - custom-designed robotic platforms
KW  - commercially-available robots
KW  - low-cost research platforms
KW  - low-cost joint actuators
KW  - torque control capabilities
KW  - hierarchical control system
KW  - virtual model control
KW  - simple foot force distribution
KW  - walking posture control system
KW  - Legged locomotion
KW  - Foot
KW  - Force
KW  - Robot sensing systems
KW  - Control systems
KW  - Engines
DO  - 10.1109/ICRA40945.2020.9197147
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Posture control for legged robots has been widely developed on custom-designed robotic platforms, with little work being done on commercially-available robots despite their potential as low-cost research platforms. This paper presents the implementation of a Walking Posture Control system on a commercially-available hexapod robot which utilizes low-cost joint actuators without torque control capabilities. The hierarchical control system employs Virtual Model Control with simple foot force distribution and a novel, position-based Foot Force Controller that enables direct force control during the leg's stance phase and active compliance control during the swing phase. Ground truth measurements in experimental tests, obtained with a Vicon motion capture system, demonstrate the improvement to posture made by the control system on uneven terrain, with the results comparing favorably to those obtained in similar tests on more sophisticated, custom-designed platforms.
ER  - 

TY  - CONF
TI  - Collaborative Multi-Robot Localization in Natural Terrain*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4529
EP  - 4535
AU  - A. Wiktor
AU  - S. Rock
PY  - 2020
KW  - autonomous underwater vehicles
KW  - filtering theory
KW  - mobile robots
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - path planning
KW  - sensor fusion
KW  - Monterey Bay
KW  - terrain relative navigation
KW  - filter architecture
KW  - collaborative multirobot localization
KW  - standard TRN
KW  - Monte Carlo simulation
KW  - inter-vehicle range measurements
KW  - autonomous underwater vehicle
KW  - multirobot information
KW  - TRN techniques
KW  - covariance intersection
KW  - Robots
KW  - Correlation
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Extraterrestrial measurements
KW  - Collaboration
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197576
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel filter architecture that allows a team of vehicles to collaboratively localize using Terrain Relative Navigation (TRN). The work explores several causes of measurement correlation that preclude the use of traditional estimators, and proposes an estimator structure that eliminates one source of measurement correlation while properly incorporating others through the use of Covariance Intersection. The result is a consistent estimator that is able to augment proven TRN techniques with multi-robot information, significantly improving localization for vehicles in uninformative terrain. The approach is demonstrated using field data from an Autonomous Underwater Vehicle (AUV) navigating with TRN in Monterey Bay and simulated inter-vehicle range measurements. In addition, a Monte Carlo simulation was used to quantify the algorithm's performance on one example mission. Monte Carlo results show that a vehicle operating in uninformative terrain has 62% lower localization error when fusing range measurements to two converged AUVs than it would using standard TRN.
ER  - 

TY  - CONF
TI  - Multi-Robot Control Using Coverage Over Time-Varying Non-Convex Domains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4536
EP  - 4542
AU  - X. Xu
AU  - Y. Diaz-Mercado
PY  - 2020
KW  - computational geometry
KW  - linearisation techniques
KW  - multi-robot systems
KW  - time-varying systems
KW  - time-varying domains
KW  - nonconvex shape
KW  - nonconvex coverage problem
KW  - control law
KW  - time-varying density
KW  - time-varying diffeomorphism
KW  - multirobot control
KW  - time-varying nonconvex domains
KW  - coverage control
KW  - Robot kinematics
KW  - Multi-robot systems
KW  - Time-varying systems
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Transforms
DO  - 10.1109/ICRA40945.2020.9196630
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the problem of a domain becoming non-convex while using coverage control of a multirobot system over time-varying domains. When the domain moves around in the workspace, its motion and the presence of obstacles might cause it to deform into some non-convex shape, and the robot team should act in a coordinating manner to maintain coverage. The proposed solution is based on a framework for constructing a diffeomorphism to transform a non-convex coverage problem into a convex one. A control law is developed to capture the effects of time variations (e.g., from a time-varying density, time-varying convex hull of the domain and time-varying diffeomorphism) in the system. Analytic expressions of each term in the control law are found for uniform density case. A simulation and robotic implementation are used to validate the proposed algorithm.
ER  - 

TY  - CONF
TI  - Efficient Large-Scale Multi-Drone Delivery Using Transit Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4543
EP  - 4550
AU  - S. Choudhury
AU  - K. Solovey
AU  - M. J. Kochenderfer
AU  - M. Pavone
PY  - 2020
KW  - autonomous aerial vehicles
KW  - computational complexity
KW  - graph theory
KW  - multi-robot systems
KW  - optimisation
KW  - near-optimal polynomial-time task allocation algorithm
KW  - delivery sequences
KW  - two-layer approach
KW  - multifaceted complexity
KW  - maximum time
KW  - comprehensive algorithmic framework
KW  - public transit vehicles
KW  - efficient large-scale multidrone delivery
KW  - transit network
KW  - bounded-suboptimal multiagent pathfinding techniques
KW  - Drones
KW  - Task analysis
KW  - Resource management
KW  - Routing
KW  - Urban areas
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197313
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of controlling a large fleet of drones to deliver packages simultaneously across broad urban areas. To conserve energy, drones hop between public transit vehicles (e.g., buses and trams). We design a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery. We address the multifaceted complexity of the problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with a near-optimal polynomial-time task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network while employing efficient bounded-suboptimal multi-agent pathfinding techniques tailored to our setting. Experiments demonstrate the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in San Francisco and Washington DC. Our results show that the framework computes solutions within a few seconds (up to 2 minutes at most) on commodity hardware, and that drones travel up to 450% of their flight range with public transit.
ER  - 

TY  - CONF
TI  - Resilience in multi-robot target tracking through reconfiguration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4551
EP  - 4557
AU  - R. K. Ramachandran
AU  - N. Fronda
AU  - G. S. Sukhatme
PY  - 2020
KW  - convex programming
KW  - covariance matrices
KW  - integer programming
KW  - Kalman filters
KW  - mobile robots
KW  - multi-robot systems
KW  - target tracking
KW  - multirobot target
KW  - resource availability
KW  - networked multirobot system
KW  - target tracking
KW  - sensing resources
KW  - computational resources
KW  - distributed Kalman filter
KW  - sensor measurement noise covariance matrix
KW  - sensing quality deteriorates
KW  - systems communication graph
KW  - sensor quality
KW  - active communication links
KW  - mixed integer semidefinite programming formulations
KW  - agent-centric strategy
KW  - team-centric strategy
KW  - greedy strategy
KW  - Robot sensing systems
KW  - Target tracking
KW  - Covariance matrices
KW  - Kalman filters
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196961
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of maintaining resource availability in a networked multi-robot system performing distributed target tracking. In our model, robots are equipped with sensing and computational resources enabling them to track a target's position using a Distributed Kalman Filter (DKF). We use the trace of each robot's sensor measurement noise covariance matrix as a measure of sensing quality. When a robot's sensing quality deteriorates, the systems communication graph is modified by adding edges such that the robot with deteriorating sensor quality may share information with other robots to improve the team's target tracking ability. This computation is performed centrally and is designed to work without a large change in the number of active communication links. We propose two mixed integer semi-definite programming formulations (an `agent-centric' strategy and a `team-centric' strategy) to achieve this goal. We implement both formulations and a greedy strategy in simulation and show that the team-centric strategy outperforms the agent-centric and greedy strategies.
ER  - 

TY  - CONF
TI  - Teleoperation of Multi-Robot Systems to Relax Topological Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4558
EP  - 4564
AU  - L. Sabattini
AU  - B. Capelli
AU  - C. Fantuzzi
AU  - C. Secchi
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - multi-robot systems
KW  - telerobotics
KW  - motion pattern
KW  - graph
KW  - multirobot teleoperation
KW  - mobile robots
KW  - topological constraints
KW  - Multi-robot systems
KW  - Force
KW  - Mobile robots
KW  - Force feedback
KW  - Collision avoidance
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9197254
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-robot systems are able to achieve common objectives exchanging information among each other. This is possible exploiting a communication structure, usually modeled as a graph, whose topological properties (such as connectivity) are very relevant in the overall performance of the multirobot system. When considering mobile robots, such properties can change over time: robots are then controlled to preserve them, thus guaranteeing the possibility, for the overall system, to achieve its goals. This, however, implies limitations on the possible motion patterns of the robots, thus reducing the flexibility of the overall multi-robot system. In this paper we introduce teleoperation as a means to reduce these limitations, allowing temporary violations of topological properties, with the aim of increasing the flexibility of the multi-robot system.
ER  - 

TY  - CONF
TI  - Eciton robotica: Design and Algorithms for an Adaptive Self-Assembling Soft Robot Collective
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4565
EP  - 4571
AU  - M. Malley
AU  - B. Haghighat
AU  - L. Houe
AU  - R. Nagpal
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - search problems
KW  - self-adjusting systems
KW  - Eciton robotica
KW  - self-assembling soft robot collective
KW  - social insects
KW  - centralized control system
KW  - army ants build bridges
KW  - flexible materials
KW  - robotic collectives
KW  - flexible robots
KW  - self-assembling robotic systems
KW  - lattice-based structures
KW  - soft robots
KW  - amorphous structures
KW  - Robot sensing systems
KW  - Bridges
KW  - Grippers
KW  - Vibrations
KW  - Self-assembly
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9196565
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Social insects successfully create bridges, rafts, nests and other structures out of their own bodies and do so with no centralized control system, simply by following local rules. For example, while traversing rough terrain, army ants (genus Eciton) build bridges which grow and dissolve in response to local traffic. Because these self-assembled structures incorporate smart, flexible materials (i.e. ant bodies) and emerge from local behavior, the bridges are adaptive and dynamic. With the goal of realizing robotic collectives with similar features, we designed a hardware system, Eciton robotica, consisting of flexible robots that can climb over each other to assemble compliant structures and communicate locally using vibration. In simulation, we demonstrate self-assembly of structures: using only local rules and information, robots build and dissolve bridges in response to local traffic and varying terrain. Unlike previous self-assembling robotic systems that focused on lattice-based structures and predetermined shapes, our system takes a new approach where soft robots attach to create amorphous structures whose final self-assembled shape can adapt to the needs of the group.
ER  - 

TY  - CONF
TI  - Stable Tool-Use with Flexible Musculoskeletal Hands by Learning the Predictive Model of Sensor State Transition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4572
EP  - 4578
AU  - K. Kawaharazuka
AU  - K. Tsuzuki
AU  - M. Onitsuka
AU  - Y. Asano
AU  - K. Okada
AU  - K. Kawasaki
AU  - M. Inaba
PY  - 2020
KW  - feedback
KW  - feedforward
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - materials handling
KW  - sensors
KW  - feedforward controls
KW  - initial contact state
KW  - predictive network
KW  - sensor state transition
KW  - actual robot sensor information
KW  - feedback control
KW  - stable tool-use
KW  - flexible musculoskeletal hands
KW  - predictive model
KW  - adaptability
KW  - impact resistance
KW  - sensors
KW  - actuators
KW  - Grasping
KW  - Muscles
KW  - Robot sensing systems
KW  - Gold
KW  - Tools
DO  - 10.1109/ICRA40945.2020.9197188
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The flexible under-actuated musculoskeletal hand is superior in its adaptability and impact resistance. On the other hand, since the relationship between sensors and actuators cannot be uniquely determined, almost all its controls are based on feedforward controls. When grasping and using a tool, the contact state of the hand gradually changes due to the inertia of the tool or impact of action, and the initial contact state is hardly kept. In this study, we propose a system that trains the predictive network of sensor state transition using the actual robot sensor information, and keeps the initial contact state by a feedback control using the network. We conduct experiments of hammer hitting, vacuuming, and brooming, and verify the effectiveness of this study.
ER  - 

TY  - CONF
TI  - Learning to Transfer Dynamic Models of Underactuated Soft Robotic Hands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4579
EP  - 4585
AU  - L. Schramm
AU  - A. Sintov
AU  - A. Boularias
PY  - 2020
KW  - dexterous manipulators
KW  - learning (artificial intelligence)
KW  - Lyapunov methods
KW  - neural nets
KW  - underactuated soft robotic hands
KW  - transfer learning
KW  - data limitations
KW  - data collection
KW  - physical robots
KW  - neural networks
KW  - transferred model
KW  - trained transition model
KW  - dynamic model
KW  - chaotic behavior
KW  - divergent behavior
KW  - upper bound
KW  - Lyapunov exponent
KW  - Adaptation models
KW  - Robots
KW  - Data models
KW  - Neural networks
KW  - Analytical models
KW  - Friction
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9197300
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Transfer learning is a popular approach to bypassing data limitations in one domain by leveraging data from another domain. This is especially useful in robotics, as it allows practitioners to reduce data collection with physical robots, which can be time-consuming and cause wear and tear. The most common way of doing this with neural networks is to take an existing neural network, and simply train it more with new data. However, we show that in some situations this can lead to significantly worse performance than simply using the transferred model without adaptation. We find that a major cause of these problems is that models trained on small amounts of data can have chaotic or divergent behavior in some regions. We derive an upper bound on the Lyapunov exponent of a trained transition model, and demonstrate two approaches that make use of this insight. Both show significant improvement over traditional fine-tuning. Experiments performed on real underactuated soft robotic hands clearly demonstrate the capability to transfer a dynamic model from one hand to another.
ER  - 

TY  - CONF
TI  - Periodic movement learning in a soft-robotic arm*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4586
EP  - 4592
AU  - P. Oikonomou
AU  - M. Khamassi
AU  - C. S. Tzafestas
PY  - 2020
KW  - end effectors
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - motion control
KW  - trajectory control
KW  - cyclic rhythmic patterns
KW  - oscillatory signals
KW  - actuator
KW  - central pattern generator
KW  - periodic motion
KW  - end-effector
KW  - model-free neurodynamic scheme
KW  - CPG model
KW  - simulation model
KW  - learning architecture
KW  - periodic movement learning
KW  - modular bio-inspired soft-robotic arm
KW  - Oscillators
KW  - Trajectory
KW  - Manipulators
KW  - Biological system modeling
KW  - Soft robotics
KW  - Mathematical model
KW  - Reinforcement learning
KW  - Central pattern generators
KW  - Soft Robotics
KW  - Rhythmic movements
DO  - 10.1109/ICRA40945.2020.9197035
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we introduce a novel technique that aims to dynamically control a modular bio-inspired soft-robotic arm in order to perform cyclic rhythmic patterns. Oscillatory signals are produced at the actuator's level by a central pattern generator (CPG), resulting in the generation of a periodic motion by the robot's end-effector. The proposed controller is based on a model-free neurodynamic scheme and is assigned with the task of training a policy that computes the parameters of the CPG model which generates a trajectory with desired features. The proposed methodology is first evaluated with a simulation model, which successfully reproduces the trained targets. Then experiments are also conducted using the real robot. Both procedures validate the efficiency of the learning architecture to successfully complete these tasks.
ER  - 

TY  - CONF
TI  - Mechanism and Model of a Soft Robot for Head Stabilization in Cancer Radiation Therapy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4609
EP  - 4615
AU  - O. Ogunmolu
AU  - X. Liu
AU  - N. Gans
AU  - R. D. Wiersma
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - cancer
KW  - deformation
KW  - elastic deformation
KW  - elasticity
KW  - electric actuators
KW  - finite element analysis
KW  - kinematics
KW  - medical image processing
KW  - medical robotics
KW  - radiation therapy
KW  - robot kinematics
KW  - stress-strain relations
KW  - soft robot
KW  - head stabilization
KW  - cancer radiation
KW  - parallel robot mechanism
KW  - constituent soft actuators
KW  - real-time motion-correction
KW  - treatment machine
KW  - stress-strain constitutive laws
KW  - inverse kinematics
KW  - radially symmetric displacement formulation
KW  - finite elastic deformation framework
KW  - Strain
KW  - Actuators
KW  - Adaptation models
KW  - Stress
KW  - Real-time systems
KW  - Robots
KW  - Cancer
DO  - 10.1109/ICRA40945.2020.9197007
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a parallel robot mechanism and the constitutive laws that govern the deformation of its constituent soft actuators. Our ultimate goal is the real-time motion-correction of a patient's head deviation from a target pose where the soft actuators control the position of the patient's cranial region on a treatment machine. We describe the mechanism, derive the stress-strain constitutive laws for the individual actuators and the inverse kinematics that prescribes a given deformation, and then present simulation results that validate our mathematical formulation. Our results demonstrate deformations consistent with our radially symmetric displacement formulation under a finite elastic deformation framework.
ER  - 

TY  - CONF
TI  - Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4616
EP  - 4622
AU  - I. Akinola
AU  - J. Varley
AU  - D. Kalashnikov
PY  - 2020
KW  - calibration
KW  - cameras
KW  - image colour analysis
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - multiple depth sensors
KW  - imperfect camera calibration
KW  - uncalibrated cameras
KW  - camera-views
KW  - single view robotic agents
KW  - voxel grid
KW  - relative pose estimation
KW  - 3D scene representations
KW  - registered output
KW  - explicit 3D representations
KW  - sensor dropout
KW  - insertion tasks
KW  - task performance
KW  - multicamera approach
KW  - uncalibrated RGB camera
KW  - precise manipulation tasks
KW  - closed-loop end-to-end learning
KW  - multiview approach
KW  - multiple uncalibrated cameras
KW  - precise 3D manipulation
KW  - Task analysis
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197181
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we present an effective multi-view approach to closed-loop end-to-end learning of precise manipulation tasks that are 3D in nature. Our method learns to accomplish these tasks using multiple statically placed but uncalibrated RGB camera views without building an explicit 3D representation such as a pointcloud or voxel grid. This multi-camera approach achieves superior task performance on difficult stacking and insertion tasks compared to single-view baselines. Single view robotic agents struggle from occlusion and challenges in estimating relative poses between points of interest. While full 3D scene representations (voxels or pointclouds) are obtainable from registered output of multiple depth sensors, several challenges complicate operating off such explicit 3D representations. These challenges include imperfect camera calibration, poor depth maps due to object properties such as reflective surfaces, and slower inference speeds over 3D representations compared to 2D images. Our use of static but uncalibrated cameras does not require camera-robot or camera-camera calibration making the proposed approach easy to setup and our use of sensor dropout during training makes it resilient to the loss of camera-views after deployment.
ER  - 

TY  - CONF
TI  - Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4623
EP  - 4629
AU  - A. Strai≈æys
AU  - M. Burke
AU  - S. Ramamoorthy
PY  - 2020
KW  - biological tissues
KW  - closed loop systems
KW  - image classification
KW  - manipulator dynamics
KW  - medical robotics
KW  - robot vision
KW  - surgery
KW  - torque measurement
KW  - joint torque measurements
KW  - closed loop control law
KW  - grapefruit cutting task
KW  - grapefruit pulp
KW  - uncertain edge
KW  - precision cutting
KW  - soft tissue
KW  - torque-based medium classification
KW  - visibility constraints
KW  - cutting trajectory
KW  - binary medium classifier
KW  - robotics
KW  - Task analysis
KW  - Trajectory
KW  - Pipelines
KW  - Torque
KW  - Robot sensing systems
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196623
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory t using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control strategy is successful in 72 % of attempts in contrast to control using a nominal trajectory, which only succeeds in 50 % of attempts.
ER  - 

TY  - CONF
TI  - Dynamic Cloth Manipulation with Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4630
EP  - 4636
AU  - R. Jangir
AU  - G. Aleny√†
AU  - C. Torras
PY  - 2020
KW  - clothing
KW  - learning (artificial intelligence)
KW  - sparse reward learning techniques
KW  - deep reinforcement learning approach
KW  - dynamic cloth manipulation tasks
KW  - rigid objects
KW  - followed trajectory
KW  - grasped points
KW  - goal positions
KW  - nongrasped points
KW  - adequate trajectories
KW  - control policy learning
KW  - sparse reward approach
KW  - engineering complex reward functions
KW  - state representations
KW  - control policy encodings
KW  - Task analysis
KW  - Manipulator dynamics
KW  - Trajectory
KW  - Textiles
KW  - Deformable models
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196659
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present a Deep Reinforcement Learning approach to solve dynamic cloth manipulation tasks. Differing from the case of rigid objects, we stress that the followed trajectory (including speed and acceleration) has a decisive influence on the final state of cloth, which can greatly vary even if the positions reached by the grasped points are the same. We explore how goal positions for non-grasped points can be attained through learning adequate trajectories for the grasped points. Our approach uses few demonstrations to improve control policy learning, and a sparse reward approach to avoid engineering complex reward functions. Since perception of textiles is challenging, we also study different state representations to assess the minimum observation space required for learning to succeed. Finally, we compare different combinations of control policy encodings, demonstrations, and sparse reward learning techniques, and show that our proposed approach can learn dynamic cloth manipulation in an efficient way, i.e., using a reduced observation space, a few demonstrations, and a sparse reward.
ER  - 

TY  - CONF
TI  - Learning to combine primitive skills: A step towards versatile robotic manipulation ¬ß
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4637
EP  - 4643
AU  - R. Strudel
AU  - A. Pashevich
AU  - I. Kalevatykh
AU  - I. Laptev
AU  - J. Sivic
AU  - C. Schmid
PY  - 2020
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - path planning
KW  - robot vision
KW  - dynamic scene changes
KW  - visual inputs
KW  - task-specific reward engineering
KW  - previous limitations
KW  - reinforcement learning approach
KW  - primitive skills
KW  - learning methods
KW  - intermediate rewards
KW  - complete task demonstrations
KW  - vision-based task planning
KW  - basic skills
KW  - synthetic demonstrations
KW  - data augmentation
KW  - manipulation tasks
KW  - UR5 robotic arm
KW  - versatile robotic manipulation
KW  - robotics
KW  - traditional task
KW  - motion planning methods
KW  - state observability
KW  - Task analysis
KW  - Robots
KW  - Planning
KW  - Learning (artificial intelligence)
KW  - Training
KW  - Trajectory
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196619
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Manipulation tasks such as preparing a meal or assembling furniture remain highly challenging for robotics and vision. Traditional task and motion planning (TAMP) methods can solve complex tasks but require full state observability and are not adapted to dynamic scene changes. Recent learning methods can operate directly on visual inputs but typically require many demonstrations and/or task-specific reward engineering. In this work we aim to overcome previous limitations and propose a reinforcement learning (RL) approach to task planning that learns to combine primitive skills. First, compared to previous learning methods, our approach requires neither intermediate rewards nor complete task demonstrations during training. Second, we demonstrate the versatility of our vision-based task planning in challenging settings with temporary occlusions and dynamic scene changes. Third, we propose an efficient training of basic skills from few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Notably, while all of our policies are learned on visual inputs in simulated environments, we demonstrate the successful transfer and high success rates when applying such policies to manipulation tasks on a real UR5 robotic arm.
ER  - 

TY  - CONF
TI  - Learning Affordance Space in Physical World for Vision-based Robotic Object Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4652
EP  - 4658
AU  - H. Wu
AU  - Z. Zhang
AU  - H. Cheng
AU  - K. Yang
AU  - J. Liu
AU  - Z. Guo
PY  - 2020
KW  - image texture
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - probability
KW  - robot vision
KW  - pixel-wise probability affordance map
KW  - image space
KW  - world space
KW  - viewpoints
KW  - multiple-object pushing
KW  - multiple-object grasping
KW  - physical world
KW  - vision-based robotic object manipulation
KW  - Affordance Space Perception Network
KW  - deep neural network
KW  - 3D affordance space
KW  - training strategy
KW  - task-agnostic framework
KW  - singular-object pushing
KW  - singular-object grasping
KW  - Robots
KW  - Task analysis
KW  - Robustness
KW  - Grasping
KW  - Data models
KW  - Calibration
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9196783
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - What is a proper representation for objects in manipulation? What would human try to perceive when manipulating a new object in a new environment? In fact, instead of focusing on the texture and illumination, human can infer the "affordance" [36] of the objects from vision. Here "affordance" describes the object's intrinsic property that affords a particular type of manipulation. In this work, we investigate whether such affordance can be learned by a deep neural network. In particular, we propose an Affordance Space Perception Network (ASPN) that takes an image as input and outputs an affordance map. Different from existing works that infer the pixel-wise probability affordance map in image space, our affordance is defined in the real world space, thus eliminates the need of hand-eye calibration. In addition, we extend the representation ability of affordance by defining it in a 3D affordance space and propose a novel training strategy to improve the performance. Trained purely with simulation data, ASPN can achieve significant performance in the real world. It is a task-agnostic framework and can handle different objects, scenes and viewpoints. Extensive real-world experiments demonstrate the accuracy and robustness of our approach. We achieve the success rates of 94.2% for singular-object pushing and 92.4% for multiple-object pushing. We also achieve the success rates of 97.2% for singular-object grasping and 95.4% for multiple-object grasping, which outperform current state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Observability Analysis of Flight State Estimation for UAVs and Experimental Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4659
EP  - 4665
AU  - P. Huang
AU  - H. Meyr
AU  - M. D√∂rpinghaus
AU  - G. Fettweis
PY  - 2020
KW  - inertial systems
KW  - Kalman filters
KW  - magnetic sensors
KW  - nonlinear filters
KW  - observability
KW  - pressure sensors
KW  - remotely operated vehicles
KW  - singular value decomposition
KW  - state estimation
KW  - UAV
KW  - cost-efficient onboard flight state estimation
KW  - robustness
KW  - MEMS-based inertial system
KW  - static pressure sensors
KW  - dynamic pressure sensors
KW  - magnetic sensor
KW  - weak magnetic field
KW  - necessary condition
KW  - system state
KW  - in-depth observability analysis
KW  - sensor data
KW  - test flights
KW  - EKF
KW  - undisturbed estimates
KW  - wind state variable
KW  - observable spaces
KW  - multisensor extended Kalman filter
KW  - singular value decomposition
KW  - SVD
KW  - glider
KW  - Observability
KW  - Mathematical model
KW  - Aerodynamics
KW  - State estimation
KW  - Global Positioning System
KW  - Magnetometers
KW  - Pressure measurement
DO  - 10.1109/ICRA40945.2020.9196635
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - UAVs require reliable, cost-efficient onboard flight state estimation that achieves high accuracy and robustness to perturbation. We analyze a multi-sensor extended Kalman filter (EKF) based on the work by Leutenegger. The EKF uses measurements from a MEMS-based inertial system, static and dynamic pressure sensors as well as GPS. As opposed to other implementations we do not use a magnetic sensor because the weak magnetic field of the earth is subject to disturbances. Observability of the state is a necessary condition for the EKF to work. In this paper, we demonstrate that the system state is observable - which is in contrast to statements in the literature - if the random nature of the air mass is taken into account. Therefore, we carry out an in-depth observability analysis based on a singular value decomposition (SVD). The numerical SVD delivers a wealth of information regarding the observable (sub)spaces. We validated the theoretical findings based on sensor data recorded in test flights on a glider. Most importantly, we demonstrate that the EKF works. It is capable of absorbing large perturbations in the wind state variable converging to the undisturbed estimates.
ER  - 

TY  - CONF
TI  - OpenVINS: A Research Platform for Visual-Inertial Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4666
EP  - 4672
AU  - P. Geneva
AU  - K. Eckenhoff
AU  - W. Lee
AU  - Y. Yang
AU  - G. Huang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - estimation theory
KW  - image filtering
KW  - Kalman filters
KW  - robot vision
KW  - SLAM (robots)
KW  - research platform
KW  - visual-inertial estimation research
KW  - open sourced codebase
KW  - visual-inertial systems
KW  - visual-inertial estimation features
KW  - on-manifold sliding window Kalman filter
KW  - consistent First-Estimates Jacobian treatments
KW  - modular type system
KW  - extendable visual-inertial system simulator
KW  - competing estimation performance
KW  - OpenVINS
KW  - online camera intrinsic calibration
KW  - open sourced algorithms
KW  - online camera extrinsic calibration
KW  - inertial sensor time offset calibration
KW  - SLAM landmarks
KW  - state management
KW  - Cameras
KW  - Current measurement
KW  - Jacobian matrices
KW  - Calibration
KW  - Documentation
KW  - Estimation
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9196524
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an open platform, termed OpenVINS, for visual-inertial estimation research for both the academic community and practitioners from industry. The open sourced codebase provides a foundation for researchers and engineers to quickly start developing new capabilities for their visual-inertial systems. This codebase has out of the box support for commonly desired visual-inertial estimation features, which include: (i) on-manifold sliding window Kalman filter, (ii) online camera intrinsic and extrinsic calibration, (iii) camera to inertial sensor time offset calibration, (iv) SLAM landmarks with different representations and consistent First-Estimates Jacobian (FEJ) treatments, (v) modular type system for state management, (vi) extendable visual-inertial system simulator, and (vii) extensive toolbox for algorithm evaluation. Moreover, we have also focused on detailed documentation and theoretical derivations to support rapid development and research, which are greatly lacked in the current open sourced algorithms. Finally, we perform comprehensive validation of the proposed OpenVINS against state-of-the-art open sourced algorithms, showing its competing estimation performance.
ER  - 

TY  - CONF
TI  - Decentralized Collaborative State Estimation for Aided Inertial Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4673
EP  - 4679
AU  - R. Jung
AU  - C. Brommer
AU  - S. Weiss
PY  - 2020
KW  - communication complexity
KW  - inertial navigation
KW  - Kalman filters
KW  - nonlinear filters
KW  - position measurement
KW  - state estimation
KW  - communication links
KW  - versatile filter formulation
KW  - independent state estimation
KW  - relative position measurements
KW  - aided inertial navigation
KW  - Q-ESEKF
KW  - IMU propagation
KW  - communication complexity
KW  - decentralized collaborative state estimation
KW  - quaternion-based error-state extended Kalman filter
KW  - CSE concept
KW  - probabilistic reinitialization
KW  - prominent benchmark datasets
KW  - Cameras
KW  - Sensors
KW  - Collaboration
KW  - State estimation
KW  - Three-dimensional displays
KW  - Calibration
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197178
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a Quaternion-based Error-State Extended Kalman Filter (Q-ESEKF) based on IMU propagation with an extension for Collaborative State Estimation (CSE) and a communication complexity of O(1) (in terms of required communication links). Our approach combines a versatile filter formulation with the concept of CSE, allowing independent state estimation on each of the agents and at the same time leveraging and statistically maintaining interdependencies between agents, after joint measurements and communication (i.e. relative position measurements) occur. We discuss the development of the overall framework and the probabilistic (re-)initialization of the agent's states upon initial or recurring joint observations. Our approach is evaluated in a simulation framework on two prominent benchmark datasets in 3D.
ER  - 

TY  - CONF
TI  - Analytic Combined IMU Integration (ACI2) For Visual Inertial Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4680
EP  - 4686
AU  - Y. Yang
AU  - B. P. Wisely Babu
AU  - C. Chen
AU  - G. Huang
AU  - L. Ren
PY  - 2020
KW  - calibration
KW  - inertial navigation
KW  - maximum likelihood estimation
KW  - Monte Carlo methods
KW  - optimisation
KW  - robot vision
KW  - sensor fusion
KW  - SLAM (robots)
KW  - analytic combined IMU integration
KW  - visual inertial navigation
KW  - batch optimization
KW  - visual sensor fusion
KW  - robotic tasks
KW  - maximum likelihood estimation
KW  - partial-fixed estimates
KW  - ACI2
KW  - inertial measurement unit
KW  - Monte-Carlo simulations
KW  - Optimization
KW  - Jacobian matrices
KW  - Maximum likelihood estimation
KW  - Time measurement
KW  - Cameras
KW  - Visualization
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9197280
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Batch optimization based inertial measurement unit (IMU) and visual sensor fusion enables high rate localization for many robotic tasks. However, it remains a challenge to ensure that the batch optimization is computationally efficient while being consistent for high rate IMU measurements without marginalization. In this paper, we derive inspiration from maximum likelihood estimation with partial-fixed estimates to provide a unified approach for handing both IMU preintegration and time-offset calibration. We present a modularized analytic combined IMU integrator (ACI2) with elegant derivations for IMU integrations, bias Jabcobians and related covariances. To simplify our derivation, we also prove that the right Jacobians for Hamilton quaterions and SO(3) are equivalent. Finally, we present a time offset calibrator that operates by fixing the linearization point for a given time offset. This reduces re-integration of the IMU measurements and thus improve efficiency. The proposed ACI2 and time-offset calibration is verified by intensive Monte-Carlo simulations generated from real world datasets. A proof-of-concept real world experiment is also conducted to verify the proposed ACI2 estimator.
ER  - 

TY  - CONF
TI  - Second-order Kinematics for Floating-base Robots using the Redundant Acceleration Feedback of an Artificial Sensory Skin
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4687
EP  - 4694
AU  - Q. Leboutet
AU  - J. R. Guadarrama-Olvera
AU  - F. Bergner
AU  - G. Cheng
PY  - 2020
KW  - calibration
KW  - estimation theory
KW  - feedback
KW  - humanoid robots
KW  - Kalman filters
KW  - manipulator kinematics
KW  - motion control
KW  - redundant manipulators
KW  - floating-base robots
KW  - redundant acceleration feedback
KW  - artificial sensory skin
KW  - estimation method
KW  - second-order kinematics
KW  - highly redundant distributed inertial feedback
KW  - linear acceleration
KW  - robot link
KW  - skin acceleration data
KW  - link level
KW  - state dimensionality reduction
KW  - main inertial measurement unit
KW  - Sigma-point Kalman filter
KW  - joint velocities
KW  - REEM-C humanoid robot
KW  - Acceleration
KW  - Robot sensing systems
KW  - Skin
KW  - Gyroscopes
KW  - Accelerometers
KW  - Acceleration Feedback
KW  - Artificial Robot Skin
KW  - Sigma-point Kalman Filter
DO  - 10.1109/ICRA40945.2020.9197169
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose a new estimation method for second-order kinematics for floating-base robots, based on highly redundant distributed inertial feedback. The linear acceleration of each robot link is measured at multiple points using a multimodal, self-configuring and self-calibrating artificial skin. The proposed algorithm is two-fold: i) the skin acceleration data is fused at the link level for state dimensionality reduction; ii) the estimated values are then fused limb-wise with data from the joint encoders and the main inertial measurement unit (IMU), using a Sigma-point Kalman filter. In this manner, it is possible to estimate the joint velocities and accelerations while avoiding the lag and noise amplification phenomena associated with conventional numerical derivation approaches. Experiments performed on the right arm and torso of a REEM-C humanoid robot, demonstrate the consistency of the proposed estimation method.
ER  - 

TY  - CONF
TI  - Clock-based time sync hronization for an event-based camera dataset acquisition platform *
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4695
EP  - 4701
AU  - V. Osadcuks
AU  - M. Pudzs
AU  - A. Zujevs
AU  - A. Pecka
AU  - A. Ardavs
PY  - 2020
KW  - cameras
KW  - data acquisition
KW  - image sensors
KW  - microcontrollers
KW  - mobile robots
KW  - optical radar
KW  - robot vision
KW  - synchronisation
KW  - monocular camera
KW  - mobile robotic platform
KW  - time synchronization architecture
KW  - time synchronization approach
KW  - accurate time synchronization
KW  - event-based camera dataset acquisition platform
KW  - dynamic visual sensor
KW  - next-generation vision sensor
KW  - event-based vision
KW  - dataset creation
KW  - temporal accuracy
KW  - high temporal resolution
KW  - evaluation task
KW  - visual data
KW  - event camera
KW  - ambient environment sensors
KW  - data acquisition
KW  - clock-based time synchronization
KW  - LIDAR
KW  - PIC32 microcontroller
KW  - Synchronization
KW  - Robot sensing systems
KW  - Cameras
KW  - Laser radar
KW  - Clocks
KW  - Voltage control
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9197303
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The Dynamic Visual Sensor is considered to be a next-generation vision sensor. Since event-based vision is in its early stage of development, a small number of datasets has been created during the last decade. Dataset creation is motivated by the need for real data from one or many sensors. Temporal accuracy of data in such datasets is crucially important since the events have high temporal resolution measured in microseconds and, during an algorithm evaluation task, such type of visual data is usually fused with data from other types of sensors. The main aim of our research is to achieve the most accurate possible time synchronization between an event camera, LIDAR, and ambient environment sensors during a session of data acquisition. All the mentioned sensors as well as a stereo and a monocular camera were installed on a mobile robotic platform. In this work, a time synchronization architecture and algorithm are proposed for time synchronization with an implementation example on a PIC32 microcontroller. The overall time synchronization approach is scalable for other sensors where there is a need for accurate time synchronization between many nodes. The evaluation results of the proposed solution are reported and discussed in the paper.
ER  - 

TY  - CONF
TI  - Model Predictive Impedance Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4702
EP  - 4708
AU  - M. Bednarczyk
AU  - H. Omran
AU  - B. Bayle
PY  - 2020
KW  - human-robot interaction
KW  - predictive control
KW  - collaborative robotics
KW  - high performance control
KW  - model predictive impedance control
KW  - human robot compliant interactions
KW  - Robots
KW  - Integrated circuit modeling
KW  - Impedance
KW  - Mathematical model
KW  - Task analysis
KW  - Collaboration
KW  - Impedance control
KW  - collaborative robotics
KW  - physical human-robot interaction
DO  - 10.1109/ICRA40945.2020.9196969
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots are more and more often designed in order to perform tasks in synergy with human operators. In this context, a current research focus for collaborative robotics lies in the design of high-performance control solutions, which ensure security in spite of unmodeled external forces. The present work provides a method based on Model Predictive Control (MPC) to allow compliant behavior when interacting with an environment, while respecting practical robotic constraints. The study shows in particular how to define the impedance control problem as a MPC problem. The approach is validated with an experimental setup including a collaborative robot. The obtained results emphasize the ability of this control strategy to solve constraints like speed, energy or jerk limits, which have a direct impact on the operator's security during human-robot compliant interactions.
ER  - 

TY  - CONF
TI  - Kinematic Modeling and Compliance Modulation of Redundant Manipulators Under Bracing Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4709
EP  - 4716
AU  - G. L. H. Johnston
AU  - A. L. Orekhov
AU  - N. Simaan
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - design engineering
KW  - dexterous manipulators
KW  - end effectors
KW  - manipulator kinematics
KW  - motion control
KW  - redundant manipulators
KW  - kinematic modeling
KW  - compliance modulation
KW  - redundant manipulators
KW  - bracing constraints
KW  - low torque actuators
KW  - passive safety reasons
KW  - human operator
KW  - in-situ collaborative robots
KW  - conflicting demands
KW  - low torque actuation
KW  - deep confined spaces
KW  - constrained kinematics
KW  - endeffector compliance
KW  - redundancy resolution framework
KW  - directional compliance
KW  - end-effector dexterity
KW  - kinematic simulation results
KW  - redundancy resolution strategy
KW  - kinematic conditioning
KW  - bracing task
KW  - admittance control framework
KW  - collaborative control
KW  - ISCR
KW  - Kinematics
KW  - Robots
KW  - Redundancy
KW  - Task analysis
KW  - Collaboration
KW  - Torque
KW  - Safety
KW  - Bracing
KW  - redundancy resolution
KW  - stiffness modulation
KW  - compliance
KW  - collaborative robots
DO  - 10.1109/ICRA40945.2020.9197387
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collaborative robots should ideally use low torque actuators for passive safety reasons. However, some applications require these collaborative robots to reach deep into confined spaces while assisting a human operator in physically demanding tasks. In this paper, we consider the use of in-situ collaborative robots (ISCRs) that balance the conflicting demands of passive safety dictating low torque actuation and the need to reach into deep confined spaces. We consider the judicious use of bracing as a possible solution to these conflicting demands and present a modeling framework that takes into account the constrained kinematics and the effect of bracing on the endeffector compliance. We then define a redundancy resolution framework that minimizes the directional compliance of the end-effector while maximizing end-effector dexterity. Kinematic simulation results show that the redundancy resolution strategy successfully decreases compliance and improves kinematic conditioning while satisfying the constraints imposed by the bracing task. Applications of this modeling framework can support future research on the choice of bracing locations and support the formation of an admittance control framework for collaborative control of ISCRs under bracing constraints. Such robots can benefit workers in the future by reducing the physiological burdens that contribute to musculoskeletal injury.
ER  - 

TY  - CONF
TI  - Successive Stiffness Increment and Time Domain Passivity Approach for Stable and High Bandwidth Control of Series Elastic Actuator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4717
EP  - 4723
AU  - C. Lee
AU  - D. -H. Kim
AU  - H. Singh
AU  - J. -H. Ryu
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - flexible manipulators
KW  - force control
KW  - haptic interfaces
KW  - human-robot interaction
KW  - impact (mechanical)
KW  - stability
KW  - telerobotics
KW  - time domain passivity approach
KW  - bandwidth control
KW  - human-robot interaction
KW  - flexible manipulators
KW  - series elastic actuator based manipulators
KW  - elastic element
KW  - system durability
KW  - actuators
KW  - SEA manipulator
KW  - system bandwidth
KW  - impedance control
KW  - system stability
KW  - successive stiffness increment approach
KW  - haptic domain
KW  - TDPA
KW  - system passivity
KW  - impact force
KW  - teleoperation
KW  - two-port electrical circuit network
KW  - size 350.0 m
KW  - size 120.0 m
KW  - Impedance
KW  - Bandwidth
KW  - Force
KW  - Actuators
KW  - Torque
KW  - Manipulators
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196995
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For safe human-robot interaction, various type of flexible manipulators have been developed. Especially series elastic actuator (SEA) based manipulators have been getting huge attention since the elastic element of SEA prevents people from injury when undesirable collision happens. Moreover, it improves system durability by absorbing impact force, which could damage actuators. However, the elastic element inside SEA manipulator causes low system bandwidth which limits the speed performance of conventional impedance control approaches. To alleviate the low bandwidth issue of impedance controlled SEA while guaranteeing system stability, we implement Time Domain Passivity Approach (TDPA) and Successive Stiffness Increment (SSI) approach, which was invented in haptic and teleoperation domain. Impedance controlled SEA is reformulated as a two-port electrical circuit network for implementing TDPA. In addition, a pair of input and output power conjugate variable, dominating the system passivity is identified for implementing SSI approach. Experimental results showed that TDPA and SSI approach can render the stiffness of the impedance controller, which decides the bandwidth, upto 350 kN/m without any stability issue, while normal impedance controller only render upto 120 kN/m. Although both of the approaches significantly increased the bandwidth of the impedance controlled SEA, TDPA slightly outperformed in stability, and SSI outperformed in tracking.
ER  - 

TY  - CONF
TI  - Arm-hand motion-force coordination for physical interactions with non-flat surfaces using dynamical systems: Toward compliant robotic massage
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4724
EP  - 4730
AU  - M. Khoramshahi
AU  - G. Henriks
AU  - A. Naef
AU  - S. S. M. Salehian
AU  - J. Kim
AU  - A. Billard
PY  - 2020
KW  - biomechanics
KW  - dexterous manipulators
KW  - force control
KW  - motion control
KW  - path planning
KW  - regression analysis
KW  - support vector machines
KW  - unified motion-force control approach
KW  - human limb
KW  - compliant robotic massage
KW  - dynamical system approach
KW  - skin surface
KW  - robot fingers
KW  - complexity increases
KW  - manipulation tasks
KW  - dynamical systems
KW  - nonflat surface
KW  - physical interactions
KW  - arm-hand motion-force coordination
KW  - desired motion patterns
KW  - unknown surface
KW  - mannequin arm
KW  - Allegro robotic hand
KW  - KUKA IIWA robotic arm
KW  - robotic fingers
KW  - DS-based impedance control
KW  - desired motions
KW  - distance-to-surface mapping
KW  - Surface impedance
KW  - Robot kinematics
KW  - Task analysis
KW  - Force
KW  - Manipulators
KW  - Thumb
DO  - 10.1109/ICRA40945.2020.9196593
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many manipulation tasks require coordinated motions for arm and fingers. Complexity increases when the task requires to control for the force at contact against a non-flat surface; This becomes even more challenging when this contact is done on a human. All these challenges are regrouped when one, for instance, massages a human limb. When massaging, the robotic arm is required to continuously adapt its orientation and distance to the limb while the robot fingers exert desired patterns of forces and motion on the skin surface. To address these challenges, we adopt a Dynamical System (DS) approach that offers a unified motion-force control approach and enables to easily coordinate multiple degrees of freedom. As each human limb may slightly differ, we learn a model of the surface using support vector regression (SVR) which enable us to obtain a distance-to-surface mapping. The gradient of this mapping, along with the DS, generates the desired motions for the interaction with the surface. A DS-based impedance control for the robotic fingers allows to control separately for force along the normal direction of the surface while moving in the tangential plane. We validate our approach using the KUKA IIWA robotic arm and Allegro robotic hand for massaging a mannequin arm covered with a skin-like material. We show that our approach allows for 1) reactive motion planning to reach for an unknown surface, 2) following desired motion patterns on the surface, and 3) exerting desired interaction forces profiles. Our results show the effectiveness of our approach; especially the robustness toward uncertainties for shape and the given location of the surface.
ER  - 

TY  - CONF
TI  - A Bio-Signal Enhanced Adaptive Impedance Controller for Lower Limb Exoskeleton
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4739
EP  - 4744
AU  - L. Xia
AU  - Y. Feng
AU  - F. Chen
AU  - X. Wu
PY  - 2020
KW  - adaptive control
KW  - biomechanics
KW  - electromyography
KW  - gait analysis
KW  - medical robotics
KW  - medical signal processing
KW  - motion control
KW  - neurocontrollers
KW  - patient rehabilitation
KW  - radial basis function networks
KW  - robot dynamics
KW  - torque control
KW  - unpredictable human body movements
KW  - complex body movements
KW  - elaborate control strategy design
KW  - uncertain dynamical parameters
KW  - human-exoskeleton interaction
KW  - lower limb exoskeleton
KW  - bio-signal enhanced adaptive impedance controller
KW  - rehabilitation lower-limb exoskeleton
KW  - exoskeleton track desired motion trajectory
KW  - radial basis function neural network enhanced adaptive impedance controller
KW  - surface electromyogram signals
KW  - neural network-based torque estimation method
KW  - joint torque
KW  - human lower extremity dynamics
KW  - human operator walking
KW  - Exoskeletons
KW  - Torque
KW  - Estimation
KW  - Muscles
KW  - Impedance
KW  - Artificial neural networks
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196774
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The problem of human-exoskeleton interaction with uncertain dynamical parameters remains an open-ended research area. It requires an elaborate control strategy design of the exoskeleton to accommodate complex and unpredictable human body movements. In this paper, we proposed a novel control approach for the lower limb exoskeleton to realize its task of assisting the human operator walking. The main challenge of this study was to determine the human lower extremity dynamics, such as the joint torque. For this purpose, we developed a neural network-based torque estimation method. It can predict the joint torques of humans with surface electromyogram signals (sEMG). Then an radial basis function neural network (RBF NN) enhanced adaptive impedance controller is employed to ensure exoskeleton track desired motion trajectory of a human operator. Algorithm performance is evaluated with two healthy subjects and the rehabilitation lower-limb exoskeleton developed by Shenzhen Institutes of Advanced Technology (SIAT).
ER  - 

TY  - CONF
TI  - Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4753
EP  - 4759
AU  - P. Karkus
AU  - A. Angelova
AU  - V. Vanhoucke
AU  - R. Jonschkowski
PY  - 2020
KW  - gradient methods
KW  - image representation
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - particle filtering (numerical methods)
KW  - robot vision
KW  - DMN architecture
KW  - end-to-end differentiable
KW  - sparse visual localization
KW  - end-to-end learning
KW  - differentiable mapping network
KW  - spatially structured view-embedding map
KW  - subsequent visual localization
KW  - learning structured map representations
KW  - Street View dataset
KW  - particle filter
KW  - gradient descent
KW  - robotics
KW  - neural network architecture
KW  - Task analysis
KW  - Visualization
KW  - Feature extraction
KW  - Neural networks
KW  - Robot kinematics
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197452
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: https://sites.google.com/view/differentiable-mapping.
ER  - 

TY  - CONF
TI  - Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4760
EP  - 4766
AU  - K. Ramachandruni
AU  - M. Babu V.
AU  - A. Majumder
AU  - S. Dutta
AU  - S. Kumar
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - image representation
KW  - learning (artificial intelligence)
KW  - video signal processing
KW  - task-specific objects
KW  - intended task
KW  - imitation learning
KW  - video demonstration
KW  - end-to-end self-supervised feature representation network
KW  - video-based task imitation
KW  - multilevel spatial attention module
KW  - spatial features
KW  - weighted combination
KW  - multiple intermediate feature maps
KW  - respective feature maps
KW  - metric learning loss
KW  - multiple view points
KW  - AT-Net features
KW  - reinforcement learning problem
KW  - attentive task-net
KW  - self supervised task-attention network
KW  - neural connections
KW  - learning task-specific feature embeddings
KW  - temporally consecutive frames
KW  - publicly available multiview pouring dataset
KW  - RL agent
KW  - Gazebo simulator
KW  - CNN pipeline
KW  - Task analysis
KW  - Measurement
KW  - Robots
KW  - Feature extraction
KW  - Training
KW  - Visualization
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197544
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes an end-to-end self-supervised feature representation network named Attentive Task-Net or AT-Net for video-based task imitation. The proposed AT-Net incorporates a novel multi-level spatial attention module to highlight spatial features corresponding to the intended task demonstrated by the expert. The neural connections in AT-Net ensure the relevant information in the demonstration is amplified and the irrelevant information is suppressed while learning task-specific feature embeddings. This is achieved by a weighted combination of multiple intermediate feature maps of the input image at different stages of the CNN pipeline. The weights of the combination are given by the compatibility scores, predicted by the attention module for respective feature maps. The AT-Net is trained using a metric learning loss which aims to decrease the distance between the feature representations of concurrent frames from multiple view points and increase the distance between temporally consecutive frames. The AT-Net features are then used to formulate a reinforcement learning problem for task imitation. Through experiments on the publicly available Multi-view pouring dataset, it is demonstrated that the output of the attention module highlights the task-specific objects while suppressing the rest of the background. The efficacy of the proposed method is further validated by qualitative and quantitative comparison with a state-of-the-art technique along with intensive ablation studies. The proposed method is implemented to imitate a pouring task where an RL agent is learned with the AT-Net in Gazebo simulator. Our findings show that the AT-Net achieves 6.5% decrease in alignment error along with a reduction in the number of training iterations by almost 155k over the state-of-the-art while satisfactorily imitating the intended task.
ER  - 

TY  - CONF
TI  - OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4767
EP  - 4773
AU  - Q. She
AU  - F. Feng
AU  - X. Hao
AU  - Q. Yang
AU  - C. Lan
AU  - V. Lomonaco
AU  - X. Shi
AU  - Z. Wang
AU  - Y. Guo
AU  - Y. Zhang
AU  - F. Qiao
AU  - R. H. M. Chan
PY  - 2020
KW  - control engineering computing
KW  - data visualisation
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - robot vision
KW  - service robots
KW  - lifelong deep learning
KW  - visual algorithms
KW  - standard computer vision datasets
KW  - adaptive visual perceptual systems
KW  - lifelong robotic vision dataset
KW  - lifelong object recognition algorithms
KW  - lifelong learning algorithms
KW  - OpenLORIS-Object dataset
KW  - object recognition task
KW  - robotic vision
KW  - ImageNet dataset
KW  - COCO dataset
KW  - Task analysis
KW  - Lighting
KW  - Object recognition
KW  - Cameras
KW  - Robot vision systems
KW  - Clutter
DO  - 10.1109/ICRA40945.2020.9196887
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The recent breakthroughs in computer vision have benefited from the availability of large representative datasets (e.g. ImageNet and COCO) for training. Yet, robotic vision poses unique challenges for applying visual algorithms developed from these standard computer vision datasets due to their implicit assumption over non-varying distributions for a fixed set of tasks. Fully retraining models each time a new task becomes available is infeasible due to computational, storage and sometimes privacy issues, while na√Øve incremental strategies have been shown to suffer from catastrophic forgetting. It is crucial for the robots to operate continuously under open-set and detrimental conditions with adaptive visual perceptual systems, where lifelong learning is a fundamental capability. However, very few datasets and benchmarks are available to evaluate and compare emerging techniques. To fill this gap, we provide a new lifelong robotic vision dataset ("OpenLORIS-Object") collected via RGB-D cameras. The dataset embeds the challenges faced by a robot in the real-life application and provides new benchmarks for validating lifelong object recognition algorithms. Moreover, we have provided a testbed of 9 state-of-the-art lifelong learning algorithms. Each of them involves 48 tasks with 4 evaluation metrics over the OpenLORIS-Object dataset. The results demonstrate that the object recognition task in the ever-changing difficulty environments is far from being solved and the bottlenecks are at the forward/backward transfer designs. Our dataset and benchmark are publicly available at https://lifelong-robotic-vision.github.io/dataset/object.
ER  - 

TY  - CONF
TI  - Geometric Pretraining for Monocular Depth Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4782
EP  - 4788
AU  - K. Wang
AU  - Y. Chen
AU  - H. Guo
AU  - L. Wen
AU  - S. Shen
PY  - 2020
KW  - geometry
KW  - image classification
KW  - learning (artificial intelligence)
KW  - video signal processing
KW  - monocular depth estimation
KW  - ImageNet-pretrained networks
KW  - semantic information
KW  - spatial information
KW  - per-pixel depth estimation
KW  - geometric-pretrained networks
KW  - geometric-transferred networks
KW  - self-supervised geometric pretraining task
KW  - conditional autoencoder-decoder structure
KW  - Task analysis
KW  - Estimation
KW  - Videos
KW  - Optical imaging
KW  - Adaptive optics
KW  - Training
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196847
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - ImageNet-pretrained networks have been widely used in transfer learning for monocular depth estimation. These pretrained networks are trained with classification losses for which only semantic information is exploited while spatial information is ignored. However, both semantic and spatial information is important for per-pixel depth estimation. In this paper, we design a novel self-supervised geometric pretraining task that is tailored for monocular depth estimation using uncalibrated videos. The designed task decouples the structure information from input videos by a simple yet effective conditional autoencoder-decoder structure. Using almost unlimited videos from the internet, networks are pretrained to capture a variety of structures of the scene and can be easily transferred to depth estimation tasks using calibrated images. Extensive experiments are used to demonstrate that the proposed geometric-pretrained networks perform better than ImageNet-pretrained networks in terms of accuracy, few-shot learning and generalization ability. Using existing learning methods, geometric-transferred networks achieve new state-of-the-art results by a large margin. The pretrained networks will be open source soon1 .
ER  - 

TY  - CONF
TI  - Joint Rotation Angle Sensing of Flexible Endoscopic Surgical Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4789
EP  - 4795
AU  - W. Lai
AU  - L. Cao
AU  - P. T. Phan
AU  - I. -W. Wu
AU  - S. C. Tjin
AU  - S. Jay Phee
PY  - 2020
KW  - Bragg gratings
KW  - closed loop systems
KW  - endoscopes
KW  - fibre optic sensors
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - surgery
KW  - closed-loop motion control
KW  - robotic endoscopic grasper
KW  - robotic joint
KW  - autonomous robotic surgery
KW  - motion hysteresis
KW  - endoscopic surgical robot
KW  - joint rotation angle sensing
KW  - motion control
KW  - tendon-sheath mechanisms
KW  - Fiber Bragg Grating
KW  - Robot sensing systems
KW  - Optical fiber sensors
KW  - Substrates
KW  - Optical fiber theory
KW  - Medical robotics
KW  - Surgery
KW  - Flexible Robots
KW  - Surgical Robotics
KW  - Fiber Optics Sensor
KW  - Tendon Sheath Mechanism.
DO  - 10.1109/ICRA40945.2020.9196549
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate motion control of surgical robots is critical for the efficiency and safety of both state-of-the-art teleoperated robotic surgery and the ultimate autonomous robotic surgery. However, fine motion control for a flexible endoscopic surgical robot is highly challenging because of the shape-dependent and speed-dependent motion hysteresis of tendon-sheath mechanisms (TSMs) in the long, tortuous, and dynamically shape-changing robot body. Aiming to achieve precise closed-loop motion control, we propose a small and flexible sensor to directly sense the large and sharp rotations of the articulated joints of a flexible endoscopic surgical robot. The sensor-a Fiber Bragg Grating (FBG) eccentrically embedded in a thin and flexible epoxy substrate-can be significantly bent with a large bending angle range of [-62.9¬∞, 75.5¬∞] and small bending radius of 6.9 mm. Mounted in-between the two pivot-connected links of a joint, the sensor will bend once the joint is actuated, resulting in the wavelength shift of the FBG. In this study, the relationship between the wavelength shift and the rotation angle of the joint was theoretically modeled and then experimentally verified before and after the installation of the sensor in a robotic endoscopic grasper. The sensor, with the calibrated model, can track the rotation of the robotic joint with an RMSE of 3.34¬∞. This small and flexible sensor has good repeatability, high sensitivity (around 147.5 pm/degree), and low hysteresis (7.72%). It is suitable for surgical robots and manipulators whose articulated joints have a large rotation angle and small bending radius.
ER  - 

TY  - CONF
TI  - Soft, Round, High Resolution Tactile Fingertip Sensors for Dexterous Robotic Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4796
EP  - 4802
AU  - B. Romero
AU  - F. Veiga
AU  - E. Adelson
PY  - 2020
KW  - dexterous manipulators
KW  - geometry
KW  - tactile sensors
KW  - tactile fingertip sensors
KW  - dexterous robotic manipulation
KW  - dexterous multifingered hands
KW  - illumination geometry
KW  - dexterous manipulation tasks
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Plastics
KW  - Geometry
KW  - Light emitting diodes
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196909
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - High resolution tactile sensors are often bulky and have shape profiles that make them awkward for use in manipulation. This becomes important when using such sensors as fingertips for dexterous multi-fingered hands, where boxy or planar fingertips limit the available set of smooth manipulation strategies. High resolution optical based sensors such as GelSight have until now been constrained to relatively flat geometries due to constraints on illumination geometry. Here, we show how to construct a rounded fingertip that utilizes a form of light piping for directional illumination. Our sensors can replace the standard rounded fingertips of the Allegro hand. They can capture high resolution maps of the contact surfaces, and can be used to support various dexterous manipulation tasks.
ER  - 

TY  - CONF
TI  - FootTile: a Rugged Foot Sensor for Force and Center of Pressure Sensing in Soft Terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4810
EP  - 4816
AU  - F. Ruppert
AU  - A. Badri-Spr√∂witz
PY  - 2020
KW  - biomechanics
KW  - force sensors
KW  - gait analysis
KW  - legged locomotion
KW  - pressure sensors
KW  - FootTile
KW  - rugged foot sensor
KW  - pressure sensing
KW  - soft terrain
KW  - sensor design
KW  - standard biomechanical devices
KW  - pressure plates
KW  - pressure distribution
KW  - ground reaction force estimation
KW  - sensing capabilities
KW  - waterproof sensor
KW  - reaction force
KW  - force plates
KW  - rough terrain
KW  - legged locomotion
KW  - granular substrate
KW  - liquid mud
KW  - mass 0.9 g
KW  - frequency 330.0 Hz
KW  - Robot sensing systems
KW  - Force
KW  - Legged locomotion
KW  - Foot
KW  - Sensor arrays
KW  - Force sensors
DO  - 10.1109/ICRA40945.2020.9197466
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present FootTile, a foot sensor for reaction force and center of pressure sensing in challenging terrain. We compare our sensor design to standard biomechanical devices, force plates and pressure plates. We show that FootTile can accurately estimate force and pressure distribution during legged locomotion. FootTile weighs 0.9 g, has a sampling rate of 330 Hz, a footprint of 10√ó10 mm and can easily be adapted in sensor range to the required load case. In three experiments, we validate: first, the performance of the individual sensor, second an array of FootTiles for center of pressure sensing and third the ground reaction force estimation during locomotion in granular substrate. We then go on to show the accurate sensing capabilities of the waterproof sensor in liquid mud, as a showcase for real world rough terrain use.
ER  - 

TY  - CONF
TI  - Learning a Control Policy for Fall Prevention on an Assistive Walking Device
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4833
EP  - 4840
AU  - V. C. V. Kumar
AU  - S. Ha
AU  - G. Sawicki
AU  - C. K. Liu
PY  - 2020
KW  - biomechanics
KW  - gait analysis
KW  - handicapped aids
KW  - humanoid robots
KW  - legged locomotion
KW  - assistive walking device
KW  - fall prevention
KW  - control policy
KW  - augmented assistive device
KW  - models realistic human gait
KW  - robust human walking policy
KW  - actuators
KW  - onboard sensors
KW  - recovery policy
KW  - fall predictor
KW  - Legged locomotion
KW  - Perturbation methods
KW  - Assistive devices
KW  - Sensors
KW  - Adaptation models
KW  - Biological system modeling
DO  - 10.1109/ICRA40945.2020.9196798
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fall prevention is one of the most important components in senior care. We present a technique to augment an assistive walking device with the ability to prevent falls. Given an existing walking device, our method develops a fall predictor and a recovery policy by utilizing the onboard sensors and actuators. The key component of our method is a robust human walking policy that models realistic human gait under a moderate level of perturbations. We use this human walking policy to provide training data for the fall predictor, as well as to teach the recovery policy how to best modify the person's gait when a fall is imminent. Our evaluation shows that the human walking policy generates walking sequences similar to those reported in biomechanics literature. Our experiments in simulation show that the augmented assistive device can indeed help recover balance from a variety of external perturbations. We also provide a quantitative method to evaluate the design choices for an assistive device.
ER  - 

TY  - CONF
TI  - Assistive Force of a Belt-type Hip Assist Suit for Lifting the Swing Leg during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4841
EP  - 4847
AU  - S. Guo
AU  - Q. Xiang
AU  - K. Hashimoto
AU  - S. Jin
PY  - 2020
KW  - belts
KW  - gait analysis
KW  - medical control systems
KW  - muscle
KW  - torque
KW  - assistive force
KW  - phase shift factor
KW  - force function
KW  - force magnitude
KW  - assistive torque
KW  - muscle force
KW  - belt-type hip assist suit
KW  - swing leg
KW  - lifting
KW  - walking
KW  - rectus femoris
KW  - ankle joints
KW  - mid-swing phase
KW  - walk ratio
KW  - Force
KW  - Hip
KW  - Legged locomotion
KW  - Muscles
KW  - Belts
KW  - Torque
KW  - Windings
DO  - 10.1109/ICRA40945.2020.9196788
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a relatively simple function of assistive force for a belt-type hip assist suit developed by the authors' group. The function, which is inspired by the muscle force of the rectus femoris, contains only two parameters, the magnitude and a phase shift factor. Thus, it can reduce the amount of calculation in generating the desired assistive force during walking. Tests were performed on three healthy subjects to confirm its effect and to investigate its influence on the motions of hip, knee and ankle joints. It was demonstrated that the effect of the assist depended greatly on the phase shift factor, i.e., the location of the peak of the assistive force in a swing period. A large effect was observed when the peak of the assistive force came at mid-swing phase. The results of the tests showed that the proposed force function could help to increase walk ratio (the ratio of step length to the number of steps per minute) by an average value of 11.2% at a force magnitude of 35 N, which could produce an assistive torque of the same order as the magnitude of the muscle force of the rectus femoris around the hip joint.
ER  - 

TY  - CONF
TI  - Soft Pneumatic System for Interface Pressure Regulation and Automated Hands-Free Donning in Robotic Prostheses
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4848
EP  - 4854
AU  - A. B. Ambrose
AU  - F. L. Hammond
PY  - 2020
KW  - human-robot interaction
KW  - medical robotics
KW  - pneumatic systems
KW  - pressure control
KW  - prosthetics
KW  - real-time systems
KW  - wearable robots
KW  - human-socket interface
KW  - wearable device
KW  - synthetic forearm model
KW  - real time pressure regulation
KW  - automated hands free donning
KW  - automated underactuated donning mechanism
KW  - soft pneumatic socket
KW  - robotic prostheses
KW  - interface pressure regulation
KW  - Sockets
KW  - Bladder
KW  - Valves
KW  - Fingers
KW  - Prosthetics
KW  - Laser beam cutting
DO  - 10.1109/ICRA40945.2020.9197405
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper discusses the design and preliminary evaluation of a soft pneumatic socket (SPS) with real-time pressure regulation and an automated underactuated donning mechanism (UDM). The ability to modulate the pressure at the human-socket interface of a prosthesis or wearable device to accommodate user's activities has the potential to make the user more comfortable. Furthermore, a hands-free, underactuated donning mechanism designed to reliably and safely don the socket onto the user may increase the convenience of prostheses and wearable devices. The pneumatic socket and donning mechanism are evaluated on synthetic forearm model designed to closely match the mechanical properties of the human forearm. The pneumatic socket was tested to determine the maximum loads it can withstand before slipping and the displacement of the socket after loading. The donning mechanism was able to successfully don the socket on to the replica forearm with a 100% success rate for the 30 trials that were tested. Both devices were also tested to determine the pressures they impart on the user. The highest pressures the socket can impart on the user is 4psi and the maximum pressure the donning mechanism imparts on the user is 0.83psi. These pressures were found to be lower than the reported pressures that cause pain and tissue damage.
ER  - 

TY  - CONF
TI  - Automated detection of soleus concentric contraction in variable gait conditions for improved exosuit control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4855
EP  - 4862
AU  - R. W. Nuckols
AU  - K. Swaminathan
AU  - S. Lee
AU  - L. Awad
AU  - C. J. Walsh
AU  - R. D. Howe
PY  - 2020
KW  - gait analysis
KW  - image segmentation
KW  - image sequences
KW  - medical robotics
KW  - muscle
KW  - patient rehabilitation
KW  - automated routine
KW  - normalized gait cycle
KW  - real-time rates
KW  - gait cycle
KW  - manual estimation
KW  - healthy individuals
KW  - persons post-stroke walking
KW  - comfortable walking speed
KW  - onset timing
KW  - automated detection
KW  - soleus concentric contraction
KW  - variable gait conditions
KW  - individualized assistance
KW  - changing gait
KW  - exosuit control strategy
KW  - muscle power
KW  - soleus muscle
KW  - positive power
KW  - low-profile ultrasound system
KW  - walking individuals
KW  - biological mechanisms
KW  - frequency 130.0 Hz
KW  - Muscles
KW  - Legged locomotion
KW  - Kinematics
KW  - Tendons
KW  - Real-time systems
KW  - Electromyography
DO  - 10.1109/ICRA40945.2020.9197428
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exosuits can reduce metabolic demand and improve gait. Controllers explicitly derived from biological mechanisms that reflect the user's joint or muscle dynamics should in theory allow for individualized assistance and enable adaptation to changing gait. With the goal of developing an exosuit control strategy based on muscle power, we present an approach for estimating, at real time rates, when the soleus muscle begins to generate positive power. A low-profile ultrasound system recorded B-mode images of the soleus in walking individuals. An automated routine using optical flow segmented the data to a normalized gait cycle and estimated the onset of concentric contraction at real-time rates (~130Hz). Segmentation error was within 1% of the gait cycle compared to using ground reaction forces. Estimation of onset of concentric contraction had a high correlation (R2=0.92) and an RMSE of 2.6% gait cycle relative to manual estimation. We demonstrated the ability to estimate the onset of concentric contraction during fixed speed walking in healthy individuals that ranged from 39.3% to 45.8% of the gait cycle and feasibility in two persons post-stroke walking at comfortable walking speed. We also showed the ability to measure a shift in onset timing to 7% earlier when the biological system adapts from level to incline walking. Finally, we provided an initial evaluation for how the onset of concentric contraction might be used to inform exosuit control in level and incline walking.
ER  - 

TY  - CONF
TI  - Soft Sensing Shirt for Shoulder Kinematics Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4863
EP  - 4869
AU  - Y. Jin
AU  - C. M. Glover
AU  - H. Cho
AU  - O. A. Araromi
AU  - M. A. Graule
AU  - N. Li
AU  - R. J. Wood
AU  - C. J. Walsh
PY  - 2020
KW  - biomechanics
KW  - biomedical measurement
KW  - capacitive sensors
KW  - coaxial cables
KW  - inertial systems
KW  - kinematics
KW  - mean square error methods
KW  - motion measurement
KW  - patient monitoring
KW  - patient rehabilitation
KW  - readout electronics
KW  - regression analysis
KW  - strain sensors
KW  - wearable motion tracking
KW  - ground truth optical motion capture system
KW  - strain sensor data
KW  - joint angle estimation
KW  - normalized root mean square errors
KW  - joint velocity estimation
KW  - recursive feature elimination-based sensor selection analysis
KW  - shoulder kinematics estimation
KW  - soft strain sensors
KW  - unobtrusive approach
KW  - noncyclic joint movements
KW  - cyclic arm movements
KW  - random arm movements
KW  - shoulder joint
KW  - customized readout electronics board
KW  - sewn microcoaxial cables
KW  - textile-based capacitive strain sensors
KW  - multidegree-of-freedom noncyclic joint movements
KW  - Tracking
KW  - Shoulder
KW  - Electrodes
KW  - Capacitive sensors
KW  - Robot sensing systems
KW  - Strain
DO  - 10.1109/ICRA40945.2020.9196586
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft strain sensors have been explored as an unobtrusive approach for wearable motion tracking. However, accurate tracking of multi degree-of-freedom (DOF) noncyclic joint movements remains a challenge. This paper presents a soft sensing shirt for tracking shoulder kinematics of both cyclic and random arm movements in 3 DOFs: adduction/abduction, horizontal flexion/extension, and internal/external rotation. The sensing shirt consists of 8 textile-based capacitive strain sensors sewn around the shoulder joint that communicate to a customized readout electronics board through sewn micro-coaxial cables. An optimized sensor design includes passive shielding and demonstrates high linearity and low hysteresis, making it suitable for wearable motion tracking. In a study with a single human subject, we evaluated the tracking capability of the integrated shirt in comparison with a ground truth optical motion capture system. An ensemble-based regression algorithm was implemented in post-processing to estimate joint angles and angular velocities from the strain sensor data. Results demonstrated root mean square errors (RMSEs) less than 4.5¬∞ for joint angle estimation and normalized root mean square errors (NRMSEs) less than 4% for joint velocity estimation. Furthermore, we applied a recursive feature elimination (RFE)-based sensor selection analysis to down select the number of sensors for future shirt designs. This sensor selection analysis found that 5 sensors out of 8 were sufficient to generate comparable accuracies.
ER  - 

TY  - CONF
TI  - Motion Reasoning for Goal-Based Imitation Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4878
EP  - 4884
AU  - D. -A. Huang
AU  - Y. -W. Chao
AU  - C. Paxton
AU  - X. Deng
AU  - L. Fei-Fei
AU  - J. C. Niebles
AU  - A. Garg
AU  - D. Fox
PY  - 2020
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - video signal processing
KW  - goal-based imitation learning
KW  - third-person video demonstration
KW  - human demonstrators
KW  - motion reasoning
KW  - motion planning
KW  - Task analysis
KW  - Trajectory
KW  - Cognition
KW  - Planning
KW  - Motion segmentation
KW  - Human-robot interaction
DO  - 10.1109/ICRA40945.2020.9197172
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach on a new dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment.
ER  - 

TY  - CONF
TI  - Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4885
EP  - 4891
AU  - S. K. Kim
AU  - E. Andrea Kirchner
AU  - F. Kirchner
PY  - 2020
KW  - electroencephalography
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - medical signal processing
KW  - learned policy
KW  - learning phases
KW  - current control strategy
KW  - robot learning
KW  - intrinsic human feedback
KW  - human-robot interaction
KW  - intrinsic interactive reinforcement learning approach
KW  - human-robot collaboration
KW  - flexible adaptation
KW  - real-world robotic applications
KW  - reinforcement signals
KW  - flexible online adaptation
KW  - learning progress
KW  - Electroencephalography
KW  - Decoding
KW  - Electronic learning
KW  - Task analysis
KW  - Human-robot interaction
KW  - Robot learning
DO  - 10.1109/ICRA40945.2020.9197538
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Flexible adaptation of learning strategy depending on online changes of the user's current intents have a high relevance in human-robot collaboration. In our previous study, we proposed an intrinsic interactive reinforcement learning approach for human-robot interaction, in which a robot learns his/her action strategy based on intrinsic human feedback that is generated in the human's brain as neural signature of the human's implicit evaluation of the robot's actions. Our approach has an inherent property that allows robots to adapt their behavior depending on online changes of the human's current intents. Such flexible adaptation is possible, since robot learning is updated in real time by human's online feedback. In this paper, the adaptivity of robot learning is tested on eight subjects who change their current control strategy by adding a new gesture to the previous used gestures. This paper evaluates the learning progress by analyzing learning phases (before and after adding a new gesture for control). The results show that the robot can adapt the previously learned policy depending on online changes of the user's intents. Especially, learning progress is interrelated with the classification performance of electroencephalograms (EEGs), which are used to measure the human's implicit evaluation of the robot's actions.
ER  - 

TY  - CONF
TI  - Object-oriented Semantic Graph Based Natural Question Generation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4892
EP  - 4898
AU  - J. Moon
AU  - B. -H. Lee
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - natural language processing
KW  - object detection
KW  - recurrent neural nets
KW  - robot vision
KW  - sequential scenes
KW  - recurrent neural network
KW  - feature extraction
KW  - autonomous robots
KW  - graph convolutional network
KW  - object-oriented semantic graphs
KW  - semantic graph mapping
KW  - natural question generation
KW  - Semantics
KW  - Feature extraction
KW  - Object oriented modeling
KW  - Neural networks
KW  - Convolution
KW  - Autonomous robots
DO  - 10.1109/ICRA40945.2020.9196563
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Generating a natural question can enable autonomous robots to propose problems according to their surroundings. However, recent studies on question generation rarely consider semantic graph mapping, which is widely used to understand environments. In this paper, we introduce a method to generate natural questions using object-oriented semantic graphs. First, a graph convolutional network extracts features from the graph. Then, a recurrent neural network generates the natural question from the extracted features. Using graphs, we can generate natural questions for both single and sequential scenes. The proposed method outperforms conventional methods on a publicly available dataset for single scenes and can generate questions for sequential scenes.
ER  - 

TY  - CONF
TI  - Towards Safe Human-Robot Collaboration Using Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4899
EP  - 4905
AU  - M. El-Shamouty
AU  - X. Wu
AU  - S. Yang
AU  - M. Albus
AU  - M. F. Huber
PY  - 2020
KW  - hazards
KW  - human-robot interaction
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - occupational safety
KW  - risk management
KW  - hazard source
KW  - safety engineers
KW  - risk assessment processes
KW  - deep RL agents
KW  - human-robot collaboration
KW  - HRC-productivity
KW  - deep reinforcement learning
KW  - systematic methodology
KW  - core components
KW  - Task analysis
KW  - Training
KW  - Hazards
KW  - Robot sensing systems
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196924
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Safety in Human-Robot Collaboration (HRC) is a bottleneck to HRC-productivity in industry. With robots being the main source of hazards, safety engineers use over-emphasized safety measures, and carry out lengthy and expensive risk assessment processes on each HRC-layout reconfiguration. Recent advances in deep Reinforcement Learning (RL) offer solutions to add intelligence and comprehensibility of the environment to robots. In this paper, we propose a framework that uses deep RL as an enabling technology to enhance intelligence and safety of the robots in HRC scenarios and, thus, reduce hazards incurred by the robots. The framework offers a systematic methodology to encode the task and safety requirements and context of applicability into RL settings. The framework also considers core components, such as behavior explainer and verifier, which aim for transferring learned behaviors from research labs to industry. In the evaluations, the proposed framework shows the capability of deep RL agents learning collision-free point-to-point motion on different robots inside simulation, as shown in the supplementary video.
ER  - 

TY  - CONF
TI  - Deep compositional robotic planners that follow natural language commands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4906
EP  - 4912
AU  - Y. -L. Kuo
AU  - B. Katz
AU  - A. Barbu
PY  - 2020
KW  - convolutional neural nets
KW  - mobile robots
KW  - natural language processing
KW  - path planning
KW  - deep compositional robotic planners
KW  - natural language commands
KW  - sampling-based robotic planner
KW  - continuous configuration space
KW  - complex command
KW  - sampling-based planner
KW  - recurrent hierarchical deep network
KW  - Robots
KW  - Task analysis
KW  - Planning
KW  - Natural languages
KW  - Aerospace electronics
KW  - Cognition
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197464
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.
ER  - 

TY  - CONF
TI  - Learning User Preferences from Corrections on State Lattices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4913
EP  - 4919
AU  - N. Wilde
AU  - D. Kuliƒá
AU  - S. L. Smith
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - mobile robots
KW  - path planning
KW  - robot programming
KW  - state lattices
KW  - autonomous mobile robots
KW  - motion planning problem
KW  - robot traffic
KW  - motion features
KW  - learned user preferences
KW  - learning from corrections
KW  - algorithm completeness proving
KW  - human robot interaction
KW  - Task analysis
KW  - Lattices
KW  - Cost function
KW  - Mobile robots
KW  - Robot motion
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9197040
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Enabling a broader range of users to efficiently deploy autonomous mobile robots requires intuitive frameworks for specifying a robot's task and behaviour. We present a novel approach using learning from corrections (LfC), where a user is iteratively presented with a solution to a motion planning problem. Users might have preferences about parts of a robot's environment that are suitable for robot traffic or that should be avoided as well as preferences on the control actions a robot can take. The robot is initially unaware of these preferences; thus, we ask the user to provide a correction to the presented path. We assume that the user evaluates paths based on environment and motion features. From a sequence of corrections we learn weights for these features, which are then considered by the motion planner, resulting in future paths that better fit the user's preferences. We prove completeness of our algorithm and demonstrate its performance in simulations. Thereby, we show that the learned preferences yield good results not only for a set of training tasks but also for test tasks, as well as for different types of user behaviour.
ER  - 

TY  - CONF
TI  - Visual Servoing-based Navigation for Monitoring Row-Crop Fields
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4920
EP  - 4926
AU  - A. Ahmadi
AU  - L. Nardi
AU  - N. Chebrolu
AU  - C. Stachniss
PY  - 2020
KW  - agricultural robots
KW  - agriculture
KW  - agrochemicals
KW  - crops
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - visual servoing
KW  - visual servoing-based navigation
KW  - autonomous navigation
KW  - field robots
KW  - precision agriculture tasks
KW  - agrochemicals
KW  - visual-based navigation framework
KW  - crop-row structure
KW  - row-crop fields monitoring
KW  - Agriculture
KW  - Navigation
KW  - Cameras
KW  - Robot vision systems
KW  - Visualization
KW  - Monitoring
DO  - 10.1109/ICRA40945.2020.9197114
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous navigation is a pre-requisite for field robots to carry out precision agriculture tasks. Typically, a robot has to navigate along a crop field multiple times during a season for monitoring the plants, for applying agrochemicals, or for performing targeted interventions. In this paper, we propose a visual-based navigation framework tailored to row-crop fields that exploits the regular crop-row structure present in fields. Our approach uses only the images from on-board cameras without the need for performing explicit localization or maintaining a map of the field. Thus, it can operate without expensive RTK-GPS solutions often used in agricultural automation systems. Our navigation approach allows the robot to follow the crop rows accurately and handles the switch to the next row seamlessly within the same framework. We implemented our approach using C++ and ROS and thoroughly tested it in several simulated fields with different shapes and sizes. We also demonstrated the system running at frame-rate on an actual robot operating on a test row-crop field. The code and data have been published.
ER  - 

TY  - CONF
TI  - Optimal Routing Schedules for Robots Operating in Aisle-Structures
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4927
EP  - 4933
AU  - F. Betti Sorbelli
AU  - S. Carpin
AU  - F. Cor√≤
AU  - A. Navarra
AU  - C. M. Pinotti
PY  - 2020
KW  - computational complexity
KW  - graph theory
KW  - mobile robots
KW  - optimisation
KW  - scheduling
KW  - COP-FR
KW  - computational complexity
KW  - optimal solutions
KW  - highly unbalanced rewards
KW  - optimal routing schedules
KW  - aisle-structures
KW  - constant-cost orienteering problem
KW  - travel budget
KW  - aisle-graph
KW  - loosely connected rows
KW  - Robots
KW  - Optimized production technology
KW  - Routing
KW  - Heuristic algorithms
KW  - Task analysis
KW  - Irrigation
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9197579
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider the Constant-cost Orienteering Problem (COP) where a robot, constrained by a limited travel budget, aims at selecting a path with the largest reward in an aisle-graph. The aisle-graph consists of a set of loosely connected rows where the robot can change lane only at either end, but not in the middle. Even when considering this special type of graphs, the orienteering problem is known to be intractable. We optimally solve in polynomial time two special cases, COP-FR where the robot can only traverse full rows, and COP-SC where the robot can access the rows only from one side. To solve the general COP, we then apply our special case algorithms as well as a new heuristic that suitably combines them. Despite its light computational complexity and being confined into a very limited class of paths, the optimal solutions for COP-FR turn out to be competitive in terms of achieved rewards even for COP. This is shown by means of extended simulations performed on both real and synthetic scenarios. Furthermore, our new heuristic for the general case outperforms state-of-art algorithms, especially for input with highly unbalanced rewards.
ER  - 

TY  - CONF
TI  - Time Optimal Motion Planning with ZMP Stability Constraint for Timber Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4934
EP  - 4940
AU  - J. Song
AU  - I. Sharf
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - mobile robots
KW  - motion control
KW  - optimisation
KW  - path planning
KW  - robot kinematics
KW  - stability
KW  - timber
KW  - time optimal motion planning
KW  - ZMP stability constraint
KW  - timber manipulation
KW  - dynamic stability-constrained optimal motion
KW  - timber harvesting machine
KW  - rough terrain
KW  - kinematics model
KW  - optimization problem
KW  - computation time
KW  - motion plan
KW  - dynamic stability constraint
KW  - zero moment point stability measure
KW  - Planning
KW  - Kinematics
KW  - Manipulator dynamics
KW  - Acceleration
KW  - Stability analysis
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9196836
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a dynamic stability-constrained optimal motion planning algorithm developed for a timber harvesting machine working on rough terrain. First, the kinematics model of the machine, and the Zero Moment Point (ZMP) stability measure is presented. Then, an approach to simplify the model to gain insight and achieve a fast solution of the optimization problem is introduced. The performance and computation time of the motion plan obtained with the simplified model is compared against that obtained with the full kinematics model of the machine with the help of MATLAB simulations. The results demonstrate feasibility of fast motion planning while satisfying the dynamic stability constraint.
ER  - 

TY  - CONF
TI  - Push and Drag: An Active Obstacle Separation Method for Fruit Harvesting Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4957
EP  - 4962
AU  - Y. Xiong
AU  - Y. Ge
AU  - P. J. From
PY  - 2020
KW  - agricultural robots
KW  - collision avoidance
KW  - control engineering computing
KW  - grippers
KW  - image colour analysis
KW  - image segmentation
KW  - industrial robots
KW  - mobile robots
KW  - object detection
KW  - robot vision
KW  - point cloud operation
KW  - deep learning
KW  - object detection
KW  - color thresholding
KW  - image processing
KW  - active obstacle separation
KW  - linear motions
KW  - zig-zag push
KW  - trajectory
KW  - separation motion
KW  - drag motions
KW  - obstacle avoidance
KW  - target fruit
KW  - fruit harvesting robots
KW  - Grippers
KW  - Robots
KW  - Three-dimensional displays
KW  - Drag
KW  - Trajectory
KW  - Force
KW  - Radio frequency
DO  - 10.1109/ICRA40945.2020.9197469
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Selectively picking a target fruit surrounded by obstacles is one of the major challenges for fruit harvesting robots. Different from traditional obstacle avoidance methods, this paper presents an active obstacle separation strategy that combines push and drag motions. The separation motion and trajectory are generated based on the 3D visual perception of the obstacle information around the target. A linear push is used to clear the obstacles from the area below the target, while a zig-zag push that contains several linear motions is proposed to push aside more dense obstacles. The zig-zag push can generate multi-directional pushes and the side-to-side motion can break the static contact force between the target and obstacles, thus helping the gripper to receive a target in more complex situations. Moreover, we propose a novel drag operation to address the issue of mis-capturing obstacles located above the target, in which the gripper drags the target to a place with fewer obstacles and then pushes back to move the obstacles aside for further detachment. Furthermore, an image processing pipeline consisting of color thresholding, object detection using deep learning and point cloud operation, is developed to implement the proposed method on a harvesting robot. Field tests show that the proposed method can improve the picking performance substantially. This method helps to enable complex clusters of fruits to be harvested with a higher success rate than conventional methods.
ER  - 


TY  - CONF
TI  - A Novel Calibration Method between a Camera and a 3D LiDAR with Infrared Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4963
EP  - 4969
AU  - S. Chen
AU  - J. Liu
AU  - X. Liang
AU  - S. Zhang
AU  - J. Hyypp√§
AU  - R. Chen
PY  - 2020
KW  - calibration
KW  - cameras
KW  - image filtering
KW  - infrared imaging
KW  - optical radar
KW  - 3D LiDAR
KW  - infrared images
KW  - infrared filter
KW  - calibration method
KW  - simultaneous location and mapping
KW  - Velodyne VLP-16 sensor
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196512
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fusions of LiDARs (light detection and ranging) and cameras have been effectively and widely employed in the communities of autonomous vehicles, virtual reality and mobile mapping systems (MMS) for different purposes, such as localization, high definition map or simultaneous location and mapping. However, the extrinsic calibration between a camera and a 3D LiDAR is a fundamental prerequisite to guarantee its performance. Some previous methods are inaccurate, have calibration error that is several times the beam divergence, and often require special calibration objects, thereby limiting their ubiquitous use for calibration. To overcome these shortcomings, we propose a novel and high-accuracy method for the extrinsic calibration between a camera and a 3D LiDAR. Our approach relies on the infrared images from a camera with an infrared filter, and the 2D-3D corresponding points in a scene with the corners of a wall can be extracted to calculate the six extrinsic parameters. Experiments using the Velodyne VLP-16 sensor show that the method can achieve an extrinsic accuracy at the level of the beam divergence, which is fully analyzed and validated from two different aspects. Therefore, the calibration method in this paper is highly accurate, effective and does not require special complicated calibration objects; thus, it meets the requirements of practical applications.
ER  - 

TY  - CONF
TI  - Online Camera-LiDAR Calibration with Sensor Semantic Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4970
EP  - 4976
AU  - Y. Zhu
AU  - C. Li
AU  - Y. Zhang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - image colour analysis
KW  - image motion analysis
KW  - optical radar
KW  - optimisation
KW  - sensor fusion
KW  - RGB camera
KW  - light detection and ranging sensor
KW  - autonomous vehicles
KW  - outdoor environment
KW  - online calibration technique
KW  - optimal rigid motion transformation
KW  - mutual information
KW  - perceived data
KW  - optimization problem
KW  - semantic features
KW  - temporally synchronized camera
KW  - autonomous driving tasks
KW  - online camera-LiDAR calibration
KW  - sensor semantic information
KW  - sensor data fusion
KW  - sensor calibration
KW  - complex settings
KW  - suboptimal results
KW  - extrinsic calibration
KW  - edge feature based auto-calibration
KW  - cutting-edge machine vision
KW  - calibration quality metric
KW  - LiDAR sensor
KW  - Calibration
KW  - Cameras
KW  - Laser radar
KW  - Image edge detection
KW  - Robot sensing systems
KW  - Semantics
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196627
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As a crucial step of sensor data fusion, sensor calibration plays a vital role in many cutting-edge machine vision applications, such as autonomous vehicles and AR/VR. Existing techniques either require quite amount of manual work and complex settings, or are unrobust and prone to produce suboptimal results. In this paper, we investigate the extrinsic calibration of an RGB camera and a light detection and ranging (LiDAR) sensor, which are two of the most widely used sensors in autonomous vehicles for perceiving the outdoor environment. Specifically, we introduce an online calibration technique that automatically computes the optimal rigid motion transformation between the aforementioned two sensors and maximizes their mutual information of perceived data, without the need of tuning environment settings. By formulating the calibration as an optimization problem with a novel calibration quality metric based on semantic features, we successfully and robustly align pairs of temporally synchronized camera and LiDAR frames in real time. Demonstrated on several autonomous driving tasks, our method outperforms state-of-the-art edge feature based auto-calibration approaches in terms of robustness and accuracy.
ER  - 

TY  - CONF
TI  - Precise 3D Calibration of Wafer Handling Robot by Visual Detection and Tracking of Elliptic-shape Wafers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4977
EP  - 4982
AU  - Z. Wang
AU  - M. Tomizuka
PY  - 2020
KW  - calibration
KW  - cameras
KW  - feature extraction
KW  - Gaussian processes
KW  - image reconstruction
KW  - image registration
KW  - image segmentation
KW  - industrial robots
KW  - optimisation
KW  - pose estimation
KW  - production engineering computing
KW  - robot kinematics
KW  - robot vision
KW  - semiconductor device manufacture
KW  - semiconductor technology
KW  - precise 3D calibration
KW  - visual detection
KW  - elliptic-shape wafers
KW  - 3D poses
KW  - robot kinematics
KW  - robust ellipse detection
KW  - tracking algorithm
KW  - calibration parameters
KW  - robot-camera system
KW  - Three-dimensional displays
KW  - Image segmentation
KW  - Robots
KW  - Optimization
KW  - Image edge detection
KW  - Calibration
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197150
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work provides a framework for the 3D calibration of wafers and a wafer handling robot by monocular vision. The proposed method precisely reconstructs the 3D poses of wafers from a set of images captured by the camera mounted on the robot. In addition, it calibrates the robot kinematics simultaneously. A robust ellipse detection and tracking algorithm based on the edge arcs is developed to recognize wafers among images. Then a joint optimization is constructed from a multi-object pose graph to solve the 3D poses of wafers and other calibration parameters of the robot-camera system. The proposed tracking method is able to associate multiple incomplete elliptic segments using a Gaussian Mixture Model-based registration algorithm. The algorithm is point-based where no feature descriptor is required. The proposed 3D pose optimization incorporates shape constraints, and is more accurate than the point-wise reconstruction produced by classic bundle adjustment methods.
ER  - 

TY  - CONF
TI  - Globally Optimal Relative Pose Estimation for Camera on a Selfie Stick
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4983
EP  - 4989
AU  - K. Joo
AU  - H. Li
AU  - T. -H. Oh
AU  - Y. Bok
AU  - I. S. Kweon
PY  - 2020
KW  - calibration
KW  - motion estimation
KW  - optimisation
KW  - pose estimation
KW  - tree searching
KW  - selfie stick
KW  - video selfie
KW  - short continuous video clip
KW  - selfie photos
KW  - camera motion
KW  - fast branch-and-bound global optimization
KW  - globally optimal relative camera pose estimation
KW  - spherical joint motion
KW  - 3-DoF search problem
KW  - Cameras
KW  - Calibration
KW  - Pose estimation
KW  - Robustness
KW  - Geometry
KW  - Transforms
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9196921
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Taking selfies has become a photographic trend nowadays. We envision the emergence of the "video selfie" capturing a short continuous video clip (or burst photography) of the user, themselves. A selfie stick is usually used, whereby a camera is mounted on a stick for taking selfie photos. In this scenario, we observe that the camera typically goes through a special trajectory along a sphere surface. Motivated by this observation, in this work, we propose an efficient and globally optimal relative camera pose estimation between a pair of two images captured by a camera mounted on a selfie stick. We exploit the special geometric structure of the camera motion constrained by a selfie stick and define its motion as spherical joint motion. By the new parametrization and calibration scheme, we show that the pose estimation problem can be reduced to a 3-DoF (degrees of freedom) search problem, instead of a generic 6-DoF problem. This allows us to derive a fast branch-and-bound global optimization, which guarantees a global optimum. Thereby, we achieve efficient and robust estimation even in the presence of outliers. By experiments on both synthetic and real-world data, we validate the performance as well as the guaranteed optimality of the proposed method.
ER  - 

TY  - CONF
TI  - Online calibration of exterior orientations of a vehicle-mounted surround-view camera system
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4990
EP  - 4996
AU  - Z. Ouyang
AU  - L. Hu
AU  - Y. Lu
AU  - Z. Wang
AU  - X. Peng
AU  - L. Kneip
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - motion estimation
KW  - path planning
KW  - road vehicles
KW  - large-scale outdoor experiments
KW  - indoor experiments
KW  - exterior orientation parameters
KW  - highly practicable online optimisation strategy
KW  - complete online optimisation strategy
KW  - camera-to-camera transformations
KW  - camera-to-vehicle rotations
KW  - camera positions
KW  - exterior orientation calibration
KW  - vision-based vehicle motion estimation
KW  - neighbouring views
KW  - extrinsic calibration
KW  - intelligent vehicle behaviour
KW  - exterior perception modality
KW  - passenger vehicles
KW  - vehicle-mounted surround-view camera system
KW  - online calibration
KW  - Cameras
KW  - Calibration
KW  - Optimization
KW  - Motion estimation
KW  - Geometry
KW  - Automobiles
KW  - Mirrors
DO  - 10.1109/ICRA40945.2020.9197127
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The increasing availability of surround-view camera systems in passenger vehicles motivates their use as an exterior perception modality for intelligent vehicle behaviour. An important problem within this context is the extrinsic calibration between the cameras, which is challenging due to the often reduced overlap between the fields of view of neighbouring views. Our work is motivated by two insights. First, we argue that the accuracy of vision-based vehicle motion estimation depends crucially on the quality of exterior orientation calibration, while design parameters for camera positions typically provide sufficient accuracy. Second, we demonstrate how planar vehicle motion related direction vectors can be used to accurately identify individual camera-to-vehicle rotations, which are more useful than the commonly and tediously derived camera-to-camera transformations. We present a complete and highly practicable online optimisation strategy to obtain the exterior orientation parameters and conclude with successful tests on simulated, indoor, and large-scale outdoor experiments.
ER  - 

TY  - CONF
TI  - Learning Camera Miscalibration Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 4997
EP  - 5003
AU  - A. Cramariuc
AU  - A. Petrov
AU  - R. Suri
AU  - M. Mittal
AU  - R. Siegwart
AU  - C. Cadena
PY  - 2020
KW  - calibration
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - deep convolutional neural network
KW  - semisynthetic dataset generation pipeline
KW  - RGB cameras
KW  - vision sensors
KW  - data-driven approach
KW  - robotic platforms
KW  - camera miscalibration detection
KW  - Cameras
KW  - Calibration
KW  - Robots
KW  - Training
KW  - Sensor systems
DO  - 10.1109/ICRA40945.2020.9197378
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not. The code is available at http://github.com/ethz-asl/camera_miscalib_detection.
ER  - 

TY  - CONF
TI  - Robotic General Parts Feeder: Bin-picking, Regrasping, and Kitting
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5004
EP  - 5010
AU  - Y. Domae
AU  - A. Noda
AU  - T. Nagatani
AU  - W. Wan
PY  - 2020
KW  - grippers
KW  - industrial manipulators
KW  - materials handling
KW  - multi-robot systems
KW  - robotic general parts feeder
KW  - multiple objects
KW  - manufacturing industry
KW  - multirobot system
KW  - kitting
KW  - automatic multiple parts feeding problem
KW  - coarse-to-fine manipulation process
KW  - multiple robot arms
KW  - MPPH
KW  - traditional parts feeder
KW  - various-shaped industrial parts
KW  - robotic bin-picking system
KW  - automatic parts feeding
KW  - mean picks per hour
KW  - Pipelines
KW  - Robot sensing systems
KW  - Shape
KW  - Grippers
KW  - Service robots
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9197056
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The automatic parts feeding of multiple objects is an unsolved problem in the manufacturing industry. In this paper, we tackle the problem by proposing a multi-robot system. The system comprises three sub-components which perform bin-picking, regrasping, and kitting. The three subcomponents divide and conquer the automatic multiple parts feeding problem by considering a coarse-to-fine manipulation process. Multiple robot arms are connected in series as a pipeline. The robots are separated into three groups to perform the roles of each sub-component. The accuracy of the state and manipulation are getting higher along with the changes of the sub-components in the pipeline. In the experimental section, the performance of the system is evaluated by using the Mean Picks Per Hour (MPPH) metric and success rate, which are compared to traditional parts feeder and manual labor. The results show that the Mean Picks Per Hour (MPPH) of the proposed system is 351 with eleven various-shaped industrial parts, which is faster than the state-of-the-art robotic bin-picking system. The lead time of the proposed system for new parts is less than that of a traditional parts feeders and/or manual labor.
ER  - 

TY  - CONF
TI  - Planning, Learning and Reasoning Framework for Robot Truck Unloading
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5011
EP  - 5017
AU  - F. Islam
AU  - A. Vemula
AU  - S. -K. Kim
AU  - A. Dornbush
AU  - O. Salzman
AU  - M. Likhachev
PY  - 2020
KW  - control engineering computing
KW  - decision making
KW  - industrial manipulators
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - path planning
KW  - production engineering computing
KW  - unloading
KW  - reasoning framework
KW  - industrial manipulator robot
KW  - real-time motion planning
KW  - complex robotic system
KW  - high-level decision-making
KW  - belief space planning
KW  - offline learning
KW  - execution module
KW  - robot truck unloading
KW  - online decision-making
KW  - Planning
KW  - Robot sensing systems
KW  - Task analysis
KW  - Decision making
KW  - Collision avoidance
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196604
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the task of autonomously unloading boxes from trucks using an industrial manipulator robot. There are multiple challenges that arise: (1) real-time motion planning for a complex robotic system carrying two articulated mechanisms, an arm and a scooper, (2) decision-making in terms of what action to execute next given imperfect information about boxes such as their masses, (3) accounting for the sequential nature of the problem where current actions affect future state of the boxes, and (4) real-time execution that interleaves high-level decision-making with lower level motion planning. In this work, we propose a planning, learning, and reasoning framework to tackle these challenges, and describe its components including motion planning, belief space planning for offline learning, online decision-making based on offline learning, and an execution module to combine decision-making with motion planning. We analyze the performance of the framework on real-world scenarios. In particular, motion planning and execution modules are evaluated in simulation and on a real robot, while offline learning and online decision-making are evaluated in simulated real-world scenarios.
ER  - 

TY  - CONF
TI  - Evaluation of Perception Latencies in a Human-Robot Collaborative Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5018
EP  - 5023
AU  - A. Aalerud
AU  - G. Hovland
PY  - 2020
KW  - humanoid robots
KW  - image sensors
KW  - optical sensors
KW  - optical tracking
KW  - EtherCAT channel
KW  - perception latency evaluation
KW  - 3D vision-based sensor system
KW  - laser-tracker system
KW  - human-robot collaborative environment
KW  - actuation system
KW  - Robot sensing systems
KW  - Measurement by laser beam
KW  - Delays
KW  - Position measurement
KW  - Collaboration
KW  - Sensor systems
DO  - 10.1109/ICRA40945.2020.9197067
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The latency in vision-based sensor systems used in human-robot collaborative environments is an important safety parameter which in most cases has been neglected by researchers. The main reason for this neglect is the lack of an accurate ground-truth sensor system with a minimal delay to benchmark the vision-sensors against. In this paper the latencies of 3D vision-based sensors are experimentally evaluated and analyzed using an accurate laser-tracker system which communicates on a dedicated EtherCAT channel with minimal delay. The experimental results in the paper demonstrate that the latency in the vision-based sensor system is many orders higher than the latency in the control and actuation system.
ER  - 

TY  - CONF
TI  - Assembly of randomly placed parts realized by using only one robot arm with a general parallel-jaw gripper
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5024
EP  - 5030
AU  - J. Zhao
AU  - X. Wang
AU  - S. Wang
AU  - X. Jiang
AU  - Y. Liu
PY  - 2020
KW  - cost reduction
KW  - dexterous manipulators
KW  - grippers
KW  - industrial manipulators
KW  - legged locomotion
KW  - materials handling
KW  - position control
KW  - robotic assembly
KW  - robotic assembly
KW  - robot arm
KW  - peg-in-hole assembly
KW  - parallel-jaw gripper
KW  - industry assembly lines
KW  - parting feeding machine
KW  - sorting process
KW  - cost reduction
KW  - grasping process
KW  - two-fingered gripper
KW  - design engineering
KW  - orientation control
KW  - offset position control
KW  - Grippers
KW  - Fasteners
KW  - Robotic assembly
KW  - Manipulators
KW  - Industries
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197396
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In industry assembly lines, parts feeding machines are widely employed as the prologue of the whole procedure. They play the role of sorting the parts randomly placed in bins to the state with specified pose. With the help of the parts feeding machines, the subsequent assembly processes by robot arm can always start from the same condition. Thus it is expected that function of parting feeding machine and the robotic assembly can be integrated with one robot arm. This scheme can provide great flexibility and can also contribute to reduce the cost. The difficulties involved in this scheme lie in the fact that in the part feeding phase, the pose of the part after grasping may be not proper for the subsequent assembly. Sometimes it can not even guarantee a stable grasp. In this paper, we proposed a method to integrate parts feeding and assembly within one robot arm. This proposal utilizes a specially designed gripper tip mounted on the jaws of a two-fingered gripper. With the modified gripper, in-hand manipulation of the grasped object is realized, which can ensure the control of the orientation and offset position of the grasped object. The proposal in this paper is verified by a simulated assembly in which a robot arm completed the assembly process including parts picking from bin and a subsequent peg-in-hole assembly.
ER  - 

TY  - CONF
TI  - Bio-Inspired Distance Estimation using the Self-Induced Acoustic Signature of a Motor-Propeller System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5047
EP  - 5053
AU  - L. Calkins
AU  - J. Lingevitch
AU  - L. McGuire
AU  - J. Geder
AU  - M. Kelly
AU  - M. M. Zavlanos
AU  - D. Sofge
AU  - D. M. Lofaro
PY  - 2020
KW  - acoustic signal processing
KW  - acoustic wave interference
KW  - aircraft control
KW  - distance measurement
KW  - microphones
KW  - oscillations
KW  - propellers
KW  - bio-inspired distance estimation
KW  - acoustic signature
KW  - motor-propeller system
KW  - single microphone
KW  - audible frequency band
KW  - power spectrum
KW  - broadband oscillation
KW  - MPS
KW  - broadband constructive-destructive interference pattern
KW  - Microphones
KW  - Acoustics
KW  - Robot sensing systems
KW  - Interference
KW  - Frequency-domain analysis
DO  - 10.1109/ICRA40945.2020.9197143
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we propose an algorithm to actively control the distance of a motor-propeller system (MPS) to a large obstacle using data from a single microphone. The method is based upon a broadband constructive/destructive interference pattern across the audible frequency band that is present when the MPS is near an obstacle. By taking the difference between the power spectrum in the obstacle-free case and the spectrum when recording near an obstacle, a broadband oscillation with respect to frequency is revealed. The frequency of this oscillation is linearly-related to the distance from the microphone to the wall. We present both static and dynamic experiments showcasing the ability of the proposed method to estimate the distance to a wall as well as actively control it.
ER  - 

TY  - CONF
TI  - A bio-inspired 3-DOF light-weight manipulator with tensegrity X-joints*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5054
EP  - 5060
AU  - B. Fasquelle
AU  - M. Furet
AU  - P. Khanna
AU  - D. Chablat
AU  - C. Chevallereau
AU  - P. Wenger
PY  - 2020
KW  - actuators
KW  - manipulator kinematics
KW  - motion control
KW  - light-weight manipulator
KW  - tensegrity X-joints
KW  - manipulators
KW  - anti-parallelogram joints
KW  - tensegrity one-degree-of-freedom mechanism
KW  - 3-degree-of-freedom manipulator
KW  - tensegrity X-joint
KW  - Bars
KW  - Springs
KW  - Birds
KW  - Neck
KW  - Manipulator dynamics
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196589
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a new kind of light-weight manipulators suitable for safe interactions. The proposed manipulators use anti-parallelogram joints in series, referred to as X-joints. Each X-joint is remotely actuated with cables and springs in parallel, thus realizing a tensegrity one-degree-of-freedom mechanism. As compared to manipulators built with simple revolute joints in series, manipulators with tensegrity X-joint offer a number of advantages, such as an intrinsic stability, variable stiffness and lower inertia. This new design was inspired by the musculosleketon architecture of the bird neck that is known to have remarkable features such as a high dexterity. The paper analyzes in detail the kinetostatics of a X-joint and proposes a 3-degree-of-freedom manipulator made of three such joints in series. Both simulation results and experiment results conducted on a test-bed prototype are presented and discussed.
ER  - 

TY  - CONF
TI  - The Lobster-inspired Antagonistic Actuation Mechanism Towards a Bending Module
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5061
EP  - 5067
AU  - Y. Chen
AU  - H. Chung
AU  - B. Chen
AU  - Y. Bao
AU  - Y. Sun
PY  - 2020
KW  - bending
KW  - biomechanics
KW  - motion control
KW  - pneumatic actuators
KW  - robot dynamics
KW  - torque control
KW  - input pressure
KW  - antagonistic soft chambers
KW  - mechanical performance
KW  - safe compliant actuation
KW  - enhanced torque output
KW  - lobster leg joint
KW  - musculoskeletal structure
KW  - bending module
KW  - lobster-inspired antagonistic actuation mechanism
KW  - maximum torque output
KW  - fabricated module
KW  - stiffness tuning
KW  - angle control
KW  - bending angle
KW  - Torque
KW  - Actuators
KW  - Legged locomotion
KW  - Hysteresis
KW  - Exoskeletons
KW  - Pneumatic systems
DO  - 10.1109/ICRA40945.2020.9196624
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes a new type of bending module inspired, in part, by the musculoskeletal structure of the lobster leg joint. The bending module proposed combines enhanced torque output, reconfigurability in assembling, safe compliant actuation, and accurate control on its mechanical performance. In this module, antagonistic soft chambers are enveloped by exoskeleton shells, and the bending angle and the stiffness can be independently adjusted by controlling the input pressure in the two chambers. Theoretical models are developed to characterize the relationships between the input pressure, bending angle, and stiffness, and a controller for angle control and stiffness tuning is constructed with experimental validation. The fabricated module can reach the maximum torque output of 109.7 N¬∑mm under 40 kPa and the stiffness range from 40 to 220 N¬∑mm/rad, demonstrating its capacity to fulfill both safe interactions and forceful tasks.
ER  - 

TY  - CONF
TI  - Emulating duration and curvature of coral snake anti-predator thrashing behaviors using a soft-robotic platform
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5068
EP  - 5074
AU  - S. M. Danforth
AU  - M. Kohler
AU  - D. Bruder
AU  - A. R. D. Rabosky
AU  - S. Kota
AU  - R. Vasudevan
AU  - T. Y. Moore
PY  - 2020
KW  - biomechanics
KW  - ecology
KW  - elastomers
KW  - mobile robots
KW  - pneumatic actuators
KW  - zoology
KW  - snake survival
KW  - soft robots
KW  - fiber-reinforced elastomeric enclosures
KW  - anti-predator behaviors
KW  - live snakes
KW  - curvature values
KW  - soft-robotic head
KW  - soft robot motion durations
KW  - distinct anti-predatory behavior
KW  - live snake observations
KW  - coral snake anti-predator thrashing
KW  - soft-robotic platform
KW  - nonlocomotory movements
KW  - animal-robot interactions
KW  - coral snakes
KW  - snake species
KW  - Soft robotics
KW  - MIMICs
KW  - Biology
KW  - Fabrication
KW  - Strain
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9197549
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a soft-robotic platform for exploring the ecological relevance of non-locomotory movements via animal-robot interactions. Coral snakes (genus Micrurus) and their mimics use vigorous, non-locomotory, and arrhythmic thrashing to deter predation. There is variation across snake species in the duration and curvature of anti-predator thrashes, and it is unclear how these aspects of motion interact to contribute to snake survival. In this work, soft robots composed of fiber-reinforced elastomeric enclosures (FREEs) are developed to emulate the anti-predator behaviors of three genera of snake. Curvature and duration of motion are estimated for both live snakes and robots, providing a quantitative assessment of the robots' ability to emulate snake poses. The curvature values of the fabricated soft-robotic head, midsection, and tail segments are found to overlap with those exhibited by live snakes. Soft robot motion durations were less than or equal to those of snakes for all three genera. Additionally, combinations of segments were selected to emulate three specific snake genera with distinct anti-predatory behavior, producing curvature values that aligned well with live snake observations.
ER  - 

TY  - CONF
TI  - Directional Mechanical Impedance of the Human Ankle During Standing with Active Muscles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5075
EP  - 5081
AU  - G. A. Ribeiro
AU  - L. N. Knop
AU  - M. Rastgaar
PY  - 2020
KW  - gait analysis
KW  - muscle
KW  - neurophysiology
KW  - prosthetics
KW  - torque
KW  - muscle activity
KW  - human ankle function
KW  - directional mechanical impedance
KW  - active muscles
KW  - standing posture
KW  - reconstructed torque
KW  - ankle states
KW  - ankle angle
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196616
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The directional mechanical impedance of the human ankle was identified from subjects in a standing posture with varying levels of muscle activity. The impedance modeled the different torque responses to angle perturbations about different axes of rotation. This work proposed a novel impedance model that incorporated the coupling between multiple degrees of freedom of the ankle and was validated theoretically and experimentally. The reconstructed torque had an average variance accounted above 94% across twelve subjects. In addition, the impedance varied between and within trials and this variation was explained by changes in the ankle states, i.e., the ankle angle, torque, and muscle activities. These results have implications in the design of new prostheses controllers and the understanding of the human ankle function.
ER  - 

TY  - CONF
TI  - Contact Surface Estimation via Haptic Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5087
EP  - 5093
AU  - H. -C. Lin
AU  - M. Mistry
PY  - 2020
KW  - haptic interfaces
KW  - legged locomotion
KW  - contact surface estimation
KW  - haptic perception
KW  - legged systems
KW  - contact force
KW  - surface geometry
KW  - vision system
KW  - harsh weather
KW  - surface information
KW  - haptic exploration
KW  - Robot sensing systems
KW  - Estimation
KW  - Friction
KW  - Legged locomotion
KW  - Foot
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196816
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Legged systems need to optimize contact force in order to maintain contacts. For this, the controller needs to have the knowledge of the surface geometry and how slippery the terrain is. We can use a vision system to realize the terrain, but the accuracy of the vision system degrades in harsh weather, and it cannot visualize the terrain if it is covered with water or grass. Also, the degree of friction cannot be directly visualized. In this paper, we propose an online method to estimate the surface information via haptic exploration. We also introduce a probabilistic criterion to measure the quality of the estimation. The method is validated on both simulation and a real robot platform.
ER  - 

TY  - CONF
TI  - Local Policy Optimization for Trajectory-Centric Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5094
EP  - 5100
AU  - P. Kolaric
AU  - D. K. Jha
AU  - A. U. Raghunathan
AU  - F. L. Lewis
AU  - M. Benosman
AU  - D. Romeres
AU  - D. Nikovski
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - nonlinear control systems
KW  - nonlinear programming
KW  - open loop systems
KW  - local policy optimization
KW  - trajectory-centric reinforcement learning
KW  - local stabilizing policy optimization
KW  - trajectory-centric model-based reinforcement learning
KW  - global policy optimization
KW  - nonlinear systems
KW  - robotic manipulation tasks
KW  - open-loop trajectory optimization
KW  - local policy synthesis
KW  - single optimization problem
KW  - nonlinear programming
KW  - Robustness
KW  - Trajectory optimization
KW  - Uncertainty
KW  - Learning (artificial intelligence)
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197058
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.
ER  - 

TY  - CONF
TI  - Automatic Snake Gait Generation Using Model Predictive Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5101
EP  - 5107
AU  - E. Hannigan
AU  - B. Song
AU  - G. Khandate
AU  - M. Haas-Heger
AU  - J. Yin
AU  - M. Ciocarlie
PY  - 2020
KW  - drag
KW  - friction
KW  - mobile robots
KW  - motion control
KW  - Pareto optimisation
KW  - predictive control
KW  - robot dynamics
KW  - trajectory control
KW  - automatic snake gait generation
KW  - undulatory gaits
KW  - snake robots
KW  - movement pattern
KW  - serpenoid curve
KW  - model predictive control
KW  - locomotion gaits
KW  - trajectory optimization
KW  - snake dynamics
KW  - anisotropic dry friction
KW  - viscous friction
KW  - fluid dynamic effects
KW  - Pareto-optimal serpenoid gaits
KW  - drag
KW  - Friction
KW  - Dynamics
KW  - Snake robots
KW  - Force
KW  - Adaptation models
KW  - Heuristic algorithms
KW  - Optimal control
DO  - 10.1109/ICRA40945.2020.9196853
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a method for generating undulatory gaits for snake robots. Instead of starting from a pre-defined movement pattern such as a serpenoid curve, we use a Model Predictive Control (MPC) approach to automatically generate effective locomotion gaits via trajectory optimization. An important advantage of this approach is that the resulting gaits are automatically adapted to the environment that is being modeled as part of the snake dynamics. To illustrate this, we use a novel model for anisotropic dry friction, along with existing models for viscous friction and fluid dynamic effects such as drag and added mass. For each of these models, gaits generated without any change in the method or its parameters are as efficient as Pareto-optimal serpenoid gaits tuned individually for each environment. Furthermore, the proposed method can also produce more complex or irregular gaits, e.g. for obstacle avoidance or executing sharp turns.
ER  - 

TY  - CONF
TI  - On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5255
EP  - 5261
AU  - V. Sadhu
AU  - S. Zonouz
AU  - D. Pompili
PY  - 2020
KW  - aerospace computing
KW  - autonomous aerial vehicles
KW  - convolutional neural nets
KW  - fault diagnosis
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - pattern classification
KW  - real-time systems
KW  - recurrent neural nets
KW  - sensor fusion
KW  - raw sensor data
KW  - drone misoperations
KW  - unmanned aerial vehicle fault cause detection
KW  - deep learning architectures
KW  - fault cause identification
KW  - drone software cyberattack
KW  - deep convolutional neural network
KW  - long short term memory neural network
KW  - autoencoder
KW  - real time sensor data classification
KW  - Drones
KW  - Computer crashes
KW  - Real-time systems
KW  - Robot sensing systems
KW  - Computer architecture
KW  - Neural networks
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197071
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).
ER  - 

TY  - CONF
TI  - GOMP: Grasp-Optimized Motion Planning for Bin Picking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5270
EP  - 5277
AU  - J. Ichnowski
AU  - M. Danielczuk
AU  - J. Xu
AU  - V. Satish
AU  - K. Goldberg
PY  - 2020
KW  - collision avoidance
KW  - concave programming
KW  - dexterous manipulators
KW  - grippers
KW  - industrial manipulators
KW  - manipulator dynamics
KW  - motion control
KW  - quadratic programming
KW  - warehouse automation
KW  - GOMP
KW  - bin-picking robot
KW  - robot dynamics
KW  - grasp planner
KW  - motion planner
KW  - robot bin picking
KW  - picks-per-hour
KW  - PPH
KW  - grasp-analysis tools
KW  - robot gripper
KW  - grasp-optimized motion planning
KW  - warehouse automation
KW  - Dex-Net
KW  - degree of freedom
KW  - sequential quadratic programming
KW  - obstacle avoidance
KW  - time-minimization
KW  - Trajectory
KW  - Grippers
KW  - Robot sensing systems
KW  - Planning
KW  - Manipulators
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197548
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Rapid and reliable robot bin picking is a critical challenge in automating warehouses, often measured in picks-per-hour (PPH). We explore increasing PPH using faster motions based on optimizing over a set of candidate grasps. The source of this set of grasps is two-fold: (1) grasp-analysis tools such as Dex-Net generate multiple candidate grasps, and (2) each of these grasps has a degree of freedom about which a robot gripper can rotate. In this paper, we present Grasp-Optimized Motion Planning (GOMP), an algorithm that speeds up the execution of a bin-picking robot's operations by incorporating robot dynamics and a set of candidate grasps produced by a grasp planner into an optimizing motion planner. We compute motions by optimizing with sequential quadratic programming (SQP) and iteratively updating trust regions to account for the non-convex nature of the problem. In our formulation, we constrain the motion to remain within the mechanical limits of the robot while avoiding obstacles. We further convert the problem to a time-minimization by repeatedly shorting a time horizon of a trajectory until the SQP is infeasible. In experiments with a UR5, GOMP achieves a speedup of 9x over a baseline planner.
ER  - 

TY  - CONF
TI  - Motion Planning and Task Allocation for a Jumping Rover Team
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5278
EP  - 5283
AU  - K. C. Tan
AU  - M. Jung
AU  - I. Shyu
AU  - C. Wan
AU  - R. Dai
PY  - 2020
KW  - collision avoidance
KW  - integer programming
KW  - linear programming
KW  - mobile robots
KW  - multi-robot systems
KW  - planetary rovers
KW  - travelling salesman problems
KW  - trees (mathematics)
KW  - jumping rover team
KW  - robotic team
KW  - unmanned ground vehicles
KW  - hybrid operational modes
KW  - multiple traveling salesman problem
KW  - mTSP
KW  - ground surface
KW  - jumping capability
KW  - optimal path
KW  - mixed-integer linear programming problem
KW  - RRT*
KW  - multiple UGV
KW  - optimized motion
KW  - customized jumping rovers
KW  - Planning
KW  - Task analysis
KW  - Smoothing methods
KW  - Resource management
KW  - Mobile robots
KW  - Wheels
KW  - Jumping Robots
KW  - Multiple Traveling Salesman Problem
KW  - Path Planning
KW  - Rapidly-exploring Random Tree
KW  - Mixed-Integer Linear Programming
DO  - 10.1109/ICRA40945.2020.9197268
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a cooperative robotic team composed of unmanned ground vehicles (UGVs) with hybrid operational modes to tackle the multiple traveling salesman problem (mTSP) with obstacles. The hybrid operational modes allow every UGV in the team to not only travel on a ground surface but also jump over obstacles. We name these UGVs jumping rovers. The jumping capability provides a flexible form of locomotion by leaping and landing on top of obstacles instead of navigating around obstacles. To solve the mTSP, an optimal path between any two objective points in an mTSP is determined by the optimized rapidly-exploring random tree method, named RRT*, and is further improved through a refined RRT* algorithm to find a smoother path between targets. We then formulate the mTSP as a mixed-integer linear programming (MILP) problem to search for the most cost-effective combination of paths for multiple UGVs. The effectiveness of the hybrid operational modes and optimized motion with assigned tasks is verified in an indoor, physical experimental environment using the customized jumping rovers.
ER  - 

TY  - CONF
TI  - Active 3D Modeling via Online Multi-View Stereo
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5284
EP  - 5291
AU  - S. Song
AU  - D. Kim
AU  - S. Jo
PY  - 2020
KW  - image reconstruction
KW  - stereo image processing
KW  - low-quality surfaces
KW  - exploration trial
KW  - active 3D modeling
KW  - online multiview stereo
KW  - large-scale structure monitoring
KW  - image acquisition
KW  - reconstruction quality
KW  - view path-planning method
KW  - online MVS system
KW  - online feedbacks
KW  - image reconstruction
KW  - camera trajectory
KW  - real-time three-dimensional model construction
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Surface reconstruction
KW  - Computational modeling
KW  - Solid modeling
KW  - Trajectory
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197089
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-view stereo (MVS) algorithms have been commonly used to model large-scale structures. When processing MVS, image acquisition is an important issue because its reconstruction quality depends heavily on the acquired images. Recently, an explore-then-exploit strategy has been used to acquire images for MVS. This method first constructs a coarse model by exploring an entire scene using a pre-allocated camera trajectory. Then, it rescans the unreconstructed regions from the coarse model. However, this strategy is inefficient because of the frequent overlap of the initial and rescanning trajectories. Furthermore, given the complete coverage of images, MVS algorithms do not guarantee an accurate reconstruction result.In this study, we propose a novel view path-planning method based on an online MVS system. This method aims to incrementally construct the target three-dimensional (3D) model in real time. View paths are continually planned based on online feedbacks from the partially constructed model. The obtained paths fully cover low-quality surfaces while maximizing the reconstruction performance of MVS. Experimental results demonstrate that the proposed method can construct high quality 3D models with one exploration trial, without any rescanning trial as in the explore-then-exploit method.
ER  - 

TY  - CONF
TI  - Reoriented Short-Cuts (RSC): An Adjustment Method for Locally Optimal Path Short-Cutting in High DoF Configuration Spaces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5292
EP  - 5298
AU  - A. C. Holston
AU  - J. -H. Kim
PY  - 2020
KW  - collision avoidance
KW  - computational complexity
KW  - Gaussian processes
KW  - graph theory
KW  - manipulators
KW  - mobile robots
KW  - optimisation
KW  - single homotopy class
KW  - asymptotic convergence
KW  - single DoF
KW  - path segments
KW  - RSC
KW  - zero-volume convergence region
KW  - adjustment method
KW  - high DoF configuration spaces
KW  - reoriented short-cuts
KW  - locally optimal path short-cutting
KW  - high degree of freedom problems
KW  - informed Gaussian sampling technique
KW  - IGS
KW  - collision checking computation
KW  - robot manipulation
KW  - rotation oriented problems
KW  - translation oriented problems
KW  - Convergence
KW  - Optimization
KW  - Robots
KW  - Planning
KW  - Complexity theory
KW  - Trajectory
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9196532
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents Reoriented Short-Cuts (RSC): A modification of the traditional Short-Cut technique, allowing almost sure, single homotopy class, asymptotic convergence in high degree of freedom (DoF) problems. An additional Informed Gaussian Sampling (IGS) technique is also introduced for convergence comparison. Traditionally, Short-Cut methods are used as a final technique to further optimize an initially found path. Typical Short-Cut methods fail as a single DoF may converge faster than the remaining, creating a zero-volume region between path segments and objects, halting further improvements. Previous attempts to solve this separate DoFs individually, drastically increasing collision checking computation. RSC and IGS control the shifting of the vertex to be Short-Cut, moving vertex positions by reorienting the line segments, removing the zero-volume convergence region. These methods are compared to similar strategies in a variety of problems including random worlds, and robot manipulation, to show the convergence across both translation and rotation oriented problems.
ER  - 

TY  - CONF
TI  - Learning Resilient Behaviors for Navigation Under Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5299
EP  - 5305
AU  - T. Fan
AU  - P. Long
AU  - W. Liu
AU  - J. Pan
AU  - R. Yang
AU  - D. Manocha
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - safety-critical software
KW  - diverse environments
KW  - environmental uncertainty
KW  - resilient behaviors
KW  - deep reinforcement learning
KW  - complex behaviors
KW  - adaptive behaviors
KW  - uncertainty-aware predictor
KW  - uncertainty-aware navigation network
KW  - neural network
KW  - safety-critical tasks
KW  - mobile robot
KW  - Uncertainty
KW  - Navigation
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Estimation
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9196785
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep reinforcement learning has great potential to acquire complex, adaptive behaviors for autonomous agents automatically. However, the underlying neural network polices have not been widely deployed in real-world applications, especially in these safety-critical tasks (e.g., autonomous driving). One of the reasons is that the learned policy cannot perform flexible and resilient behaviors as traditional methods to adapt to diverse environments. In this paper, we consider the problem that a mobile robot learns adaptive and resilient behaviors for navigating in unseen uncertain environments while avoiding collisions. We present a novel approach for uncertainty-aware navigation by introducing an uncertainty-aware predictor to model the environmental uncertainty, and we propose a novel uncertainty-aware navigation network to learn resilient behaviors in the prior unknown environments. To train the proposed uncertainty-aware network more stably and efficiently, we present the temperature decay training paradigm, which balances exploration and exploitation during the training process. Our experimental evaluation demonstrates that our approach can learn resilient behaviors in diverse environments and generate adaptive trajectories according to environmental uncertainties.
ER  - 

TY  - CONF
TI  - Nonlinear Vector-Projection Control for Agile Fixed-Wing Unmanned Aerial Vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5314
EP  - 5320
AU  - J. C. Hern√°ndez Ram√≠rez
AU  - M. Nahon
PY  - 2020
KW  - aerospace components
KW  - aircraft control
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - cascade control
KW  - helicopters
KW  - mobile robots
KW  - nonlinear control systems
KW  - trajectory control
KW  - nonlinear vector-projection control
KW  - agile fixed-wing aircraft
KW  - fixed-wing platforms
KW  - nonlinear control strategy
KW  - autonomous flight
KW  - cascaded control structure
KW  - inner attitude control loop
KW  - Special Orthornormal group
KW  - outer position control loop
KW  - thrust command
KW  - attitude references
KW  - lift forces
KW  - agile fixed-wing unmanned aerial vehicles
KW  - rotorcraft
KW  - Aircraft
KW  - Control systems
KW  - Attitude control
KW  - Aerodynamics
KW  - Position measurement
KW  - Aircraft propulsion
DO  - 10.1109/ICRA40945.2020.9196838
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Agile fixed-wing aircraft integrate the efficient, high-speed capabilities of conventional fixed-wing platforms with the extreme maneuverability of rotorcraft. This work presents a nonlinear control strategy that harnesses these capabilities to enable autonomous flight through aggressive, time-constrained, three-dimensional trajectories. The cascaded control structure consists of two parts; an inner attitude control loop developed on the Special Orthornormal group that avoids singularities commonly associated with other parametrizations, and an outer position control loop that jointly determines the thrust command and attitude references by implementing a novel vector-projection algorithm. The objective of the algorithm is to decouple roll from the reference attitude to ensure that thrust and lift forces can always be pointed such that position errors converge to zero. The proposed control system represents a single, unified solution that remains effective throughout the aircraft's flight envelope, including aerobatic operation. Controller performance is verified through simulations and experimental flight tests; results show the unified control scheme is capable of performing a wide range of operations that would normally require multiple, single-purpose controllers, and their associated switching logic.
ER  - 

TY  - CONF
TI  - Adaptive Nonlinear Control of Fixed-Wing VTOL with Airflow Vector Sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5321
EP  - 5327
AU  - X. Shi
AU  - P. Spieler
AU  - E. Tang
AU  - E. -S. Lupu
AU  - P. Tokumaru
AU  - S. -J. Chung
PY  - 2020
KW  - adaptive control
KW  - aerodynamics
KW  - aerospace components
KW  - aircraft control
KW  - computational fluid dynamics
KW  - mobile robots
KW  - nonlinear control systems
KW  - rotors
KW  - tracking
KW  - adaptive nonlinear control
KW  - airflow vector sensing
KW  - complex aerodynamic interactions
KW  - linear force models
KW  - rotor forces
KW  - three-dimensional airflow sensor
KW  - custom-built fixed-wing VTOL
KW  - force prediction
KW  - baseline flight controllers
KW  - fixed-wing vertical take-off and landing aircraft
KW  - Force
KW  - Aerodynamics
KW  - Atmospheric modeling
KW  - Aircraft
KW  - Rotors
KW  - Propellers
KW  - Adaptation models
DO  - 10.1109/ICRA40945.2020.9197344
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fixed-wing vertical take-off and landing (VTOL) aircraft pose a unique control challenge that stems from complex aerodynamic interactions between wings and rotors. Thus, accurate estimation of external forces is indispensable for achieving high performance flight. In this paper, we present a composite adaptive nonlinear tracking controller for a fixed- wing VTOL. The method employs online adaptation of linear force models, and generates accurate estimation for wing and rotor forces in real-time based on information from a three-dimensional airflow sensor. The controller is implemented on a custom-built fixed-wing VTOL, which shows improved velocity tracking and force prediction during the transition stage from hover to forward flight, compared to baseline flight controllers.
ER  - 

TY  - CONF
TI  - The Reconfigurable Aerial Robotic Chain: Modeling and Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5328
EP  - 5334
AU  - H. Nguyen
AU  - T. Dang
AU  - K. Alexis
PY  - 2020
KW  - aerospace robotics
KW  - control system synthesis
KW  - mobile robots
KW  - position control
KW  - ARC-Alpha prototype
KW  - multilinked microaerial vehicles
KW  - reconfigurable aerial robotic chain
KW  - multiple parallel angular controllers
KW  - model predictive position control loop
KW  - controller design
KW  - connected aerial vehicles
KW  - system dynamics
KW  - system extendability
KW  - distributed sensing
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Payloads
KW  - Shape
KW  - Prototypes
DO  - 10.1109/ICRA40945.2020.9197184
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper overviews the system design, modeling and control of the Aerial Robotic Chain. This new design corresponds to a reconfigurable robotic system of systems consisting of multilinked micro aerial vehicles that presents the ability to cross narrow sections, morph its shape, ferry significant payloads, offer the potential of distributed sensing and processing, and enable system extendability. We present the system dynamics for any number of connected aerial vehicles, followed by the controller design involving a model predictive position control loop combined with multiple parallel angular controllers on SO(3). Evaluation studies both in simulation and through experiments based on our ARC-Alpha prototype are depicted and involve coordinated maneuvering and shape configuration to cross narrow windows.
ER  - 

TY  - CONF
TI  - Direct Acceleration Feedback Control of Quadrotor Aerial Vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5335
EP  - 5341
AU  - M. Hamandi
AU  - M. Tognon
AU  - A. Franchi
PY  - 2020
KW  - acceleration control
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - feedback
KW  - helicopters
KW  - motion control
KW  - position control
KW  - position measurement
KW  - regression analysis
KW  - three-term control
KW  - direct feedback
KW  - novel regression-based filter
KW  - commanded propeller speeds
KW  - low-latency acceleration measurements
KW  - control feedback
KW  - low frequency position measurements
KW  - PID strategy
KW  - noisy acceleration measurements
KW  - quadrotor aerial vehicles
KW  - direct acceleration feedback control
KW  - Acceleration
KW  - Accelerometers
KW  - Propellers
KW  - Robustness
KW  - Attitude control
KW  - Zirconium
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196557
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we propose to control a quadrotor through direct acceleration feedback. The proposed method, while simple in form, alleviates the need for accurate estimation of platform parameters such as mass and propeller effectiveness. In order to use efficaciously the noisy acceleration measurements in direct feedback, we propose a novel regression-based filter that exploits the knowledge on the commanded propeller speeds, and extracts smooth platform acceleration with minimal delay. Our tests show that the controller exhibits a few millimeter error when performing real world tasks with fast changing mass and effectiveness, e.g., in pick and place operation and in turbulent conditions. Finally, we benchmark the direct acceleration controller against the PID strategy and show the clear advantage of using high-frequency and low-latency acceleration measurements directly in the control feedback, especially in the case of low frequency position measurements that are typical for real outdoor conditions.
ER  - 

TY  - CONF
TI  - Trajectory Tracking Nonlinear Model Predictive Control for an Overactuated MAV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5342
EP  - 5348
AU  - M. Brunner
AU  - K. Bodie
AU  - M. Kamel
AU  - M. Pantic
AU  - W. Zhang
AU  - J. Nieto
AU  - R. Siegwart
PY  - 2020
KW  - actuators
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - microrobots
KW  - mobile robots
KW  - nonlinear control systems
KW  - observers
KW  - optimal control
KW  - predictive control
KW  - robot dynamics
KW  - robot kinematics
KW  - trajectory control
KW  - trajectory tracking nonlinear model predictive control
KW  - omnidirectional microaerial vehicles
KW  - free space
KW  - rigid body model based approach
KW  - receding horizon
KW  - optimal wrench commands
KW  - mechanical design
KW  - optimal actuator commands
KW  - disturbance observer
KW  - offset-free tracking
KW  - 6DoF trajectories
KW  - Rotors
KW  - Resource management
KW  - Actuators
KW  - Trajectory
KW  - Force
KW  - Quaternions
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197005
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents a method to control omnidirectional micro aerial vehicles (OMAVs) for the tracking of 6-DoF trajectories in free space. A rigid body model based approach is applied in a receding horizon fashion to generate optimal wrench commands that can be constrained to meet limits given by the mechanical design and actuators of the platform. Allocation of optimal actuator commands is performed in a separate step. A disturbance observer estimates forces and torques that may arise from unmodeled dynamics or external disturbances and fuses them into the optimization to achieve offset-free tracking. Experiments on a fully overactuated MAV show the tracking performance and compare it against a classical PD-based controller.
ER  - 

TY  - CONF
TI  - Optimal Oscillation Damping Control of cable-Suspended Aerial Manipulator with a Single IMU Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5349
EP  - 5355
AU  - Y. S. Sarkisov
AU  - M. Jun Kim
AU  - A. Coelho
AU  - D. Tsetserukou
AU  - C. Ott
AU  - K. Kondak
PY  - 2020
KW  - control system synthesis
KW  - damping
KW  - feedback
KW  - linear quadratic control
KW  - linearisation techniques
KW  - manipulators
KW  - nonlinear control systems
KW  - optimal control
KW  - pendulums
KW  - robust control
KW  - vibration control
KW  - control action
KW  - simplified SAM model
KW  - model uncertainties
KW  - optimal oscillation damping control
KW  - cable-suspended aerial manipulator
KW  - single IMU sensor
KW  - double pendulum
KW  - output feedback linear quadratic regulation problem
KW  - minimal energy consumption
KW  - Oscillators
KW  - Damping
KW  - Manipulators
KW  - Robot sensing systems
KW  - Task analysis
KW  - Cranes
DO  - 10.1109/ICRA40945.2020.9197055
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a design of oscillation damping control for the cable-Suspended Aerial Manipulator (SAM). The SAM is modeled as a double pendulum, and it can generate a body wrench as a control action. The main challenge is the fact that there is only one onboard IMU sensor which does not provide full information on the system state. To overcome this difficulty, we design a controller motivated by a simplified SAM model. The proposed controller is very simple yet robust to model uncertainties. Moreover, we propose a gain tuning rule by formulating the proposed controller in the form of output feedback linear quadratic regulation problem. Consequently, it is possible to quickly dampen oscillations with minimal energy consumption. The proposed approach is validated through simulations and experiments.
ER  - 

TY  - CONF
TI  - TUNERCAR: A Superoptimization Toolchain for Autonomous Racing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5356
EP  - 5362
AU  - M. O‚ÄôKelly
AU  - H. Zheng
AU  - A. Jain
AU  - J. Auckley
AU  - K. Luong
AU  - R. Mangharam
PY  - 2020
KW  - automobiles
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - random processes
KW  - search problems
KW  - autonomous vehicles
KW  - online planning
KW  - TUNERCAR
KW  - superoptimization toolchain
KW  - autonomous racing
KW  - vehicle parameters
KW  - autonomous racecar
KW  - systems infrastructure
KW  - parallel implementation
KW  - CMA-ES
KW  - lap time
KW  - naive random search
KW  - racing strategy
KW  - Optimization
KW  - Sociology
KW  - Statistics
KW  - Vehicle dynamics
KW  - Hardware
KW  - Robots
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197080
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - TUNERCAR is a toolchain that jointly optimizes racing strategy, planning methods, control algorithms, and vehicle parameters for an autonomous racecar. In this paper, we detail the target hardware, software, simulators, and systems infrastructure for this toolchain. Our methodology employs a parallel implementation of CMA-ES which enables simulations to proceed 6 times faster than real-world rollouts. We show our approach can reduce the lap times in autonomous racing, given a fixed computational budget. For all tested tracks, our method provides the lowest lap time, and relative improvements in lap time between 7-21%. We demonstrate improvements over a naive random search method with equivalent computational budget of over 15 seconds/lap, and improvements over expert solutions of over 2 seconds/lap. We further compare the performance of our method against hand-tuned solutions submitted by over 30 international teams, comprised of graduate students working in the field of autonomous vehicles. Finally, we discuss the effectiveness of utilizing an online planning mechanism to reduce the reality gap between our simulation and actual tests.
ER  - 

TY  - CONF
TI  - Risk Assessment and Planning with Bidirectional Reachability for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5363
EP  - 5369
AU  - M. -Y. Yu
AU  - R. Vasudevan
AU  - M. Johnson-Roberson
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - path planning
KW  - risk analysis
KW  - road safety
KW  - road traffic control
KW  - road vehicles
KW  - autonomous driving
KW  - subsequent planning
KW  - risk assessment algorithms
KW  - ego vehicle
KW  - risk-inducing factors
KW  - bidirectional reachability
KW  - risk planning
KW  - Risk management
KW  - Planning
KW  - Prediction algorithms
KW  - Autonomous vehicles
KW  - Robot sensing systems
KW  - Navigation
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9197491
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Risk assessment to quantify the danger associated with taking a certain action is critical to navigating safely through crowded urban environments during autonomous driving. Risk assessment and subsequent planning is usually done by first tracking and predicting trajectories of other agents, such as vehicles and pedestrians, and then choosing an action to avoid future collisions. However, few existing risk assessment algorithms handle occlusion and other sensory limitations effectively. One either assesses the risk in the worst-case scenario and thus makes the ego vehicle overly conservative, or predicts as many hidden agents as possible and thus makes the computation intensive. This paper explores the possibility of efficient risk assessment under occlusion via both forward and backward reachability. The proposed algorithm can not only identify the location of risk-inducing factors, but can also be used during motion planning. The proposed method is evaluated on various four-way highly occluded intersections with up to five other vehicles in the scene. Compared with other risk assessment algorithms, the proposed method shows better efficiency, meaning that the ego vehicle reaches the goal at a higher speed. In addition, it also lowers the median collision rate by 7.5√ó when compared to state of the art techniques.
ER  - 

TY  - CONF
TI  - Game theoretic decision making based on real sensor data for autonomous vehicles‚Äô maneuvers in high traffic
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5378
EP  - 5384
AU  - M. Garz√≥n
AU  - A. Spalanzani
PY  - 2020
KW  - decision making
KW  - game theory
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - road vehicles
KW  - sensors
KW  - sensor data
KW  - autonomous vehicles
KW  - iterative multiplayer game
KW  - game model
KW  - ego-vehicle
KW  - vehicle-to-vehicle communication
KW  - traffic simulator
KW  - game theoretic decision making
KW  - cognitive hierarchy reasoning
KW  - Monte Carlo reinforcement learning
KW  - Games
KW  - Automobiles
KW  - Mathematical model
KW  - Game theory
KW  - Autonomous vehicles
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197430
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an approach for implementing game theoretic decision making in combination with realistic sensory data input so as to allow an autonomous vehicle to perform maneuvers, such as lane change or merge in high traffic scenarios. The main novelty of this work, is the use of realistic sensory data input to obtain the observations as input of an iterative multi-player game in a realistic simulator. The game model allows to anticipate reactions of additional vehicles to the movements of the ego-vehicle without using any specific coordination or vehicle-to-vehicle communication. Moreover, direct information from the simulator, such as position or speed of the vehicles is also avoided.The solution of the game is based on cognitive hierarchy reasoning and it uses Monte Carlo reinforcement learning in order to obtain a near-optimal policy towards a specific goal. Moreover, the game proposed is capable of solving different situations using a single policy. The system has been successfully tested and compared with previous techniques using a realistic hybrid simulator, where the ego-vehicle and its sensors are simulated on a 3D simulator and the additional vehicles' behavior is obtained from a traffic simulator.
ER  - 

TY  - CONF
TI  - Driving in Dense Traffic with Model-Free Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5385
EP  - 5392
AU  - D. M. Saxena
AU  - S. Bae
AU  - A. Nakhaei
AU  - K. Fujimura
AU  - M. Likhachev
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - predictive control
KW  - road traffic control
KW  - traffic engineering computing
KW  - dense traffic
KW  - model-free reinforcement learning
KW  - control methods
KW  - autonomous vehicle
KW  - obstacle-free volume
KW  - deep reinforcement learning
KW  - continuous control policy
KW  - target road lane
KW  - model-predictive control-based algorithms
KW  - Roads
KW  - Autonomous vehicles
KW  - Learning (artificial intelligence)
KW  - Task analysis
KW  - Benchmark testing
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197132
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Traditional planning and control methods could fail to find a feasible trajectory for an autonomous vehicle to execute amongst dense traffic on roads. This is because the obstacle-free volume in spacetime is very small in these scenarios for the vehicle to drive through. However, that does not mean the task is infeasible since human drivers are known to be able to drive amongst dense traffic by leveraging the cooperativeness of other drivers to open a gap. The traditional methods fail to take into account the fact that the actions taken by an agent affect the behaviour of other vehicles on the road. In this work, we rely on the ability of deep reinforcement learning to implicitly model such interactions and learn a continuous control policy over the action space of an autonomous vehicle. The application we consider requires our agent to negotiate and open a gap in the road in order to successfully merge or change lanes. Our policy learns to repeatedly probe into the target road lane while trying to find a safe spot to move in to. We compare against two model-predictive control-based algorithms and show that our policy outperforms them in simulation. As part of this work, we introduce a benchmark for driving in dense traffic for use by the community.
ER  - 

TY  - CONF
TI  - Enhancing Game-Theoretic Autonomous Car Racing Using Control Barrier Functions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5393
EP  - 5399
AU  - G. Notomista
AU  - M. Wang
AU  - M. Schwager
AU  - M. Egerstedt
PY  - 2020
KW  - collision avoidance
KW  - computer games
KW  - game theory
KW  - game-theoretic autonomous car
KW  - control barrier functions
KW  - two-player racing game
KW  - autonomous ego vehicle
KW  - opponent vehicle
KW  - two-car racing game
KW  - game-theoretic control method hinges
KW  - sensitivity-enhanced Nash equilibrium
KW  - Collision avoidance
KW  - Trajectory
KW  - Robots
KW  - Acceleration
KW  - Safety
KW  - Bicycles
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9196757
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider a two-player racing game, where an autonomous ego vehicle has to be controlled to race against an opponent vehicle, which is either autonomous or human-driven. The approach to control the ego vehicle is based on a Sensitivity-ENhanced NAsh equilibrium seeking (SENNA) method, which uses an iterated best response algorithm in order to optimize for a trajectory in a two-car racing game. This method exploits the interactions between the ego and the opponent vehicle that take place through a collision avoidance constraint. This game-theoretic control method hinges on the ego vehicle having an accurate model and correct knowledge of the state of the opponent vehicle. However, when an accurate model for the opponent vehicle is not available, or the estimation of its state is corrupted by noise, the performance of the approach might be compromised. For this reason, we augment the SENNA algorithm by enforcing Permissive RObust SafeTy (PROST) conditions using control barrier functions. The objective is to successfully overtake or to remain in the front of the opponent vehicle, even when the information about the latter is not fully available. The successful synergy between SENNA and PROST-antithetical to the notable rivalry between the two namesake Formula 1 drivers-is demonstrated through extensive simulated experiments.
ER  - 

TY  - CONF
TI  - SPRINT: Subgraph Place Recognition for INtelligent Transportation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5408
EP  - 5414
AU  - Y. Latif
AU  - A. -D. Doan
AU  - T. -J. Chin
AU  - I. Reid
PY  - 2020
KW  - image recognition
KW  - image retrieval
KW  - intelligent transportation systems
KW  - nearest neighbour methods
KW  - traffic engineering computing
KW  - scalable place recognition
KW  - environmental conditions
KW  - visual place recognition
KW  - mobile robotics
KW  - image information
KW  - image acquisition process
KW  - k nearest neighbours
KW  - image retrieval
KW  - temporal relations
KW  - inference time reduction
KW  - databases
KW  - publicly sourced data
KW  - subgraph place recognition for intelligent transportation
KW  - SPRINT
KW  - Image recognition
KW  - Hidden Markov models
KW  - Robots
KW  - Symmetric matrices
KW  - Cameras
KW  - Image retrieval
DO  - 10.1109/ICRA40945.2020.9196522
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual place recognition is an important problem in mobile robotics which aims to localize a robot using image information alone. Recent methods have shown promising results for place recognition under varying environmental conditions by exploiting the sequential nature of the image acquisition process. We show that by using k nearest neighbours based image retrieval as the backend, and exploiting the structure of the image acquisition process which introduces temporal relations between images in the database, the location of possible matches can be restricted to a subset of all the images seen so far. In effect, the original problem space can thus be restricted to a significantly smaller subspace, reducing the inference time significantly. This is particularly important for scalable place recognition over databases containing millions of images. We present large scale experiments using publicly sourced data that show the computational performance of the proposed method under varying environmental conditions.
ER  - 

TY  - CONF
TI  - OneShot Global Localization: Instant LiDAR-Visual Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5415
EP  - 5421
AU  - S. Ratz
AU  - M. Dymczyk
AU  - R. Siegwart
AU  - R. Dub√©
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image matching
KW  - image segmentation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optical radar
KW  - pose estimation
KW  - robot vision
KW  - learning-based descriptors
KW  - point cloud segments
KW  - degree-of-freedom
KW  - LiDAR scan
KW  - neural network architecture
KW  - segment retrieval rates
KW  - LiDAR-only description
KW  - OneShot global localization
KW  - autonomous navigation tasks
KW  - LiDAR-visual pose estimation
KW  - LiDAR-only approach
KW  - degree-of-freedom pose
KW  - NCLT dataset
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Robots
KW  - Sensors
KW  - Image segmentation
KW  - Neural networks
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197458
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Globally localizing in a given map is a crucial ability for robots to perform a wide range of autonomous navigation tasks. This paper presents OneShot - a global localization algorithm that uses only a single 3D LiDAR scan at a time, while outperforming approaches based on integrating a sequence of point clouds. Our approach, which does not require the robot to move, relies on learning-based descriptors of point cloud segments and computes the full 6 degree-of-freedom pose in a map. The segments are extracted from the current LiDAR scan and are matched against a database using the computed descriptors. Candidate matches are then verified with a geometric consistency test. We additionally present a strategy to further improve the performance of the segment descriptors by augmenting them with visual information provided by a camera. For this purpose, a custom-tailored neural network architecture is proposed. We demonstrate that our LiDAR-only approach outperforms a state-of-the-art baseline on a sequence of the KITTI dataset and also evaluate its performance on the challenging NCLT dataset. Finally, we show that fusing in visual information boosts segment retrieval rates by up to 26% compared to LiDAR-only description.
ER  - 

TY  - CONF
TI  - Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5446
EP  - 5452
AU  - M. Lechner
AU  - R. Hasani
AU  - D. Rus
AU  - R. Grosu
PY  - 2020
KW  - collision avoidance
KW  - control engineering computing
KW  - eigenvalues and eigenfunctions
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - neurocontrollers
KW  - recurrent neural nets
KW  - mobile robot
KW  - traditional robotic control suits
KW  - profound task-specific knowledge
KW  - building
KW  - testing control software
KW  - deep learning
KW  - end-to-end solutions
KW  - minimal knowledge
KW  - end-to-end linear dynamical systems
KW  - LDS
KW  - robotic domains
KW  - regularization loss component
KW  - learning algorithm
KW  - learned autonomous system
KW  - simulated robotic experiments
KW  - stabilizing method
KW  - end-to-end robot learning scheme
KW  - recurrent neural network compartment
KW  - gershgorin loss
KW  - Stability analysis
KW  - Eigenvalues and eigenfunctions
KW  - Recurrent neural networks
KW  - Training
KW  - Robots
KW  - Heuristic algorithms
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196608
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.
ER  - 

TY  - CONF
TI  - Mini-Batched Online Incremental Learning Through Supervisory Teleoperation with Kinesthetic Coupling*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5453
EP  - 5459
AU  - H. Latifee
AU  - A. Pervez
AU  - J. -H. Ryu
AU  - D. Lee
PY  - 2020
KW  - expectation-maximisation algorithm
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - telerobotics
KW  - kinesthetic coupling
KW  - online incremental learning approach
KW  - task execution
KW  - teleoperation-based teaching
KW  - partial demonstration
KW  - dynamic authority distribution
KW  - modified partial trajectory
KW  - rhythmic peg-in-hole teleoperation task
KW  - partial modification
KW  - task operation
KW  - mini-batched online incremental learning
KW  - supervisory teleoperation
KW  - expectation-maximization algorithm
KW  - Task analysis
KW  - Couplings
KW  - Force
KW  - Trajectory
KW  - Robots
KW  - Education
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9197444
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose an online incremental learning approach through teleoperation which allows an operator to partially modify a learned model, whenever it is necessary, during task execution. Compared to conventional incremental learning approaches, the proposed approach is applicable for teleoperation-based teaching and it needs only partial demonstration without any need to obstruct the task execution. Dynamic authority distribution and kinesthetic coupling between the operator and the agent helps the operator to correctly perceive the exact instance where modification needs to be asserted in the agent's behaviour online using partial trajectory. For this, we propose a variation of the Expectation-Maximization algorithm for updating original model through mini batches of the modified partial trajectory. The proposed approach reduces human workload and latency for a rhythmic peg-in-hole teleoperation task where online partial modification is required during the task operation.
ER  - 

TY  - CONF
TI  - Recurrent Neural Network Control of a Hybrid Dynamical Transfemoral Prosthesis with EdgeDRNN Accelerator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5460
EP  - 5466
AU  - C. Gao
AU  - R. Gehlhar
AU  - A. D. Ames
AU  - S. -C. Liu
AU  - T. Delbruck
PY  - 2020
KW  - artificial limbs
KW  - gait analysis
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - medical computing
KW  - medical robotics
KW  - neurocontrollers
KW  - PD control
KW  - recurrent neural nets
KW  - AMPRO3 prosthetic leg
KW  - real-time dynamical system
KW  - human-prosthesis system
KW  - prosthesis control
KW  - recurrent neural network control
KW  - hybrid dynamical transfemoral prosthesis
KW  - control methods
KW  - modulating behaviors
KW  - dynamical robotic assistive devices
KW  - behavioral cloning
KW  - powered transfemoral prostheses
KW  - Gated Recurrent Unit
KW  - custom hardware accelerator
KW  - prosthesis controller
KW  - RNN inference
KW  - nominal PD controller
KW  - EdgeDRNN accelerator
KW  - Prosthetics
KW  - Legged locomotion
KW  - PD control
KW  - Trajectory
KW  - Real-time systems
KW  - Knee
DO  - 10.1109/ICRA40945.2020.9196984
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Lower leg prostheses could improve the life quality of amputees by increasing comfort and reducing energy to locomote, but currently control methods are limited in modulating behaviors based upon the human's experience. This paper describes the first steps toward learning complex controllers for dynamical robotic assistive devices. We provide the first example of behavioral cloning to control a powered transfemoral prostheses using a Gated Recurrent Unit (GRU) based recurrent neural network (RNN) running on a custom hardware accelerator that exploits temporal sparsity. The RNN is trained on data collected from the original prosthesis controller. The RNN inference is realized by a novel EdgeDRNN accelerator in real-time. Experimental results show that the RNN can replace the nominal PD controller to realize end-to-end control of the AMPRO3 prosthetic leg walking on flat ground and unforeseen slopes with comparable tracking accuracy. EdgeDRNN computes the RNN about 240 times faster than real time, opening the possibility of running larger networks for more complex tasks in the future. Implementing an RNN on this real-time dynamical system with impacts sets the ground work to incorporate other learned elements of the human-prosthesis system into prosthesis control.
ER  - 

TY  - CONF
TI  - Cross-context Visual Imitation Learning from Demonstrations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5467
EP  - 5473
AU  - S. Yang
AU  - W. Zhang
AU  - W. Lu
AU  - H. Wang
AU  - Y. Li
PY  - 2020
KW  - learning (artificial intelligence)
KW  - robots
KW  - general imitation learning method
KW  - robotic system
KW  - context translation model
KW  - depth prediction model
KW  - multimodal inverse dynamics model
KW  - depth observation
KW  - inverse model maps
KW  - multimodal observations
KW  - cross-context learning advantage
KW  - cross-context visual imitation learning
KW  - color observation
KW  - block stacking tasks
KW  - Context modeling
KW  - Robots
KW  - Task analysis
KW  - Inverse problems
KW  - Visualization
KW  - Predictive models
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196868
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Imitation learning enables robots to learn a task by simply watching the demonstration of the task. Current imitation learning methods usually require the learner and demonstrator to occur in the same context. This limits their scalability to practical applications. In this paper, we propose a more general imitation learning method which allows the learner and the demonstrator to come from different contexts, such as different viewpoints, backgrounds, and object positions and appearances. Specifically, we design a robotic system consisting of three models: context translation model, depth prediction model and multi-modal inverse dynamics model. First, the context translation model translates the demonstration to the context of learner from a different context. Then combining the color observation and depth observation as inputs, the inverse model maps the multi-modal observations into actions to reproduce the demonstration, where the depth observation is provided by a depth prediction model. By performing the block stacking tasks both in simulation and real world, we prove the cross-context learning advantage of the proposed robotic system over other systems.
ER  - 

TY  - CONF
TI  - Improving Generalisation in Learning Assistance by Demonstration for Smart Wheelchairs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5474
EP  - 5480
AU  - V. Schettino
AU  - Y. Demiris
PY  - 2020
KW  - Gaussian processes
KW  - generalisation (artificial intelligence)
KW  - handicapped aids
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - wheelchairs
KW  - Gaussian process
KW  - learning assistance by demonstration
KW  - human agent
KW  - machine learning models
KW  - dimensionality reduction techniques
KW  - learning system
KW  - custom teleoperation
KW  - LAD
KW  - generalisation capability
KW  - assistive power
KW  - learned assistive policy
KW  - customised assistance
KW  - smart wheelchairs
KW  - Wheelchairs
KW  - Training
KW  - Robots
KW  - Vehicles
KW  - Haptic interfaces
KW  - Sensors
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197490
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning Assistance by Demonstration (LAD) is concerned with using demonstrations of a human agent to teach a robot how to assist another human. The concept has previously been used with smart wheelchairs to provide customised assistance to individuals with driving difficulties. A basic premise of this technique is that the learned assistive policy should be able to generalise to environments different than the ones used for training; but this has not been tested before. In this work we evaluate the assistive power and the generalisation capability of LAD using our custom teleoperation and learning system for smart wheelchairs, while seeking to improve it by experimenting with different combinations of dimensionality reduction techniques and machine learning models. Using Autoencoders to reduce the dimension of laserscan data and a Gaussian Process as the learning model, we achieved a 23% improvement in prediction performance against the combination used by the latest work on the field. Using this model to assist a driver exposed to a simulated disability, we observed a 9.8% reduction in track completion times when compared to driving without assistance.
ER  - 

TY  - CONF
TI  - Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5481
EP  - 5487
AU  - M. Naumann
AU  - L. Sun
AU  - W. Zhan
AU  - M. Tomizuka
PY  - 2020
KW  - behavioural sciences computing
KW  - driver information systems
KW  - learning (artificial intelligence)
KW  - path planning
KW  - road safety
KW  - road traffic
KW  - road vehicles
KW  - vehicles
KW  - human drivers
KW  - interactive driving
KW  - cooperative behavior
KW  - dense traffic
KW  - autonomous vehicles
KW  - safer interaction
KW  - cost function selection
KW  - cost function structures
KW  - inverse reinforcement learning
KW  - human driven trajectories
KW  - human driving behavior imitation
KW  - human driving behavior explanation
KW  - theory of mind
KW  - Cost function
KW  - Trajectory
KW  - Vehicles
KW  - Roads
KW  - Learning (artificial intelligence)
KW  - Safety
KW  - Planning
KW  - Automated vehicles
KW  - cost function
KW  - inverse reinforcement learning
KW  - imitation learning
KW  - cooperative motion planning
DO  - 10.1109/ICRA40945.2020.9196795
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous vehicles are sharing the road with human drivers. In order to facilitate interactive driving and cooperative behavior in dense traffic, a thorough understanding and representation of other traffic participants' behavior are necessary. Cost functions (or reward functions) have been widely used to describe the behavior of human drivers since they can not only explicitly incorporate the rationality of human drivers and the theory of mind (TOM), but also share similarity with the motion planning problem of autonomous vehicles. Hence, more human-like driving behavior and comprehensible trajectories can be generated to enable safer interaction and cooperation. However, the selection of cost functions in different driving scenarios is not trivial, and there is no systematic summary and analysis for cost function selection and learning from a variety of driving scenarios. In this work, we aim to investigate to what extent cost functions are suitable for explaining and imitating human driving behavior. Further, we focus on how cost functions differ from each other in different driving scenarios. Towards this goal, we first comprehensively review existing cost function structures in literature. Based on that, we point out required conditions for demonstrations to be suitable for inverse reinforcement learning (IRL). Finally, we use IRL to explore suitable features and learn cost function weights from human driven trajectories in three different scenarios.
ER  - 

TY  - CONF
TI  - Magnetic Sensor Based Topographic Localization for Automatic Dislocation of Ingested Button Battery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5488
EP  - 5494
AU  - J. Liu
AU  - H. Sugiyama
AU  - T. Nakayama
AU  - S. Miyashita
PY  - 2020
KW  - cells (electric)
KW  - coils
KW  - ferromagnetic materials
KW  - magnetic sensors
KW  - medical robotics
KW  - Hall-effect sensors
KW  - LR44 button battery
KW  - magnet-containing capsule
KW  - magnetic sensor
KW  - topographic localization
KW  - ingested button battery
KW  - stomach
KW  - battery lands
KW  - ingestible magnetic robot
KW  - button battery retrieval
KW  - localization method
KW  - magnetic sensors
KW  - direct magnetic field
KW  - trilateration method
KW  - Batteries
KW  - Magnetic sensors
KW  - Magnetic resonance imaging
KW  - Magnetization
KW  - Magnetic separation
KW  - Stomach
DO  - 10.1109/ICRA40945.2020.9196546
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A button battery accidentally ingested by a toddler or small child can cause severe damage to the stomach within a short period of time. Once a battery lands on the surface of the esophagus or stomach, it can run a current in the tissue and induce a chemical reaction resulting in injury. Following our previous work where we presented an ingestible magnetic robot for button battery retrieval, this study presents a remotely achieved novel localization method of a button battery with commonly available magnetic sensors (Hall-effect sensors). By applying a direct magnetic field to the button battery using an electromagnetic coil, the battery is magnetized, and hence it becomes able to be sensed by Hall-effect sensors. Using a trilateration method, we were able to detect the locations of an LR44 button battery and other ferromagnetic materials at variable distances. Additional four electromagnetic coils were used to autonomously navigate a magnet-containing capsule to dislocate the battery from the affected site.
ER  - 

TY  - CONF
TI  - A Fully Actuated Body-Mounted Robotic Assistant for MRI-Guided Low Back Pain Injection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5495
EP  - 5501
AU  - G. Li
AU  - N. A. Patel
AU  - W. Liu
AU  - D. Wu
AU  - K. Sharma
AU  - K. Cleary
AU  - J. Fritz
AU  - I. Iordachita
PY  - 2020
KW  - actuators
KW  - biomedical MRI
KW  - medical image processing
KW  - medical robotics
KW  - mobile robots
KW  - needles
KW  - optical tracking
KW  - position control
KW  - telerobotics
KW  - fully actuated body-mounted robotic assistant
KW  - MRI-guided low back pain injection
KW  - needle alignment module
KW  - remotely actuated needle driver module
KW  - fully actuated robot
KW  - remote actuation design
KW  - actuation electronics
KW  - robot frame
KW  - minimal visible image degradation
KW  - size 0.53 mm to 1.45 mm
KW  - size 0.7 mm
KW  - Needles
KW  - Robots
KW  - Magnetic resonance imaging
KW  - Pain
KW  - Force
KW  - Back
DO  - 10.1109/ICRA40945.2020.9197534
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper reports the development of a fully actuated body-mounted robotic assistant for MRI-guided low back pain injection. The robot is designed with a 4-DOF needle alignment module and a 2-DOF remotely actuated needle driver module. The 6-DOF fully actuated robot can operate inside the scanner bore during imaging; hence, minimizing the need of moving the patient in or out of the scanner during the procedure, and thus potentially reducing the procedure time and streamlining the workflow. The robot is built with a lightweight and compact structure that can be attached directly to the patient's lower back using straps; therefore, attenuating the effect of patient motion by moving with the patient. The novel remote actuation design of the needle driver module with beaded chain transmission can reduce the weight and profile on the patient, as well as minimize the imaging degradation caused by the actuation electronics. The free space positioning accuracy of the system was evaluated with an optical tracking system, demonstrating the mean absolute errors (MAE) of the tip position to be 0.99¬±0.46 mm and orientation to be 0.99¬±0.65¬∞. Qualitative imaging quality evaluation was performed¬± on a human volunteer, revealing minimal visible image degradation that should not affect the procedure. The mounting stability of the system was assessed on a human volunteer, indicating the 3D position variation of target movement with respect to the robot frame to be less than 0.7 mm.
ER  - 

TY  - CONF
TI  - Fault Tolerant Control in Shape-Changing Internal Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5502
EP  - 5508
AU  - L. Balasubramanian
AU  - T. Wray
AU  - D. D. Damian
PY  - 2020
KW  - biological tissues
KW  - fault tolerance
KW  - medical robotics
KW  - redundancy
KW  - surgery
KW  - shape-changing internal robots
KW  - human body
KW  - foreign body
KW  - in-vivo robot
KW  - implantable robot
KW  - fault tolerant control
KW  - fault diagnosis
KW  - redundancy-based control
KW  - fault-tolerant capabilities
KW  - fault risks
KW  - implantable robots
KW  - surgical robots
KW  - Implants
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Robot sensing systems
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196989
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is known that the interior of the human body is one of the most adverse environments for a foreign body, such as an in-vivo robot, and vice-versa. As robots operating in-vivo are increasingly recognized for their capabilities and potential for improved therapies, it is important to ensure their safety, especially for long term treatments when little supervision can be provided. We introduce an implantable robot that is flexible, extendable and symmetric, thus changing shape and size. This design allows the implementation of an effective fault tolerant control, with features such as physical polling for fault diagnosis, retraction and redundancy-based control switching at fault. We demonstrate the fault-tolerant capabilities for an implantable robot that elongates tubular tissues by applying tension to the tissue. In benchtop tests, we show a reduction of the fault risks by at least 83%. The study provides a valuable methodology to enhance safety and efficacy of implantable and surgical robots, and thus to accelerate their adoption.
ER  - 

TY  - CONF
TI  - Evaluation of Increasing Camera Baseline on Depth Perception in Surgical Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5509
EP  - 5515
AU  - A. Avinash
AU  - A. E. Abdelaal
AU  - S. E. Salcudean
PY  - 2020
KW  - endoscopes
KW  - medical robotics
KW  - surgery
KW  - visual perception
KW  - camera baseline
KW  - depth perception
KW  - surgical robotics
KW  - robot-assisted surgery
KW  - surgical trocar
KW  - fixed baseline
KW  - stereoscopic pickup camera
KW  - da Vinci surgical system
KW  - robot-assisted surgical systems
KW  - flexible baseline design
KW  - adaptive baseline camera design
KW  - clinical stereoendoscopes
KW  - clinical stereoendoscopes
KW  - Cameras
KW  - Robot vision systems
KW  - Surgery
KW  - Stereo image processing
KW  - Endoscopes
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197235
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we evaluate the effect of increasing camera baselines on depth perception in robot-assisted surgery. Restricted by the diameter of the surgical trocar through which they are inserted, current clinical stereo endoscopes have a fixed baseline of 5.5 mm. To overcome this restriction, we propose using a stereoscopic "pickup" camera with a side-firing design that allows for larger baselines. We conducted a user study with baselines of 10 mm, 15 mm, 20 mm, and 30 mm to evaluate the effect of increasing baseline on depth perception when used with the da Vinci surgical system. Subjects (N=28) were recruited and asked to rank differently sized poles, mounted at a distance of 200 mm from the cameras, according to their increasing order of height when viewed under different baseline conditions. The results showed that subjects performed better as the baseline was increased with the best performance at a 20 mm baseline. This preliminary proof-of-concept study shows that there is opportunity to improve depth perception in robot-assisted surgical systems with a change in endoscope design philosophy. In this paper, we present this change with our side-firing "pickup" camera and its flexible baseline design. Ultimately, this serves as the first step towards an adaptive baseline camera design that maximizes depth perception in surgery.
ER  - 

TY  - CONF
TI  - Toward Autonomous Robotic Micro-Suturing using Optical Coherence Tomography Calibration and Path Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5516
EP  - 5522
AU  - Y. Tian
AU  - M. Draelos
AU  - G. Tang
AU  - R. Qian
AU  - A. Kuo
AU  - J. Izatt
AU  - K. Hauser
PY  - 2020
KW  - biological tissues
KW  - biomedical optical imaging
KW  - calibration
KW  - control engineering computing
KW  - iterative methods
KW  - medical image processing
KW  - medical robotics
KW  - mobile robots
KW  - needles
KW  - optical tomography
KW  - path planning
KW  - phantoms
KW  - pose estimation
KW  - surgery
KW  - autonomous robotic microsuturing
KW  - prior autonomous suturing robots
KW  - planned entry
KW  - suture needle
KW  - animal tissue
KW  - tissue phantoms
KW  - exit points
KW  - iterative closest points
KW  - keypoint identification
KW  - wound detection
KW  - robot-needle transforms
KW  - robot-OCT
KW  - imaging feedback
KW  - 3D optical coherence tomography system
KW  - robotic suturing system
KW  - soft tissue
KW  - sub-millimeter precision
KW  - human surgeons
KW  - robotic automation
KW  - path planning
KW  - optical coherence tomography calibration
KW  - Needles
KW  - Robots
KW  - Wounds
KW  - Calibration
KW  - Surgery
KW  - Imaging
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196834
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic automation has the potential to assist human surgeons in performing suturing tasks in microsurgery, and in order to do so a robot must be able to guide a needle with sub-millimeter precision through soft tissue. This paper presents a robotic suturing system that uses 3D optical coherence tomography (OCT) system for imaging feedback. Calibration of the robot-OCT and robot-needle transforms, wound detection, keypoint identification, and path planning are all performed automatically. The calibration method handles pose uncertainty when the needle is grasped using a variant of iterative closest points. The path planner uses the identified wound shape to calculate needle entry and exit points to yield an evenly-matched wound shape after closure. Experiments on tissue phantoms and animal tissue demonstrate that the system can pass a suture needle through wounds with 0.200 mm overall accuracy in achieving the planned entry and exit points, and over 20√ó more precise than prior autonomous suturing robots.
ER  - 

TY  - CONF
TI  - Improved Multiple Objects Tracking based Autonomous Simultaneous Magnetic Actuation & Localization for WCE
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5523
EP  - 5529
AU  - Y. Xu
AU  - K. Li
AU  - Z. Zhao
AU  - M. Q. . -H. Meng
PY  - 2020
KW  - biological organs
KW  - biomedical optical imaging
KW  - endoscopes
KW  - interpolation
KW  - medical image processing
KW  - object tracking
KW  - phantoms
KW  - autonomous simultaneous magnetic actuation & localization
KW  - wireless capsule endoscopy
KW  - gastrointestinal examinations
KW  - clinical applications
KW  - rotating magnet
KW  - robotic arm
KW  - internal magnetic ring
KW  - magnetic fields
KW  - external sensor array
KW  - spherical linear interpolation
KW  - actuation-localization loop
KW  - Bezier Curve Gradient
KW  - normal vector fitting
KW  - frequency 25.0 Hz
KW  - Magnetic moments
KW  - Magnetic separation
KW  - Object tracking
KW  - Magnetic resonance imaging
KW  - Interpolation
KW  - Actuators
KW  - Fitting
KW  - Simultaneous Magnetic Actuation and Localization
KW  - Multiple Objects Tracking
KW  - Wireless Capsule Endoscopy
DO  - 10.1109/ICRA40945.2020.9197142
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Wireless Capsule Endoscopy (WCE) has the advantage of reducing the invasiveness and pain of gastrointestinal examinations. In this work, we propose a system aimed at autonomously accelerating and locating the WCE inside the intestine for clinical applications. A rotating magnet controlled by a robotic arm is placed outside the patient's body to actuate the capsule with an internal magnetic ring, and the magnetic fields of the two sources are measured by an external sensor array. The original Multiple Objects Tracking method is improved by combining Normal Vector Fitting, B√©zier Curve Gradient, and Spherical Linear Interpolation to estimate the 6-D pose of the WCE from a 5-D pose sequence. In order to close the actuation-localization loop, a strategy is presented to react to different states of the capsule. The proposed method is validated via experiments on phantoms as well as on animal intestines. The localization of the capsule shows an accuracy of 3.5mm in position and 9.4¬∞ in orientation, and the average update frequency of the estimated 6-D pose reaches 25Hz.
ER  - 

TY  - CONF
TI  - Probe-before-step walking strategy for multi-legged robots on terrain with risk of collapse
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5530
EP  - 5536
AU  - E. Tennakoon
AU  - T. Peynot
AU  - J. Roberts
AU  - N. Kottege
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - mobile robots
KW  - stability
KW  - probe-before-step walking strategy
KW  - multilegged robots
KW  - rough terrain
KW  - safe footholds
KW  - hexapod robot
KW  - terrain probing approach
KW  - follow-the-leader strategy
KW  - stabilisation
KW  - Legged locomotion
KW  - Probes
KW  - Foot
KW  - Robot sensing systems
KW  - Force
KW  - Australia
DO  - 10.1109/ICRA40945.2020.9197154
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-legged robots are effective at traversing rough terrain. However, terrains that include collapsible footholds (i.e. regions that can collapse when stepped on) remain a significant challenge, especially since such situations can be extremely difficult to anticipate using only exteroceptive sensing. State-of-the-art methods typically use various stabilisation techniques to regain balance and counter changing footholds. However, these methods are likely to fail if safe footholds are sparse and spread out or if the robot does not respond quickly enough after a foothold collapse. This paper presents a novel method for multi-legged robots to probe and test the terrain for collapses using its legs while walking. The proposed method improves on existing terrain probing approaches, and integrates the probing action into a walking cycle. A follow-the-leader strategy with a suitable gait and stance is presented and implemented on a hexapod robot. The proposed method is experimentally validated, demonstrating the robot can safely traverse terrain containing collapsible footholds.
ER  - 

TY  - CONF
TI  - Representing Multi-Robot Structure through Multimodal Graph Embedding for the Selection of Robot Teams
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5576
EP  - 5582
AU  - B. Reily
AU  - C. Reardon
AU  - H. Zhang
PY  - 2020
KW  - directed graphs
KW  - mobile robots
KW  - multi-robot systems
KW  - unsupervised learning
KW  - multirobot system
KW  - multirobot structure
KW  - human-robot teaming
KW  - multimodal graph embedding
KW  - robot teams selection
KW  - directed graphs
KW  - asymmetrical relationships
KW  - unsupervised learning
KW  - physical robots
KW  - multifaceted internal structures
KW  - graph embedding-based division methods
KW  - Robots
KW  - Task analysis
KW  - Multi-robot systems
KW  - Indexes
KW  - Organizations
KW  - Resource management
KW  - Biology
DO  - 10.1109/ICRA40945.2020.9197389
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-robot systems of increasing size and complexity are used to solve large-scale problems, such as area exploration and search and rescue. A key decision in human-robot teaming is dividing a multi-robot system into teams to address separate issues or to accomplish a task over a large area. In order to address the problem of selecting teams in a multi-robot system, we propose a new multimodal graph embedding method to construct a unified representation that fuses multiple information modalities to describe and divide a multi-robot system. The relationship modalities are encoded as directed graphs that can encode asymmetrical relationships, which are embedded into a unified representation for each robot. Then, the constructed multimodal representation is used to determine teams based upon unsupervised learning. We per-form experiments to evaluate our approach on expert-defined team formations, large-scale simulated multi-robot systems, and a system of physical robots. Experimental results show that our method successfully decides correct teams based on the multifaceted internal structures describing multi-robot systems, and outperforms baseline methods based upon only one mode of information, as well as other graph embedding-based division methods.
ER  - 

TY  - CONF
TI  - MAMS-A*: Multi-Agent Multi-Scale A*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5583
EP  - 5589
AU  - J. Lim
AU  - P. Tsiotras
PY  - 2020
KW  - graph theory
KW  - multi-agent systems
KW  - path planning
KW  - search problems
KW  - local search
KW  - local inconsistency conditions
KW  - common subgraph
KW  - provably optimal path
KW  - informative graph
KW  - search algorithm
KW  - distributed agents
KW  - single-query shortest path
KW  - search space
KW  - common environment
KW  - multiresolution graph
KW  - multiagent multiscale A*
KW  - Planning
KW  - Hypercubes
KW  - Search problems
KW  - Legged locomotion
KW  - Wavelet transforms
KW  - Aerospace engineering
KW  - Electronic mail
DO  - 10.1109/ICRA40945.2020.9197045
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a multi-scale forward search algorithm for distributed agents to solve single-query shortest path planning problems. Each agent first builds a representation of its own search space of the common environment as a multi-resolution graph, it communicates with the other agents the result of its local search, and it uses received information from other agents to refine its own graph and update the local inconsistency conditions. As a result, all agents attain a common subgraph that includes a provably optimal path in the most informative graph available among all agents, if one exists, without necessarily communicating the entire graph. We prove the completeness and optimality of the proposed algorithm, and present numerical results supporting the advantages of the proposed approach.
ER  - 

TY  - CONF
TI  - Connectivity Maintenance: Global and Optimized approach through Control Barrier Functions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5590
EP  - 5596
AU  - B. Capelli
AU  - L. Sabattini
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - optimal control
KW  - optimisation
KW  - connectivity maintenance
KW  - Control Barrier functions
KW  - multirobot system
KW  - local connectivity
KW  - global connectivity
KW  - formation control
KW  - Control Barrier Function
KW  - control strategy
KW  - Robots
KW  - Laplace equations
KW  - Multi-robot systems
KW  - Eigenvalues and eigenfunctions
KW  - Maintenance engineering
KW  - Task analysis
KW  - Control systems
DO  - 10.1109/ICRA40945.2020.9197109
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Connectivity maintenance is an essential aspect to consider while controlling a multi-robot system. In general, a multi-robot system should be connected to obtain a certain common objective. Connectivity must be kept regardless of the control strategy or the objective of the multi-robot system. Two main methods exist for connectivity maintenance: keep the initial connections (local connectivity) or allow modifications to the initial connections, but always keeping the overall system connected (global connectivity). In this paper we present a method that allows, at the same time, to maintain global connectivity and to implement the desired control strategy (e.g., consensus, formation control, coverage), all in an optimized fashion. For this purpose, we defined and implemented a Control Barrier Function that can incorporate constraints and objectives. We provide a mathematical proof of the method, and we demonstrate its versatility with simulations of different applications.
ER  - 

TY  - CONF
TI  - Controller Synthesis for Infinitesimally Shape-Similar Formations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5597
EP  - 5603
AU  - I. Buckley
AU  - M. Egerstedt
PY  - 2020
KW  - control system synthesis
KW  - decentralised control
KW  - multi-robot systems
KW  - multirobot team
KW  - network structure
KW  - controller synthesis
KW  - formation control
KW  - communication requirements
KW  - differential-drive robots
KW  - infinitesimally shape-similar formations
KW  - network topology
KW  - Robot sensing systems
KW  - Robot kinematics
KW  - Lyapunov methods
KW  - Trajectory
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9196591
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The interplay between network topology and the interaction modalities of a multi-robot team fundamentally impact the types of formations that can be achieved. To explore the trade-offs between network structure and the sensing and communication capabilities of individual robots, this paper applies controller synthesis to formation control of infinitesimally shape-similar frameworks, for which maintaining the relative angles between robots ensures invariance of the framework to translation, rotation, and uniform scaling. Beginning with the development of a controller for the sole purpose of maintaining the formation, the controller-synthesis approach is introduced as a mechanism for incorporating user- designated objectives while ensuring that the formation is maintained. Both centralized and decentralized formulations of the synthesized controller are presented, the resulting sensing and communication requirements are discussed, and the method is demonstrated on a team of differential-drive robots.
ER  - 

TY  - CONF
TI  - A Distributed Source Term Estimation Algorithm for Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5604
EP  - 5610
AU  - F. Rahbar
AU  - A. Martinoli
PY  - 2020
KW  - distributed control
KW  - mobile robots
KW  - multi-robot systems
KW  - airborne chemicals
KW  - mobile sensing systems
KW  - intelligent systems
KW  - odor source localization
KW  - homogeneous multirobot systems
KW  - multiple mobile robots
KW  - distributed system
KW  - distributed source term estimation
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Estimation
KW  - Navigation
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196959
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Finding sources of airborne chemicals with mobile sensing systems finds applications in safety, security, and emergency situations related to medical, domestic, and environmental domains. Given the often critical nature of all the applications, it is important to reduce the amount of time necessary to accomplish this task through intelligent systems and algorithms. In this paper, we extend a previously presented algorithm based on source term estimation for odor source localization for homogeneous multi-robot systems. By gradually increasing the level of coordination among multiple mobile robots, we study the benefits of a distributed system on reducing the amount of time and resources necessary to achieve the task at hand. The method has been evaluated systematically through high-fidelity simulations and in a wind tunnel emulating realistic and repeatable conditions in different coordination scenarios and with different number of robots.
ER  - 

TY  - CONF
TI  - Weighted Buffered Voronoi Cells for Distributed Semi-Cooperative Behavior
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5611
EP  - 5617
AU  - A. Pierson
AU  - W. Schwarting
AU  - S. Karaman
AU  - D. Rus
PY  - 2020
KW  - collision avoidance
KW  - computational geometry
KW  - game theory
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - selfish agent
KW  - relative cell
KW  - collision-free configuration
KW  - egoistic weights
KW  - altruistic agents
KW  - distributed semi
KW  - semicooperative multiagent navigation policies
KW  - collision avoidance
KW  - dynamic weights
KW  - lower relative weight
KW  - buffered distance
KW  - agent weights
KW  - selfish behavior
KW  - prioritized behavior
KW  - weighted buffered Voronoi tessellation
KW  - weighted buffered Voronoi cells
KW  - Navigation
KW  - Robot kinematics
KW  - Collision avoidance
KW  - Games
KW  - Planning
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196686
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces the Weighted Buffered Voronoi tessellation, which allows us to define distributed, semicooperative multi-agent navigation policies with guarantees on collision avoidance. We generate the Voronoi cells with dynamic weights that bias the boundary towards the agent with the lower relative weight while always maintaining a buffered distance between two agents. By incorporating agent weights, we can encode selfish or prioritized behavior among agents, where a more selfish agent will have a larger relative cell over less selfish agents. We consider this semi-cooperative since agents do not cooperate in symmetric ways. Furthermore, when all agents start in a collision-free configuration and plan their control actions within their cells, we prove that no agents will collide. Simulations demonstrate the performance of our algorithm for agents navigating to goal locations in a position-swapping game. We observe that agents with more egoistic weights consistently travel shorter paths to their goal than more altruistic agents.
ER  - 

TY  - CONF
TI  - Learning to Control Reconfigurable Staged Soft Arms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5618
EP  - 5624
AU  - A. Nicolai
AU  - G. Olson
AU  - Y. Meng√º√ß
AU  - G. A. Hollinger
PY  - 2020
KW  - actuators
KW  - calibration
KW  - end effectors
KW  - learning (artificial intelligence)
KW  - tapered arm configurations
KW  - soft robotic actuators
KW  - arm configurations
KW  - quasistatic model only control
KW  - controller baseline
KW  - control reconfigurable staged soft
KW  - system load states
KW  - soft arm configurations
KW  - stage approach
KW  - LSTM calibration routine
KW  - current load state
KW  - control input generation step
KW  - generalized quasistatic model
KW  - learned load model
KW  - Load modeling
KW  - Actuators
KW  - Soft robotics
KW  - Kinematics
KW  - Calibration
KW  - Training
KW  - Training data
DO  - 10.1109/ICRA40945.2020.9197516
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we present a novel approach for modeling, and classifying between, the system load states introduced when constructing staged soft arm configurations. Through a two stage approach: (1) an LSTM calibration routine is used to identify the current load state then (2) a control input generation step combines a generalized quasistatic model with the learned load model. Our experiments show that accounting for system load allows us to more accurately control tapered arm configurations. We analyze the performance of our method using soft robotic actuators and show it is capable of classifying between different arm configurations at a rate greater than 95%. Additionally, our method is capable of reducing the end-effector error of quasistatic model only control to within 1 cm of our controller baseline.
ER  - 

TY  - CONF
TI  - Modeling and Analysis of SMA Actuator Embedded in Stretchable Coolant Vascular Pursuing Artificial Muscles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5641
EP  - 5646
AU  - J. Jeong
AU  - C. Hoon Park
AU  - K. -U. Kyung
PY  - 2020
KW  - coils
KW  - coolants
KW  - cooling
KW  - design engineering
KW  - energy consumption
KW  - industrial robots
KW  - muscle
KW  - shape memory effects
KW  - springs (mechanical)
KW  - SMA coil spring
KW  - flexible response soft actuator
KW  - coolant circulation system
KW  - thermomechanical heat transfer model
KW  - maximum actuation frequency
KW  - SMA actuator embedded
KW  - stretchable coolant vascular pursuing artificial muscles
KW  - muscle-like SMA actuator
KW  - active cooling system
KW  - energy consumption
KW  - heating phase
KW  - design engineering
KW  - robots structure
KW  - shape memory alloy
KW  - frequency 0.5 Hz
KW  - Springs
KW  - Coolants
KW  - Actuators
KW  - Mathematical model
KW  - Force
KW  - Wires
DO  - 10.1109/ICRA40945.2020.9197090
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a muscle-like SMA (Shape Memory Alloy) actuator with an active cooling system for efficient response. An SMA coil spring is embedded into a stretchable coolant vascular for soften structure of robots. In order to design a flexible, lightweight, and fast-response soft actuator with the SMA coil spring and coolant circulation system, a modeling based approach has been conducted. Analysis of coolant effects has been conducted in aspects of heating speed, cooling speed, and energy consumption based on both theoretical and empirical studies. From thermomechanical and heat transfer model between SMA and coolant, the actuation times in the case of heating and cooling phase have been estimated. From experimental results, Mineral oil is selected as the optimal coolant, and the maximum actuation frequency was measured as 0.5Hz for 40% contraction lifting 1kg.
ER  - 

TY  - CONF
TI  - Grasping Unknown Objects by Coupling Deep Reinforcement Learning, Generative Adversarial Networks, and Visual Servoing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5655
EP  - 5662
AU  - O. -M. Pedersen
AU  - E. Misimi
AU  - F. Chaumette
PY  - 2020
KW  - grippers
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object recognition
KW  - pose estimation
KW  - robot vision
KW  - visual servoing
KW  - semicompliant objects
KW  - grasping experiments
KW  - deep learning
KW  - inaccurate agent gripper
KW  - visual servoing grasping task
KW  - CycleGAN
KW  - DRL
KW  - deep reinforcement learning grasping agent
KW  - generative adversarial networks
KW  - Grasping
KW  - Robots
KW  - Grippers
KW  - Training
KW  - Task analysis
KW  - Gallium nitride
KW  - Image segmentation
DO  - 10.1109/ICRA40945.2020.9197196
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a novel approach for transferring a deep reinforcement learning (DRL) grasping agent from simulation to a real robot, without fine tuning in the real world. The approach utilises a CycleGAN to close the reality gap between the simulated and real environments, in a reverse real-to-sim manner, effectively "tricking" the agent into believing it is still in the simulator. Furthermore, a visual servoing (VS) grasping task is added to correct for inaccurate agent gripper pose estimations derived from deep learning. The proposed approach is evaluated by means of real grasping experiments, achieving a success rate of 83 % on previously seen objects, and the same success rate for previously unseen, semi-compliant objects. The robustness of the approach is demonstrated by comparing it with two baselines, DRL plus CycleGAN, and VS only. The results clearly show that our approach outperforms both baselines.
ER  - 

TY  - CONF
TI  - Incorporating Motion Planning Feasibility Considerations during Task-Agent Assignment to Perform Complex Tasks Using Mobile Manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5663
EP  - 5670
AU  - A. M. Kabir
AU  - S. Thakar
AU  - P. M. Bhatt
AU  - R. K. Malhan
AU  - P. Rajendran
AU  - B. C. Shah
AU  - S. K. Gupta
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - task-agent assignment
KW  - complex tasks
KW  - multiarm mobile manipulators
KW  - multiple robotic agents
KW  - expensive motion planning queries
KW  - speed-up techniques
KW  - spatial constraint checking
KW  - conservative surrogates
KW  - symbolic conditions
KW  - high-DOF robotic agents
KW  - Task analysis
KW  - Planning
KW  - Manipulators
KW  - Robot kinematics
KW  - Containers
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196667
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multi-arm mobile manipulators can be represented as a combination of multiple robotic agents from the perspective of task-assignment and motion planning. Depending upon the task, agents might collaborate or work independently. Integrating motion planning with task-agent assignment is a computationally slow process as infeasible assignments can only be detected through expensive motion planning queries. We present three speed-up techniques for addressing this problem-(1) spatial constraint checking using conservative surrogates for motion planners, (2) instantiating symbolic conditions for pruning infeasible assignments, and (3) efficiently caching and reusing previously generated motion plans. We show that the developed method is useful for real-world operations that require complex interaction and coordination among high-DOF robotic agents.
ER  - 

TY  - CONF
TI  - Learning to Scaffold the Development of Robotic Manipulation Skills
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5671
EP  - 5677
AU  - L. Shao
AU  - T. Migimatsu
AU  - J. Bohg
PY  - 2020
KW  - learning systems
KW  - manipulators
KW  - position control
KW  - robust control
KW  - peg insertion
KW  - inaccurate motor control
KW  - robotic manipulation skills
KW  - shallow-depth insertion
KW  - wrench manipulation
KW  - robot positions
KW  - learning loops
KW  - learning system
KW  - scaffold manipulation skill learning
KW  - robot actions
KW  - robust manipulation
KW  - Task analysis
KW  - Tools
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Robustness
KW  - Motor drives
DO  - 10.1109/ICRA40945.2020.9197134
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning contact-rich, robotic manipulation skills is a challenging problem due to the high-dimensionality of the state and action space as well as uncertainty from noisy sensors and inaccurate motor control. To combat these factors and achieve more robust manipulation, humans actively exploit contact constraints in the environment. By adopting a similar strategy, robots can also achieve more robust manipulation. In this paper, we enable a robot to autonomously modify its environment and thereby discover how to ease manipulation skill learning. Specifically, we provide the robot with fixtures that it can freely place within the environment. These fixtures provide hard constraints that limit the outcome of robot actions. Thereby, they funnel uncertainty from perception and motor control and scaffold manipulation skill learning. We propose a learning system that consists of two learning loops. In the outer loop, the robot positions the fixture in the workspace. In the inner loop, the robot learns a manipulation skill and after a fixed number of episodes, returns the reward to the outer loop. Thereby, the robot is incentivised to place the fixture such that the inner loop quickly achieves a high reward. We demonstrate our framework both in simulation and in the real world on three tasks: peg insertion, wrench manipulation and shallow- depth insertion. We show that manipulation skill learning is dramatically sped up through this way of scaffolding.
ER  - 

TY  - CONF
TI  - Online Replanning in Belief Space for Partially Observable Task and Motion Problems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5678
EP  - 5684
AU  - C. R. Garrett
AU  - C. Paxton
AU  - T. Lozano-P√©rez
AU  - L. P. Kaelbling
AU  - D. Fox
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - execution system
KW  - deterministic cost-sensitive planning
KW  - hybrid belief states
KW  - partially observable problems
KW  - online replanning
KW  - belief space
KW  - multistep manipulation tasks
KW  - autonomous robot
KW  - Planning
KW  - Task analysis
KW  - Bayes methods
KW  - Manipulators
KW  - Aerospace electronics
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196681
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To solve multi-step manipulation tasks in the real world, an autonomous robot must take actions to observe its environment and react to unexpected observations. This may require opening a drawer to observe its contents or moving an object out of the way to examine the space behind it. Upon receiving a new observation, the robot must update its belief about the world and compute a new plan of action. In this work, we present an online planning and execution system for robots faced with these challenges. We perform deterministic cost-sensitive planning in the space of hybrid belief states to select likely-to-succeed observation actions and continuous control actions. After execution and observation, we replan using our new state estimate. We initially enforce that planner reuses the structure of the unexecuted tail of the last plan. This both improves planning efficiency and ensures that the overall policy does not undo its progress towards achieving the goal. Our approach is able to efficiently solve partially observable problems both in simulation and in a real-world kitchen.
ER  - 

TY  - CONF
TI  - A Code for Unscented Kalman Filtering on Manifolds (UKF-M)
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5701
EP  - 5708
AU  - M. Brossard
AU  - A. Barrau
AU  - S. Bonnabel
PY  - 2020
KW  - Kalman filters
KW  - Lie groups
KW  - manifolds
KW  - nonlinear filters
KW  - public domain software
KW  - state estimation
KW  - UKF-M
KW  - Lie groups
KW  - state estimation problems
KW  - unscented Kalman filtering-manifolds
KW  - filtering performance
KW  - independent open-source
KW  - Python frameworks
KW  - Matlab frameworks
KW  - online repositories
KW  - https://github.com/CAOR-MINES-ParisTech/ukfm
KW  - Manifolds
KW  - Kalman filters
KW  - Two dimensional displays
KW  - Dispersion
KW  - Robots
KW  - Covariance matrices
DO  - 10.1109/ICRA40945.2020.9197489
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The present paper introduces a novel methodology for Unscented Kalman Filtering (UKF) on manifolds that extends previous work by the authors on UKF on Lie groups. Beyond filtering performance, the main interests of the approach are its versatility, as the method applies to numerous state estimation problems, and its simplicity of implementation for practitioners not being necessarily familiar with manifolds and Lie groups. We have developed the method on two independent open-source Python and Matlab frameworks we call UKF-M, for quickly implementing and testing the approach. The online repositories contain tutorials, documentation, and various relevant robotics examples that the user can readily reproduce and then adapt, for fast prototyping and benchmarking. The code is available at https://github.com/CAOR-MINES-ParisTech/ukfm.
ER  - 

TY  - CONF
TI  - Efficient and precise sensor fusion for non-linear systems with out-of-sequence measurements by example of mobile robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5709
EP  - 5715
AU  - P. B√∂hmler
AU  - J. Dziedzitz
AU  - P. Hopfgarten
AU  - T. Specker
AU  - R. Lange
PY  - 2020
KW  - delays
KW  - linear systems
KW  - mobile robots
KW  - nonlinear systems
KW  - sensor fusion
KW  - state estimation
KW  - precise sensor fusion
KW  - nonlinear systems
KW  - out-of-sequence measurements
KW  - mobile robotics
KW  - precise state estimation
KW  - sensor fusion algorithm
KW  - sensor fusion approaches
KW  - suitable approaches
KW  - consumer robot use-case
KW  - delays
KW  - fusion process
KW  - estimation bias
KW  - estimation performance
KW  - Robot sensing systems
KW  - Delays
KW  - Current measurement
KW  - Sensor fusion
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9197032
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For most applications in mobile robotics, precise state estimation is essential. Typically, the state estimation is based on the fusion of data from different sensors. In practice, these sensors differ in their characteristics and measurements are available to the sensor fusion algorithm only with delay. Based on a brief survey of sensor fusion approaches that consider delayed and out-of-sequence availability of measurements, suitable approaches for applications in mobile robotics are identified. In a consumer robot use-case, experiments show that the estimation is biased if delayed availability of measurements is not considered appropriately. However, if delays are considered in the fusion process, the estimation bias is reduced to almost zero and in consequence, the estimation performance is distinctly improved. Two computational favorable approximative methods are described and provide almost the same accuracy as - theoretically optimal - brute-force filter recalculation at much lower and well-distributed computational costs.
ER  - 

TY  - CONF
TI  - UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5716
EP  - 5723
AU  - J. Tian
AU  - W. Cheung
AU  - N. Glaser
AU  - Y. -C. Liu
AU  - Z. Kira
PY  - 2020
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - probability
KW  - sensor fusion
KW  - uncertainty handling
KW  - multimodal fusion
KW  - unanticipated input degradation
KW  - multiple sensor modalities
KW  - deep learning architectures
KW  - modality-specific output softmax probabilities
KW  - uncertainty measures
KW  - uncertainty-scaled output
KW  - fusion architectures
KW  - probabilistic noisy-fusion
KW  - data-dependent spatial temperature scaling
KW  - uncertainty-aware fusion
KW  - UNO
KW  - uncertainty-aware noisy fusion
KW  - Uncertainty
KW  - Degradation
KW  - Training
KW  - Noise measurement
KW  - Robot sensing systems
KW  - Entropy
DO  - 10.1109/ICRA40945.2020.9197266
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradation across their input modalities, especially when they must generalize to degradation not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradation. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, as compared to state-of-art fusion architectures, on a range of degradation (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training.
ER  - 

TY  - CONF
TI  - Intermittent GPS-aided VIO: Online Initialization and Calibration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5724
EP  - 5731
AU  - W. Lee
AU  - K. Eckenhoff
AU  - P. Geneva
AU  - G. Huang
PY  - 2020
KW  - calibration
KW  - cameras
KW  - distance measurement
KW  - Global Positioning System
KW  - Monte Carlo methods
KW  - sensor fusion
KW  - GPS reference frame
KW  - GPS signal
KW  - intermittent GPS-aided VIO
KW  - online initialization
KW  - IMU-camera data fusion
KW  - intermittent GPS measurements
KW  - sensor fusion
KW  - spatiotemporal sensor calibration
KW  - sensor reference frames
KW  - online calibration method
KW  - reference frame initialization procedure
KW  - GPS sensor noise
KW  - GPS-VIO system
KW  - VIO reference frame
KW  - online calibration
KW  - robust GPS-aided visual inertial odometry
KW  - GPS-IMU extrinsics
KW  - Monte-Carlo simulations
KW  - large-scale real-world experiment
KW  - Global Positioning System
KW  - Robot sensing systems
KW  - Calibration
KW  - Cameras
KW  - Cloning
KW  - Robustness
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197029
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an efficient and robust GPS-aided visual inertial odometry (GPS-VIO) system that fuses IMU-camera data with intermittent GPS measurements. To perform sensor fusion, spatiotemporal sensor calibration and initialization of the transform between the sensor reference frames are required. We propose an online calibration method for both the GPS-IMU extrinsics and time offset as well as a reference frame initialization procedure that is robust to GPS sensor noise. In addition, we prove the existence of four unobservable directions of the GPS-VIO system when estimating in the VIO reference frame, and advocate a state transformation to the GPS reference frame for full observability. We extensively evaluate the proposed approach in Monte-Carlo simulations where we investigate the system's robustness to different levels of GPS noise and loss of GPS signal, and additionally study the hyper-parameters used in the initialization procedure. Finally, the proposed system is validated in a large-scale real-world experiment.
ER  - 

TY  - CONF
TI  - A Mathematical Framework for IMU Error Propagation with Applications to Preintegration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5732
EP  - 5738
AU  - A. Barrau
AU  - S. Bonnabel
PY  - 2020
KW  - inertial navigation
KW  - mathematical analysis
KW  - measurement errors
KW  - measurement uncertainty
KW  - position measurement
KW  - sensor fusion
KW  - units (measurement)
KW  - inertial navigation equations
KW  - log-linearity property
KW  - mathematical framework
KW  - IMU error propagation
KW  - inertial measurement units
KW  - Lie group SE
KW  - sensors
KW  - rotating Earth
KW  - centrifugal force
KW  - Coriolis effect
KW  - Earth
KW  - Mathematical model
KW  - Force
KW  - Robot kinematics
KW  - Sensors
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197492
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To fuse information from inertial measurement units (IMU) with other sensors one needs an accurate model for IMU error propagation in terms of position, velocity and orientation, a triplet we call extended pose. In this paper we leverage a nontrivial result, namely log-linearity of inertial navigation equations based on the recently introduced Lie group SE2(3), to transpose the recent methodology of Barfoot and Furgale for associating uncertainty with poses (position, orientation) of SE(3) when using noisy wheel speeds, to the case of extended poses (position, velocity, orientation) of SE2(3) when using noisy IMUs. Besides, our approach to extended poses combined with log-linearity property allows revisiting the theory of preintegration on manifolds and reaching a further theoretic level in this field. We show exact preintegration formulas that account for rotating Earth, that is, centrifugal force and Coriolis effect, may be derived as a byproduct.
ER  - 

TY  - CONF
TI  - Radar-Inertial Ego-Velocity Estimation for Visually Degraded Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5739
EP  - 5746
AU  - A. Kramer
AU  - C. Stahoviak
AU  - A. Santamaria-Navarro
AU  - A. -a. Agha-mohammadi
AU  - C. Heckman
PY  - 2020
KW  - inertial navigation
KW  - millimetre wave radar
KW  - mobile robots
KW  - optimisation
KW  - radar imaging
KW  - radar receivers
KW  - visual perception
KW  - millimeter-wave radar-on-a-chip sensor
KW  - inertial measurement unit
KW  - batch optimization
KW  - sliding window
KW  - radar-inertial ego-velocity estimation
KW  - visually degraded environments
KW  - mobile robot body-frame velocity
KW  - radar-inertial velocity estimates
KW  - visual-inertial approach
KW  - Robot sensing systems
KW  - Radar measurements
KW  - Doppler radar
KW  - Doppler effect
KW  - Estimation
KW  - Velocity measurement
DO  - 10.1109/ICRA40945.2020.9196666
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach for estimating the body-frame velocity of a mobile robot. We combine measurements from a millimeter-wave radar-on-a-chip sensor and an inertial measurement unit (IMU) in a batch optimization over a sliding window of recent measurements. The sensor suite employed is lightweight, low-power, and is invariant to ambient lighting conditions. This makes the proposed approach an attractive solution for platforms with limitations around payload and longevity, such as aerial vehicles conducting autonomous exploration in perceptually degraded operating conditions, including subterranean environments. We compare our radar-inertial velocity estimates to those from a visual-inertial (VI) approach. We show the accuracy of our method is comparable to VI in conditions favorable to VI, and far exceeds the accuracy of VI when conditions deteriorate.
ER  - 

TY  - CONF
TI  - Position-based Impedance Control of a 2-DOF Compliant Manipulator for a Facade Cleaning Operation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5765
EP  - 5770
AU  - T. Kim
AU  - S. Yoo
AU  - H. S. Kim
AU  - T. Seo
PY  - 2020
KW  - actuators
KW  - ball screws
KW  - buildings (structures)
KW  - cleaning
KW  - compliance control
KW  - force control
KW  - force measurement
KW  - force sensors
KW  - manipulators
KW  - mobile robots
KW  - position control
KW  - robust control
KW  - service robots
KW  - walls
KW  - wet-type cleaning manipulator
KW  - system stability
KW  - control performance
KW  - robust force measurement mechanism
KW  - position-based impedance control
KW  - facade cleaning operation
KW  - series elastic actuator
KW  - force sensor
KW  - contact force
KW  - cleaning performance
KW  - ball screw mechanism
KW  - 2-DOF compliant manipulator
KW  - manipulator stiffness
KW  - force tracking
KW  - Manipulators
KW  - Force
KW  - Cleaning
KW  - Buildings
KW  - Force measurement
KW  - Sea measurements
KW  - Impedance
DO  - 10.1109/ICRA40945.2020.9197478
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the design of a compliant manipulator using a series elastic actuator (SEA) and a mechanism for precisely measuring the force acting on the contact part of the manipulator without using a force sensor. It is important to maintain a constant contact force between the compliant manipulator and the wall in order to guarantee cleaning performance, and the ball screw mechanism is used to adapt to changes in the distance and the angle. Position-based impedance control is used to maintain a constant contact force when the manipulator interacts with the wall of the building, and the results confirm that the system stability is guaranteed when using SEA, regardless of the variation in the actual stiffness of the manipulator. The results of extensive experimentation using the test bench demonstrate the force tracking performance against various types of wall changes using the stiff wet-type cleaning manipulator. The results indicate that the stiffness of SEA affects the force tracking performance and system stability under the condition of the manipulator and environment interaction, and that the system stability and control performance can be improved by applying a robust force measurement mechanism to noise.
ER  - 

TY  - CONF
TI  - Robust, Locally Guided Peg-in-Hole using Impedance-Controlled Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5771
EP  - 5777
AU  - K. Nottensteiner
AU  - F. Stulp
AU  - A. Albu-Sch√§ffer
PY  - 2020
KW  - geometry
KW  - position control
KW  - robots
KW  - robust control
KW  - state estimation
KW  - torque control
KW  - impedance-controlled robots
KW  - robust peg-in-hole
KW  - locally guided peg-in-hole
KW  - autonomous execution
KW  - peg-in-hole problem
KW  - robotic setup
KW  - robust behavior
KW  - mesh data
KW  - task-specific weight
KW  - motion generation step
KW  - joint torque measurements
KW  - sampling-based state estimation framework
KW  - peg-in-hole assembly tasks
KW  - Task analysis
KW  - Uncertainty
KW  - Geometry
KW  - Robot sensing systems
KW  - Robustness
KW  - State estimation
DO  - 10.1109/ICRA40945.2020.9196986
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach for the autonomous, robust execution of peg-in-hole assembly tasks. We build on a sampling-based state estimation framework, in which samples are weighted according to their consistency with the position and joint torque measurements. The key idea is to reuse these samples in a motion generation step, where they are assigned a second task-specific weight. The algorithm thereby guides the peg towards the goal along the configuration space. An advantage of the approach is that the user only needs to provide: the geometry of the objects as mesh data, as well as a rough estimate of the object poses in the workspace, and a desired goal state. Another advantage is that the local, online nature of our algorithm leads to robust behavior under uncertainty. The approach is validated in the case of our robotic setup and under varying uncertainties for the classical peg-in-hole problem subject to two different geometries.
ER  - 

TY  - CONF
TI  - Design of Spatial Admittance for Force-Guided Assembly of Polyhedral Parts in Single Point Frictional Contact
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5801
EP  - 5807
AU  - S. Huang
AU  - J. M. Schimmels
PY  - 2020
KW  - force control
KW  - friction
KW  - industrial manipulators
KW  - mechanical contact
KW  - robotic assembly
KW  - polyhedral parts
KW  - frictional contact
KW  - frictionless contact
KW  - nonlinear equations
KW  - error reduction function
KW  - frictional cases
KW  - misalignment-reducing conditions
KW  - contact configurations
KW  - force-guided assembly
DO  - 10.1109/ICRA40945.2020.9197075
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper identifies conditions for designing the appropriate spatial admittance to achieve reliable force-guided assembly of polyhedral parts for cases in which a single frictional contact occurs between the two parts. This work is an extension of previous work in which frictionless contact was considered. This paper presents a way to characterize friction without solving a set of complicated non-linear equations. We show that, by modifying the error reduction function and evaluating the function bounds associated with friction, the procedures developed for frictionless contact apply to the frictional cases. Thus, for bounded misalignments, if an admittance satisfies the misalignment-reducing conditions at a finite number of contact configurations, then the admittance will also satisfy the conditions at all intermediate configurations for any value of friction less than the specified upper bound.
ER  - 

TY  - CONF
TI  - Characterisation of Self-locking High-contraction Electro-ribbon Actuators*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5856
EP  - 5861
AU  - M. Taghavi
AU  - T. Helps
AU  - J. Rossiter
PY  - 2020
KW  - electric actuators
KW  - microactuators
KW  - microsensors
KW  - solenoids
KW  - contraction variation
KW  - electro-ribbon actuator
KW  - high-contraction electro-ribbon actuators
KW  - fluidically-driven technologies
KW  - electrically-driven actuators
KW  - high power equivalent
KW  - mammalian muscle
KW  - dielectrophoretic liquid zipping
KW  - low-power-consumption solenoids
KW  - valves
KW  - Actuators
KW  - Electrodes
KW  - Dielectric liquids
KW  - Force
KW  - Insulators
KW  - Dielectrics
KW  - Permittivity
DO  - 10.1109/ICRA40945.2020.9196849
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Actuators are essential devices that exert force and do work. The contraction of an actuator (how much it can shorten) is an important property that strongly influences its applications, especially in engineering and robotics. While high contractions have been achieved by thermally- or fluidically-driven technologies, electrically-driven actuators typically cannot contract by more than 50%. Recently developed electro-ribbon actuators are simple, low cost, scalable electroactive devices powered by dielectrophoretic liquid zipping (DLZ) that exhibit high efficiency (~70%), high power equivalent to mammalian muscle (~100 W/kg), contractions exceeding 99%. We characterise the electro-ribbon actuator and explore contraction variation with voltage and load. We describe the unique self-locking behaviour of the electro-ribbon actuator which could allow for low-power-consumption solenoids and valves. Finally, we show the interdependence of constituent material properties and the important role that material choice plays in maximising performance.
ER  - 

TY  - CONF
TI  - Helically Wrapped Supercoiled Polymer (HW-SCP) Artificial Muscles: Design, Characterization, and Modeling
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5862
EP  - 5868
AU  - T. Tsabedze
AU  - C. Mullen
AU  - R. Coulter
AU  - S. Wade
AU  - J. Zhang
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - coils
KW  - hysteresis
KW  - magnetic hysteresis
KW  - manipulator dynamics
KW  - medical robotics
KW  - muscle
KW  - pneumatic actuators
KW  - robotic muscles
KW  - force generation
KW  - force performance
KW  - force production
KW  - HW-SCP actuators
KW  - Preisach hysteresis model
KW  - polynomial model
KW  - high-performance artificial muscles
KW  - helically wrapped supercoiled polymer artificial muscles
KW  - strain performance
KW  - Actuators
KW  - Strain
KW  - Force
KW  - Mathematical model
KW  - Muscles
KW  - Robots
KW  - Yarn
DO  - 10.1109/ICRA40945.2020.9197330
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Supercoiled polymer (SCP) artificial muscles exhibit many desirable properties such as large contractions and high power density. However, their full potential as robotic muscles is challenged by insufficient strain or force generation - non-mandrel-coiled SCP actuators produce up to 10-20% strain; mandrel-coiled SCP actuators often lift up to 10-30g of weight. It is strongly desired but difficult to obtain SCP actuators that produce large strain and large force. In this paper, the design, characterization, and modeling of helically wrapped SCP (HW-SCP) actuators are presented, which can produce up to 40-60% strain and lift more than 90g of weight. By adjusting their configuration parameters, their strain and force performance can be changed. Experiments are conducted to characterize the force production, strain, and speed of HW-SCP actuators. A Preisach hysteresis model and a polynomial model are adopted to accurately capture the actuator behaviors. This work contributes to high-performance artificial muscles.
ER  - 

TY  - CONF
TI  - A Variable Stiffness Soft Continuum Robot Based on Pre-charged Air, Particle Jamming, and Origami
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5869
EP  - 5875
AU  - Y. Li
AU  - T. Ren
AU  - Y. Chen
AU  - M. Z. Q. Chen
PY  - 2020
KW  - bellows
KW  - bending
KW  - compressive strength
KW  - continuum mechanics
KW  - design engineering
KW  - manipulator dynamics
KW  - mobile robots
KW  - rigidity
KW  - structural design
KW  - control systems
KW  - pre-charged air
KW  - particle jamming
KW  - origami structure
KW  - internal chambers
KW  - spine-like chamber
KW  - identical chambers
KW  - spine chamber
KW  - pressurized air
KW  - air chambers
KW  - robot expansion-contraction
KW  - stiffness variation mechanism
KW  - lateral stiffness
KW  - axial stiffness
KW  - prototype robot
KW  - variable stiffness soft continuum robot
KW  - Robots
KW  - Jamming
KW  - Force
KW  - Tendons
KW  - Electron tubes
KW  - Wires
KW  - Valves
KW  - Soft continuum robot
KW  - variable stiffness
KW  - pre-charged air
KW  - particle jamming
KW  - origami
DO  - 10.1109/ICRA40945.2020.9196729
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft continuum robots have many applications such as medical surgeries, service industries, rescue tasks, and underwater exploration. Flexibility and good accessibility of such robots are the key reasons for their popularity. However, the complexity of their structural design and control systems limit their broader applications. In this paper, a novel variable stiffness soft continuum robot based on pre-charged air, particle jamming, and origami is proposed. The robot is a bellow-like origami structure with internal chambers. A spine-like chamber is filled with particles, and three identical chambers surrounding the spine chamber are filled with pressurized air. When the origami structure is compressed, the particles are jammed by the compression force and the increased pressure of the three air chambers, thus increasing the overall stiffness of the robot. The robot expansion-contraction and bending are controlled by three tendons. An analytical model of the proposed stiffness variation mechanism has been developed. The effects of various parameters on the lateral and axial stiffness of the soft continuum robot have been investigated by experimental studies. A prototype robot has been fabricated to demonstrate grasping operations.
ER  - 

TY  - CONF
TI  - SwarmRail: A Novel Overhead Robot System for Indoor Transport and Mobile Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5905
EP  - 5911
AU  - M. G√∂rner
AU  - F. Benedikt
AU  - F. Grimmel
AU  - T. Hulin
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - multi-robot systems
KW  - rails
KW  - robotic manipulator arm overhead
KW  - continuous overhead manipulation
KW  - rail crossings
KW  - robot swarm
KW  - mobile SwarmRail units
KW  - single rail network
KW  - indoor transport
KW  - mobile manipulation
KW  - omnidirectional mobile platform
KW  - rail profiles
KW  - rail-structure
KW  - overhead robot system
KW  - Rails
KW  - Wheels
KW  - Robot sensing systems
KW  - Manipulators
KW  - Layout
DO  - 10.1109/ICRA40945.2020.9196972
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - SwarmRail represents a novel solution to overhead manipulation from a mobile unit that drives in an aboveground rail-structure. The concept is based on the combination of omnidirectional mobile platform and L-shaped rail profiles that form a through-going central gap. This gap makes possible mounting a robotic manipulator arm overhead at the underside of the mobile platform. Compared to existing solutions, SwarmRail enables continuous overhead manipulation while traversing rail crossings. It also can be operated in a robot swarm, as it allows for concurrent operation of a group of mobile SwarmRail units inside a single rail network. Experiments on a first functional demonstrator confirm the functional capability of the concept. Potential fields of applications reach from industry over logistics to vertical farming.
ER  - 

TY  - CONF
TI  - Fast Local Planning and Mapping in Unknown Off-Road Terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5912
EP  - 5918
AU  - T. Overbye
AU  - S. Saripalli
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - motion control
KW  - remotely operated vehicles
KW  - SLAM (robots)
KW  - trajectory control
KW  - off-road terrain
KW  - on-line mapping
KW  - planning solution
KW  - obstacle detection
KW  - terrain gradient map
KW  - simple cost map
KW  - adaptable cost map
KW  - optimal paths
KW  - control input space
KW  - kinematic forward simulation
KW  - generated feasible trajectories
KW  - optimal trajectory
KW  - time operation
KW  - frequency 10.0 Hz
KW  - frequency 30.0 Hz
KW  - Trajectory
KW  - Robots
KW  - Planning
KW  - Aerospace electronics
KW  - Microsoft Windows
KW  - Three-dimensional displays
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196848
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.
ER  - 

TY  - CONF
TI  - Scaled Autonomy: Enabling Human Operators to Control Robot Fleets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5942
EP  - 5948
AU  - G. Swamy
AU  - S. Reddy
AU  - S. Levine
AU  - A. D. Dragan
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - telerobotics
KW  - utility function
KW  - real-world mobile robot navigation
KW  - robot fleets control
KW  - autonomous robots
KW  - human operator
KW  - teleoperation
KW  - Task analysis
KW  - Mathematical model
KW  - Navigation
KW  - Autonomous robots
KW  - Hardware
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196792
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a simulated navigation task. We also run a hardware demonstration that illustrates how our method can be applied to a real-world mobile robot navigation task.
ER  - 

TY  - CONF
TI  - An Actor-Critic Approach for Legible Robot Motion Planner
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5949
EP  - 5955
AU  - X. Zhao
AU  - T. Fan
AU  - D. Wang
AU  - Z. Hu
AU  - T. Han
AU  - J. Pan
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - recurrent neural nets
KW  - actor-critic approach
KW  - legible robot motion planner
KW  - human-robot collaboration
KW  - human partners
KW  - mutual learning
KW  - legibility evaluator
KW  - policy network
KW  - deep reinforcement learning
KW  - sequence model
KW  - Seq2Seq
KW  - motion predictor
KW  - maps motion
KW  - legible reward
KW  - human-subject experiments
KW  - real-human data
KW  - recurrent neural networks based sequence
KW  - Task analysis
KW  - Robot motion
KW  - Robot kinematics
KW  - Biological neural networks
KW  - Trajectory
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197102
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In human-robot collaboration, it is crucial for the robot to make its intentions clear and predictable to the human partners. Inspired by the mutual learning and adaptation of human partners, we suggest an actor-critic approach for a legible robot motion planner. This approach includes two neural networks and a legibility evaluator: 1) A policy network based on deep reinforcement learning (DRL); 2) A Recurrent Neural Networks (RNNs) based sequence to sequence (Seq2Seq) model as a motion predictor; 3) A legibility evaluator that maps motion to legible reward. Through a series of human-subject experiments, we demonstrate that with a simple handicraft function and no real-human data, our method lead to improved collaborative performance against a baseline method and a non-prediction method.
ER  - 

TY  - CONF
TI  - Intuitive 3D Control of a Quadrotor in User Proximity with Pointing Gestures
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5964
EP  - 5971
AU  - B. Gromov
AU  - J. Guzzi
AU  - L. M. Gambardella
AU  - A. Giusti
PY  - 2020
KW  - aerospace computing
KW  - aircraft control
KW  - control engineering computing
KW  - helicopters
KW  - interactive devices
KW  - position control
KW  - virtual reality
KW  - pointing gestures
KW  - quadrotor
KW  - pointing ray
KW  - push button
KW  - joystick control
KW  - intuitive 3D control
KW  - user proximity
KW  - 3D piloting task
KW  - virtual workspace surfaces
KW  - position control
KW  - human robot interaction
KW  - Three-dimensional displays
KW  - Drones
KW  - Robot kinematics
KW  - Shape
KW  - Robot sensing systems
KW  - Aerospace electronics
DO  - 10.1109/ICRA40945.2020.9196654
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach for controlling the position of a quadrotor in 3D space using pointing gestures; the task is difficult because it is in general ambiguous to infer where, along the pointing ray, the robot should go. We propose and validate a pragmatic solution based on a push button acting as a simple additional input device which switches between different virtual workspace surfaces. Results of a study involving ten subjects show that the approach performs well on a challenging 3D piloting task, where it compares favorably with joystick control.
ER  - 

TY  - CONF
TI  - Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5972
EP  - 5978
AU  - T. Yuan
AU  - H. Liu
AU  - L. Fan
AU  - Z. Zheng
AU  - T. Gao
AU  - Y. Zhu
AU  - S. -C. Zhu
PY  - 2020
KW  - belief networks
KW  - cognition
KW  - cognitive systems
KW  - graph theory
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - robots
KW  - robot knowledge
KW  - human interactions
KW  - graphical model
KW  - object states
KW  - parse graph
KW  - single-view spatiotemporal parsing
KW  - learned representation
KW  - inference algorithm
KW  - joint pg
KW  - effective reasoning
KW  - inference capability
KW  - states joint inference
KW  - human beliefs
KW  - socio-cognitive ability
KW  - false-beliefs
KW  - individual parse graph
KW  - small object tracking dataset
KW  - Robots
KW  - Cognition
KW  - Visualization
KW  - Graphical models
KW  - Psychology
KW  - Noise measurement
KW  - Object tracking
DO  - 10.1109/ICRA40945.2020.9197355
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Aiming to understand how human (false-)belief- a core socio-cognitive ability-would affect human interactions with robots, this paper proposes to adopt a graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs. Specifically, a parse graph (pg) is learned from a single-view spatiotemporal parsing by aggregating various object states along the time; such a learned representation is accumulated as the robot's knowledge. An inference algorithm is derived to fuse individual pg from all robots across multi-views into a joint pg, which affords more effective reasoning and inference capability to overcome the errors originated from a single view. In the experiments, through the joint inference over pgs, the system correctly recognizes human (false-)belief in various settings and achieves better cross-view accuracy on a challenging small object tracking dataset.
ER  - 

TY  - CONF
TI  - Audiovisual cognitive architecture for autonomous learning of face localisation by a Humanoid Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5979
EP  - 5985
AU  - J. Gonzalez-Billandon
AU  - A. Sciutti
AU  - M. Tata
AU  - G. Sandini
AU  - F. Rea
PY  - 2020
KW  - cognitive systems
KW  - humanoid robots
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - robot vision
KW  - audiovisual cognitive architecture
KW  - autonomous learning
KW  - face localisation
KW  - humanoid robot
KW  - deep learning algorithms
KW  - cognitive framework
KW  - audiovisual attention
KW  - learning generalization process
KW  - machine learning
KW  - HRI
DO  - 10.1109/ICRA40945.2020.9196829
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Newborn infants are naturally attracted to human faces, a crucial source of information for social interaction. In robotics, acquisition of such information is crucial and social robots should also learn to exhibit such social skill. Deep learning algorithms are valid candidates to address the problem of face localisation. However, a major drawback of these methods is the large amount of data and human supervision needed in the training procedure. In this work, we propose a cognitive architecture to address autonomous learning from raw sensory signals without supervision. We demonstrate the success of our cognitive framework for the task of face localisation. The proposed cognitive architecture builds on existing work and uses audiovisual attention and a proactive stereo vision mechanism to autonomously direct a robot's attentive focus towards human faces. The gathered information is used to incrementally generate a dataset that can be used to train a state-of-the-art deep network. The learning system imitates the typical learning process of infants and enhances the learning generalization process by leveraging on the interaction experience with people. The integration of HRI with machine learning, inspired by early development in humans, constitutes an innovative approach for improving autonomous learning in robots.
ER  - 

TY  - CONF
TI  - Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5986
EP  - 5993
AU  - A. Candela
AU  - S. Kodgule
AU  - K. Edelson
AU  - S. Vijayarangan
AU  - D. R. Thompson
AU  - E. Noe Dobrea
AU  - D. Wettergreen
PY  - 2020
KW  - aerospace robotics
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - planetary rovers
KW  - planetary rover exploration combining remote
KW  - active spectroscopic mapping
KW  - planetary rover missions
KW  - heavy reliance
KW  - ground control
KW  - real-time information
KW  - autonomous mapping
KW  - exploration approach
KW  - planetary rovers
KW  - machine learning model
KW  - rover measurements
KW  - spectroscopic data
KW  - information theory
KW  - nonmyopic path
KW  - exploration productivity
KW  - actual rover
KW  - Feature extraction
KW  - Extraterrestrial measurements
KW  - Robot kinematics
KW  - Adaptation models
KW  - Productivity
KW  - Mars
DO  - 10.1109/ICRA40945.2020.9196973
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Maintaining high levels of productivity for planetary rover missions is very difficult due to limited communication and heavy reliance on ground control. There is a need for autonomy that enables more adaptive and efficient actions based on real-time information. This paper presents an autonomous mapping and exploration approach for planetary rovers. We first describe a machine learning model that actively combines remote and rover measurements for mapping. We focus on spectroscopic data because they are commonly used to investigate surface composition. We then incorporate notions from information theory and non-myopic path planning to improve exploration productivity. Finally, we demonstrate the feasibility and successful performance of our approach via spectroscopic investigations of Cuprite, Nevada; a well-studied region of mineralogical and geological interest. We first perform a detailed analysis in simulations, and then validate those results with an actual rover in the field in Nevada.
ER  - 

TY  - CONF
TI  - Magnetic Docking Mechanism for Free-flying Space Robots with Spherical Surfaces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 5994
EP  - 5999
AU  - K. WATANABE
PY  - 2020
KW  - aerospace robotics
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - position control
KW  - regression analysis
KW  - control capability
KW  - slider guide
KW  - robot position error
KW  - magnetic docking mechanism
KW  - free-flying space robots
KW  - spherical surfaces
KW  - autonomous operation
KW  - International Space Station
KW  - fixed guide mechanism
KW  - machine learning technique
KW  - Gaussian process regression
KW  - GPR technique
KW  - Robots
KW  - Force
KW  - Magnetic levitation
KW  - Computer interfaces
KW  - Shape
KW  - Magnetoelasticity
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197423
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The autonomous operation of robots in the International Space Station (ISS) is required to maximize the use of limited resources and enable astronauts to concentrate more on valuable tasks. To achieve these goals, we are developing a station where the robot approaches, docks, charges, and then undocks. In this paper, the authors propose a magnetic docking mechanism for free-flying robots with spherical surfaces that makes it possible for a robot to dock securely without requiring highly precise guidance, navigation and control capability. By making use of a slider guide and repelling pairs of magnets, this mechanism can achieve tolerance for larger robot position error as compared with the conventional fixed guide mechanism. The experimental results show that the proposed mechanism can effectively enlarge the acceptable error range of poses, and also reduce acceleration at the moment of impact. We also introduce a model to predict the success or failure of docking from the contact condition of the robot and the guide by using a machine learning technique - Gaussian Process Regression (GPR). The prediction results shows that the learnt model can express the contact condition of successful docking.
ER  - 

TY  - CONF
TI  - Barefoot Rover: a Sensor-Embedded Rover Wheel Demonstrating In-Situ Engineering and Science Extractions using Machine Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6000
EP  - 6006
AU  - Y. Marchetti
AU  - J. Lightholder
AU  - E. Junkins
AU  - M. Cross
AU  - L. Mandrake
AU  - A. Fraeman
PY  - 2020
KW  - aerospace computing
KW  - aerospace control
KW  - aerospace robotics
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - planetary rovers
KW  - planetary surfaces
KW  - wheels
KW  - barefoot rover
KW  - machine learning
KW  - 2D pressure grid
KW  - electrochemical impedance spectroscopy sensor
KW  - in-situ sensing
KW  - sensor-embedded rover wheel
KW  - in-situ engineering
KW  - planetary exploration missions
KW  - Wheels
KW  - Robot sensing systems
KW  - Rocks
KW  - Instruments
KW  - Measurement
KW  - Surface impedance
KW  - DC motors
DO  - 10.1109/ICRA40945.2020.9197500
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we demonstrate an instrumented wheel concept which utilizes a 2D pressure grid, an electrochemical impedance spectroscopy (EIS) sensor and machine learning (ML) to extract meaningful metrics from the interaction between the wheel and surface terrain. These include continuous slip/skid estimation, balance, and sharpness for engineering applications. Estimates of surface hydration, texture, terrain patterns, and regolith physical properties such as cohesion and angle of internal friction are additionally calculated for science applications. Traditional systems rely on post-processing of visual images and vehicle telemetry to estimate these metrics. Through in-situ sensing, these metrics can be calculated in near real time and made available to onboard science and engineering autonomy applications. This work aims to provide a deployable system for future planetary exploration missions to increase science and engineering capabilities through increased knowledge of the terrain.
ER  - 

TY  - CONF
TI  - Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6007
EP  - 6013
AU  - P. F. Proen√ßa
AU  - Y. Gao
PY  - 2020
KW  - aerospace computing
KW  - image classification
KW  - learning (artificial intelligence)
KW  - mixture models
KW  - neural nets
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - space vehicles
KW  - photorealistic rendering
KW  - on-orbit proximity operations
KW  - 6D pose estimation
KW  - monocular pose estimation
KW  - Unreal Engine 4
KW  - neural networks
KW  - deep learning framework
KW  - orientation soft classification
KW  - orientation ambiguity
KW  - mixture model
KW  - URSO
KW  - spacecraft pose estimation
KW  - Space vehicles
KW  - Quaternions
KW  - Pose estimation
KW  - Earth
KW  - Cameras
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197244
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - On-orbit proximity operations in space rendezvous, docking and debris removal require precise and robust 6D pose estimation under a wide range of lighting conditions and against highly textured background, i.e., the Earth. This paper investigates leveraging deep learning and photorealistic rendering for monocular pose estimation of known uncooperative spacecraft. We first present a simulator built on Unreal Engine 4, named URSO, to generate labeled images of spacecraft orbiting the Earth, which can be used to train and evaluate neural networks. Secondly, we propose a deep learning framework for pose estimation based on orientation soft classification, which allows modelling orientation ambiguity as a mixture model. This framework was evaluated both on URSO datasets and the European Space Agency pose estimation challenge. In this competition, our best model achieved 3rd place on the synthetic test set and 2nd place on the real test set. Moreover, our results show the impact of several architectural and training aspects, and we demonstrate qualitatively how models learned on URSO datasets can perform on real images from space.
ER  - 

TY  - CONF
TI  - Concurrent Parameter Identification and Control for Free-Floating Robotic Systems During On-Orbit Servicing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6014
EP  - 6020
AU  - O. -O. Christidi-Loumpasefski
AU  - G. Rekleitis
AU  - E. Papadopoulos
PY  - 2020
KW  - aerospace control
KW  - aerospace robotics
KW  - Jacobian matrices
KW  - mobile robots
KW  - motion control
KW  - parameter estimation
KW  - path planning
KW  - position control
KW  - concurrent parameter identification
KW  - free-floating robotic system
KW  - on-orbit servicing
KW  - uncertain parameters
KW  - fast parameter identification method
KW  - accurate parameter estimates
KW  - system dynamic properties
KW  - control scheme compensates
KW  - robotic servicer base
KW  - parameter information
KW  - transposed Jacobian controller
KW  - RW angular momentum disturbance rejection
KW  - OOS tasks
KW  - Task analysis
KW  - Parameter estimation
KW  - Aerospace electronics
KW  - Manipulator dynamics
KW  - Adaptive control
DO  - 10.1109/ICRA40945.2020.9197187
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To control a free-floating robotic system with uncertain parameters in OOS tasks with high accuracy, a fast parameter identification method, previously developed by the authors, is enhanced further and used concurrently with a controller. The method provides accurate parameter estimates, without any prior knowledge of any system dynamic properties. This control scheme compensates for the accumulated angular momentum on the reaction wheels (RWs), which acts as a disturbance to the robotic servicer base. While any controller using parameter information can be used, a transposed Jacobian controller, modified to include RW angular momentum disturbance rejection, is employed here. Threedimensional simulations demonstrate the method's validity.
ER  - 

TY  - CONF
TI  - A Dual Quaternion-Based Discrete Variational Approach for Accurate and Online Inertial Parameter Estimation in Free-Flying obots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6021
EP  - 6027
AU  - M. Ekal
AU  - R. Ventura
PY  - 2020
KW  - aerospace robotics
KW  - attitude control
KW  - control system synthesis
KW  - linear matrix inequalities
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - recursive estimation
KW  - robust control
KW  - dual quaternion-based discrete variational approach
KW  - online inertial parameter estimation
KW  - free-flying robots
KW  - model-based motion control
KW  - rigid body inertial parameter estimation
KW  - discrete dual quaternion equations
KW  - variational mechanics
KW  - linear parameter estimation problem
KW  - standard localization algorithms
KW  - rotational inertia
KW  - linear matrix inequality constraints
KW  - pseudoinertia matrix
KW  - recursive semidefinite programming
KW  - Quaternions
KW  - Mathematical model
KW  - Parameter estimation
KW  - Robots
KW  - Linear matrix inequalities
KW  - Estimation
KW  - Programming
DO  - 10.1109/ICRA40945.2020.9196852
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The performance of model-based motion control for free-flying robots relies on accurate estimation of their parameters. In this work, a method of rigid body inertial parameter estimation which relies on a variational approach is presented. Instead of discretizing the continuous equations of motion, discrete dual quaternion equations based on variational mechanics are used to formulate a linear parameter estimation problem. This method depends only on the pose of the rigid body obtained from standard localization algorithms. Recursive semi-definite programming is used to estimate the inertial parameters (mass, rotational inertia and center of mass offset) online. Linear Matrix Inequality constraints based on the pseudo-inertia matrix ensure that the estimates obtained are fully physically consistent. Simulation results demonstrate that this method is robust to disturbances and the produced estimates are at least one order of magnitude more accurate when compared to discretization using finite differences.
ER  - 

TY  - CONF
TI  - Unified Intrinsic and Extrinsic Camera and LiDAR Calibration under Uncertainties
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6028
EP  - 6034
AU  - J. K√ºmmerle
AU  - T. K√ºhner
PY  - 2020
KW  - calibration
KW  - cameras
KW  - optical radar
KW  - LiDAR calibration
KW  - intrinsic parameters
KW  - probabilistic sense
KW  - cameras
KW  - probabilistic formulation
KW  - camera model
KW  - additional LiDAR measurements
KW  - intrinsic camera calibration
KW  - state-of-the-art calibration precision
KW  - extrinsic parameters
KW  - Cameras
KW  - Calibration
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Image edge detection
KW  - Detectors
DO  - 10.1109/ICRA40945.2020.9197496
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many approaches for camera and LiDAR calibration are presented in literature but none of them estimates all intrinsic and extrinsic parameters simultaneously and therefore optimally in a probabilistic sense.In this work, we present a method to simultaneously estimate intrinsic and extrinsic parameters of cameras and LiDARs in a unified problem. We derive a probabilistic formulation that enables flawless integration of different measurement types without hand-tuned weights. An arbitrary number of cameras and LiDARs can be calibrated simultaneously. Measurements are not required to be time-synchronized. The method is designed to work with any camera model.In evaluation, we show that additional LiDAR measurements significantly improve intrinsic camera calibration. Further, we show on real data that our method achieves state-of-the-art calibration precision with high reliability.
ER  - 

TY  - CONF
TI  - AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6035
EP  - 6041
AU  - J. Rebello
AU  - A. Fung
AU  - S. L. Waslander
PY  - 2020
KW  - calibration
KW  - cameras
KW  - sensitivity analysis
KW  - SLAM (robots)
KW  - calibration parameters
KW  - calibration sensitivity analysis
KW  - joint angle noise
KW  - joint angle values
KW  - calibration code
KW  - dynamic camera clusters
KW  - visual SLAM
KW  - time-varying set
KW  - extrinsic calibration transformations
KW  - DCC calibration accuracy
KW  - configuration space
KW  - pixel re-projection error
KW  - fiducial target
KW  - dynamic camera cluster
KW  - pose-loop error optimization
KW  - Cameras
KW  - Calibration
KW  - Robot vision systems
KW  - Simultaneous localization and mapping
KW  - Vehicle dynamics
KW  - Optimization
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9197217
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.
ER  - 

TY  - CONF
TI  - Analytic Plane Covariances Construction for Precise Planarity-based Extrinsic Calibration of Camera and LiDAR
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6042
EP  - 6048
AU  - G. Koo
AU  - J. Kang
AU  - B. Jang
AU  - N. Doh
PY  - 2020
KW  - calibration
KW  - feature extraction
KW  - optical radar
KW  - stereo image processing
KW  - planar feature correspondences
KW  - plane parameter covariance
KW  - 3D corner points
KW  - plane measurement covariance
KW  - out-of-plane errors
KW  - analytic plane covariances construction
KW  - plane parameter errors
KW  - planarity-based extrinsic calibration
KW  - LiDAR
KW  - Three-dimensional displays
KW  - Calibration
KW  - Cameras
KW  - Laser radar
KW  - Linear programming
KW  - Solid modeling
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197149
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planarity of checkerboards is a widely used feature for extrinsic calibration of camera and LiDAR. In this study, we propose two analytically derived covariances of (i) plane parameters and (ii) plane measurement, for precise extrinsic calibration of camera and LiDAR. These covariances allow the graded approach in planar feature correspondences by exploiting the uncertainty of a set of given features in calibration. To construct plane parameter covariance, we employ the error model of 3D corner points and the analytically formulated plane parameter errors. Next, plane measurement covariance is directly derived from planar regions of point clouds using the out-of-plane errors. In simulation validation, our method is compared to an existing uncertainty-excluding method using the different number of target poses and the different levels of noise. In field experiment, we validated the applicability of the proposed analytic plane covariances for precise calibration using the basic planarity-based method and the latest planarity-and-linearity-based method.
ER  - 

TY  - CONF
TI  - An End-Effector Wrist Module for the Kinematically Redundant Manipulation of Arm-Type Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6075
EP  - 6080
AU  - Y. -H. Chang
AU  - Y. -C. Liu
AU  - C. -C. Lan
PY  - 2020
KW  - collision avoidance
KW  - end effectors
KW  - industrial robots
KW  - motion control
KW  - redundant manipulators
KW  - end effector path tracking
KW  - 6-DoF robot
KW  - 8-DoF robot
KW  - redundant robot
KW  - kinematically redundant manipulation
KW  - industrial arm-type robots
KW  - dexterity
KW  - roll-pitch-roll wrist configuration
KW  - singularity free motion
KW  - end effector wrist module
KW  - collision avoidance
KW  - Wrist
KW  - Collision avoidance
KW  - Kinematics
KW  - Redundancy
KW  - Jacobian matrices
KW  - Service robots
KW  - Kinematically redundant manipulation
KW  - wrist module
KW  - roll-pitch-yaw
KW  - wrist singularity
KW  - inverse kinematics
DO  - 10.1109/ICRA40945.2020.9197258
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Industrial arm-type robots have multiple degrees-of-freedom (DoFs) and high dexterity but the use of the roll-pitch-roll wrist configuration yields singularities inside the reachable workspace. Excessive joint velocities will occur when encountering these singularities. Arm-type robots currently don't have enough dexterity to move the end-effector path away from the wrist singularities. Robots with redundant DoFs can be used to provide additional dexterity to avoid the singularities and reduce the excessive joint velocity. An end-effector wrist module is proposed to provide two redundant DoFs when interfaced with an existing 6-DoF robot. The new 8-DoF robot has a compact roll-pitch-yaw wrist that has no singularities inside the reachable workspace. The highly redundant robot can also be used to avoid collisions in various directions. Path tracking simulation examples are provided to show the advantages of the proposed design when compared with existing redundant or nonredundant robots. We expect that this module can serve as a cost-effective solution in applications where singularity-free motion or collision-free motion is required.
ER  - 

TY  - CONF
TI  - Online Trajectory Planning for an Industrial Tractor Towing Multiple Full Trailers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6089
EP  - 6095
AU  - H. Zhao
AU  - W. Chen
AU  - S. Zhou
AU  - Z. Liu
AU  - F. Zheng
AU  - Y. -H. Liu
PY  - 2020
KW  - agricultural machinery
KW  - nonlinear dynamical systems
KW  - path planning
KW  - trajectory control
KW  - vehicle dynamics
KW  - vehicle dynamics model
KW  - online trajectory planning
KW  - car-like tractor
KW  - passive full trailers
KW  - motion planning
KW  - complex nonlinear dynamics
KW  - simulation based prediction
KW  - industrial tractor-trailers vehicle
KW  - obstacle free trajectories
KW  - Agricultural machinery
KW  - Vehicle dynamics
KW  - Planning
KW  - Trajectory
KW  - Dynamics
KW  - Wheels
KW  - Lead
DO  - 10.1109/ICRA40945.2020.9196656
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel solution for online trajectory planning of a full-size tractor-trailers vehicle composed of a car-like tractor and arbitrary number of passive full trailers. The motion planning problem for such systems was rarely addressed due to the complex nonlinear dynamics. A simulation-based prediction method is proposed to easily handle the complicated nonlinear dynamics and efficiently generate the obstacle-free and dynamically feasible trajectories. The vehicle dynamics model and a two-layer controller are used in the prediction. Implementation results on the real-world full-size industrial tractor-trailers vehicle are presented to validate the performance of the proposed methods.
ER  - 

TY  - CONF
TI  - A Bio-Inspired Transportation Network for Scalable Swarm Foraging
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6120
EP  - 6126
AU  - Q. Lu
AU  - G. M. Fricke
AU  - T. Tsuno
AU  - M. E. Moses
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - multi-robot systems
KW  - swarm intelligence
KW  - transportation
KW  - interrobot collisions
KW  - swarm robot foraging
KW  - scale-invariant swarm foraging algorithm
KW  - hierarchical branching transportation network
KW  - ubiquitous fractal branching networks
KW  - bioinspired transportation network
KW  - Robots
KW  - Transportation
KW  - Biology
KW  - Collision avoidance
KW  - Scalability
KW  - Task analysis
KW  - Explosions
DO  - 10.1109/ICRA40945.2020.9196762
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Scalability is a significant challenge for robot swarms. Generally, larger groups of cooperating robots produce more inter-robot collisions, and in swarm robot foraging, larger search arenas result in larger travel costs. This paper demonstrates a scale-invariant swarm foraging algorithm that ensures that each robot finds and delivers targets to a central collection zone at the same rate regardless of the size of the swarm or the search area. Dispersed mobile depots aggregate locally collected targets and transport them to a central place via a hierarchical branching transportation network. This approach is inspired by ubiquitous fractal branching networks such as tree branches and animal cardiovascular networks that deliver resources to cells and determine the scale and pace of life. We demonstrate that biological scaling laws predict how quickly robots forage in simulations of up to thousands of robots searching over thousands of square meters. We then use biological scaling to predict the capacity of depot robots that overcome scaling constraints to produce scale-invariant robot swarms. We verify the claims for large swarms in simulation and implement a simple depot design in hardware.
ER  - 

TY  - CONF
TI  - Stance Control Inspired by Cerebellum Stabilizes Reflex-Based Locomotion on HyQ Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6127
EP  - 6133
AU  - G. Urbain
AU  - V. Barasuol
AU  - C. Semini
AU  - J. Dambre
AU  - F. wyffels
PY  - 2020
KW  - compliant mechanisms
KW  - feedback
KW  - legged locomotion
KW  - motion control
KW  - predictive control
KW  - robot dynamics
KW  - robust control
KW  - reflex based dynamic locomotion
KW  - stance control
KW  - cerebellum
KW  - legged robotics
KW  - central pattern generators
KW  - cyclic motion
KW  - robotic locomotion
KW  - reflex feedback
KW  - musculoskeletal simulation models
KW  - compliant quadruped robots
KW  - predictive control
KW  - gravity compensation mechanism
KW  - HyQ robot
KW  - stability module
KW  - robust locomotion
KW  - Legged locomotion
KW  - Cerebellum
KW  - Foot
KW  - Stability analysis
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196523
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Advances in legged robotics are strongly rooted in animal observations. A clear illustration of this claim is the generalization of Central Pattern Generators (CPG), first identified in the cat spinal cord, to generate cyclic motion in robotic locomotion. Despite a global endorsement of this model, physiological and functional experiments in mammals have also indicated the presence of descending signals from the cerebellum, and reflex feedback from the lower limb sensory cells, that closely interact with CPGs. To this day, these interactions are not fully understood. In some studies, it was demonstrated that pure reflex-based locomotion in the absence of oscillatory signals could be achieved in realistic musculoskeletal simulation models or small compliant quadruped robots. At the same time, biological evidence has attested the functional role of the cerebellum for predictive control of balance and stance within mammals. In this paper, we promote both approaches and successfully apply reflex-based dynamic locomotion, coupled with a balance and gravity compensation mechanism, on the state-of-art HyQ robot. We discuss the importance of this stability module to ensure a correct foot lift-off and maintain a reliable gait. The robotic platform is further used to test two different architectural hypotheses inspired by the cerebellum. An analysis of experimental results demonstrates that the most biologically plausible alternative also leads to better results for robust locomotion.
ER  - 

TY  - CONF
TI  - Error estimation and correction in a spiking neural network for map formation in neuromorphic hardware
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6134
EP  - 6140
AU  - R. Kreiser
AU  - G. Waibel
AU  - N. Armengol
AU  - A. Renner
AU  - Y. Sandamirskaya
PY  - 2020
KW  - error correction
KW  - mobile robots
KW  - neural chips
KW  - neural nets
KW  - path planning
KW  - pose estimation
KW  - SLAM (robots)
KW  - error correction
KW  - SNN mechanism
KW  - neuromorphic device
KW  - form-factor neuromorphic chip
KW  - spiking neural network
KW  - map formation
KW  - neuromorphic hardware
KW  - neural networks
KW  - robot control
KW  - error estimation
KW  - simultaneous localization and mapping
KW  - robot pose estimation
KW  - SNN-based SLAM
KW  - path integration speed
KW  - Neurons
KW  - Robots
KW  - Sociology
KW  - Statistics
KW  - Light emitting diodes
KW  - Neuromorphics
KW  - Synapses
DO  - 10.1109/ICRA40945.2020.9197498
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Neuromorphic hardware offers computing platforms for the efficient implementation of spiking neural networks (SNNs) that can be used for robot control. Here, we present such an SNN on a neuromorphic chip that solves a number of tasks related to simultaneous localization and mapping (SLAM): forming a map of an unknown environment and, at the same time, estimating the robot's pose. In particular, we present an SNN mechanism to detect and estimate errors when the robot revisits a known landmark and updates both the map and the path integration speed to reduce the error. The whole system is fully realized in a neuromorphic device, showing the feasibility of a purely SNN-based SLAM, which could be efficiently implemented in a small form-factor neuromorphic chip.
ER  - 

TY  - CONF
TI  - Adaptive Visual Shock Absorber with Visual-based Maxwell Model Using a Magnetic Gear
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6163
EP  - 6168
AU  - S. Tanaka
AU  - K. Koyama
AU  - T. Senoo
AU  - M. Ishikawa
PY  - 2020
KW  - feedback
KW  - gears
KW  - impact (mechanical)
KW  - plastic deformation
KW  - position control
KW  - robot dynamics
KW  - shock absorbers
KW  - velocity control
KW  - adaptive visual shock absorber
KW  - visual-based Maxwell model
KW  - feedback control
KW  - high-speed visual object tracking
KW  - velocity control
KW  - object noncontact state
KW  - object contact
KW  - magnetic gear response
KW  - plastic deformation control
KW  - Strain
KW  - Visualization
KW  - Force
KW  - Shock absorbers
KW  - Magnetic gears
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9197504
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, a visual shock absorber capable of adapting to free-fall objects with various weights and speeds is designed and realized. The key element is a magnetic gear to passively absorb shock in the moment of contact, which is difficult for traditional feedback control to deal with. The magnetic gear allows the seamless transfer of control from the non-contact state to the contact state. 1000 Hz high-speed visual object tracking is used for preparation with position and velocity control in the object non-contact state. In the moment of object contact, the high backdrivability of the magnetic gear response by hardware provides high responsiveness to external force. After the impact, the plastic deformation control of a parallel-expressed Maxwell model handles the contact state.
ER  - 

TY  - CONF
TI  - Slip-Based Nonlinear Recursive Backstepping Path Following Controller for Autonomous Ground Vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6169
EP  - 6175
AU  - M. Xin
AU  - K. Zhang
AU  - D. Lackner
AU  - M. A. Minor
PY  - 2020
KW  - compensation
KW  - control nonlinearities
KW  - convergence
KW  - feedback
KW  - feedforward
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - observers
KW  - path planning
KW  - position control
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - stability
KW  - steering systems
KW  - variable structure systems
KW  - kinematic controller
KW  - feedforward slip compensation
KW  - variable structure controller
KW  - graceful motion
KW  - yaw rate commands
KW  - backstepping dynamic controller
KW  - robust steering commands
KW  - output feedback control
KW  - autonomous ground vehicles
KW  - vehicle steering control
KW  - graceful lateral motion
KW  - couples yaw-rate based path
KW  - steering angle
KW  - heading error
KW  - slip-based nonlinear recursive backstepping path following controller
KW  - observer based sideslip estimates
KW  - path following accuracy
KW  - error convergence
KW  - slip-based kinematic model
KW  - dynamic model
KW  - path following error
KW  - robustness
KW  - high gain observer
KW  - stability analysis
KW  - Kinematics
KW  - Vehicle dynamics
KW  - Backstepping
KW  - Tracking
KW  - Dynamics
KW  - Tires
KW  - Convergence
DO  - 10.1109/ICRA40945.2020.9197165
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path following accuracy and error convergence with graceful motion in vehicle steering control is challenging due to the competing nature of these requirements, especially across a range of operating speeds. This work is founded upon slip-based kinematic and dynamic models, which allow derivation of controllers considering error due to sideslip and the mapping between steering commands and graceful lateral motion. A novel recursive backstepping steering controller is proposed that better couples yaw-rate based path following commands to steering angle and rate. Observer based sideslip estimates are combined with heading error in the kinematic controller to provide feedforward slip compensation. Path following error is compensated by a Variable Structure Controller (VSC) to balance graceful motion, path following error, and robustness. Yaw rate commands are used by a backstepping dynamic controller to generate robust steering commands. A High Gain Observer (HGO) estimates sideslip and yaw rate for output feedback control. Stability analysis is provided and peaking is addressed. Field experimental results evaluate the work and provide comparisons to MPC.
ER  - 

TY  - CONF
TI  - Fast and Safe Path-Following Control using a State-Dependent Directional Metric
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6176
EP  - 6182
AU  - Z. Li
AU  - √ñ. Arslan
AU  - N. Atanasov
PY  - 2020
KW  - collision avoidance
KW  - control system synthesis
KW  - Lyapunov methods
KW  - navigation
KW  - path planning
KW  - stability
KW  - safe path-following control
KW  - fast navigation
KW  - safe autonomous navigation
KW  - control policy design
KW  - ellipsoidal trajectory
KW  - ellipsoidal bounds
KW  - control design
KW  - local environment geometry
KW  - medial obstacles
KW  - virtual reference governor system
KW  - system safety
KW  - Lyapunov-function-based designs
KW  - state-dependent directional metric
KW  - quadratic state-dependent distance metric
KW  - Euclidean-norm design
KW  - stability
KW  - collision avoidance
KW  - Robots
KW  - Trajectory
KW  - Safety
KW  - Measurement
KW  - Navigation
KW  - Collision avoidance
KW  - Aerospace electronics
DO  - 10.1109/ICRA40945.2020.9197377
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers the problem of fast and safe autonomous navigation in partially known environments. Our main contribution is a control policy design based on ellipsoidal trajectory bounds obtained from a quadratic state-dependent distance metric. The ellipsoidal bounds are used to embed directional preference in the control design, leading to system behavior that is adapted to local environment geometry, carefully considering medial obstacles while paying less attention to lateral ones. We use a virtual reference governor system to adaptively follow a desired navigation path, slowing down when system safety may be violated and speeding up otherwise. The resulting controller is able to navigate complex environments faster than common Euclidean-norm and Lyapunov-function-based designs, while retaining stability and collision avoidance guarantees.
ER  - 

TY  - CONF
TI  - Backlash-Compensated Active Disturbance Rejection Control of Nonlinear Multi-Input Series Elastic Actuators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6183
EP  - 6189
AU  - B. DeBoon
AU  - S. Nokleby
AU  - C. Rossa
PY  - 2020
KW  - active disturbance rejection control
KW  - actuators
KW  - clutches
KW  - compensation
KW  - elasticity
KW  - force control
KW  - gears
KW  - manipulator dynamics
KW  - nonlinear control systems
KW  - position measurement
KW  - power transmission (mechanical)
KW  - three-term control
KW  - hybrid motor-brake-clutch series elastic actuator
KW  - positional measurement error
KW  - backlash-compensated active disturbance rejection control
KW  - nonlinear multiinput series elastic actuators
KW  - passive compliance
KW  - force-controlled robotic manipulators
KW  - elastic element
KW  - dedicated torque sensors
KW  - deflection control
KW  - nonlinear deformation
KW  - torque requirements
KW  - mechanical backlash
KW  - multiinput active disturbance rejection controller
KW  - error-based controllers
KW  - backlash-compensated ADRC
KW  - Actuators
KW  - Torque
KW  - Springs
KW  - Brakes
KW  - DC motors
KW  - Sea measurements
KW  - Hysteresis motors
DO  - 10.1109/ICRA40945.2020.9196657
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Series elastic actuators with passive compliance have been gaining increasing popularity in force-controlled robotic manipulators. One of the reasons is the actuator's ability to infer the applied torque by measuring the deflection of the elastic element as opposed to directly with dedicated torque sensors. Proper deflection control is pinnacle to achieve a desired output torque and, therefore, small deviances in positional measurements or a nonlinear deformation can have adverse effects on performance. In applications with larger torque requirements, the actuators typically use gear reductions which inherently result in mechanical backlash. This combined with the nonlinear behaviour of the elastic element and unmodelled dynamics, can severely compromise force fidelity.This paper proposes a backlash compensating active disturbance rejection controller (ADRC) for multi-input series elastic actuators. In addition to proper deflection control, a multiinput active disturbance rejection controller is derived and implemented experimentally to mitigate any unmodelled nonlinearities or perturbations to the plant model. The controller is experimentally validated on a hybrid motor-brake-clutch series elastic actuator and the controller performance is compared against traditional error-based controllers. It is shown that the backlash compensated ADRC outperforms classical PID and ADRC methods and is a viable solution to positional measurement error in elastic actuators.
ER  - 

TY  - CONF
TI  - On Generalized Homogenization of Linear Quadrotor Controller
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6190
EP  - 6195
AU  - S. Wang
AU  - A. Polyakov
AU  - G. Zheng
PY  - 2020
KW  - control system synthesis
KW  - feedback
KW  - helicopters
KW  - linearisation techniques
KW  - nonlinear control systems
KW  - generalized homogenization
KW  - linear quadrotor controller
KW  - generalized homogeneity
KW  - implicit homogeneous feedback design
KW  - tuning rules
KW  - quadrotor QDrone
KW  - Symmetric matrices
KW  - Generators
KW  - Linear matrix inequalities
KW  - Robustness
KW  - Closed loop systems
KW  - Lyapunov methods
DO  - 10.1109/ICRA40945.2020.9197116
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A novel scheme for an "upgrade" of a linear control algorithm to a non-linear one is developed based on the concepts of a generalized homogeneity and an implicit homogeneous feedback design. Some tuning rules for a guaranteed improvement of a regulation quality are proposed. Theoretical results are confirmed by real experiments with the quadrotor QDrone of Quanser‚Ñ¢.
ER  - 


TY  - CONF
TI  - In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6203
EP  - 6209
AU  - J. Liang
AU  - A. Handa
AU  - K. V. Wyk
AU  - V. Makoviychuk
AU  - O. Kroemer
AU  - D. Fox
PY  - 2020
KW  - coprocessors
KW  - graphics processing units
KW  - manipulators
KW  - object tracking
KW  - optimisation
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - complex contact dynamics
KW  - GPU-accelerated parallel robot simulations
KW  - sample-based optimizers
KW  - contact feedback
KW  - robot-object interactions
KW  - GPU-accelerated robotic simulation
KW  - robot hand
KW  - vision-based methods
KW  - particle filters
KW  - static grasp setting
KW  - in-hand object pose tracking
KW  - manipulation
KW  - physics simulation
KW  - forward model
KW  - point cloud distance error
KW  - Pose estimation
KW  - Robot sensing systems
KW  - Physics
KW  - Heuristic algorithms
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9197117
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tracking the pose of an object while it is being held and manipulated by a robot hand is difficult for vision-based methods due to significant occlusions. Prior works have explored using contact feedback and particle filters to localize in-hand objects. However, they have mostly focused on the static grasp setting and not when the object is in motion, as doing so requires modeling of complex contact dynamics. In this work, we propose using GPU-accelerated parallel robot simulations and derivative-free, sample-based optimizers to track in-hand object poses with contact feedback during manipulation. We use physics simulation as the forward model for robot-object interactions, and the algorithm jointly optimizes for the state and the parameters of the simulations, so they better match with those of the real world. Our method runs in real-time (30Hz) on a single GPU, and it achieves an average point cloud distance error of 6mm in simulation experiments and 13mm in the real-world ones.
ER  - 

TY  - CONF
TI  - Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6210
EP  - 6217
AU  - B. Wen
AU  - C. Mitash
AU  - S. Soorian
AU  - A. Kimmel
AU  - A. Sintov
AU  - K. E. Bekris
PY  - 2020
KW  - image registration
KW  - pose estimation
KW  - occlusion-aware pose estimation
KW  - adaptive hands
KW  - manipulation tasks
KW  - within-hand manipulation
KW  - robot hand
KW  - depth-based framework
KW  - robust pose estimation
KW  - adaptive hand
KW  - efficient parallel search
KW  - point cloud
KW  - robust global registration
KW  - object types
KW  - object pose hypotheses
KW  - short response times
KW  - in-hand 6D object pose estimation
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Robot sensing systems
KW  - Robustness
KW  - Computational modeling
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9197350
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https://github.com/wenbowen123/icra20-hand-object-pose.
ER  - 

TY  - CONF
TI  - Robust 6D Object Pose Estimation by Learning RGB-D Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6218
EP  - 6224
AU  - M. Tian
AU  - L. Pan
AU  - M. H. Ang
AU  - G. Hee Lee
PY  - 2020
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - optimisation
KW  - pose estimation
KW  - regression analysis
KW  - robot vision
KW  - video signal processing
KW  - RGB-D features
KW  - robotic manipulation
KW  - local optimization approach
KW  - distance between closest point pairs
KW  - rotation ambiguity
KW  - symmetric objects
KW  - rotation regression
KW  - local-optimum problem
KW  - object location
KW  - point-wise vectors
KW  - robust 6D object pose estimation
KW  - discrete-continuous formulation
KW  - LINEMOD
KW  - YCB-Video
KW  - Feature extraction
KW  - Pose estimation
KW  - Three-dimensional displays
KW  - Robustness
KW  - Uncertainty
KW  - Image segmentation
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197555
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete- continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https://github.com/mentian/object-posenet.
ER  - 

TY  - CONF
TI  - Split Deep Q-Learning for Robust Object Singulation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6225
EP  - 6231
AU  - I. Sarantopoulos
AU  - M. Kiatos
AU  - Z. Doulgeri
AU  - S. Malassiotis
PY  - 2020
KW  - collision avoidance
KW  - grippers
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - policy learning
KW  - split deep Q-learning
KW  - robust object singulation
KW  - robotic manipulation
KW  - robotic applications
KW  - grasping techniques
KW  - pushing policy
KW  - lateral pushing movements
KW  - reinforcement learning
KW  - optimal push policies
KW  - split DQN
KW  - target object extraction
KW  - Grasping
KW  - Task analysis
KW  - Clutter
KW  - Image segmentation
KW  - Robustness
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196647
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Extracting a known target object from a pile of other objects in a cluttered environment is a challenging robotic manipulation task encountered in many robotic applications. In such conditions, the target object touches or is covered by adjacent obstacle objects, thus rendering traditional grasping techniques ineffective. In this paper, we propose a pushing policy aiming at singulating the target object from its surrounding clutter, by means of lateral pushing movements of both the neighboring objects and the target object until sufficient 'grasping room' has been achieved. To achieve the above goal we employ reinforcement learning and particularly Deep Qlearning (DQN) to learn optimal push policies by trial and error. A novel Split DQN is proposed to improve the learning rate and increase the modularity of the algorithm. Experiments show that although learning is performed in a simulated environment the transfer of learned policies to a real environment is effective thanks to robust feature selection. Finally, we demonstrate that the modularity of the algorithm allows the addition of extra primitives without retraining the model from scratch.
ER  - 

TY  - CONF
TI  - 6-DOF Grasping for Target-driven Object Manipulation in Clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6232
EP  - 6238
AU  - A. Murali
AU  - A. Mousavian
AU  - C. Eppner
AU  - C. Paxton
AU  - D. Fox
PY  - 2020
KW  - manipulators
KW  - object detection
KW  - robot vision
KW  - 6DOF grasping
KW  - cluttered environments
KW  - manipulator
KW  - robotic platform
KW  - target-driven object manipulation
KW  - point cloud observations
KW  - collision checking module
KW  - grasp sequence
KW  - Clutter
KW  - Three-dimensional displays
KW  - Grasping
KW  - Grippers
KW  - Robots
KW  - Collision avoidance
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197318
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found here.
ER  - 

TY  - CONF
TI  - Single Shot 6D Object Pose Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6239
EP  - 6245
AU  - K. Kleeberger
AU  - M. F. Huber
PY  - 2020
KW  - convolutional neural nets
KW  - object detection
KW  - pose estimation
KW  - regression analysis
KW  - stereo image processing
KW  - rigid objects
KW  - depth images
KW  - convolutional neural network
KW  - 3D input data
KW  - volume elements
KW  - optimized end-to-end
KW  - multiple objects
KW  - single shot 6D object pose estimation
KW  - single shot approach
KW  - object pose network
KW  - regression task
KW  - GPU
KW  - synthetic data
KW  - public benchmark datasets
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Data models
KW  - Task analysis
KW  - Pose estimation
KW  - Image segmentation
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9197207
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we introduce a novel single shot approach for 6D object pose estimation of rigid objects based on depth images. For this purpose, a fully convolutional neural network is employed, where the 3D input data is spatially discretized and pose estimation is considered as a regression task that is solved locally on the resulting volume elements. With 65 fps on a GPU, our Object Pose Network (OP-Net) is extremely fast, is optimized end-to-end, and estimates the 6D pose of multiple objects in the image simultaneously. Our approach does not require manually 6D pose-annotated real-world datasets and transfers to the real world, although being entirely trained on synthetic data. The proposed method is evaluated on public benchmark datasets, where we can demonstrate that state-of-the-art methods are significantly outperformed.
ER  - 

TY  - CONF
TI  - MulRan: Multimodal Range Dataset for Urban Place Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6246
EP  - 6253
AU  - G. Kim
AU  - Y. S. Park
AU  - Y. Cho
AU  - J. Jeong
AU  - A. Kim
PY  - 2020
KW  - geophysical image processing
KW  - geophysical techniques
KW  - image recognition
KW  - mobile robots
KW  - object recognition
KW  - optical radar
KW  - radar imaging
KW  - robot vision
KW  - multimodal range dataset
KW  - radio detection and ranging
KW  - light detection and ranging
KW  - urban environment
KW  - range sensor-based place recognition
KW  - 6D baseline trajectories
KW  - place recognition ground truth
KW  - image-format data
KW  - time-stamped 1D intensity arrays
KW  - polar images
KW  - image data
KW  - radar place recognition method
KW  - LiDAR
KW  - longer-range measurements
KW  - urban place recognition
KW  - MulRan
KW  - Laser radar
KW  - Radar imaging
KW  - Three-dimensional displays
KW  - Urban areas
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197298
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a multimodal range dataset namely for radio detection and ranging (radar) and light detection and ranging (LiDAR) specifically targeting the urban environment. By extending our workshop paper [1] to a larger scale, this dataset focuses on the range sensor-based place recognition and provides 6D baseline trajectories of a vehicle for place recognition ground truth. Provided radar data support both raw-level and image-format data, including a set of time-stamped 1D intensity arrays and 360¬∞ polar images, respectively. In doing so, we provide flexibility between raw data and image data depending on the purpose of the research. Unlike existing datasets, our focus is at capturing both temporal and structural diversities for range-based place recognition research. For evaluation, we applied and validated that our previous location descriptor and its search algorithm [2] are highly effective for radar place recognition method. Furthermore, the result shows that radar-based place recognition outperforms LiDAR-based one exploiting its longer-range measurements. The dataset is available from https://sites.google.com/view/mulran-pr.
ER  - 

TY  - CONF
TI  - GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6254
EP  - 6260
AU  - S. Du
AU  - H. Guo
AU  - Y. Chen
AU  - Y. Lin
AU  - X. Meng
AU  - L. Wen
AU  - F. -Y. Wang
PY  - 2020
KW  - cameras
KW  - optimisation
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - GPO
KW  - global plane optimization
KW  - homography estimation
KW  - homography decomposition
KW  - monocular SLAM initialization
KW  - monocular simultaneous localization and mapping problem
KW  - camera poses
KW  - chessboard dataset
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Optimization
KW  - Matrix decomposition
KW  - Transmission line matrix methods
KW  - Estimation
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196970
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the ne-tuned baselines in both accuracy and real-time.
ER  - 

TY  - CONF
TI  - Large-Scale Volumetric Scene Reconstruction using LiDAR
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6261
EP  - 6267
AU  - T. K√ºhner
AU  - J. K√ºmmerle
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image reconstruction
KW  - image representation
KW  - optical radar
KW  - large-scale 3D scene reconstruction
KW  - autonomous driving
KW  - volumetric depth fusion
KW  - indoor applications
KW  - commodity RGB-D cameras
KW  - high reconstruction quality
KW  - LiDAR sensors
KW  - autonomous cars
KW  - large-scale mapping
KW  - urban area
KW  - meshed representation
KW  - real world application
KW  - large-scale volumetric scene reconstruction
KW  - distance 3.7 km
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Image reconstruction
KW  - Graphics processing units
KW  - Sensor fusion
KW  - Weight measurement
DO  - 10.1109/ICRA40945.2020.9197388
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Large-scale 3D scene reconstruction is an important task in autonomous driving and other robotics applications as having an accurate representation of the environment is necessary to safely interact with it. Reconstructions are used for numerous tasks ranging from localization and mapping to planning. In robotics, volumetric depth fusion is the method of choice for indoor applications since the emergence of commodity RGB-D cameras due to its robustness and high reconstruction quality. In this work we present an approach for volumetric depth fusion using LiDAR sensors as they are common on most autonomous cars. We present a framework for large-scale mapping of urban areas considering loop closures. Our method creates a meshed representation of an urban area from recordings over a distance of 3.7km with a high level of detail on consumer graphics hardware in several minutes. The whole process is fully automated and does not need any user interference. We quantitatively evaluate our results from a real world application. Also, we investigate the effects of the sensor model that we assume on reconstruction quality by using synthetic data.
ER  - 

TY  - CONF
TI  - Topological Mapping for Manhattan-like Repetitive Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6268
EP  - 6274
AU  - S. S. Puligilla
AU  - S. Tourani
AU  - T. Vaidya
AU  - U. S. Parihar
AU  - R. Kiran Sarvadevabhatla
AU  - K. M. Krishna
PY  - 2020
KW  - convolutional neural nets
KW  - graph theory
KW  - image representation
KW  - optimisation
KW  - SLAM (robots)
KW  - topology
KW  - Manhattan properties
KW  - topological graph
KW  - unoptimized Pose Graph
KW  - topological Manhattan relations
KW  - ground-truth Pose Graph
KW  - real-world indoor warehouse scenes
KW  - Manhattan-like repetitive environments
KW  - topological mapping framework
KW  - neighbouring nodes
KW  - indoor warehouse setting
KW  - warehouse topological construct
KW  - deep convolutional network
KW  - Siamese-style neural network
KW  - backend pose graph optimization framework
KW  - Manhattan graph aided loop closure relations
KW  - Topology
KW  - Network topology
KW  - Simultaneous localization and mapping
KW  - Neural networks
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197520
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.
ER  - 

TY  - CONF
TI  - Robust RGB-D Camera Tracking using Optimal Key-frame Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6275
EP  - 6281
AU  - K. M. Han
AU  - Y. J. Kim
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - image reconstruction
KW  - image sequences
KW  - integer programming
KW  - interpolation
KW  - iterative methods
KW  - motion estimation
KW  - SLAM (robots)
KW  - optimal key-frame selection
KW  - integer programming
KW  - VO method
KW  - camera motion
KW  - elastic-fusion
KW  - discontinuous camera motions
KW  - robust RGB-D camera tracking
KW  - adaptive visual odometry
KW  - TUM benchmark sequences
KW  - camera trajectory errors
KW  - iterative closed point
KW  - Cameras
KW  - Robustness
KW  - Optimization
KW  - Tracking
KW  - Iterative closest point algorithm
KW  - Robot vision systems
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197021
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel RGB-D camera tracking system that robustly reconstructs hand-held RGB-D camera sequences. The robustness of our system is achieved by two independent features of our method: adaptive visual odometry (VO) and integer programming-based key-frame selection. Our VO method adaptively interpolates the camera motion results of the direct VO (DVO) and the iterative closed point (ICP) to yield more optimal results than existing methods such as Elastic-Fusion. Moreover, our key-frame selection method locates globally optimum key-frames using a comprehensive objective function in a deterministic manner rather than heuristic or experience-based rules that prior methods mostly rely on. As a result, our method can complete reconstruction even if the camera fails to be tracked due to discontinuous camera motions, such as kidnap events, when conventional systems need to backtrack the scene. We validated our tracking system on 25 TUM benchmark sequences against state-of-the-art works, such as ORBSLAM2, Elastic-Fusion, and DVO SLAM, and experimentally showed that our method has smaller and more robust camera trajectory errors than these systems.
ER  - 

TY  - CONF
TI  - Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6282
EP  - 6287
AU  - S. Li
AU  - E. √ñzt√ºrk
AU  - C. De Wagter
AU  - G. C. H. E. de Croon
AU  - D. Izzo
PY  - 2020
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - helicopters
KW  - mobile robots
KW  - neural nets
KW  - optimisation
KW  - time optimal control
KW  - power optimality
KW  - time optimality
KW  - deep neural network
KW  - robotic applications
KW  - optimality principles
KW  - deep network representations
KW  - aggressive online control
KW  - time-optimal maneuvers
KW  - offline optimal control method
KW  - aggressive quadrotor control
KW  - Trajectory
KW  - Optimal control
KW  - Stability analysis
KW  - Neural networks
KW  - Delays
KW  - Training
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9197443
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Optimal control holds great potential to improve a variety of robotic applications. The application of optimal control on-board limited platforms has been severely hindered by the large computational requirements of current state of the art implementations. In this work, we make use of a deep neural network to directly map the robot states to control actions. The network is trained offline to imitate the optimal control computed by a time consuming direct nonlinear method. A mixture of time optimality and power optimality is considered with a continuation parameter used to select the predominance of each objective. We apply our networks (termed G&CNets) to aggressive quadrotor control, first in simulation and then in the real world. We give insight into the factors that influence the `reality gap' between the quadrotor model used by the offline optimal control method and the real quadrotor. Furthermore, we explain how we set up the model and the control structure on-board of the real quadrotor to successfully close this gap and perform time-optimal maneuvers in the real world. Finally, G&CNet's performance is compared to state-of-the-art differential-flatness-based optimal control methods. We show, in the experiments, that G&CNets lead to significantly faster trajectory execution due to, in part, the less restrictive nature of the allowed state-to-input mappings.
ER  - 

TY  - CONF
TI  - Refined Analysis of Asymptotically-Optimal Kinodynamic Planning in the State-Cost Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6344
EP  - 6350
AU  - M. Kleinbort
AU  - E. Granados
AU  - K. Solovey
AU  - R. Bonalli
AU  - K. E. Bekris
AU  - D. Halperin
PY  - 2020
KW  - boundary-value problems
KW  - motion control
KW  - optimal control
KW  - piecewise constant techniques
KW  - robot dynamics
KW  - trees (mathematics)
KW  - asymptotically-optimal kinodynamic planning
KW  - state-cost space
KW  - AO-RRT
KW  - tree-based planner
KW  - motion planning
KW  - kinodynamic constraints
KW  - optimality proof
KW  - piecewise-constant control function
KW  - two-point boundary-value
KW  - Lipschitz-continuity
KW  - Trajectory
KW  - Planning
KW  - Aerospace electronics
KW  - Robots
KW  - Collision avoidance
KW  - Cost function
KW  - Space exploration
DO  - 10.1109/ICRA40945.2020.9197236
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel analysis of AO-RRT: a tree-based planner for motion planning with kinodynamic constraints, originally described by Hauser and Zhou (AO-X, 2016). AO-RRT explores the state-cost space and has been shown to efficiently obtain high-quality solutions in practice without relying on the availability of a computationally-intensive two-point boundary-value solver. Our main contribution is an optimality proof for the single-tree version of the algorithm-a variant that was not analyzed before. Our proof only requires a mild and easily-verifiable set of assumptions on the problem and system: Lipschitz-continuity of the cost function and the dynamics. In particular, we prove that for any system satisfying these assumptions, any trajectory having a piecewise-constant control function and positive clearance from the obstacles can be approximated arbitrarily well by a trajectory found by AORRT. We also discuss practical aspects of AORRT and present experimental comparisons of variants of the algorithm.
ER  - 

TY  - CONF
TI  - Robust quadcopter control with artificial vector fields*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6381
EP  - 6387
AU  - A. M. C. Rezende
AU  - V. M. Gon√ßalves
AU  - A. H. D. Nunes
AU  - L. C. A. Pimenta
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - helicopters
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - robust control
KW  - time-varying systems
KW  - robust quadcopter control
KW  - artificial vector fields
KW  - path tracking control strategy
KW  - control laws
KW  - vector field
KW  - controlled second order integrator
KW  - quadcopter model
KW  - input-to-state stable
KW  - control inputs
KW  - Robots
KW  - Vehicle dynamics
KW  - Robustness
KW  - Convergence
KW  - Level set
KW  - Mathematical model
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196605
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This article presents a path tracking control strategy for a quadcopter to follow a time varying curve. The control is based on artificial vector fields. The construction of the field is based on a well known technique in the literature. Next, control laws are developed to impose the behavior of the vector field to a second order integrator model. Finally, control laws are developed to impose the dynamics of the controlled second order integrator to a quadcopter model, which assumes the thrust and the angular rates as input commands. Asymptotic convergence of the whole system is proved by showing that the individual systems in cascade connection are input-to-state stable. We also analyze the influence of norm-bounded disturbances in the control inputs to evaluate the robustness of the controller. We show that bounded disturbances originate limited deviations from the target curve. Simulations and a real robot experiment exemplify and validate the developed theory.
ER  - 

TY  - CONF
TI  - Simulation-Based Reinforcement Learning for Real-World Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6411
EP  - 6418
AU  - B. Osi≈Ñski
AU  - A. Jakubowski
AU  - P. Ziƒôcina
AU  - P. Mi≈Ço≈õ
AU  - C. Galias
AU  - S. Homoceanu
AU  - H. Michalewski
PY  - 2020
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - road vehicles
KW  - traffic engineering computing
KW  - simulation-based reinforcement learning
KW  - real-world autonomous driving
KW  - driving system
KW  - real-world vehicle
KW  - driving policy
KW  - RGB images
KW  - single camera
KW  - semantic segmentation
KW  - synthetic data
KW  - real-world data
KW  - segmentation network
KW  - real-world experiments
KW  - sim-to-real policy transfer
KW  - real-world performance
KW  - Training
KW  - Visualization
KW  - Learning (artificial intelligence)
KW  - Semantics
KW  - Robots
KW  - Image segmentation
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196730
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.
ER  - 

TY  - CONF
TI  - Driving Style Encoder: Situational Reward Adaptation for General-Purpose Planning in Automated Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6419
EP  - 6425
AU  - S. Rosbach
AU  - V. James
AU  - S. Gro√üjohann
AU  - S. Homoceanu
AU  - X. Li
AU  - S. Roth
PY  - 2020
KW  - learning (artificial intelligence)
KW  - path planning
KW  - predictive control
KW  - road traffic control
KW  - situational reward adaptation
KW  - general-purpose planning algorithms
KW  - automated driving
KW  - planning algorithm
KW  - driving kinematics
KW  - linear reward function
KW  - driving situation
KW  - deep learning approach
KW  - situation-dependent reward functions
KW  - sampled driving policies
KW  - driving style
KW  - planning cycle
KW  - Planning
KW  - Neural networks
KW  - Entropy
KW  - Kinematics
KW  - Machine learning
KW  - Tuning
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196778
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - General-purpose planning algorithms for automated driving combine mission, behavior, and local motion planning. Such planning algorithms map features of the environment and driving kinematics into complex reward functions. To achieve this, planning experts often rely on linear reward functions. The specification and tuning of these reward functions is a tedious process and requires significant experience. Moreover, a manually designed linear reward function does not generalize across different driving situations. In this work, we propose a deep learning approach based on inverse reinforcement learning that generates situation-dependent reward functions. Our neural network provides a mapping between features and actions of sampled driving policies of a model-predictive control-based planner and predicts reward functions for upcoming planning cycles. In our evaluation, we compare the driving style of reward functions predicted by our deep network against clustered and linear reward functions. Our proposed deep learning approach outperforms clustered linear reward functions and is at par with linear reward functions with a-priori knowledge about the situation.
ER  - 

TY  - CONF
TI  - Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6426
EP  - 6432
AU  - S. K. Jayaraman
AU  - D. M. Tilbury
AU  - X. Jessie Yang
AU  - A. K. Pradhan
AU  - L. P. Robert
PY  - 2020
KW  - behavioural sciences computing
KW  - human-robot interaction
KW  - navigation
KW  - path planning
KW  - pedestrians
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - virtual reality
KW  - automated vehicle interactions
KW  - safe navigation
KW  - automated vehicles
KW  - pedestrian interactions
KW  - human-driven vehicles
KW  - HDV
KW  - hybrid systems model
KW  - constant velocity dynamics
KW  - long-term pedestrian trajectory prediction
KW  - immersive virtual environment
KW  - AV interactions
KW  - pedestrian crosswalk behavior analysis
KW  - pedestrian crosswalk behavior prediction
KW  - gap acceptance behavior
KW  - AV motion planning
KW  - IVE
KW  - Predictive models
KW  - Trajectory
KW  - Legged locomotion
KW  - Vehicle dynamics
KW  - Virtual environments
KW  - Planning
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197347
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians' trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians' gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further, we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.
ER  - 

TY  - CONF
TI  - The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6433
EP  - 6438
AU  - D. Barnes
AU  - M. Gadd
AU  - P. Murcutt
AU  - P. Newman
AU  - I. Posner
PY  - 2020
KW  - CW radar
KW  - distance measurement
KW  - FM radar
KW  - Global Positioning System
KW  - millimetre wave radar
KW  - optical radar
KW  - road vehicle radar
KW  - Oxford Radar RobotCar dataset
KW  - radar extension
KW  - millimetre-wave FMCW scanning radar data
KW  - central Oxford route
KW  - truth optimised radar odometry
KW  - autonomous vehicles
KW  - environmental conditions
KW  - sensor modalities
KW  - AD 2019 01
KW  - urban driving
KW  - weather condition
KW  - traffic condition
KW  - lighting condition
KW  - Navtech CTS350-X radar
KW  - Velodyne HDL-32E 3D LIDAR
KW  - GPS-INS receiver
KW  - ori.ox.ac.uk/datasets/radar-robotear-dataset
KW  - size 280.0 km
KW  - memory size 4.7 TByte
KW  - Robot sensing systems
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Azimuth
KW  - Calibration
DO  - 10.1109/ICRA40945.2020.9196884
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.
ER  - 

TY  - CONF
TI  - Multi-modal Experts Network for Autonomous Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6439
EP  - 6445
AU  - S. Fang
AU  - A. Choromanska
PY  - 2020
KW  - computational complexity
KW  - control engineering computing
KW  - expert systems
KW  - inference mechanisms
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - road vehicles
KW  - sensory data
KW  - autonomous driving
KW  - autonomous vehicles
KW  - computational complexity
KW  - multistage training procedure
KW  - end-to-end learning
KW  - multimodal experts network architecture
KW  - inference time step
KW  - mixed discrete-continuous policy
KW  - Laser radar
KW  - Feature extraction
KW  - Cameras
KW  - Training
KW  - Autonomous vehicles
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197459
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - End-to-end learning from sensory data has shown promising results in autonomous driving. While employing many sensors enhances world perception and should lead to more robust and reliable behavior of autonomous vehicles, it is challenging to train and deploy such network and at least two problems are encountered in the considered setting. The first one is the increase of computational complexity with the number of sensing devices. The other is the phenomena of network overfitting to the simplest and most informative input. We address both challenges with a novel, carefully tailored multi-modal experts network architecture and propose a multi-stage training procedure. The network contains a gating mechanism, which selects the most relevant input at each inference time step using a mixed discrete-continuous policy. We demonstrate the plausibility of the proposed approach on our 1/6 scale truck equipped with three cameras and one LiDAR.
ER  - 

TY  - CONF
TI  - Localising PMDs through CNN Based Perception of Urban Streets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6454
EP  - 6460
AU  - M. Jayasuriya
AU  - J. Arukgoda
AU  - R. Ranasinghe
AU  - G. Dissanayake
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - feature extraction
KW  - Kalman filters
KW  - learning (artificial intelligence)
KW  - nonlinear filters
KW  - object detection
KW  - robot vision
KW  - common environmental landmarks
KW  - point features
KW  - higher level information
KW  - common vision based approaches
KW  - low level hand
KW  - EKF framework
KW  - practical CNN
KW  - typical suburban streets
KW  - localiser
KW  - PMD
KW  - CNN based perception
KW  - urban streets
KW  - localisation scheme
KW  - complementary approaches
KW  - outdoor vision based localisation
KW  - convolutional neural networks
KW  - necessary perceptual information
KW  - camera images
KW  - lane markings
KW  - manhole covers
KW  - vector distance
KW  - binary image
KW  - ground surface boundaries
KW  - CNN based detection
KW  - novel extended Kalman filter
KW  - Feature extraction
KW  - Transforms
KW  - Cameras
KW  - Semantics
KW  - Two dimensional displays
KW  - Data mining
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196639
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The main contribution of this paper is a novel Extended Kalman Filter (EKF) based localisation scheme that fuses two complementary approaches to outdoor vision based localisation. This EKF is aided by a front end consisting of two Convolutional Neural Networks (CNNs) that provide the necessary perceptual information from camera images. The first approach involves a CNN based extraction of information corresponding to artefacts such as curbs, lane markings, and manhole covers to localise on a vector distance transform representation of a binary image of these ground surface boundaries. The second approach involves a CNN based detection of common environmental landmarks such as tree trunks and light poles, which are represented as point features on a sparse map. Utilising CNNs to obtain higher level information about the environment enables this framework to avoid the typical pitfalls of common vision based approaches that use low level hand crafted features for localisation. The EKF framework makes it possible to deal with false positives and missed detections that are inevitable in a practical CNN, to produce a location estimate together with its associated uncertainty. Experiments using a Personal Mobility Device (PMD) driven in typical suburban streets are presented to demonstrate the effectiveness of the proposed localiser.
ER  - 

TY  - CONF
TI  - Hybrid Localization using Model- and Learning-Based Methods: Fusion of Monte Carlo and E2E Localizations via Importance Sampling
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6469
EP  - 6475
AU  - N. Akai
AU  - T. Hirayama
AU  - H. Murase
PY  - 2020
KW  - convolutional neural nets
KW  - importance sampling
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - neurocontrollers
KW  - particle filtering (numerical methods)
KW  - path planning
KW  - motion model
KW  - importance sampling
KW  - convolutional neural network
KW  - CNN
KW  - Monte Carlo localization
KW  - particle filter
KW  - hybrid localization method
KW  - learning-based method
KW  - model-based method
KW  - E2E localization
KW  - MCL
KW  - CNN predictions
KW  - posterior distributions
KW  - Atmospheric measurements
KW  - Particle measurements
KW  - Proposals
KW  - Predictive models
KW  - Fuses
KW  - Learning systems
KW  - Monte Carlo methods
DO  - 10.1109/ICRA40945.2020.9196568
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a hybrid localization method that fuses Monte Carlo localization (MCL) and convolutional neural network (CNN)-based end-to-end (E2E) localization. MCL is based on particle filter and requires proposal distributions to sample the particles. The proposal distribution is generally predicted using a motion model. However, because the motion model cannot handle unanticipated errors, the predicted distribution is sometimes inaccurate. The use of other ideal proposal distributions, such as the measurement model, can improve robustness against such unanticipated errors. This technique is called importance sampling (IS). However, it is difficult to sample the particles from such ideal distributions because they are not represented in the closed form. Recent works have proved that CNNs with dropout layers represent the posterior distributions over their outputs conditioned on the inputs and the CNN predictions are equivalent to sampling the outputs from the posterior. Therefore, the proposed method utilizes a CNN to sample the particles and fuses them with MCL via IS. Consequently, the advantages of both MCL and E2E localization can be simultaneously leveraged while preventing their disadvantages. Experiments demonstrate that the proposed method can smoothly estimate the robot pose, similar to the model-based method, and quickly re-localize it from the failures, similar to the learning-based method.
ER  - 

TY  - CONF
TI  - Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6491
EP  - 6497
AU  - B. Patel
AU  - T. D. Barfoot
AU  - A. P. Schoellig
PY  - 2020
KW  - autonomous aerial vehicles
KW  - distance measurement
KW  - Global Positioning System
KW  - image filtering
KW  - image registration
KW  - image sensors
KW  - mobile robots
KW  - multi-robot systems
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - robot vision
KW  - Google Earth images
KW  - georeferenced rendered images
KW  - dense mutual information technique
KW  - outdoor GPS-denied environments
KW  - image registrations
KW  - gimballed visual odometry pipeline
KW  - visual localization
KW  - robust global pose estimation
KW  - multirotor UAV
KW  - typical feature-based localizer
KW  - Cameras
KW  - Image registration
KW  - Three-dimensional displays
KW  - Visualization
KW  - Earth
KW  - Robustness
KW  - Google
DO  - 10.1109/ICRA40945.2020.9196606
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We estimate the global pose of a multirotor UAV by visually localizing images captured during a flight with Google Earth images pre-rendered from known poses. We metrically localize real images with georeferenced rendered images using a dense mutual information technique to allow accurate global pose estimation in outdoor GPS-denied environments. We show the ability to consistently localize throughout a sunny summer day despite major lighting changes while demonstrating that a typical feature-based localizer struggles under the same conditions. Successful image registrations are used as measurements in a filtering framework to apply corrections to the pose estimated by a gimballed visual odometry pipeline. We achieve less than 1 m and 1¬∞ RMSE on a 303 m flight and less than 3 m and 3¬∞ RMSE on six 1132 m flights as low as 36 m above ground level conducted at different times of the day from sunrise to sunset.
ER  - 

TY  - CONF
TI  - Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6498
EP  - 6505
AU  - L. Hermann
AU  - M. Argus
AU  - A. Eitel
AU  - A. Amiranashvili
AU  - W. Burgard
AU  - T. Brox
PY  - 2020
KW  - computer vision
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - shaped reward functions
KW  - ACGD
KW  - policy transfer
KW  - real-world manipulation tasks
KW  - sim-to-real visuomotor control
KW  - reinforcement learning
KW  - adaptive curriculum generation
KW  - vision-based control policies
KW  - Task analysis
KW  - Training
KW  - Robots
KW  - Trajectory
KW  - Learning (artificial intelligence)
KW  - Adaptation models
KW  - Stacking
DO  - 10.1109/ICRA40945.2020.9197108
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose Adaptive Curriculum Generation from Demonstrations (ACGD) for reinforcement learning in the presence of sparse rewards. Rather than designing shaped reward functions, ACGD adaptively sets the appropriate task difficulty for the learner by controlling where to sample from the demonstration trajectories and which set of simulation parameters to use. We show that training vision-based control policies in simulation while gradually increasing the difficulty of the task via ACGD improves the policy transfer to the real world. The degree of domain randomization is also gradually increased through the task difficulty. We demonstrate zero-shot transfer for two real-world manipulation tasks: pick-and-stow and block stacking. A video showing the results can be found at https://lmb.informatik.uni-freiburg.de/projects/curriculum/.
ER  - 

TY  - CONF
TI  - Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep Visuomotor Policies for Manipulation in Clutter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6506
EP  - 6512
AU  - P. Abolghasemi
AU  - L. B√∂l√∂ni
PY  - 2020
KW  - clutter
KW  - computer vision
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - data augmentation procedure
KW  - network architectures
KW  - implicit attention ASOR-IA
KW  - explicit attention ASOR-EA
KW  - training data
KW  - uncluttered environment
KW  - cluttered environments
KW  - end-to-end training
KW  - attentive deep visuomotor policies
KW  - end-to-end train multitask deep visuomotor policies
KW  - robotic manipulation
KW  - reinforcement learning
KW  - end-to-end LfD architectures
KW  - Accept Synthetic Objects as Real
KW  - Clutter
KW  - Robots
KW  - Training data
KW  - Task analysis
KW  - Training
KW  - Encoding
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197552
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent research demonstrated that it is feasible to end-to-end train multi-task deep visuomotor policies for robotic manipulation using variations of learning from demonstration (LfD) and reinforcement learning (RL). In this paper, we extend the capabilities of end-to-end LfD architectures to object manipulation in clutter. We start by introducing a data augmentation procedure called Accept Synthetic Objects as Real (ASOR). Using ASOR we develop two network architectures: implicit attention ASOR-IA and explicit attention ASOR-EA. Both architectures use the same training data (demonstrations in uncluttered environments) as previous approaches. Experimental results show that ASOR-IA and ASOR-EA succeed in a significant fraction of trials in cluttered environments where previous approaches never succeed. In addition, we find that both ASOR-IA and ASOR-EA outperform previous approaches even in uncluttered environments, with ASOR-EA performing better even in clutter compared to the previous best baseline in an uncluttered environment.
ER  - 

TY  - CONF
TI  - Learning of Exception Strategies in Assembly Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6521
EP  - 6527
AU  - B. Nemec
AU  - M. Simoniƒç
AU  - A. Ude
PY  - 2020
KW  - humanoid robots
KW  - mobile robots
KW  - position control
KW  - principal component analysis
KW  - robotic assembly
KW  - humanoid robots
KW  - LfD framework
KW  - exception strategies
KW  - peg-in-hole task
KW  - Franka-Emika Panda robot
KW  - assembly tasks
KW  - assembly policy
KW  - Robot sensing systems
KW  - Task analysis
KW  - Trajectory
KW  - Robot kinematics
KW  - Databases
KW  - Statistical learning
DO  - 10.1109/ICRA40945.2020.9197480
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Assembly tasks performed with a robot often fail due to unforeseen situations, regardless of the fact that we carefully learned and optimized the assembly policy. This problem is even more present in humanoid robots acting in an unstructured environment where it is not possible to anticipate all factors that might lead to the failure of the given task. In this work, we propose a concurrent LfD framework, which associates demonstrated exception strategies to the given context. Whenever a failure occurs, the proposed algorithm generalizes past experience regarding the current context and generates an appropriate policy that solves the assembly issue. For this purpose, we applied PCA on force/torque data, which generates low dimensional descriptor of the current context. The proposed framework was validated in a peg-in-hole (PiH) task using Franka-Emika Panda robot.
ER  - 

TY  - CONF
TI  - An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6544
EP  - 6550
AU  - A. Munawar
AU  - N. Srishankar
AU  - G. S. Fischer
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - manipulators
KW  - medical computing
KW  - medical robotics
KW  - surgery
KW  - telerobotics
KW  - virtual reality
KW  - real-time simulation
KW  - interactive manipulation
KW  - human-readable front-end interface
KW  - commercially available haptic devices
KW  - game controllers
KW  - da Vinci Research Kit
KW  - real-time haptic feedback
KW  - multiuser training
KW  - manipulation problems
KW  - soft-body manipulation
KW  - open-source framework
KW  - interactive soft-body simulations
KW  - real-time training
KW  - master telemanipulators
KW  - Visualization
KW  - Computational modeling
KW  - Real-time systems
KW  - Training
KW  - Robots
KW  - Faces
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9197573
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an open-source framework that provides a low barrier to entry for real-time simulation, visualization, and interactive manipulation of user-specifiable soft-bodies, environments, and robots (using a human-readable front-end interface). The simulated soft-bodies can be interacted by a variety of input interface devices including commercially available haptic devices, game controllers, and the Master Tele-Manipulators (MTMs) of the da Vinci Research Kit (dVRK) with real-time haptic feedback. We propose this framework for carrying out multi-user training, user-studies, and improving the control strategies for manipulation problems. In this paper, we present the associated challenges to the development of such a framework and our proposed solutions. We also demonstrate the performance of this framework with examples of soft-body manipulation and interaction with various input devices.
ER  - 

TY  - CONF
TI  - Towards 5-DoF Control of an Untethered Magnetic Millirobot via MRI Gradient Coils
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6551
EP  - 6557
AU  - O. Erin
AU  - D. Antonelli
AU  - M. E. Tiryaki
AU  - M. Sitti
PY  - 2020
KW  - biomedical MRI
KW  - medical image processing
KW  - medical robotics
KW  - microrobots
KW  - path planning
KW  - surgery
KW  - untethered magnetic millirobot
KW  - MRI gradient coils
KW  - electromagnetic field gradients
KW  - magnetic resonance imaging devices
KW  - power untethered magnetic robots
KW  - MRI devices
KW  - magnetic pulling forces
KW  - drug delivery
KW  - MRI-powered untethered magnetic robots
KW  - orientation control
KW  - three-dimensional fluids
KW  - 3-DoF position control
KW  - path-planning-based 5-DoF control algorithm
KW  - optimal controller
KW  - robot manufacturing errors
KW  - pitch angle
KW  - neutral pitching angle
KW  - 3D Bezier curves
KW  - worst-case path-tracking error
KW  - position-tracking error
KW  - orientation-tracking error
KW  - pitch angles
KW  - future MRI-powered active imaging
KW  - laser surgery
KW  - biopsy robots
KW  - Magnetic resonance imaging
KW  - Robots
KW  - Magnetic devices
KW  - Coils
KW  - Three-dimensional displays
KW  - Force
KW  - Medical robotics
KW  - miniature robots
KW  - magnetic actuation
KW  - magnetic resonance imaging
KW  - optimal control
DO  - 10.1109/ICRA40945.2020.9196692
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electromagnetic field gradients generated by magnetic resonance imaging (MRI) devices pave the way to power untethered magnetic robots remotely. This innovative use of MRI devices allows exerting magnetic pulling forces on untethered magnetic robots, which could be used for navigation, diagnosis, drug delivery and therapeutic procedures inside a human body. So far, MRI-powered untethered magnetic robots lack simultaneous position and orientation control inside three-dimensional (3D) fluids, and therefore, their control has been limited to 3-DoF position control. In this paper, we present a path-planning-based 5-DoF control algorithm to steer and control an MRI-powered untethered robot's position and orientation simultaneously in 3D workspaces in fluids. Eventhough the simulation results show that the proposed optimal controller can successfully control the robot for 5-DoF, in the experiments, we observe a reduced 5-DoF controllability due to the robot manufacturing errors, which result in pitch angle to remain at around the neutral pitching angle at the steady state. The proposed controller was evaluated to track four different paths (linear, planar-horizontal, planar-vertical and 3D paths) generated by 3D Bezier curves. The worst-case path-tracking error was observed for 3D path-following experiments. For this case, the position-tracking error was 2.7¬±1.8 mm, and the orientation-tracking error was 13.5¬± 28.7 and 3.7¬± 10.2 degrees for yaw and pitch angles, respectively. The overall path is completed within 19.6 seconds with 23.6 mm overall displacement and 61.2 and 41.2 degrees of yaw and pitch angle rotation, respectively. Such robots can be used in future MRI-powered active imaging, laser surgery and biopsy robots inside a fluid-filled stomach type of organs.
ER  - 

TY  - CONF
TI  - Balance of Humanoid Robots in a Mix of Fixed and Sliding Multi-Contact Scenarios
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6590
EP  - 6596
AU  - S. Samadi
AU  - S. Caron
AU  - A. Tanguy
AU  - A. Kheddar
PY  - 2020
KW  - approximation theory
KW  - humanoid robots
KW  - legged locomotion
KW  - quadratic programming
KW  - humanoid robots
KW  - multilegged robots
KW  - multicontact setting
KW  - desired sliding-task motions
KW  - center-of-mass
KW  - admissible convex area
KW  - contact positions
KW  - CoM support area
KW  - CSA
KW  - appropriate CoM position
KW  - multiple fixed sliding contacts
KW  - HRP-4 humanoid robot
KW  - quadratic programming
KW  - QP optimization problems
KW  - Humanoid robots
KW  - Mathematical model
KW  - Friction
KW  - Task analysis
KW  - Gravity
KW  - Humanoid and multi-legged robots
KW  - balance
KW  - multi-contacts
KW  - sliding contacts
DO  - 10.1109/ICRA40945.2020.9197253
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study deals with the balance of humanoid or multi-legged robots in a multi-contact setting where a chosen subset of contacts is undergoing desired sliding-task motions. One method to keep balance is to hold the center-of-mass (CoM) within an admissible convex area. This area is calculated based on the contact positions and forces. We introduce a methodology to compute this CoM support area (CSA) for multiple fixed and intentionally sliding contacts. To select the most appropriate CoM position within CSA, we account for (i) constraints of multiple fixed and sliding contacts, (ii) desired wrench distribution for contacts, and (iii) desired CoM position (eventually dictated by other tasks). These are formulated as a quadratic programming (QP) optimization problems. We illustrate our approach with pushing against a wall and wiping, and conducted experiments using the HRP-4 humanoid robot.
ER  - 

TY  - CONF
TI  - Fast Whole-Body Motion Control of Humanoid Robots with Inertia Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6597
EP  - 6603
AU  - G. Ficht
AU  - S. Behnke
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - position control
KW  - predictive control
KW  - robot dynamics
KW  - reduced five mass model
KW  - analytical solution
KW  - mass distribution
KW  - inertial properties
KW  - desired foot positioning
KW  - CRB inertia properties
KW  - model predictive control
KW  - dynamic kicking motion
KW  - humanoid robots
KW  - inertia constraints
KW  - analytical method
KW  - whole-body motions
KW  - fast whole-body motion control
KW  - humanoid open platform robot
KW  - desired composite rigid body inertia
KW  - Legged locomotion
KW  - Foot
KW  - Kinematics
KW  - Humanoid robots
KW  - Hip
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197322
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce a new, analytical method for generating whole-body motions for humanoid robots, which approximate the desired Composite Rigid Body (CRB) inertia. Our approach uses a reduced five mass model, where four of the masses are attributed to the limbs and one is used for the trunk. This compact formulation allows for finding an analytical solution that combines the kinematics with mass distribution and inertial properties of a humanoid robot. The positioning of the masses in Cartesian space is then directly used to obtain joint angles with relations based on simple geometry. Motions are achieved through the time evolution of poses generated through the desired foot positioning and CRB inertia properties. As a result, we achieve short computation times in the order of tens of microseconds. This makes the method suited for applications with limited computation resources, or leaving them to be spent on higher-layer tasks such as model predictive control. The approach is evaluated by performing a dynamic kicking motion with an igus¬Æ Humanoid Open Platform robot.
ER  - 

TY  - CONF
TI  - SL1M: Sparse L1-norm Minimization for contact planning on uneven terrain
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6604
EP  - 6610
AU  - S. Tonneau
AU  - D. Song
AU  - P. Fernbach
AU  - N. Mansard
AU  - M. Ta√Øx
AU  - A. Del Prete
PY  - 2020
KW  - integer programming
KW  - legged locomotion
KW  - linear programming
KW  - minimisation
KW  - trajectory control
KW  - kinematic reachability
KW  - contact effectors
KW  - quasistatic COM trajectory
KW  - quasiflat contacts
KW  - contact surfaces
KW  - SL1M
KW  - uneven terrain
KW  - legged locomotion
KW  - combinatorial contact selection problem
KW  - mixed-integer optimization solvers
KW  - sparsity properties
KW  - L1 norm minimization techniques
KW  - online contact replanning
KW  - sparse L1-norm minimization
KW  - Silicon
KW  - Planning
KW  - Foot
KW  - Minimization
KW  - Kinematics
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197371
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - One of the main challenges of planning legged locomotion in complex environments is the combinatorial contact selection problem. Recent contributions propose to use integer variables to represent which contact surface is selected, and then to rely on modern mixed-integer (MI) optimization solvers to handle this combinatorial issue. To reduce the computational cost of MI, we exploit the sparsity properties of L1 norm minimization techniques to relax the contact planning problem into a feasibility linear program. Our approach accounts for kinematic reachability of the center of mass (COM) and of the contact effectors. We ensure the existence of a quasi-static COM trajectory by restricting our plan to quasi-flat contacts. For planning 10 steps with less than 10 potential contact surfaces for each phase, our approach is 50 to 100 times faster that its MI counterpart, which suggests potential applications for online contact re-planning. The method is demonstrated in simulation with the humanoid robots HRP-2 and Talos over various scenarios.
ER  - 

TY  - CONF
TI  - Finding Locomanipulation Plans Quickly in the Locomotion Constrained Manifold
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6611
EP  - 6617
AU  - S. J. Jorgensen
AU  - M. Vedantam
AU  - R. Gupta
AU  - H. Cappel
AU  - L. Sentis
PY  - 2020
KW  - end effectors
KW  - humanoid robots
KW  - legged locomotion
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - robot programming
KW  - locomanipulation plans
KW  - locomotion constrained manifold
KW  - end-effector trajectory
KW  - injective locomotion constraint manifold
KW  - locomotion scheme
KW  - admissible manipulation trajectories
KW  - weighted-A* graph search
KW  - planner output
KW  - contact transitions
KW  - path progression trajectory
KW  - whole-body kinodynamic locomanipulation plan
KW  - locomanipulability region
KW  - edge transition feasibility
KW  - NASA Valkyrie robot platform
KW  - dynamic locomotion approach
KW  - example locomanipulation scenarios
KW  - divergent-component-of-motion
KW  - Trajectory
KW  - Task analysis
KW  - Foot
KW  - Manifolds
KW  - Pelvis
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197533
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a method that finds locomanipulation plans that perform simultaneous locomotion and manipulation of objects for a desired end-effector trajectory. Key to our approach is to consider an injective locomotion constraint manifold that defines the locomotion scheme of the robot and then using this constraint manifold to search for admissible manipulation trajectories. The problem is formulated as a weighted-A* graph search whose planner output is a sequence of contact transitions and a path progression trajectory to construct the whole-body kinodynamic locomanipulation plan. We also provide a method for computing, visualizing, and learning the locomanipulability region, which is used to efficiently evaluate the edge transition feasibility during the graph search. Numerical simulations are performed with the NASA Valkyrie robot platform that utilizes a dynamic locomotion approach, called the divergent-component-of-motion (DCM), on two example locomanipulation scenarios.
ER  - 

TY  - CONF
TI  - Force-based Control of Bipedal Balancing on Dynamic Terrain with the "Tallahassee Cassie" Robotic Platform
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6618
EP  - 6624
AU  - J. White
AU  - D. Swart
AU  - C. Hubicki
PY  - 2020
KW  - force control
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - PD control
KW  - position control
KW  - springs (mechanical)
KW  - bipedal control
KW  - minimal model information
KW  - dynamic impacts
KW  - walking running controllers
KW  - modeling information
KW  - force-based control
KW  - bipedal balancing
KW  - Tallahassee Cassie robotic platform
KW  - bipedal robots
KW  - force-based double support balancing controller
KW  - dynamic terrain scenarios
KW  - robotic bipedal platform
KW  - minimal information
KW  - robot model
KW  - individual links
KW  - pelvis-centric pelvis positions
KW  - commanding pelvis positions
KW  - model-free PD controller
KW  - Pelvis
KW  - Legged locomotion
KW  - Foot
KW  - Dynamics
KW  - Vehicle dynamics
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196725
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Out in the field, bipedal robots need to travel on terrain that is uneven, non-rigid, and sometimes moving beneath their feet. We present a force-based double support balancing controller for such dynamic terrain scenarios for bipedal robots, and test it on the robotic bipedal platform "Tallahassee Cassie." The presented controller relies on minimal information about the robot model, requiring its kinematics and overall weight, but not inertias of individual links or components. The controller is pelvis-centric, commanding pelvis positions in Cartesian space, which a model-free PD controller converts to motor torques in joint space. By commanding forces, torques, and a frontal center of pressure in this fashion, Tallahassee Cassie is capable of balancing on a variety of scenarios, from a lifting/sliding platform, to soft foam, to a sudden drop. These results show the potential for bipedal control to balance successfully despite minimal model information, the presence of large dynamic impacts-e.g., falling through trap door, and soft series-spring deflections. These results motivate future work for walking and running controllers on dynamic terrain with relatively low reliance on modeling information.
ER  - 

TY  - CONF
TI  - Dense r-robust formations on lattices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6633
EP  - 6639
AU  - L. Guerrero-Bonilla
AU  - D. Salda√±a
AU  - V. Kumar
PY  - 2020
KW  - energy consumption
KW  - multi-robot systems
KW  - network theory (graphs)
KW  - cubic lattices
KW  - dense r-robust formations
KW  - robot networks
KW  - malicious robots
KW  - defective robots
KW  - high energy consumption
KW  - communication network
KW  - robot formations
KW  - square lattices
KW  - triangular lattices
KW  - Lattices
KW  - Robot kinematics
KW  - Communication networks
KW  - Robustness
KW  - Robot sensing systems
KW  - Energy consumption
DO  - 10.1109/ICRA40945.2020.9196683
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot networks are susceptible to fail under the presence of malicious or defective robots. Resilient networks in the literature require high connectivity and large communication ranges, leading to high energy consumption in the communication network. This paper presents robot formations with guaranteed resiliency that use smaller communication ranges than previous results in the literature. The formations can be built on triangular and square lattices in the plane, and cubic lattices in the three-dimensional space. We support our theoretical framework with simulations.
ER  - 

TY  - CONF
TI  - Optimizing Topologies for Probabilistically Secure Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6640
EP  - 6646
AU  - R. Wehbe
AU  - R. K. Williams
PY  - 2020
KW  - combinatorial mathematics
KW  - computational complexity
KW  - graph theory
KW  - matrix algebra
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - optimisation
KW  - set theory
KW  - statistical distributions
KW  - multirobot system
KW  - MRS
KW  - robot interactions
KW  - probability distribution
KW  - optimal solution
KW  - rooted k-connections problem
KW  - graph transformations
KW  - weighted matroid intersection algorithm
KW  - edge set
KW  - interaction graph
KW  - optimal security solution
KW  - secure multirobot systems
KW  - Robots
KW  - Probabilistic logic
KW  - Security
KW  - Optimization
KW  - Topology
KW  - Observers
KW  - Multi-robot systems
DO  - 10.1109/ICRA40945.2020.9197249
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we optimize the interaction graph of a multi-robot system (MRS) by maximizing its probability of security while requiring the MRS to have the fewest edges possible. Edges that represent robot interactions exist according to a probability distribution and security is defined using the control theoretic notion of left invertibility. To compute an optimal solution to our problem, we first start by reducing our problem to a variation of the rooted k-connections problem using three graph transformations. Then, we apply a weighted matroid intersection algorithm (WMIA) on matroids defined on the edge set of the interaction graph. Although the optimal solution can be found in polynomial time, MRSs are dynamic and their topologies may change faster than the rate at which the optimal security solution can be found. To cope with dynamic behavior, we present two heuristics that relax optimality but execute with much lower time complexity. Finally, we validate our results through Monte Carlo simulations.
ER  - 

TY  - CONF
TI  - Efficient Communication in Large Multi-robot Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6647
EP  - 6653
AU  - A. Dutta
AU  - A. Ghosh
AU  - S. Sisley
AU  - O. P. Kreidl
PY  - 2020
KW  - multi-robot systems
KW  - peer-to-peer computing
KW  - radiocommunication
KW  - communication routing
KW  - ground-level communication
KW  - multirobot coordination frameworks
KW  - multirobot system
KW  - multirobot networks
KW  - peer-to-peer radio communication
KW  - Robot kinematics
KW  - Routing
KW  - Multi-robot systems
KW  - Complexity theory
KW  - Relays
KW  - Communication networks
DO  - 10.1109/ICRA40945.2020.9196672
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To achieve coordination in a multi-robot system, the robots typically resort to some form of communication among each other. In most of the multi-robot coordination frameworks, high-level coordination strategies are studied but `how' the ground-level communication takes place, is assumed to be taken care of by another program. In this paper, we study the communication routing problem for large multi-robot systems where the robots have limited communication ranges. The objective is to send a message from a robot to another in the network, routed through a low number of other robots. To this end, we propose a communication model between any pair of robots using peer-to-peer radio communication. Our proposed model is generic to any type of message and guarantees a low hop routing between any pair of robots in this network. These help the robots to exchange large messages (e.g., multi-spectral images) in a short amount of time. Results show that our proposed approach easily scales up to 1000 robots while drastically reducing the space complexity for maintaining the network information.
ER  - 

TY  - CONF
TI  - CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6654
EP  - 6660
AU  - R. Ghosh
AU  - J. P. Jansch-Porto
AU  - C. Hsieh
AU  - A. Gosse
AU  - M. Jiang
AU  - H. Taylor
AU  - P. Du
AU  - S. Mitra
AU  - G. Dullerud
PY  - 2020
KW  - control engineering computing
KW  - control system synthesis
KW  - learning (artificial intelligence)
KW  - middleware
KW  - mobile computing
KW  - mobile robots
KW  - multi-threading
KW  - path planning
KW  - program debugging
KW  - specification languages
KW  - heterogeneous distributed coordination
KW  - libraries
KW  - development tools
KW  - application development processes
KW  - mobile computing
KW  - machine learning
KW  - CyPhyHouse
KW  - debugging
KW  - distributed mobile robotic applications
KW  - distributed applications
KW  - Koord programming language
KW  - controller design
KW  - distributed network protocols
KW  - platform-independent middleware
KW  - path planning
KW  - multithreaded simulator
KW  - Koord applications
KW  - application code
KW  - heterogeneous agents
KW  - heterogeneous mobile platforms
KW  - design cycles
KW  - robotic testbed
KW  - distributed task allocation
KW  - deployment toolchain
KW  - hardware-agnostic application
KW  - Robot kinematics
KW  - Task analysis
KW  - Middleware
KW  - Collision avoidance
KW  - Python
DO  - 10.1109/ICRA40945.2020.9196513
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse-a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.
ER  - 

TY  - CONF
TI  - Chance Constrained Simultaneous Path Planning and Task Assignment for Multiple Robots with Stochastic Path Costs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6661
EP  - 6667
AU  - F. Yang
AU  - N. Chakraborty
PY  - 2020
KW  - distributed algorithms
KW  - graph theory
KW  - multi-robot systems
KW  - path planning
KW  - probability
KW  - stochastic processes
KW  - simultaneous path planning
KW  - multiple robots
KW  - stochastic path costs
KW  - stochastic edge costs
KW  - robot team
KW  - stochastic travel costs
KW  - chance-constrained simultaneous task assignment
KW  - deterministic simultaneous task assignment
KW  - shortest paths
KW  - task locations
KW  - linear assignment problem
KW  - CC-STAP
KW  - Robots
KW  - Task analysis
KW  - Collision avoidance
KW  - Path planning
KW  - Resource management
KW  - Random variables
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197354
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel algorithm for simultaneous task assignment and path planning on a graph (or roadmap) with stochastic edge costs. In this problem, the initially unassigned robots and tasks are located at known positions in a roadmap. We want to assign a unique task to each robot and compute a path for the robot to go to its assigned task location. Given the mean and variance of travel cost of each edge, our goal is to develop algorithms that, with high probability, the total path cost of the robot team is below a minimum value in any realization of the stochastic travel costs. We formulate the problem as a chance-constrained simultaneous task assignment and path planning problem (CC-STAP). We prove that the optimal solution of CC-STAP can be obtained by solving a sequence of deterministic simultaneous task assignment and path planning problems in which the travel cost is a linear combination of mean and variance of the edge cost. We show that the deterministic problem can be solved in two steps. In the first step, robots compute the shortest paths to the task locations and in the second step, the robots solve a linear assignment problem with the costs obtained in the first step. We also propose a distributed algorithm that solves CC-STAP near-optimally. We present simulation results on randomly generated networks and data to demonstrate that our algorithm is scalable with the number of robots (or tasks) and the size of the network.
ER  - 

TY  - CONF
TI  - Optimal Topology Selection for Stable Coordination of Asymmetrically Interacting Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6668
EP  - 6674
AU  - P. Mukherjee
AU  - M. Santilli
AU  - A. Gasparri
AU  - R. K. Williams
PY  - 2020
KW  - integer programming
KW  - mathematical programming
KW  - motion control
KW  - multi-robot systems
KW  - topology
KW  - stable coordinated motion
KW  - robot-to-robot interactions
KW  - asymmetric interaction topologies
KW  - multirobot motion
KW  - mixed integer semidefinite programming
KW  - multirobot systems
KW  - asymmetric interactions
KW  - optimal topology selection
KW  - Robot kinematics
KW  - Topology
KW  - Robot sensing systems
KW  - Multi-robot systems
KW  - Symmetric matrices
KW  - Laplace equations
DO  - 10.1109/ICRA40945.2020.9196822
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we address the problem of optimal topology selection for stable coordination of multi-robot systems with asymmetric interactions. This problem arises naturally for multi-robot systems that interact based on sensing, e.g., with limited field of view (FOV) cameras. From our previous efforts on motion control in such settings, we have shown that not all interaction topologies yield stable coordinated motion when asymmetry exists. At the same time, not all robot-to-robot interactions are of equal quality, and thus we seek to optimize asymmetric interaction topologies subject to the constraint that the topology yields stable multi-robot motion. In this context, we formulate an optimal topology selection problem (OTSP) as a mixed integer semidefinite programming (MISDP) problem to compute optimal topologies that yield stable coordinated motion. Simulation results are provided to corroborate the effectiveness of the proposed OTSP formulation.
ER  - 

TY  - CONF
TI  - Non-Prehensile Manipulation in Clutter with Human-In-The-Loop
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6723
EP  - 6729
AU  - R. Papallas
AU  - M. R. Dogar
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - path planning
KW  - human-operator guided planning
KW  - low-level planner
KW  - fully autonomous sampling-based planners
KW  - human-in-the-loop
KW  - high-level plan
KW  - control-based randomized planning
KW  - pushing-based manipulation
KW  - clutter
KW  - nonprehensile manipulation
KW  - Robots
KW  - Planning
KW  - Clutter
KW  - Aerospace electronics
KW  - 1/f noise
KW  - Task analysis
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196689
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a human-operator guided planning approach to pushing-based manipulation in clutter. Most recent approaches to manipulation in clutter employs randomized planning. The problem, however, remains a challenging one where the planning times are still in the order of tens of seconds or minutes, and the success rates are low for difficult instances of the problem. We build on these control-based randomized planning approaches, but we investigate using them in conjunction with human-operator input. In our framework, the human operator supplies a high-level plan, in the form of an ordered sequence of objects and their approximate goal positions. We present experiments in simulation and on a real robotic setup, where we compare the success rate and planning times of our human-in-the-loop approach with fully autonomous sampling-based planners. We show that with a minimal amount of human input, the low-level planner can solve the problem faster and with higher success rates.
ER  - 

TY  - CONF
TI  - PuzzleFlex: kinematic motion of chains with loose joints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6730
EP  - 6737
AU  - S. Lensgraf
AU  - K. Itani
AU  - Y. Zhang
AU  - Z. Sun
AU  - Y. Wu
AU  - A. Q. Li
AU  - B. Zhu
AU  - E. Whiting
AU  - W. Wang
AU  - D. Balkcom
PY  - 2020
KW  - linear programming
KW  - mechanical stability
KW  - mobile robots
KW  - motion control
KW  - robot kinematics
KW  - tolerance analysis
KW  - PuzzleFlex
KW  - loose joints
KW  - free motions
KW  - planar assembly
KW  - rigid bodies
KW  - local distance constraints
KW  - configuration space velocities
KW  - linear programming formulation
KW  - structural stability perturbation analysis
KW  - tolerance analysis
KW  - Robots
KW  - Kinematics
KW  - Analytical models
KW  - Shape
KW  - Jacobian matrices
KW  - Aerospace electronics
KW  - Linear programming
DO  - 10.1109/ICRA40945.2020.9196854
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method of computing free motions of a planar assembly of rigid bodies connected by loose joints. Joints are modeled using local distance constraints, which are then linearized with respect to configuration space velocities, yielding a linear programming formulation that allows analysis of systems with thousands of rigid bodies. Potential applications include analysis of collections of modular robots, structural stability perturbation analysis, tolerance analysis for mechanical systems, and formation control of mobile robots.
ER  - 

TY  - CONF
TI  - Accurate Vision-based Manipulation through Contact Reasoning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6738
EP  - 6744
AU  - A. Kloss
AU  - M. Bauza
AU  - J. Wu
AU  - J. B. Tenenbaum
AU  - A. Rodriguez
AU  - J. Bohg
PY  - 2020
KW  - control engineering computing
KW  - inference mechanisms
KW  - manipulators
KW  - neural nets
KW  - robot vision
KW  - state estimation
KW  - vision-based manipulation
KW  - contact reasoning
KW  - contact interactions
KW  - motion optimization
KW  - state estimation
KW  - state representation
KW  - neural networks
KW  - Shape
KW  - Predictive models
KW  - Planning
KW  - Analytical models
KW  - Robots
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197409
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning contact interactions is one of the core challenges of many robotic tasks. Optimizing contact locations while taking dynamics into account is computationally costly and, in environments that are only partially observable, executing contact-based tasks often suffers from low accuracy. We present an approach that addresses these two challenges for the problem of vision-based manipulation. First, we propose to disentangle contact from motion optimization. Thereby, we improve planning efficiency by focusing computation on promising contact locations. Second, we use a hybrid approach for perception and state estimation that combines neural networks with a physically meaningful state representation. In simulation and real-world experiments on the task of planar pushing, we show that our method is more efficient and achieves a higher manipulation accuracy than previous vision-based approaches.
ER  - 

TY  - CONF
TI  - A Probabilistic Framework for Constrained Manipulations and Task and Motion Planning under Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6745
EP  - 6751
AU  - J. -S. Ha
AU  - D. Driess
AU  - M. Toussaint
PY  - 2020
KW  - geometric programming
KW  - manipulators
KW  - path planning
KW  - stochastic processes
KW  - trajectory control
KW  - probabilistic framework
KW  - constrained manipulations
KW  - motion planning
KW  - manipulation planning framework
KW  - hierarchical structure
KW  - logic rules
KW  - trajectory optimization
KW  - large-scale sequential manipulation
KW  - tool-use planning problems
KW  - LGP formulation
KW  - stochastic domain
KW  - posterior path distribution
KW  - Gaussian path distribution
KW  - logic-geometric programming
KW  - Robots
KW  - Planning
KW  - Trajectory optimization
KW  - Skeleton
KW  - Programming
DO  - 10.1109/ICRA40945.2020.9196840
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Logic-Geometric Programming (LGP) is a powerful motion and manipulation planning framework, which represents hierarchical structure using logic rules that describe discrete aspects of problems, e.g., touch, grasp, hit, or push, and solves the resulting smooth trajectory optimization. The expressive power of logic allows LGP for handling complex, large-scale sequential manipulation and tool-use planning problems. In this paper, we extend the LGP formulation to stochastic domains. Based on the control-inference duality, we interpret LGP in a stochastic domain as fitting a mixture of Gaussians to the posterior path distribution, where each logic pro le defines a single Gaussian path distribution. The proposed framework enables a robot to prioritize various interaction modes and to acquire interesting behaviors such as contact exploitation for uncertainty reduction, eventually providing a composite control scheme that is reactive to disturbance.
ER  - 

TY  - CONF
TI  - Planning with Selective Physics-based Simulation for Manipulation Among Movable Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6752
EP  - 6758
AU  - M. Suhail Saleem
AU  - M. Likhachev
PY  - 2020
KW  - computer simulation
KW  - manipulators
KW  - path planning
KW  - planning model
KW  - robot manipulator
KW  - planning with selective physics based simulation
KW  - Planning
KW  - Collision avoidance
KW  - Manipulators
KW  - Physics
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197451
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Use of physics-based simulation as a planning model enables a planner to reason and generate plans that involve non-trivial interactions with the world. For example, grasping a milk container out of a cluttered refrigerator may involve moving a robot manipulator in between other objects, pushing away the ones that are moveable and avoiding interactions with certain fragile containers. A physics-based simulator allows a planner to reason about the effects of interactions with these objects and to generate a plan that grasps the milk container successfully. The use of physics-based simulation for planning however is underutilized. One of the reasons for it being that physics-based simulations are typically way too slow for being used within a planning loop that typically requires tens of thousands of actions to be evaluated within a matter of a second or two. In this work, we develop a planning algorithm that tries to address this challenge. In particular, it builds on the observation that only a small number of actions actually need to be simulated using physics, and the remaining set of actions, such as moving an arm around obstacles, can be evaluated using a much simpler internal planning model, e.g., a simple collision-checking model. Motivated by this, we develop an algorithm called Planning with Selective Physics-based Simulation that automatically discovers what should be simulated with physics and what can utilize an internal planning model for pick-and-place tasks.
ER  - 

TY  - CONF
TI  - Hybrid Differential Dynamic Programming for Planar Manipulation Primitives
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6759
EP  - 6765
AU  - N. Doshi
AU  - F. R. Hogan
AU  - A. Rodriguez
PY  - 2020
KW  - closed loop systems
KW  - dynamic programming
KW  - linear systems
KW  - manipulators
KW  - stability
KW  - switching systems (control)
KW  - trajectory control
KW  - hybrid trajectories
KW  - planar manipulation primitives
KW  - hybrid differential dynamic programming
KW  - closed-loop execution
KW  - frictional contact switches
KW  - hybrid DDP
KW  - finite horizon trajectories
KW  - linear stabilizing controllers
KW  - planar pivoting
KW  - hybrid switches
KW  - pose-to-pose closed-loop trajectories
KW  - planar pushing
KW  - Trajectory
KW  - Planning
KW  - Contacts
KW  - Heuristic algorithms
KW  - Force
KW  - Convergence
KW  - Friction
DO  - 10.1109/ICRA40945.2020.9197414
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a hybrid differential dynamic programming (DDP) algorithm for closed-loop execution of manipulation primitives with frictional contact switches. Planning and control of these primitives is challenging as they are hybrid, under-actuated, and stochastic. We address this by developing hybrid DDP both to plan finite horizon trajectories with a few contact switches and to create linear stabilizing controllers. We evaluate the performance and computational cost of our framework in ablations studies for two primitives: planar pushing and planar pivoting. We find that generating pose-to-pose closed-loop trajectories from most configurations requires only a couple (one to two) hybrid switches and can be done in reasonable time (one to five seconds). We further demonstrate that our controller stabilizes these hybrid trajectories on a real pushing system. A video describing our work can be found at https://youtu.be/YGSe4cUfq6Q.
ER  - 

TY  - CONF
TI  - Deep Depth Fusion for Black, Transparent, Reflective and Texture-Less Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6766
EP  - 6772
AU  - C. -Y. Chai
AU  - Y. -P. Wu
AU  - S. -L. Tsao
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image fusion
KW  - iterative methods
KW  - robot vision
KW  - stereo image processing
KW  - black objects
KW  - stereo cameras
KW  - depth values
KW  - structured-light camera
KW  - light path
KW  - depth fusion model
KW  - high-quality point clouds
KW  - short-range robotic applications
KW  - fusion weights
KW  - depth images
KW  - fused depth
KW  - depth prediction
KW  - stereo model
KW  - iterative closest point algorithm
KW  - deep depth fusion
KW  - transparent objects
KW  - reflective objects
KW  - Cameras
KW  - Robot vision systems
KW  - Three-dimensional displays
KW  - Color
KW  - Image color analysis
KW  - Prediction algorithms
DO  - 10.1109/ICRA40945.2020.9196894
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Structured-light and stereo cameras, which are widely used to construct point clouds for robotic applications, have different limitations on estimating depth values. Structured-light cameras fail in black, transparent, and reflective objects, which influence the light path; stereo cameras fail in texture-less objects. In this work, we propose a depth fusion model that complements these two types of methods to generate high-quality point clouds for short-range robotic applications. The model first determines the fusion weights from the two input depth images and then refines the fused depth using color features. We construct a dataset containing the aforementioned challenging objects and report the performance of our proposed model. The results reveal that our method reduces the average L1 distance on depth prediction by 75% and 52% compared with the original depth output of the structured-light camera and the stereo model, respectively. A noticeable improvement on the Iterative Closest Point (ICP) algorithm can be achieved by using the refined depth images output from our method.
ER  - 

TY  - CONF
TI  - LiDAR-enhanced Structure-from-Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6773
EP  - 6779
AU  - W. Zhen
AU  - Y. Hu
AU  - H. Yu
AU  - S. Scherer
PY  - 2020
KW  - cameras
KW  - image enhancement
KW  - image matching
KW  - image texture
KW  - motion estimation
KW  - optical radar
KW  - radar imaging
KW  - stereo image processing
KW  - rotating LiDAR
KW  - stereo camera pair
KW  - sensor motions
KW  - image matching
KW  - maturing technique
KW  - inspection purposes
KW  - LiDAR-enhanced structure-from-motion estimation
KW  - LiDAR-enhanced SfM pipeline algorithms
KW  - pipeline
KW  - Laser radar
KW  - Cameras
KW  - Pipelines
KW  - Three-dimensional displays
KW  - Visualization
KW  - Robustness
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197030
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Although Structure-from-Motion (SfM) as a maturing technique has been widely used in many applications, state-of-the-art SfM algorithms are still not robust enough in certain situations. For example, images for inspection purposes are often taken in close distance to obtain detailed textures, which will result in less overlap between images and thus decrease the accuracy of estimated motion. In this paper, we propose a LiDAR-enhanced SfM pipeline that jointly processes data from a rotating LiDAR and a stereo camera pair to estimate sensor motions. We show that incorporating LiDAR helps to effectively reject falsely matched images and significantly improve the model consistency in large-scale environments. Experiments are conducted in different environments to test the performance of the proposed pipeline and comparison results with the state-of-the-art SfM algorithms are reported.
ER  - 

TY  - CONF
TI  - Low Latency And Low-Level Sensor Fusion For Automotive Use-Cases
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6780
EP  - 6786
AU  - M. Pollach
AU  - F. Schiegg
AU  - A. Knoll
PY  - 2020
KW  - belief networks
KW  - image fusion
KW  - object detection
KW  - sensor synchronization
KW  - multiple sensors
KW  - object detection
KW  - low-level sensor fusion
KW  - automotive use-cases
KW  - probabilistic low level automotive sensor fusion approach
KW  - camera data
KW  - associated data
KW  - sensor modalities
KW  - probabilistic fusion
KW  - association method
KW  - Cameras
KW  - Three-dimensional displays
KW  - Object detection
KW  - Sensor fusion
KW  - Robot sensing systems
KW  - Two dimensional displays
KW  - Radar
KW  - sensor fusion
KW  - object detection
KW  - Bayesian networks
DO  - 10.1109/ICRA40945.2020.9196717
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work proposes a probabilistic low level automotive sensor fusion approach using LiDAR, RADAR and camera data. The method is stateless and directly operates on associated data from all sensor modalities. Tracking is not used, in order to reduce the object detection latency and create existence hypotheses per frame. The probabilistic fusion uses input from 3D and 2D space. An association method using a combination of overlap and distance metrics, avoiding the need for sensor synchronization is proposed. A Bayesian network executes the sensor fusion. The proposed approach is compared with a state of the art fusion system, which is using multiple sensors of the same modality and relies on tracking for object detection. Evaluation was done using low level sensor data recorded in an urban environment. The test results show that the low level sensor fusion reduces the object detection latency.
ER  - 

TY  - CONF
TI  - Robot-Assisted and Wearable Sensor-Mediated Autonomous Gait Analysis¬ß
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6795
EP  - 6802
AU  - H. Zhang
AU  - Z. Chen
AU  - D. Zanotto
AU  - Y. Guo
PY  - 2020
KW  - gait analysis
KW  - kinematics
KW  - medical computing
KW  - mobile robots
KW  - regression analysis
KW  - support vector machines
KW  - autonomous gait analysis system
KW  - mobile robot
KW  - custom-engineered instrumented insoles
KW  - on-board RGB-D sensor
KW  - inertial sensors
KW  - force sensitive resistors
KW  - robot companion
KW  - walking exercises
KW  - support vector regression models
KW  - fundamental kinematic gait parameters
KW  - optical motion capture system
KW  - SVR models
KW  - autonomous mobile robots
KW  - out-of-the-lab gait analysis
KW  - robot-assisted wearable sensor-mediated autonomous gait analysis
KW  - Robot sensing systems
KW  - Legged locomotion
KW  - Task analysis
KW  - Instruments
KW  - Robot kinematics
KW  - Wearable Technology
KW  - Instrumented Footwear
KW  - Gait Analysis
KW  - Assistive Robotics
KW  - SportSole
DO  - 10.1109/ICRA40945.2020.9197571
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose an autonomous gait analysis system consisting of a mobile robot and custom-engineered instrumented insoles. The robot is equipped with an on-board RGB-D sensor, the insoles feature inertial sensors and force sensitive resistors. This system is motivated by the need for a robot companion to engage older adults in walking exercises. Support vector regression (SVR) models were developed to extract accurate estimates of fundamental kinematic gait parameters (i.e., stride length, velocity, foot clearance, and step length), from data collected with the robot's on-board RGB-D sensor and with the instrumented insoles during straight walking and turning tasks. The accuracy of each model was validated against ground-truth data measured by an optical motion capture system with N=10 subjects. Results suggest that the combined use of wearable and robot's sensors yields more accurate gait estimates than either sub-system used independently. Additionally, SVR models are robust to inter-subject variability and type of walking task (i.e., straight walking vs. turning), thereby making it unnecessary to collect subject-specific or task-specific training data for the models. These findings indicate the potential of the synergistic use of autonomous mobile robots and wearable sensors for accurate out-of-the-lab gait analysis.
ER  - 

TY  - CONF
TI  - A Control Framework Definition to Overcome Position/Interaction Dynamics Uncertainties in Force-Controlled Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6819
EP  - 6825
AU  - L. Roveda
AU  - N. Castaman
AU  - P. Franceschi
AU  - S. Ghidoni
AU  - N. Pedrocchi
PY  - 2020
KW  - control engineering computing
KW  - damping
KW  - force control
KW  - image colour analysis
KW  - image sensors
KW  - industrial manipulators
KW  - pose estimation
KW  - position control
KW  - production engineering computing
KW  - control framework definition
KW  - force-controlled tasks
KW  - industrial robots
KW  - increasing autonomy
KW  - manipulator
KW  - working environment
KW  - robust behavior
KW  - industrial interaction tasks
KW  - uncertain working scenes
KW  - 6D pose estimation
KW  - featureless parts
KW  - variable damping impedance controller
KW  - adaptive saturation PI
KW  - outer loop
KW  - high accuracy force control
KW  - force error
KW  - overshoots avoidance
KW  - task uncertainties
KW  - positioning errors
KW  - assembly task
KW  - force-tracking task
KW  - force overshoots
KW  - reference force
KW  - Force
KW  - Pose estimation
KW  - Impedance
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Robots
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9197141
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Within the Industry 4.0 context, industrial robots need to show increasing autonomy. The manipulator has to be able to react to uncertainties/changes in the working environment, displaying a robust behavior. In this paper, a control framework is proposed to perform industrial interaction tasks in uncertain working scenes. The proposed methodology relies on two components: i) a 6D pose estimation algorithm aiming to recognize large and featureless parts; ii) a variable damping impedance controller (inner loop) enhanced by an adaptive saturation PI (outer loop) for high accuracy force control (i.e., zero steady-state force error and force overshoots avoidance). The proposed methodology allows to be robust w.r.t. task uncertainties (i.e. , positioning errors and interaction dynamics). The proposed approach has been evaluated in an assembly task of a side-wall panel to be installed inside the aircraft cabin. As a test platform, the KUKA iiwa 14 R820 has been used together with the Microsoft Kinect 2.0 as RGB-D sensor. Experiments show the reliability in the 6D pose estimation and the high-performance in the force-tracking task, avoiding force overshoots while achieving the tracking of the reference force.
ER  - 

TY  - CONF
TI  - Identification of Compliant Contact Parameters and Admittance Force Modulation on a Non-stationary Compliant Surface
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6826
EP  - 6832
AU  - L. Wijayarathne
AU  - F. L. Hammond
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - manipulators
KW  - medical robotics
KW  - motion control
KW  - parameter estimation
KW  - position control
KW  - surgery
KW  - position-based adaptive force controller
KW  - interaction forces
KW  - nonstationary environments
KW  - fast parameter estimation
KW  - compliant contact parameters
KW  - admittance force modulation
KW  - nonstationary compliant surface
KW  - safety-critical applications
KW  - mechanical probing strategy
KW  - environmental impedance parameters
KW  - compliant environments
KW  - robotic manipulator autonomous control
KW  - manipulator controller design
KW  - surgical tasks
KW  - motion compensation
KW  - Force
KW  - Impedance
KW  - Force control
KW  - Probes
KW  - Surface impedance
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9196897
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Although autonomous control of robotic manipulators has been studied for several decades, they are not commonly used in safety-critical applications due to lack of safety and performance guarantees - many of them concerning the modulation of interaction forces. This paper presents a mechanical probing strategy for estimating the environmental impedance parameters of compliant environments, independent a manipulator's controller design, and configuration. The parameter estimates are used in a position-based adaptive force controller to enable control of interaction forces in compliant, stationary, and non-stationary environments. This approach is targeted for applications where the workspace is constrained and non-stationary, and where force control is critical to task success. These applications include surgical tasks involving manipulation of compliant, delicate, moving tissues. Results show fast parameter estimation and successful force modulation that compensates for motion.
ER  - 

TY  - CONF
TI  - Force Adaptation in Contact Tasks with Dynamical Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6841
EP  - 6847
AU  - W. Amanhoud
AU  - M. Khoramshahi
AU  - M. Bonnesoeur
AU  - A. Billard
PY  - 2020
KW  - adaptive control
KW  - force control
KW  - learning systems
KW  - manipulator dynamics
KW  - motion control
KW  - radial basis function networks
KW  - robots
KW  - uncertain systems
KW  - online adaptation
KW  - state-dependent force correction model
KW  - force error
KW  - collaborative cleaning task
KW  - task adaptation
KW  - adaptive force control
KW  - reactive behaviours
KW  - adaptive behaviours
KW  - force adaptation
KW  - contact tasks
KW  - robot dynamics
KW  - force tracking accuracy
KW  - compensation model
KW  - adaptive framework
KW  - force generation
KW  - time-invariant dynamical system framework
KW  - radial basis functions
KW  - KUKA LWR IV+ robotic arm
KW  - Force
KW  - Robots
KW  - Task analysis
KW  - Surface impedance
KW  - Dynamics
KW  - Impedance
KW  - Tracking
KW  - Force Control
KW  - Compliance and Impedance Control
KW  - Physical Human-Robot Interaction
DO  - 10.1109/ICRA40945.2020.9197509
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many tasks such as finishing operations, achieving accurate force tracking is essential. However, uncertainties in the robot dynamics and the environment limit the force tracking accuracy. Learning a compensation model for these uncertainties to reduce the force error is an effective approach to overcome this limitation. However, this approach requires an adaptive and robust framework for motion and force generation. In this paper, we use the time-invariant Dynamical System (DS) framework for force adaptation in contact tasks. We propose to improve force tracking accuracy through online adaptation of a state-dependent force correction model encoded with Radial Basis Functions (RBFs). We evaluate our method with a KUKA LWR IV+ robotic arm. We show its efficiency to reduce the force error to a negligible amount with different target forces and robot velocities. Furthermore, we study the effect of the hyper-parameters and provide a guideline for their selection. We showcase a collaborative cleaning task with a human by integrating our method to previous works to achieve force, motion, and task adaptation at the same time. Thereby, we highlight the benefits of using adaptive force control in real-world environments where we need reactive and adaptive behaviours in response to interactions with the environment.
ER  - 

TY  - CONF
TI  - Weakly Supervised Silhouette-based Semantic Scene Change Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6861
EP  - 6867
AU  - K. Sakurada
AU  - M. Shibuya
AU  - W. Wang
PY  - 2020
KW  - feature extraction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - object detection
KW  - weakly supervised silhouette-based semantic scene change detection
KW  - novel semantic scene change detection scheme
KW  - semantic change detection network
KW  - large-scale dataset
KW  - specific dataset
KW  - semantic extraction
KW  - change detection task
KW  - siamese network structure
KW  - publicly available dataset
KW  - Semantics
KW  - Image segmentation
KW  - Cameras
KW  - Training
KW  - Task analysis
KW  - Satellites
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9196985
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we create a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https://github.com/xdspacelab/sscdnet.
ER  - 

TY  - CONF
TI  - 3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6868
EP  - 6875
AU  - L. Du
AU  - J. Tan
AU  - X. Xue
AU  - L. Chen
AU  - H. Wen
AU  - J. Feng
AU  - J. Li
AU  - X. Zhang
PY  - 2020
KW  - feature selection
KW  - image enhancement
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - robust joint 3D semantic-instance segmentation
KW  - human scene perception process
KW  - CFSM
KW  - coupled feature selection module
KW  - reciprocal semantic instance feature selection
KW  - 3DCFS
KW  - robust 3D point cloud segmentation framework
KW  - Semantics
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Logic gates
KW  - Training
KW  - Euclidean distance
DO  - 10.1109/ICRA40945.2020.9197242
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost. Codes are available at: https://github.com/Biotan/3DCFS.
ER  - 

TY  - CONF
TI  - Who2com: Collaborative Perception via Learnable Handshake Communication
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6876
EP  - 6883
AU  - Y. -C. Liu
AU  - J. Tian
AU  - C. -Y. Ma
AU  - N. Glaser
AU  - C. -W. Kuo
AU  - Z. Kira
PY  - 2020
KW  - aircraft communication
KW  - autonomous aerial vehicles
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - multi-agent systems
KW  - multi-robot systems
KW  - neural nets
KW  - visual perception
KW  - degraded sensor data
KW  - compressed request
KW  - aerial robots
KW  - semantic segmentation task
KW  - collaborative perception
KW  - learnable handshake communication
KW  - local observations
KW  - neighboring agents
KW  - multiagent reinforcement learning
KW  - bandwidth-sensitive manner
KW  - scene understanding tasks
KW  - communication protocols
KW  - multistage handshake communication mechanism
KW  - neural network
KW  - Who2com
KW  - AirSim simulator
KW  - AirSim-CP dataset
KW  - Task analysis
KW  - Bandwidth
KW  - Semantics
KW  - Training
KW  - Collaboration
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197364
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose the problem of collaborative perception, where robots can combine their local observations with those of neighboring agents in a learnable way to improve accuracy on a perception task. Unlike existing work in robotics and multi-agent reinforcement learning, we formulate the problem as one where learned information must be shared across a set of agents in a bandwidth-sensitive manner to optimize for scene understanding tasks such as semantic segmentation. Inspired by networking communication protocols, we propose a multi-stage handshake communication mechanism where the neural network can learn to compress relevant information needed for each stage. Specifically, a target agent with degraded sensor data sends a compressed request, the other agents respond with matching scores, and the target agent determines who to connect with (i.e., receive information from). We additionally develop the AirSim-CP dataset and metrics based on the AirSim simulator where a group of aerial robots perceive diverse landscapes, such as roads, grasslands, buildings, etc. We show that for the semantic segmentation task, our handshake communication method significantly improves accuracy by approximately 20% over decentralized baselines, and is comparable to centralized ones using a quarter of the bandwidth.
ER  - 

TY  - CONF
TI  - Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6884
EP  - 6890
AU  - Z. Landgraf
AU  - F. Falck
AU  - M. Bloesch
AU  - S. Leutenegger
AU  - A. J. Davison
PY  - 2020
KW  - data visualisation
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - SLAM (robots)
KW  - view-based labelling
KW  - spatial AI systems
KW  - real-time height map fusion
KW  - map-based labelling
KW  - generated scene model
KW  - input view-wise data
KW  - estimate labels
KW  - clear groups
KW  - labelling scenes
KW  - semantic labels
KW  - geometric models
KW  - persistent scene representations
KW  - real-time SLAM
KW  - map-based semantic labelling
KW  - Labeling
KW  - Semantics
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Image reconstruction
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196843
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Generally capable Spatial AI systems must build persistent scene representations where geometric models are combined with meaningful semantic labels. The many approaches to labelling scenes can be divided into two clear groups: view-based which estimate labels from the input view-wise data and then incrementally fuse them into the scene model as it is built; and map-based which label the generated scene model. However, there has so far been no attempt to quantitatively compare view-based and map-based labelling. Here, we present an experimental framework and comparison which uses real-time height map fusion as an accessible platform for a fair comparison, opening up the route to further systematic research in this area.
ER  - 

TY  - CONF
TI  - Generative Modeling of Environments with Scene Grammars and Variational Inference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6891
EP  - 6897
AU  - G. Izatt
AU  - R. Tedrake
PY  - 2020
KW  - grammars
KW  - inference mechanisms
KW  - mobile robots
KW  - object detection
KW  - probability
KW  - trees (mathematics)
KW  - variational inference algorithm
KW  - observed environments
KW  - nontrivial manipulation-relevant datasets
KW  - scene grammars
KW  - discrete variables
KW  - continuous variables
KW  - probabilistic generative model
KW  - scene trees
KW  - hierarchical relationships
KW  - labeled parse trees
KW  - Grammar
KW  - Robots
KW  - Production
KW  - Testing
KW  - Training
KW  - Random variables
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196910
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to understand how a robot will perform in the open world, we aim to establish a quantitative understanding of the distribution of environments that a robot will face when when it is deployed. However, even restricting attention only to the distribution of objects in a scene, these distributions over environments are nontrivial: they describe mixtures of discrete and continuous variables related to the number, type, poses, and attributes of objects in the scene. We describe a probabilistic generative model that uses scene trees to capture hierarchical relationships between collections of objects, as well as a variational inference algorithm for tuning that model to best match a set of observed environments without any need for tediously labeled parse trees. We demonstrate that this model can accurately capture the distribution of a pair of nontrivial manipulation-relevant datasets and be deployed as a density estimator and outlier detector for novel environments.
ER  - 

TY  - CONF
TI  - SHOP-VRB: A Visual Reasoning Benchmark for Object Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6898
EP  - 6904
AU  - M. Nazarczuk
AU  - K. Mikolajczyk
PY  - 2020
KW  - control engineering computing
KW  - image representation
KW  - inference mechanisms
KW  - manipulators
KW  - natural language processing
KW  - query processing
KW  - robot vision
KW  - text analysis
KW  - object perception
KW  - robotics applications
KW  - object grasping
KW  - object properties
KW  - visual text data
KW  - household objects
KW  - natural language descriptions
KW  - question-answer pairs
KW  - visual reasoning queries
KW  - scene semantic representations
KW  - symbolic program execution
KW  - disentangled representation
KW  - visual inputs
KW  - textual inputs
KW  - symbolic programs
KW  - reasoning process
KW  - SHOP-VRB
KW  - visual reasoning benchmark
KW  - object manipulation
KW  - Visualization
KW  - Cognition
KW  - Benchmark testing
KW  - Robots
KW  - Image color analysis
KW  - Task analysis
KW  - Plastics
DO  - 10.1109/ICRA40945.2020.9197332
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results from the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.
ER  - 

TY  - CONF
TI  - Sensorization of a Continuum Body Gripper for High Force and Delicate Object Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6913
EP  - 6919
AU  - J. Hughes
AU  - S. Li
AU  - D. Rus
PY  - 2020
KW  - closed loop systems
KW  - deformation
KW  - dexterous manipulators
KW  - feedback
KW  - grippers
KW  - tactile sensors
KW  - delicate object grasping
KW  - universal grasping
KW  - heavy bulky items
KW  - lightweight delicate objects
KW  - highly flexible latex bladders
KW  - Magic Ball origami gripper
KW  - tactile sensing
KW  - proprioceptive sensing
KW  - sensor feedback
KW  - closed loop controller
KW  - continuum body gripper sensorization
KW  - high force object grasping
KW  - Grippers
KW  - Force
KW  - Robot sensing systems
KW  - Strain
KW  - Bladder
KW  - Mechanical sensors
DO  - 10.1109/ICRA40945.2020.9196603
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The goal of achieving `universal grasping' where many objects can be handled with minimal control input is the focus of much research due to potential high impact applications ranging from grocery packing to recycling. However, many of the grippers developed suffer from limited sensing capabilities which can prevent handing of both heavy bulky items and also lightweight delicate objects which require fine control when grasping. Sensorizing such grippers is often challenging due to the highly deformable surfaces. We propose a novel sensing approach which uses highly flexible latex bladders. By measuring changes in the air pressure of the bladders, normal force and longitudinal strain can be measured. These sensors have been integrated into a `Magic Ball' origami gripper to provide both tactile and proprioceptive sensing. The sensors show reasonable sensitivity and repeatability, are durable and low-cost, and can be easily integrated into the gripper without affecting performance. When the sensors are used for classification, they enabled identification of 10 objects with over 90% accuracy, and also allow failure to be detected through slippage detection. A control algorithm has been developed which uses the sensor feedback to extend the capabilities of the gripper to include both delicate and strong grasping. It is shown that this closed loop controller enables delicate grasping of potato chips; 80% of those tested were grasped without damage.
ER  - 

TY  - CONF
TI  - A Soft Gripper with Retractable Nails for Advanced Grasping and Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6928
EP  - 6934
AU  - S. Jain
AU  - T. Stalin
AU  - V. Subramaniam
AU  - J. Agarwal
AU  - P. V. y Alvarado
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - manipulator dynamics
KW  - path planning
KW  - pneumatic actuators
KW  - manipulation tasks
KW  - normal grasping forces
KW  - delicate pinch grasps
KW  - robotic grasping tasks
KW  - soft gripper
KW  - advanced grasping
KW  - retractable finger nails
KW  - reconfigurable palm
KW  - finger nail mechanism
KW  - Nails
KW  - Grippers
KW  - Grasping
KW  - Three-dimensional displays
KW  - Actuators
KW  - Payloads
KW  - Fingers
DO  - 10.1109/ICRA40945.2020.9197259
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This study describes the enhancement of a vacuum actuated soft gripper's grasping capabilities using retractable finger nails and an active re-configurable palm. The finger nail mechanism is pneumatically actuated and enables the gripper to perform complex grasping and manipulation tasks with high repeatability. The retracted nails can exert normal grasping forces of up to 1.8N and enable grasping of objects up to 200Œºm thick from flat surfaces, while allowing the gripper to execute delicate pinch grasps without complex trajectory or grasp planning. A wide array of robotic grasping tasks that were not possible without nails are also described.
ER  - 

TY  - CONF
TI  - Real-time Continuous Hand Motion Myoelectric Decoding by Automated Data Labeling*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6951
EP  - 6957
AU  - X. Hu
AU  - H. Zeng
AU  - D. Chen
AU  - J. Zhu
AU  - A. Song
PY  - 2020
KW  - biomechanics
KW  - electromyography
KW  - gesture recognition
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - medical signal processing
KW  - prosthetics
KW  - sEMG data
KW  - hand motion measurement
KW  - automated data labeling neural network
KW  - hand motion myoelectric decoding
KW  - hand motion labels
KW  - unsupervised neural network
KW  - unlabeled sEMG
KW  - hand motion signals
KW  - bio-signals
KW  - surface electromyography array
KW  - dataset collecting
KW  - Muscles
KW  - Neurons
KW  - Wrist
KW  - Training
KW  - Feature extraction
KW  - Neural networks
KW  - Principal component analysis
DO  - 10.1109/ICRA40945.2020.9197286
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper an automated data labeling (ADL) neural network is proposed to streamline dataset collecting for real-time predicting the continuous motion of hand and wrist, these gestures are only decoded from a surface electromyography (sEMG) array of eight channels. Unlike collecting both the bio-signals and hand motion signals as samples and labels in supervised learning, this algorithm only collects unlabeled sEMG into an unsupervised neural network, in which the hand motion labels are auto-generated. The coefficient of determination (R2) for three DOFs, i.e. wrist flex/extension, wrist pro/supination, hand open/close, was 0.86, 0.89 and 0.87 respectively. The comparison between real motion labels and auto-generated labels shows that the latter has earlier response than former. The results of Fitts' law test indicate that ADL has capability of controlling multi-DOFs simultaneously even though the training set only contains sEMG data from single DOF gesture. Moreover, no more hand motion measurement needed which greatly helps upper limb amputee imagine the gesture of residual limb to control a dexterous prosthesis.
ER  - 

TY  - CONF
TI  - Towards Proactive Navigation: A Pedestrian-Vehicle Cooperation Based Behavioral Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6958
EP  - 6964
AU  - M. Kabtoul
AU  - A. Spalanzani
AU  - P. Martinet
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - remotely operated vehicles
KW  - road traffic control
KW  - road vehicles
KW  - trajectory control
KW  - pedestrian-vehicle cooperation
KW  - autonomous vehicle
KW  - intelligent transportation
KW  - autonomous navigation research
KW  - safe proactive navigation
KW  - pedestrian-vehicle interaction behavioral model
KW  - interaction scenario
KW  - quantitative time-varying function
KW  - cooperation estimation
KW  - cooperation-based trajectory planning model
KW  - Navigation
KW  - Space vehicles
KW  - Predictive models
KW  - Strain
KW  - Task analysis
KW  - Trajectory
KW  - Autonomous vehicles
DO  - 10.1109/ICRA40945.2020.9196669
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Developing autonomous vehicles capable of navigating safely and socially around pedestrians is a major challenge in intelligent transportation. This challenge cannot be met without understanding pedestrians' behavioral response to an autonomous vehicle, and the task of building a clear and quantitative description of the pedestrian to vehicle interaction remains a key milestone in autonomous navigation research. As a step towards safe proactive navigation in a space shared with pedestrians, this work introduces a pedestrian-vehicle interaction behavioral model. The model estimates the pedestrian's cooperation with the vehicle in an interaction scenario by a quantitative time-varying function. Using this cooperation estimation the pedestrian's trajectory is predicted by a cooperation-based trajectory planning model. Both parts of the model are tested and validated using real-life recorded scenarios of pedestrian-vehicle interaction. The model is capable of describing and predicting agents' behaviors when interacting with a vehicle in both lateral and frontal crossing scenarios.
ER  - 

TY  - CONF
TI  - Studying Navigation as a Form of Interaction: a Design Approach for Social Robot Navigation Methods*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6965
EP  - 6972
AU  - P. Scales
AU  - O. Aycard
AU  - V. Auberg√©
PY  - 2020
KW  - human-robot interaction
KW  - mobile robots
KW  - motion control
KW  - navigation
KW  - social robot navigation methods
KW  - social navigation methods
KW  - human sciences fields
KW  - mobile robot navigation
KW  - robot behavior
KW  - social hierarchy
KW  - socio-physical context
KW  - robot motion
KW  - human-robot interaction
KW  - human behavior
KW  - Navigation
KW  - Biological system modeling
KW  - Robot sensing systems
KW  - Mobile robots
KW  - Design methodology
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197037
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Social Navigation methods attempt to integrate knowledge from Human Sciences fields such as the notion of Proxemics into mobile robot navigation. They are often evaluated in simulations, or lab conditions with informed participants, and studies of the impact of the robot behavior on humans are rare. Humans communicate and interact through many vectors, among which are motion and positioning, which can be related to social hierarchy and the socio-physical context. If a robot is to be deployed among humans, the methods it uses should be designed with this in mind. This work acts as the first step in an ongoing project in which we explore how to design navigation methods for mobile robots destined to be deployed among humans. We aim to consider navigation as more than just a functionality of the robot, and to study the impact of robot motion on humans. In this paper, we focus on the person-following task. We selected a state of the art person-following method as the basis for our method, which we modified and extended in order for it to be more general and adaptable. We conducted pilot experiments using this method on a real mobile robot in ecological contexts. We used results from the experiments to study the Human-Robot Interaction as a whole by analysing both the person-following method and the human behavior. Our preliminary results show that the way in which the robot followed a person had an impact on the interaction that emerged between them.
ER  - 

TY  - CONF
TI  - Robot Plan Model Generation and Execution with Natural Language Interface*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6973
EP  - 6978
AU  - K. -M. Yang
AU  - K. -H. Seo
AU  - S. H. Kang
AU  - Y. Lim
PY  - 2020
KW  - human computer interaction
KW  - mobile robots
KW  - natural language interfaces
KW  - path planning
KW  - human instructions
KW  - service environments
KW  - robot plan model generation
KW  - natural language interface
KW  - interaction inconvenient
KW  - verbal interaction-based method
KW  - human involvement
KW  - human user
KW  - unclear instructions
KW  - reactive plan model
KW  - Task analysis
KW  - Service robots
KW  - Natural languages
KW  - Human-robot interaction
KW  - Robot sensing systems
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196987
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Verbal interaction between a human and a robot may play a key role in conveying suitable directions for a robot to achieve the goal of a user's request. However, a robot may need to correct task plans or make new decisions with human help, which would make the interaction inconvenient and also increase the interaction time. In this paper, we propose a new verbal interaction-based method that can generate plan models and execute proper actions without human involvement in the middle of performing a task by a robot. To understand the verbal behaviors of humans when giving instructions to a robot, we first conducted a brief user study and found that a human user does not explicitly express the required task. To handle such unclear instructions by a human, we propose two different algorithms that can generate a component of new plan models based on intents and entities parsed from natural language and can resolve the unclear entities existed in human instructions. An experimental scenario with a robot, Cozmo, was tried in the lab environment to test whether or not the proposed method could generate an appropriate plan model. As a result, we found that the robot could successfully accomplish the task following human instructions and also found that the number of interactions and components in the plan model could be reduced as opposed to the general reactive plan model. In the future, we are going to improve the automated process of generating plan models and apply various scenarios under different service environments and robots.
ER  - 

TY  - CONF
TI  - Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6979
EP  - 6985
AU  - J. Jin
AU  - N. M. Nguyen
AU  - N. Sakib
AU  - D. Graves
AU  - H. Yao
AU  - M. Jagersand
PY  - 2020
KW  - collision avoidance
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - robot dynamics
KW  - robot programming
KW  - time-efficient path planning behavior
KW  - dynamic crowds
KW  - social-safety-awareness
KW  - reinforcement learning
KW  - 2D laser scans
KW  - mapless collision-avoidance navigation
KW  - ego-safety
KW  - pedestrians
KW  - robot tests
KW  - Collision avoidance
KW  - Navigation
KW  - Training
KW  - Robot sensing systems
KW  - Lasers
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197148
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of mapless collision-avoidance navigation where humans are present using 2D laser scans. Our proposed method uses ego-safety to measure collision from the robot's perspective and social-safety to measure the impact of robot's actions on surrounding pedestrians. Specifically, the social-safety part predicts the intrusion impact of the robot's action into the interaction area with surrounding humans. We train the policy using reinforcement learning on a simple simulator and directly evaluate the learned policy in Gazebo and real robot tests. Experiments show the learned policy smoothly transferred to different scenarios without any fine tuning. We observe that our method demonstrates time-efficient path planning behavior with high success rate in the mapless navigation task. Furthermore, we test our method in a navigation task among dynamic crowds, considering both low and high volume traffic. Our learned policy demonstrates cooperative behavior that actively drives our robot into traffic flows while showing respect to nearby pedestrians. Evaluation videos are at https://sites.google.com/view/ssw-batman.
ER  - 

TY  - CONF
TI  - Steering Control of Magnetic Helical Swimmers in Swirling Flows due to Confinement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 6994
EP  - 7000
AU  - H. O. Caldag
AU  - S. Yesilyurt
PY  - 2020
KW  - biomechanics
KW  - cell motility
KW  - computational fluid dynamics
KW  - flow simulation
KW  - hydrodynamics
KW  - magnetic actuators
KW  - microrobots
KW  - mobile robots
KW  - Navier-Stokes equations
KW  - position control
KW  - propulsion
KW  - swirling flow
KW  - trajectory control
KW  - vortices
KW  - prospective robotic agents
KW  - rotating magnetic field
KW  - magnetized swimmer
KW  - helical tail
KW  - helical paths
KW  - pusher-mode swimmers
KW  - rotating magnetic head
KW  - microswimmers
KW  - swimmer orientation
KW  - render orientation-based methods
KW  - confined swimmer
KW  - control law
KW  - swimmer position
KW  - swirling flow
KW  - helical pusher-mode trajectories
KW  - steering control
KW  - magnetic helical swimmers
KW  - Magnetic fields
KW  - Magnetosphere
KW  - Magnetic confinement
KW  - Magnetohydrodynamics
KW  - Propulsion
KW  - Navigation
KW  - microswimmers
KW  - helical swimming
KW  - low Reynolds number
KW  - steering
KW  - control
KW  - stability
DO  - 10.1109/ICRA40945.2020.9196521
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Artificial microswimmers are prospective robotic agents especially in biomedical applications. A rotating magnetic field can actuate a magnetized swimmer with a helical tail and enable propulsion. Such swimmers exhibit several modes of instability. Inside conduits, for example, hydrodynamic interactions with the boundaries lead to helical paths for pusher-mode swimmers; in this mode the helical tail pushes a rotating magnetic head. State-of-the-art in controlled navigation of micro-swimmers is based on aligning the swimmer orientation according to a reference path, thereby requiring both swimmer orientation and position to be known. Object-orientation is hard to track especially in in vivo scenarios which render orientation-based methods practically unfeasible. Here, we show that the kinematics for a confined swimmer can be linearized by assuming a low wobbling angle. This allows for a control law solely based on the swimmer position. The approach is demonstrated through experiments and two different numerical models: the first is based on the resistive force theory for a swimmer inside a swirling flow represented by a forced vortex and the second is a computational fluid dynamics model, which solves Stokes equations for a swimmer inside a circular channel. Helical pusher-mode trajectories are suppressed significantly for the straight path following problem. The error in real-life experiments remains comparable to those in the state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Sim2real gap is non-monotonic with robot complexity for morphology-in-the-loop flapping wing design
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7001
EP  - 7007
AU  - K. Rosser
AU  - J. Kok
AU  - J. Chahl
AU  - J. Bongard
PY  - 2020
KW  - aerospace components
KW  - aerospace control
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - biological exemplars
KW  - robot design
KW  - morphology-in-the-loop flapping wing design
KW  - robot complexity
KW  - sim2real gap
KW  - design complexity
KW  - sim2real transfer
KW  - high performance robot morphologies
KW  - parameterised morphology design space
KW  - flapping wing flight
KW  - machine learning
KW  - Morphology
KW  - Robots
KW  - Shape
KW  - Finite element analysis
KW  - Complexity theory
KW  - Computational modeling
KW  - Machine learning
KW  - morphology
KW  - simulation to reality
KW  - evolution
KW  - bio-inspired
DO  - 10.1109/ICRA40945.2020.9196539
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Morphology of a robot design is important to its ability to achieve a stated goal and therefore applying machine learning approaches that incorporate morphology in the design space can provide scope for significant advantage. Our study is set in a domain known to be reliant on morphology: flapping wing flight. We developed a parameterised morphology design space that draws features from biological exemplars and apply automated design to produce a set of high performance robot morphologies in simulation. By performing sim2real transfer on a selection, for the first time we measured the shape of the reality gap for variations in design complexity. We found for the flapping wing that the reality gap changes non-monotonically with complexity, suggesting that certain morphology details narrow the gap more than others, and that such details could be identified and further optimised in a future end-to-end automated morphology design process.
ER  - 

TY  - CONF
TI  - A Linearized Model for an Ornithopter in Gliding Flight: Experiments and Simulations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7008
EP  - 7014
AU  - R. Lopez-Lopez
AU  - V. Perez-Sanchez
AU  - P. Ramon-Soria
AU  - A. Mart√≠n-Alc√°ntara
AU  - R. Fernandez-Feria
AU  - B. C. Arrue
AU  - A. Ollero
PY  - 2020
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - linearisation techniques
KW  - position control
KW  - velocity control
KW  - longitudinal gliding flight configuration
KW  - aerodynamic forces
KW  - linearized potential theory
KW  - flat plate
KW  - flapping-wing episodes
KW  - linear potential theory
KW  - steady-state descent
KW  - terminal velocity
KW  - pitching
KW  - gliding angles
KW  - tail position
KW  - flapping-wing configuration
KW  - flight velocity
KW  - climbing episodes
KW  - realistic simulation tool
KW  - flapping frequencies
KW  - ornithopter flight
KW  - linearized model
KW  - flapping-wings UAV
KW  - Unreal Engine 4
KW  - Numerical models
KW  - Force
KW  - Mathematical model
KW  - Hardware
KW  - Steady-state
KW  - Computed tomography
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9196929
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work studies the accuracy of a simple but effective analytical model for a flapping-wings UAV in longitudinal gliding flight configuration comparing it with experimental results of a real ornithopter. The aerodynamic forces are modeled following the linearized potential theory for a flat plate in gliding configuration, extended to flapping-wing episodes modeled also by the (now unsteady) linear potential theory, which are studied numerically. In the gliding configuration, the model reaches a steady-state descent at given terminal velocity and pitching and gliding angles, governed by the wings and tail position. In the flapping-wing configuration, it is noticed that the vehicle can increase its flight velocity and perform climbing episodes. A realistic simulation tool based on Unreal Engine 4 was developed to visualize the effect of the tail position and flapping frequencies and amplitudes on the ornithopter flight in real time. The paper also includes the experimental validation of the gliding flight and the data has been released for the community.
ER  - 

TY  - CONF
TI  - Towards biomimicry of a bat-style perching maneuver on structures: the manipulation of inertial dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7015
EP  - 7021
AU  - A. Ramezani
PY  - 2020
KW  - aerospace control
KW  - angular momentum
KW  - biomimetics
KW  - closed loop systems
KW  - gears
KW  - geometry
KW  - mobile robots
KW  - motion control
KW  - remotely operated vehicles
KW  - robot dynamics
KW  - bat-style perching maneuver
KW  - aerial drone designs
KW  - aerial flip turns
KW  - landing surface
KW  - zero-angular-momentum turns
KW  - detachable landing gear
KW  - closed-loop manipulations
KW  - biomimicry
KW  - inertial dynamics manipulation
KW  - bat flight characteristics
KW  - dynamical system
KW  - geometric conservation properties
KW  - Aerodynamics
KW  - Manipulator dynamics
KW  - Mathematical model
KW  - Robot sensing systems
KW  - Birds
DO  - 10.1109/ICRA40945.2020.9197376
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The flight characteristics of bats remarkably have been overlooked in aerial drone designs. Unlike other animals, bats leverage the manipulation of inertial dynamics to exhibit aerial flip turns when they perch. Inspired by this unique maneuver, this work develops and uses a tiny robot called Harpoon to demonstrate that the preparation for upside-down landing is possible through: 1) reorientation towards the landing surface through zero-angular-momentum turns and 2) reaching to the surface through shooting a detachable landing gear. The closed-loop manipulations of inertial dynamics takes place based on a symplectic description of the dynamical system (body and appendage), which is known to exhibit an excellent geometric conservation properties.
ER  - 

TY  - CONF
TI  - Bioinspired object motion filters as the basis of obstacle negotiation in micro aerial systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7022
EP  - 7028
AU  - R. Zhou
AU  - H. -T. Lin
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - feedback
KW  - image filtering
KW  - image motion analysis
KW  - image sequences
KW  - microrobots
KW  - object detection
KW  - robot vision
KW  - object motion filters
KW  - obstacle negotiation
KW  - microaerial systems
KW  - biological visual guidance
KW  - machine vision system
KW  - motion vision
KW  - dense optic flow map
KW  - insect vision inspired object motion filter model
KW  - microracing drone
KW  - proximaldistal object separation
KW  - early-stage motion detection
KW  - feedback control loop
KW  - Visualization
KW  - Kernel
KW  - Optical filters
KW  - Drones
KW  - Band-pass filters
KW  - Biological system modeling
KW  - Insects
KW  - motion vision
KW  - obstacle avoidance
KW  - visual guidance
DO  - 10.1109/ICRA40945.2020.9196752
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - All animals and robots that move in the world must navigate to a goal while clearing obstacles. Using vision to accomplish such task has several advantages in cost and payload, which explains the prevalence of biological visual guidance. However, the computational overhead has been an obvious concern when increasing number of pixels and frames that need to be analyzed in real-time for a machine vision system. The use of motion vision and optic flow has been a popular bio-inspired solution for this problem. However, many early-stage motion detection approaches rely on special hardware (e.g. event-cameras) or extensive computation (e.g. dense optic flow map). Here we demonstrate a method to combine an insect vision inspired object motion filter model with simple visual guidance rules to fly through a cluttered environment. We have implemented a complete feedback control loop in a micro racing drone and achieved proximaldistal object separation through only two object motion filters. We discuss the key constraints and the scalability of this approach for future development.
ER  - 

TY  - CONF
TI  - ARCSnake: An Archimedes‚Äô Screw-Propelled, Reconfigurable Serpentine Robot for Complex Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7029
EP  - 7034
AU  - D. A. Schreiber
AU  - F. Richter
AU  - A. Bilan
AU  - P. V. Gavrilov
AU  - H. Man Lam
AU  - C. H. Price
AU  - K. C. Carpenter
AU  - M. C. Yip
PY  - 2020
KW  - mobile robots
KW  - propulsion
KW  - robot dynamics
KW  - orientation control
KW  - versatile serpentine robot platform
KW  - screw threads
KW  - mechanical design
KW  - electrical design
KW  - reconfigurable serpentine robot
KW  - serpentine robots
KW  - screw propulsion
KW  - ARCSnake robot
KW  - omni-wheel drive-like motions
KW  - NASA-JPL EELS program
KW  - NASA-JPL Exobiology Extant Life Surveyor program
KW  - Robots
KW  - Fasteners
KW  - Brushless motors
KW  - Skin
KW  - Propulsion
KW  - Torque
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9196968
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the design and performance of a new locomotion strategy for serpentine robots using screw propulsion. The ARCSnake robot comprises serially linked, identical modules, each incorporating an Archimedes' screw for propulsion and a universal joint (U-Joint) for orientation control. When serially chained, these modules form a versatile serpentine robot platform which enables the robot to reshape its body configuration for varying environments, typical of a snake. Furthermore, the Archimedes' screws allow for novel omni-wheel drive-like motions by speed controlling their screw threads. This paper considers the mechanical and electrical design, as well as the software architecture for realizing a fully integrated system. The system includes 3N actuators for N segments, each controlled using a BeagleBone Black with a customized power-electronics cape, a 9 Degrees of Freedom (DoF) Inertial Measurement Unit (IMU), and a scalable communication channel over ROS. This robot serves as the first proof-of-concept demonstration of the NASA-JPL Exobiology Extant Life Surveyor (EELS) program that aims to deliver scientific instrumentation deep within the plume vents, caves, and ice sheets of Enceladus and Europa in search for extant lifeforms*.
ER  - 

TY  - CONF
TI  - GPR-based Subsurface Object Detection and Reconstruction Using Random Motion and DepthNet
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7035
EP  - 7041
AU  - J. Feng
AU  - L. Yang
AU  - H. Wang
AU  - Y. Song
AU  - J. Xiao
PY  - 2020
KW  - feature extraction
KW  - geophysical image processing
KW  - geophysical techniques
KW  - ground penetrating radar
KW  - image reconstruction
KW  - inspection
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - radar detection
KW  - radar imaging
KW  - stereo image processing
KW  - GPR-based subsurface object detection
KW  - DepthNet
KW  - Ground Penetrating Radar
KW  - nondestructive evaluation devices
KW  - underground scene
KW  - GPR based inspection
KW  - underground targets
KW  - pose information
KW  - GPR device
KW  - GPR image
KW  - B-scan data
KW  - GPR scan data
KW  - B-scan image
KW  - Geophysical Survey System Inc.
KW  - synthetic GPR data
KW  - B-scan feature detection
KW  - underground target depth prediction
KW  - GSSI
KW  - visual inertial fusion module
KW  - VIF module
KW  - gprMax3.0 simulator
KW  - NDE devices
KW  - 3D GPR migration
KW  - dielectric prediction system
KW  - deep neural network module
KW  - Ground penetrating radar
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Dielectrics
KW  - Feature extraction
KW  - Object detection
KW  - Inspection
DO  - 10.1109/ICRA40945.2020.9197043
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ground Penetrating Radar (GPR) is one of the most important non-destructive evaluation (NDE) devices to detect the subsurface objects (i.e. rebars, utility pipes) and reveal the underground scene. One of the biggest challenges in GPR based inspection is the subsurface targets reconstruction. In order to address this issue, this paper presents a 3D GPR migration and dielectric prediction system to detect and reconstruct underground targets. This system is composed of three modules: 1) visual inertial fusion (VIF) module to generate the pose information of GPR device, 2) deep neural network module (i.e., DepthNet) which detects B-scan of GPR image, extracts hyperbola features to remove the noise in B-scan data and predicts dielectric to determine the depth of the objects, 3) 3D GPR migration module which synchronizes the pose information with GPR scan data processed by DepthNet to reconstruct and visualize the 3D underground targets. Our proposed DepthNet processes the GPR data by removing the noise in B-scan image as well as predicting depth of subsurface objects. For DepthNet model training and testing, we collect the real GPR data in the concrete test pit at Geophysical Survey System Inc. (GSSI) and create the synthetic GPR data by using gprMax3.0 simulator. The dataset we create includes 350 labeled GPR images. The DepthNet achieves an average accuracy of 92.64% for B-scan feature detection and an 0.112 average error for underground target depth prediction. In addition, the experimental results verify that our proposed method improve the migration accuracy and performance in generating 3D GPR image compared with the traditional migration methods.
ER  - 

TY  - CONF
TI  - Real-time Stereo Visual Servoing for Rose Pruning with Robotic Arm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7050
EP  - 7056
AU  - H. Cuevas-Velasquez
AU  - A. -J. Gallego
AU  - R. Tylecek
AU  - J. Hemming
AU  - B. van Tuijl
AU  - A. Mencarelli
AU  - R. B. Fisher
PY  - 2020
KW  - cutting
KW  - end effectors
KW  - gardening
KW  - real-time systems
KW  - robot vision
KW  - service robots
KW  - stereo image processing
KW  - visual servoing
KW  - multiple cameras
KW  - single stereo camera
KW  - end effector
KW  - robotic arm
KW  - real time stereo visual servoing
KW  - automated robotic rose cutter
KW  - rose bush pruning
KW  - gardening
KW  - rose pruning robots
KW  - Cameras
KW  - Robot vision systems
KW  - Manipulators
KW  - Real-time systems
KW  - Pipelines
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197272
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents a working pipeline which integrates hardware and software in an automated robotic rose cutter. To the best of our knowledge, this is the first robot able to prune rose bushes in a natural environment. Unlike similar approaches like tree stem cutting, the proposed method does not require to scan the full plant, have multiple cameras around the bush, or assume that a stem does not move. It relies on a single stereo camera mounted on the end-effector of the robot and real-time visual servoing to navigate to the desired cutting location on the stem. The evaluation of the whole pipeline shows a good performance in a garden with unconstrained conditions, where finding and approaching a specific location on a stem is challenging due to occlusions caused by other stems and dynamic changes caused by the wind.
ER  - 

TY  - CONF
TI  - Slip-Limiting Controller for Redundant Line-Suspended Robots: Application to Line Ranger
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7081
EP  - 7087
AU  - P. Hamelin
AU  - P. -L. Richard
AU  - M. Lepage
AU  - M. Lagac√©
AU  - A. Sartor
AU  - G. Lambert
AU  - C. H√©bert
AU  - N. Pouliot
PY  - 2020
KW  - angular velocity control
KW  - angular velocity measurement
KW  - centralised control
KW  - mobile robots
KW  - motion control
KW  - power transmission lines
KW  - service robots
KW  - wheels
KW  - wheel slippage
KW  - slip-limiting controller
KW  - redundant line-suspended robots
KW  - v-shaped wheels
KW  - wheel radius
KW  - wheel angular velocity measurements
KW  - slip limitation
KW  - control allocation algorithm
KW  - high-level velocity controller
KW  - centralized control
KW  - line ranger
KW  - Wheels
KW  - Mobile robots
KW  - Angular velocity
KW  - Robot sensing systems
KW  - Resource management
KW  - Velocity measurement
DO  - 10.1109/ICRA40945.2020.9196832
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a slip-limiting controller for redundant line-suspended robots is presented. This kind of robot is usually equipped with v-shaped wheels, which brings uncertainty about the effective wheel radius, particularly when crossing obstacles. The proposed algorithm is able to estimate and limit wheel slippage in the presence of such uncertainty, relying only on wheel angular velocity measurements. Slip limitation occurs in the control allocation algorithm and hence is decoupled from the high-level velocity controller, allowing a broad applicability in centralized control approaches. Experimental results on Line Ranger show that it effectively reduces wheel slippage compared to traditional centralized control while being more energy efficient than traditional decentralized control approaches.
ER  - 

TY  - CONF
TI  - Interval Search Genetic Algorithm Based on Trajectory to Solve Inverse Kinematics of Redundant Manipulators and Its Application
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7088
EP  - 7094
AU  - D. Wu
AU  - W. Zhang
AU  - M. Qin
AU  - B. Xie
PY  - 2020
KW  - end effectors
KW  - genetic algorithms
KW  - kinematics
KW  - redundant manipulators
KW  - search problems
KW  - trajectory control
KW  - tunnels
KW  - interval search strategy
KW  - reference point strategy
KW  - redundant manipulators
KW  - continuous motion
KW  - interval search genetic algorithm
KW  - inverse kinematics problem
KW  - parametric joint angle method
KW  - population continuity strategy
KW  - trajectory control
KW  - evolutionary generation
KW  - fitness function
KW  - tunnel shotcrete robot
KW  - end effector
KW  - Manipulators
KW  - Kinematics
KW  - Sociology
KW  - Statistics
KW  - Trajectory
KW  - Genetic algorithms
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196890
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a new method is proposed to solve the inverse kinematics problem of redundant manipulators. This method demonstrates superior performance on continuous motion by combining interval search genetic algorithm based on trajectory which we propose with parametric joint angle method. In this method, population continuity strategy is utilized to improve search speed and reduce evolutionary generation, interval search strategy is introduced to enhance the search ability and overcome the influence of singularity, and reference point strategy is used to avoid sudden changes of joint variables. By introducing those three strategies, this method is especially suitable for redundant manipulators that perform continuous motion. It can not only obtain solutions of inverse kinematics quickly, but also ensure the motion continuity of manipulator and accuracy of the end effector. Moreover, this algorithm can also perform multi-objective tasks by adjusting the fitness function. Finally, this algorithm is applied to an 8 degree of freedom tunnel shotcrete robot. Field experiments and data analysis show that the algorithm can solve the problem quickly in industrial field, and ensure the motion continuity and accuracy.
ER  - 

TY  - CONF
TI  - Analytical Expressions of Serial Manipulator Jacobians and their High-Order Derivatives based on Lie Theory*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7095
EP  - 7100
AU  - Z. Fu
AU  - E. Spyrakos-Papastavridis
AU  - Y. -h. Lin
AU  - J. S. Dai
PY  - 2020
KW  - approximation theory
KW  - end effectors
KW  - iterative methods
KW  - Jacobian matrices
KW  - Lie algebras
KW  - manipulator kinematics
KW  - manipulator Jacobian
KW  - high-order derivatives
KW  - Lie theory
KW  - higher-order derivatives
KW  - higher-order Jacobian derivatives
KW  - serial manipulator kinematics
KW  - joint variables
KW  - joint-space coordinates
KW  - serial manipulator Jacobians
KW  - task-space Cartesian coordinates
KW  - inertial-fixed frames
KW  - body-fixed frames
KW  - KUKA LRB iiwa7 R800 manipulator
KW  - Jacobian matrices
KW  - Manipulators
KW  - Kinematics
KW  - Acceleration
KW  - Fasteners
DO  - 10.1109/ICRA40945.2020.9197131
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Serial manipulator kinematics provide a mapping between joint variables in joint-space coordinates, and end-effector configurations in task-space Cartesian coordinates. Velocity mappings are represented via the manipulator Jacobian produced by direct differentiation of the forward kinematics. Acquisition of acceleration, jerk, and snap expressions, typically utilized for accurate trajectory-tracking, requires the computation of high-order Jacobian derivatives. As compared to conventional numerical/D-H approaches, this paper proposes a novel methodology to derive the Jacobians and their high-order derivatives symbolically, based on Lie theory, which requires that the derivatives are calculated with respect to each joint variable and time. Additionally, the technique described herein yields a mathematically sound solution to the high-order Jacobian derivatives, which distinguishes it from other relevant works. Performing computations with respect to the two inertial-fixed and body-fixed frames, the analytical form of the spatial and body Jacobians are derived, as well as their higher-order derivatives, without resorting to any approximations, whose expressions would depend explicitly on the joint state and the choice of reference frames. The proposed method provides more tractable computation of higher-order Jacobian derivatives, while its effectiveness has been verified by conducting a comparative analysis based on experimental data extracted from a KUKA LRB iiwa7 R800 manipulator.
ER  - 

TY  - CONF
TI  - Inverse Kinematics for Serial Kinematic Chains via Sum of Squares Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7101
EP  - 7107
AU  - F. Mariƒá
AU  - M. Giamou
AU  - S. Khoubyarian
AU  - I. Petroviƒá
AU  - J. Kelly
PY  - 2020
KW  - convex programming
KW  - end effectors
KW  - manipulator kinematics
KW  - mobile robots
KW  - polynomials
KW  - position control
KW  - degrees of freedom
KW  - articulated robots
KW  - globally optimal solution
KW  - serial manipulators
KW  - kinematic constraints
KW  - highly redundant serial kinematic chains
KW  - joint limit constraints
KW  - inverse kinematics problem
KW  - convex optimization techniques
KW  - numerical methods
KW  - nonlinear problem
KW  - feasible joint configurations
KW  - task-related workspace constraints
KW  - sum of squares optimization
KW  - Kinematics
KW  - Optimization
KW  - Conferences
KW  - Automation
KW  - Robots
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196704
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Inverse kinematics is a fundamental challenge for articulated robots: fast and accurate algorithms are needed for translating task-related workspace constraints and goals into feasible joint configurations. In general, inverse kinematics for serial kinematic chains is a difficult nonlinear problem, for which closed form solutions cannot easily be obtained. Therefore, computationally efficient numerical methods that can be adapted to a general class of manipulators are of great importance. In this paper, we use convex optimization techniques to solve the inverse kinematics problem with joint limit constraints for highly redundant serial kinematic chains with spherical joints in two and three dimensions. This is accomplished through a novel formulation of inverse kinematics as a nearest point problem, and with a fast sum of squares solver that exploits the sparsity of kinematic constraints for serial manipulators. Our method has the advantages of post-hoc certification of global optimality and a runtime that scales polynomially with the number of degrees of freedom. Additionally, we prove that our convex relaxation leads to a globally optimal solution when certain conditions are met, and demonstrate empirically that these conditions are common and represent many practical instances. Finally, we provide an open source implementation of our algorithm.
ER  - 

TY  - CONF
TI  - Multi-task closed-loop inverse kinematics stability through semidefinite programming
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7108
EP  - 7114
AU  - J. Marti-Saumell
AU  - A. Santamaria-Navarro
AU  - C. Ocampo-Martinez
AU  - J. Andrade-Cetto
PY  - 2020
KW  - closed loop systems
KW  - control system synthesis
KW  - discrete time systems
KW  - humanoid robots
KW  - linear matrix inequalities
KW  - Lyapunov methods
KW  - mathematical programming
KW  - mobile robots
KW  - robot kinematics
KW  - stability
KW  - multitask closed-loop inverse kinematics stability
KW  - multiobjective task resolution
KW  - humanoid robots
KW  - local stability problem
KW  - closed-loop inverse kinematics algorithm
KW  - highly redundant robots
KW  - system stability
KW  - closed-loop control gains
KW  - semidefinite programming problem
KW  - discrete-time Lyapunov stability condition
KW  - SDP optimization problem
KW  - stability conditions
KW  - Task analysis
KW  - Stability analysis
KW  - Robots
KW  - Kinematics
KW  - Thermal stability
KW  - Numerical stability
KW  - Asymptotic stability
DO  - 10.1109/ICRA40945.2020.9196750
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Today's complex robotic designs comprise in some cases a large number of degrees of freedom, enabling for multi-objective task resolution (e.g., humanoid robots or aerial manipulators). This paper tackles the local stability problem of a hierarchical closed-loop inverse kinematics algorithm for such highly redundant robots. We present a method to guarantee this system stability by performing an online tuning of the closed-loop control gains. We define a semi-definite programming problem (SDP) with these gains as decision variables and a discrete-time Lyapunov stability condition as a linear matrix inequality, constraining the SDP optimization problem and guaranteeing the local stability of the prioritized tasks. To the best of authors' knowledge, this work represents the first mathematical development of an SDP formulation that introduces these stability conditions for a multi-objective closed-loop inverse kinematic problem for highly redundant robots. The validity of the proposed approach is demonstrated through simulation case studies, including didactic examples and a Matlab toolbox for the benefit of the community.
ER  - 

TY  - CONF
TI  - Securing Industrial Operators with Collaborative Robots: Simulation and Experimental Validation for a Carpentry task
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7128
EP  - 7134
AU  - N. Benhabib
AU  - V. Padois
AU  - D. Daney
PY  - 2020
KW  - industrial robots
KW  - machine tools
KW  - milling
KW  - mobile robots
KW  - safety
KW  - wood
KW  - industrial operators
KW  - collaborative robot
KW  - carpentry task
KW  - robotic assistance strategy
KW  - machine-tool
KW  - wood milling
KW  - accidentogenic aspect
KW  - physical model
KW  - tooling process
KW  - safety
KW  - Task analysis
KW  - Cutting tools
KW  - Milling
KW  - Force
KW  - Service robots
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197161
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, a robotic assistance strategy is developed to improve the safety in an artisanal task that involves a strong interaction between a machine-tool and an operator. Wood milling is chosen as a pilot task due to its importance in carpentry and its accidentogenic aspect. A physical model of the tooling process including a human is proposed and a simulator is thereafter developed to better understand situations that are dangerous for the craftsman. This simulator is validated with experiments on three subjects using an harmless mock-up. This validation shows the pertinence of the proposed control approach for the collaborative robot used to increase the safety of the task.
ER  - 

TY  - CONF
TI  - Learning Shape-based Representation for Visual Localization in Extremely Changing Conditions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7135
EP  - 7141
AU  - H. -G. Jeon
AU  - S. Im
AU  - J. Oh
AU  - M. Hebert
PY  - 2020
KW  - convolutional neural nets
KW  - disasters
KW  - image representation
KW  - image texture
KW  - learning (artificial intelligence)
KW  - natural scenes
KW  - pose estimation
KW  - shape recognition
KW  - shape-based representation
KW  - visual localization
KW  - extremely changing conditions
KW  - convolutional neural network
KW  - layout changes
KW  - approximate scene coordinates
KW  - scene layout
KW  - CNN
KW  - stylized images
KW  - estimated dominant planes
KW  - query images
KW  - simulated disaster dataset
KW  - reliable camera pose predictions
KW  - Visualization
KW  - Shape
KW  - Cameras
KW  - Semantics
KW  - Robustness
KW  - Geometry
KW  - Buildings
DO  - 10.1109/ICRA40945.2020.9196842
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual localization is an important task for applications such as navigation and augmented reality, but is a challenging problem when there are changes in scene appearances through day, seasons, or environments. In this paper, we present a convolutional neural network (CNN)-based approach for visual localization across normal to drastic appearance variations such as pre- and post-disaster cases. Our approach aims to address two key challenges: (1) to reduce the biases based on scene textures as in traditional CNNs, our model learns a shape-based representation by training on stylized images; (2) to make the model robust against layout changes, our approach uses the estimated dominant planes of query images as approximate scene coordinates. Our method is evaluated on various scenes including a simulated disaster dataset to demonstrate the effectiveness of our method in significant changes of scene layout. Experimental results show that our method provides reliable camera pose predictions in various changing conditions.
ER  - 

TY  - CONF
TI  - Trajectory Planning with Safety Guaranty for a Multirotor based on the Forward and Backward Reachability Analysis
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7142
EP  - 7148
AU  - H. Seo
AU  - C. Youngdong Son
AU  - D. Lee
AU  - H. Jin Kim
PY  - 2020
KW  - aircraft control
KW  - collision avoidance
KW  - helicopters
KW  - reachability analysis
KW  - robust control
KW  - set theory
KW  - trajectory control
KW  - obstacle avoidance
KW  - risk free flight
KW  - backward reachable sets
KW  - forward reachable sets
KW  - robust trajectory planning algorithm
KW  - safety guarantee
KW  - multirotor
KW  - Hamilton-Jacobi reachability analysis
KW  - Trajectory
KW  - Planning
KW  - Safety
KW  - Reachability analysis
KW  - Robustness
KW  - Optimization
KW  - Aerospace engineering
DO  - 10.1109/ICRA40945.2020.9196760
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planning a trajectory with guaranteed safety is a core part for a risk-free flight of a multirotor. If a trajectory planner only aims to ensure safety, it may generate trajectories which overly bypass risky regions and prevent the system from achieving specific missions. This work presents a robust trajectory planning algorithm which simultaneously guarantees the safety and reachability to the target state in the presence of unknown disturbances. We first characterize how the forward and backward reachable sets (FRSs and BRSs) are constructed by using Hamilton-Jacobi reachability analysis. Based on the analysis, we present analytic expressions for the reachable sets and then propose minimal ellipsoids which closely approximate the reachable sets. In the planning process, we optimize the reference trajectory to connect the FRSs and BRSs, while avoiding obstacles. By combining the FRSs and BRSs, we can guarantee that any state inside of the initial set reaches the target set. We validate the proposed algorithm through a simulation of traversing a narrow gap.
ER  - 

TY  - CONF
TI  - A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7149
EP  - 7155
AU  - S. Bansal
AU  - A. Bajcsy
AU  - E. Ratner
AU  - A. D. Dragan
AU  - C. J. Tomlin
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - continuous time systems
KW  - motion control
KW  - path planning
KW  - predictive control
KW  - probability
KW  - reachability analysis
KW  - robots
KW  - stochastic processes
KW  - probabilistic predictive models
KW  - human behavior
KW  - future motion
KW  - observation models
KW  - state predictions
KW  - robot motion plan
KW  - human behavioral data
KW  - human motion prediction
KW  - Hamilton-Jacobi reachability problem
KW  - continuous-time dynamical system
KW  - model parameters
KW  - worst-case forward reachable set
KW  - future state distributions
KW  - robust planning
KW  - Hamilton-Jacobi reachability-based framework
KW  - human motion analysis
KW  - safe planning
KW  - real-world autonomous systems
KW  - Predictive models
KW  - Robots
KW  - Stochastic processes
KW  - Planning
KW  - Data models
KW  - Computational modeling
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197257
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. Although this enables data and priors to improve the human model, observation models are difficult to specify and priors may be incorrect, leading to erroneous state predictions that can degrade the safety of the robot motion plan. In this work, we seek to design a predictor which is more robust to misspecified models and priors, but can still leverage human behavioral data online to reduce conservatism in a safe way. To do this, we cast human motion prediction as a Hamilton-Jacobi reachability problem in the joint state space of the human and the belief over the model parameters. We construct a new continuous-time dynamical system, where the inputs are the observations of human behavior, and the dynamics include how the belief over the model parameters change. The results of this reachability computation enable us to both analyze the effect of incorrect priors on future predictions in continuous state and time, as well as to make predictions of the human state in the future. We compare our approach to the worst-case forward reachable set and a stochastic predictor which uses Bayesian inference and produces full future state distributions. Our comparisons in simulation and in hardware demonstrate how our framework can enable robust planning while not being overly conservative, even when the human model is inaccurate. Videos of our experiments can be found at the project website1.
ER  - 

TY  - CONF
TI  - Enhancing Privacy in Robotics via Judicious Sensor Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7156
EP  - 7165
AU  - S. Eick
AU  - A. I. Ant√≥n
PY  - 2020
KW  - data privacy
KW  - design
KW  - robots
KW  - sensors
KW  - judicious sensor selection
KW  - roboticists
KW  - robot design
KW  - robotics journals
KW  - privacy preservation
KW  - robot lifecycle
KW  - privacy impact assessments
KW  - privacy enhancement
KW  - Robot sensing systems
KW  - Privacy
KW  - Data privacy
KW  - Cameras
KW  - Task analysis
KW  - Law
KW  - privacy
KW  - privacy by design
KW  - robotics
KW  - robot design
KW  - sensor selection
KW  - compliance
KW  - privacy impact assessments
DO  - 10.1109/ICRA40945.2020.9196983
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Roboticists are grappling with how to address privacy in robot design at a time when regulatory frameworks around the world increasingly require systems to be engineered to preserve and protect privacy. This paper surveys the top robotics journals and conferences over the past four decades to identify contributions with respect to privacy in robot design. Our survey revealed that less than half of one percent of the ~89,120 papers in our study even mention the word privacy. Herein, we propose privacy preserving approaches for roboticists to employ in robot design, including, assessing a robot's purpose and environment; ensuring privacy by design by selecting sensors that do not collect information that is not essential to the core objectives of that robot; embracing both privacy and performance as fundamental design challenges to be addressed early in the robot lifecycle; and performing privacy impact assessments.
ER  - 

TY  - CONF
TI  - Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7166
EP  - 7172
AU  - S. Li
AU  - O. Bastani
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - nonlinear control systems
KW  - nonlinear dynamical systems
KW  - predictive control
KW  - probability
KW  - robust control
KW  - stochastic processes
KW  - stochastic systems
KW  - backup policy
KW  - learned policy
KW  - control policy
KW  - additive stochastic disturbances
KW  - nominal dynamics
KW  - stochastic nonlinear dynamical systems
KW  - stochastic dynamics
KW  - safe reinforcement learning
KW  - robust model predictive shielding
KW  - stochastic systems
KW  - statistical learning theory
KW  - backup controller
KW  - tube-based robust nonlinear model predictive controller
KW  - Safety
KW  - Robustness
KW  - Stochastic processes
KW  - Robots
KW  - Trajectory
KW  - Nonlinear dynamical systems
KW  - Heuristic algorithms
DO  - 10.1109/ICRA40945.2020.9196867
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a framework for safe reinforcement learning that can handle stochastic nonlinear dynamical systems. We focus on the setting where the nominal dynamics are known, and are subject to additive stochastic disturbances with known distribution. Our goal is to ensure the safety of a control policy trained using reinforcement learning, e.g., in a simulated environment. We build on the idea of model predictive shielding (MPS), where a backup controller is used to override the learned policy as needed to ensure safety. The key challenge is how to compute a backup policy in the context of stochastic dynamics. We propose to use a tube-based robust nonlinear model predictive controller (NMPC) as the backup controller. We estimate the tubes using sampled trajectories, leveraging ideas from statistical learning theory to obtain high-probability guarantees. We empirically demonstrate that our approach can ensure safety in stochastic systems, including cart-pole and a non-holonomic particle with random obstacles.
ER  - 

TY  - CONF
TI  - Segregation of Heterogeneous Swarms of Robots in Curves
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7173
EP  - 7179
AU  - E. B. Ferreira Filho
AU  - L. C. A. Pimenta
PY  - 2020
KW  - collision avoidance
KW  - decentralised control
KW  - mobile robots
KW  - multi-robot systems
KW  - topology
KW  - decentralized control strategy
KW  - heterogeneous robot swarms
KW  - formation control
KW  - collision avoidance strategy
KW  - multiple heterogeneous robots
KW  - heterogeneous swarm segregation
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Heuristic algorithms
KW  - Topology
KW  - Convergence
KW  - Damping
DO  - 10.1109/ICRA40945.2020.9196851
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a decentralized control strategy to reach segregation in heterogeneous robot swarms distributed in curves. The approach is based on a formation control algorithm applied to each robot and a heuristics to compute the distance between the groups, i.e. the distance from the beginning of the curve. We consider that robots can communicate through a fixed underlying topology and also when they are within a certain distance. A convergence proof with a collision avoidance strategy is presented. Simulations and experimental results show that our approach allows a swarm of multiple heterogeneous robots to segregate into groups.
ER  - 

TY  - CONF
TI  - A Fast, Accurate, and Scalable Probabilistic Sample-Based Approach for Counting Swarm Size
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7180
EP  - 7185
AU  - H. Wang
AU  - M. Rubenstein
PY  - 2020
KW  - control engineering computing
KW  - distributed algorithms
KW  - multi-robot systems
KW  - particle swarm optimisation
KW  - path planning
KW  - counting swarm
KW  - distributed algorithm
KW  - neighboring robots
KW  - robot swarm
KW  - Robots
KW  - Estimation
KW  - Shape
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Random variables
KW  - Clocks
DO  - 10.1109/ICRA40945.2020.9196529
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes a distributed algorithm for computing the number of robots in a swarm, only requiring communication with neighboring robots. The algorithm can adjust the estimated count when the number of robots in the swarm changes, such as the addition or removal of robots. Probabilistic guarantees are given, which show the accuracy of this method, and the trade-off between accuracy, speed, and adaptability to changing numbers. The proposed approach is demonstrated in simulation as well as a real swarm of robots.
ER  - 

TY  - CONF
TI  - Bayes Bots: Collective Bayesian Decision-Making in Decentralized Robot Swarms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7186
EP  - 7192
AU  - J. T. Ebert
AU  - M. Gauci
AU  - F. Mallmann-Trenn
AU  - R. Nagpal
PY  - 2020
KW  - Bayes methods
KW  - decision making
KW  - multi-robot systems
KW  - collective Bayesian decision-making
KW  - decentralized robot swarms
KW  - distributed Bayesian algorithm
KW  - spatially distributed feature
KW  - farm field
KW  - robotics
KW  - decentralized Bayesian algorithms
KW  - sparsely distributed robots
KW  - decision-making accuracy
KW  - bio-inspired positive feedback
KW  - fixed-time benchmark algorithm
KW  - Bayes bots
KW  - bio-inspired approaches
KW  - Robot sensing systems
KW  - Bayes methods
KW  - Decision making
KW  - Classification algorithms
KW  - Task analysis
KW  - Color
DO  - 10.1109/ICRA40945.2020.9196584
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a distributed Bayesian algorithm for robot swarms to classify a spatially distributed feature of an environment. This type of "go/no-go" decision appears in applications where a group of robots must collectively choose whether to take action, such as determining if a farm field should be treated for pests. Previous bio-inspired approaches to decentralized decision-making in robotics lack a statistical foundation, while decentralized Bayesian algorithms typically require a strongly connected network of robots. In contrast, our algorithm allows simple, sparsely distributed robots to quickly reach accurate decisions about a binary feature of their environment. We investigate the speed vs. accuracy tradeoff in decision-making by varying the algorithm's parameters. We show that making fewer, less-correlated observations can improve decision-making accuracy, and that a well-chosen combination of prior and decision threshold allows for fast decisions with a small accuracy cost. Both speed and accuracy also improved with the addition of bio-inspired positive feedback. This algorithm is also adaptable to the difficulty of the environment. Compared to a fixed-time benchmark algorithm with accuracy guarantees, our Bayesian approach resulted in equally accurate decisions, while adapting its decision time to the difficulty of the environment.
ER  - 

TY  - CONF
TI  - Supervisory Control of Robot Swarms Using Public Events
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7193
EP  - 7199
AU  - Y. K. Lopes
AU  - S. M. Trenkwalder
AU  - A. B. Leal
AU  - T. J. Dodd
AU  - R. Gro√ü
PY  - 2020
KW  - discrete event systems
KW  - mobile robots
KW  - multi-robot systems
KW  - robot swarms
KW  - public events
KW  - supervisory control theory
KW  - formal framework
KW  - discrete event systems
KW  - correct-by-construction controllers
KW  - swarm robotics systems
KW  - extended SCT framework
KW  - mobile robots
KW  - e-puck robots
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Mobile robots
KW  - Generators
KW  - Supervisory control
DO  - 10.1109/ICRA40945.2020.9197418
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Supervisory Control Theory (SCT) provides a formal framework for controlling discrete event systems. It has recently been used to generate correct-by-construction controllers for swarm robotics systems. Current SCT frameworks are limited, as they support only (private) events that are observable within the same robot. In this paper, we propose an extended SCT framework that incorporates (public) events that are shared among robots. The extended framework allows to model formally the interactions among the robots. It is evaluated using a case study, where a group of mobile robots need to synchronise their movements in space and time-a requirement that is specified at the formal level. We validate our approach through experiments with groups of e-puck robots.
ER  - 

TY  - CONF
TI  - Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7226
EP  - 7232
AU  - B. Abbyasov
AU  - R. Lavrenov
AU  - A. Zakiev
AU  - K. Yakovlev
AU  - M. Svinin
AU  - E. Magid
PY  - 2020
KW  - control engineering computing
KW  - laser ranging
KW  - mobile robots
KW  - SLAM (robots)
KW  - solid modelling
KW  - Gazebo world construction
KW  - grayscale image
KW  - 3D solid model
KW  - robot simulators
KW  - simulated physical environment
KW  - 2D image
KW  - 2D laser range finder data
KW  - Gazebo simulator
KW  - 3D Collada
KW  - simultaneous localization and mapping
KW  - real-time factor
KW  - SLAM missions
KW  - RTF
KW  - Tools
KW  - Solid modeling
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Gray-scale
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196621
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.
ER  - 

TY  - CONF
TI  - A ROS Gazebo plugin to simulate ARVA sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7233
EP  - 7239
AU  - J. Cacace
AU  - N. Mimmo
AU  - L. Marconi
PY  - 2020
KW  - aerospace communication
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - operating systems (computers)
KW  - radio transceivers
KW  - rescue robots
KW  - robot programming
KW  - sensors
KW  - ROS Gazebo plugin
KW  - forefront technology
KW  - Search & Rescue operations
KW  - ARVA sensor simulation
KW  - transceiver sensor
KW  - Appareil de Recherche de Victims en Avalanche
KW  - Unmanned Aerial Vehicle
KW  - Receivers
KW  - Transmitters
KW  - Sensors
KW  - Electromagnetics
KW  - Antennas
KW  - Robots
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA40945.2020.9196914
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the problem to simulate ARVA sensors using ROS and Gazebo. ARVA is a French acronym which stands for Appareil de Recherche de Victims en Avalanche and represents the forefront technology adopted in Search & Rescue operations to localize victims of avalanches buried under the snow. The aim of this paper is to describe the mathematical and theoretical background of the transceiver, discussing its implementation and integration with ROS allowing researchers to develop faster and smarter Search &Rescue strategies based on ARVA receiver data. To assess the effectiveness of the proposed sensor model, We present a simulation scenario in which an Unmanned Aerial Vehicle equipped with the transceiver sensor performs a basic S&R pattern using the output of ARVA system.
ER  - 

TY  - CONF
TI  - Is That a Chair? Imagining Affordances Using Simulations of an Articulated Human Body
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7240
EP  - 7246
AU  - H. Wu
AU  - D. Misra
AU  - G. S. Chirikjian
PY  - 2020
KW  - CAD
KW  - cameras
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - pose estimation
KW  - articulated human body
KW  - object affordances
KW  - physical interactions
KW  - physical simulations
KW  - arbitrarily oriented object
KW  - physical sitting interaction
KW  - object affordance reasoning
KW  - object classification
KW  - chair classification
KW  - appearance-based deep learning methods
KW  - affordances imagining
KW  - synthetic 3D CAD models
KW  - training data
KW  - functional pose predictions
KW  - depth camera
KW  - Robots
KW  - Solid modeling
KW  - Cognition
KW  - Physics
KW  - Three-dimensional displays
KW  - Data models
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197384
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For robots to exhibit a high level of intelligence in the real world, they must be able to assess objects for which they have no prior knowledge. Therefore, it is crucial for robots to perceive object affordances by reasoning about physical interactions with the object. In this paper, we propose a novel method to provide robots with an ability to imagine object affordances using physical simulations. The class of chair is chosen here as an initial category of objects to illustrate a more general paradigm. In our method, the robot "imagines" the affordance of an arbitrarily oriented object as a chair by simulating a physical sitting interaction between an articulated human body and the object. This object affordance reasoning is used as a cue for object classification (chair vs non-chair). Moreover, if an object is classified as a chair, the affordance reasoning can also predict the upright pose of the object which allows the sitting interaction to take place. We call this type of poses the functional pose. We demonstrate our method in chair classification on synthetic 3D CAD models. Although our method uses only 30 models for training, it outperforms appearance-based deep learning methods, which require a large amount of training data, when the upright orientation is not assumed to be known a priori. In addition, we showcase that the functional pose predictions of our method align well with human judgments on both synthetic models and real objects scanned by a depth camera.
ER  - 

TY  - CONF
TI  - Toward Sim-to-Real Directional Semantic Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7247
EP  - 7253
AU  - S. Iqbal
AU  - J. Tremblay
AU  - A. Campbell
AU  - K. Leung
AU  - T. To
AU  - J. Cheng
AU  - E. Leitch
AU  - D. McKay
AU  - S. Birchfield
PY  - 2020
KW  - control engineering computing
KW  - end effectors
KW  - grippers
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - rendering (computer graphics)
KW  - robot vision
KW  - directional semantic grasping
KW  - deep reinforcement learning
KW  - double deep Q-network
KW  - robot simulator
KW  - rendering
KW  - monocular RGB images
KW  - wrist mounted camera
KW  - cartesian robot control
KW  - crossentropy method
KW  - domain randomization
KW  - end effector
KW  - Grippers
KW  - Grasping
KW  - Cameras
KW  - Training
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197310
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of directional semantic grasping, that is, grasping a specific object from a specific direction. We approach the problem using deep reinforcement learning via a double deep Q-network (DDQN) that learns to map downsampled RGB input images from a wrist-mounted camera to Q-values, which are then translated into Cartesian robot control commands via the cross-entropy method (CEM). The network is learned entirely on simulated data generated by a custom robot simulator that models both physical reality (contacts) and perceptual quality (high-quality rendering). The reality gap is bridged using domain randomization. The system is an example of end-to-end (mapping input monocular RGB images to output Cartesian motor commands) grasping of objects from multiple pre-defined object-centric orientations, such as from the side or top. We show promising results in both simulation and the real world, along with some challenges faced and the need for future research in this area.
ER  - 

TY  - CONF
TI  - Inferring the Material Properties of Granular Media for Robotic Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 2770
EP  - 2777
AU  - C. Matl
AU  - Y. Narang
AU  - R. Bajcsy
AU  - F. Ramos
AU  - D. Fox
PY  - 2020
KW  - Bayes methods
KW  - calibration
KW  - granular flow
KW  - granular materials
KW  - industrial robots
KW  - rolling friction
KW  - sliding friction
KW  - material properties
KW  - granular media
KW  - robotic tasks
KW  - cereal grains
KW  - plastic resin pellets
KW  - robotics-integrated industries
KW  - pharmaceutical development
KW  - accurate simulation
KW  - hardware framework
KW  - fast physics simulator
KW  - granular materials
KW  - real-world depth images
KW  - grain formations
KW  - likelihood-free Bayesian inference
KW  - calibrated simulator
KW  - unseen granular formations
KW  - simulator predictions
KW  - Robots
KW  - Friction
KW  - Numerical models
KW  - Bayes methods
KW  - Material properties
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197063
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Granular media (e.g., cereal grains, plastic resin pellets, and pills) are ubiquitous in robotics-integrated industries, such as agriculture, manufacturing, and pharmaceutical development. This prevalence mandates the accurate and efficient simulation of these materials. This work presents a software and hardware framework that automatically calibrates a fast physics simulator to accurately simulate granular materials by inferring material properties from real-world depth images of granular formations (i.e., piles and rings). Specifically, coefficients of sliding friction, rolling friction, and restitution of grains are estimated from summary statistics of grain formations using likelihood-free Bayesian inference. The calibrated simulator accurately predicts unseen granular formations in both simulation and experiment; furthermore, simulator predictions are shown to generalize to more complex tasks, including using a robot to pour grains into a bowl, as well as to create a desired pattern of piles and rings.
ER  - 

TY  - CONF
TI  - KETO: Learning Keypoint Representations for Tool Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7278
EP  - 7285
AU  - Z. Qin
AU  - K. Fang
AU  - Y. Zhu
AU  - L. Fei-Fei
AU  - S. Savarese
PY  - 2020
KW  - control engineering computing
KW  - image representation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neural nets
KW  - KETO
KW  - tool manipulation
KW  - informative representation
KW  - task-specific keypoints
KW  - 3D point clouds
KW  - tool object
KW  - deep neural network
KW  - informative description
KW  - self-supervised robot interactions
KW  - task environment
KW  - manipulation tasks
KW  - task success rates
KW  - keypoint prediction
KW  - tool generation
KW  - learned representations
KW  - keypoint representation learning
KW  - Tools
KW  - Task analysis
KW  - Robots
KW  - Visualization
KW  - Force
KW  - Three-dimensional displays
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196971
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We aim to develop an algorithm for robots to manipulate novel objects as tools for completing different task goals. An efficient and informative representation would facilitate the effectiveness and generalization of such algorithms. For this purpose, we present KETO, a framework of learning keypoint representations of tool-based manipulation. For each task, a set of task-specific keypoints is jointly predicted from 3D point clouds of the tool object by a deep neural network. These keypoints offer a concise and informative description of the object to determine grasps and subsequent manipulation actions. The model is learned from self-supervised robot interactions in the task environment without the need for explicit human annotations. We evaluate our framework in three manipulation tasks with tool use. Our model consistently outperforms state-of-the-art methods in terms of task success rates. Qualitative results of keypoint prediction and tool generation are shown to visualize the learned representations.
ER  - 

TY  - CONF
TI  - Learning to See before Learning to Act: Visual Pre-training for Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7286
EP  - 7293
AU  - L. Yen-Chen
AU  - A. Zeng
AU  - S. Song
AU  - P. Isola
AU  - T. -Y. Lin
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - robot vision
KW  - visual priors
KW  - vision-based manipulation
KW  - transfer learning
KW  - passive vision task
KW  - data distribution
KW  - active manipulation task
KW  - affordance maps
KW  - vision networks
KW  - zero-shot adaptation
KW  - zero robotic experience
KW  - visual pre-training
KW  - object detection
KW  - object manipulation
KW  - affordance prediction networks
KW  - Task analysis
KW  - Robots
KW  - Visualization
KW  - Predictive models
KW  - Grasping
KW  - Head
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197331
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task (i.e., the data distribution does not depend on the agent's decisions), then adapted to perform an active manipulation task (i.e., the data distribution does depend on the agent's decisions). We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves ~ 80% success rate at picking up novel objects.
ER  - 

TY  - CONF
TI  - Contact-based in-hand pose estimation using Bayesian state estimation and particle filtering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7294
EP  - 7299
AU  - F. von Drigalski
AU  - S. Taniguchi
AU  - R. Lee
AU  - T. Matsubara
AU  - M. Hamaya
AU  - K. Tanaka
AU  - Y. Ijiri
PY  - 2020
KW  - assembling
KW  - Bayes methods
KW  - calibration
KW  - force sensors
KW  - grippers
KW  - industrial manipulators
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - robotic assembly
KW  - state estimation
KW  - Bayesian state estimation
KW  - particle filtering
KW  - industrial assembly tasks
KW  - force sensor
KW  - robotic gripper
KW  - rigid object
KW  - contact based inhand pose estimation
KW  - Pose estimation
KW  - Grippers
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9196640
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In industrial assembly tasks, the position of an object grasped by the robot has to be known with high precision in order to insert or place it. In real applications, this problem is commonly solved by jigs that are specially produced for each part. However, they significantly limit flexibility and are prohibitive when the target parts change often, so a flexible method to localize parts with high accuracy after grasping is desired. To solve this problem, we propose a method that can estimate the position of an object in the robot's hand to sub-millimeter precision, and can improve its estimate incrementally, using only minimal calibration and a force sensor. Our method is applicable to any robotic gripper and any rigid object that the gripper can hold, and requires only a force sensor. We demonstrate that the method can determine the position of an object to a precision of under 1 mm without using any part-specific jigs or equipment.
ER  - 

TY  - CONF
TI  - A Single Multi-Task Deep Neural Network with Post-Processing for Object Detection with Reasoning and Robotic Grasp Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7300
EP  - 7306
AU  - D. Park
AU  - Y. Seo
AU  - D. Shin
AU  - J. Choi
AU  - S. Y. Chun
PY  - 2020
KW  - humanoid robots
KW  - inference mechanisms
KW  - manipulators
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - robotic grasp detection
KW  - object detection
KW  - separate networks
KW  - target objects
KW  - single RGB-D camera
KW  - multitask DNN
KW  - accurate detections
KW  - relationship reasoning
KW  - state-of-the-art performance
KW  - object grasping tasks
KW  - humanoid robot
KW  - single multitask deep neural network
KW  - deep neural network based object
KW  - network output
KW  - high-level reasoning
KW  - VMRD
KW  - Cornell datasets
KW  - Robots
KW  - Grasping
KW  - Cognition
KW  - Task analysis
KW  - Neural networks
KW  - Grippers
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197179
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Applications of deep neural network (DNN) based object and grasp detections could be expanded significantly when the network output is processed by a high-level reasoning over relationship of objects. Recently, robotic grasp detection and object detection with reasoning have been investigated using DNNs. There have been efforts to combine these multitasks using separate networks so that robots can deal with situations of grasping specific target objects in the cluttered, stacked, complex piles of novel objects from a single RGB-D camera. We propose a single multi-task DNN that yields accurate detections of objects, grasp position and relationship reasoning among objects. Our proposed methods yield state-of-the-art performance with the accuracy of 98.6% and 74.2% with the computation speed of 33 and 62 frame per second on VMRD and Cornell datasets, respectively. Our methods also yielded 95.3% grasp success rate for novel object grasping tasks with a 4-axis robot arm and 86.7% grasp success rate in cluttered novel objects with a humanoid robot.
ER  - 

TY  - CONF
TI  - Practical Persistence Reasoning in Visual SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7307
EP  - 7313
AU  - Z. Hashemifar
AU  - K. Dantu
PY  - 2020
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - static environments
KW  - dynamic environments
KW  - persistence filters
KW  - ORB-SLAM
KW  - visual SLAM algorithm
KW  - persistence filtering
KW  - persistence reasoning
KW  - semistatic environments
KW  - Simultaneous localization and mapping
KW  - Visualization
KW  - Estimation
KW  - Cognition
KW  - Probability
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196913
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many existing SLAM approaches rely on the assumption of static environments for accurate performance. However, several robot applications require them to traverse repeatedly in semi-static or dynamic environments. There has been some recent research interest in designing persistence filters to reason about persistence in such scenarios. Our goal in this work is to incorporate such persistence reasoning in visual SLAM. To this end, we incorporate persistence filters [1] into ORB-SLAM, a well-known visual SLAM algorithm. We observe that the simple integration of their proposal results in inefficient persistence reasoning. Through a series of modifications and using two locally collected datasets, we demonstrate the utility of such persistence filtering as well as our customizations in ORB-SLAM. Overall, incorporating persistence filtering could result in a significant reduction in map size (about 30% in the best case) and a corresponding reduction in run-time while retaining similar accuracy to methods that use much larger maps.
ER  - 

TY  - CONF
TI  - FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7322
EP  - 7328
AU  - T. Zhang
AU  - H. Zhang
AU  - Y. Li
AU  - Y. Nakamura
AU  - L. Zhang
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - dynamic environments
KW  - visual SLAM
KW  - moving objects
KW  - static environment features
KW  - lead
KW  - wrong camera motion estimation
KW  - dense RGB-D SLAM solution
KW  - camera ego-motion estimation
KW  - static background reconstructions
KW  - optical flow residuals
KW  - dynamic semantics
KW  - RGB-D point clouds
KW  - camera tracking
KW  - background reconstruction
KW  - dense reconstruction results
KW  - dynamic scenes
KW  - static environments
KW  - dynamic dense RGB-D SLAM
KW  - Cameras
KW  - Dynamics
KW  - Optical imaging
KW  - Simultaneous localization and mapping
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197349
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.
ER  - 

TY  - CONF
TI  - Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7344
EP  - 7350
AU  - R. Michelmore
AU  - M. Wicker
AU  - L. Laurenti
AU  - L. Cardelli
AU  - Y. Gal
AU  - M. Kwiatkowska
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - collision avoidance
KW  - decision making
KW  - inference mechanisms
KW  - neurocontrollers
KW  - probability
KW  - road safety
KW  - Bayesian inference methods
KW  - uncertainty computation
KW  - pointwise uncertainty measures
KW  - end-to-end Bayesian controllers
KW  - autonomous driving scenarios
KW  - Bayesian neural networks
KW  - sensor noise
KW  - controller behaviour
KW  - safety guarantees
KW  - neural network controllers
KW  - end-to-end autonomous driving control
KW  - statistical guarantees
KW  - uncertainty quantification
KW  - Uncertainty
KW  - Safety
KW  - Autonomous vehicles
KW  - Neural networks
KW  - Automobiles
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196844
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep neural network controllers for autonomous driving have recently benefited from significant performance improvements, and have begun deployment in the real world. Prior to their widespread adoption, safety guarantees are needed on the controller behaviour that properly take account of the uncertainty within the model as well as sensor noise. Bayesian neural networks, which assume a prior over the weights, have been shown capable of producing such uncertainty measures, but properties surrounding their safety have not yet been quantified for use in autonomous driving scenarios. In this paper, we develop a framework based on a state-of-the-art simulator for evaluating end-to-end Bayesian controllers. In addition to computing pointwise uncertainty measures that can be computed in real time and with statistical guarantees, we also provide a method for estimating the probability that, given a scenario, the controller keeps the car safe within a finite horizon. We experimentally evaluate the quality of uncertainty computation by three Bayesian inference methods in different scenarios and show how the uncertainty measures can be combined and calibrated for use in collision avoidance. Our results suggest that uncertainty estimates can greatly aid decision making in autonomous driving.
ER  - 

TY  - CONF
TI  - Autonomously Navigating a Surgical Tool Inside the Eye by Learning from Demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7351
EP  - 7357
AU  - J. W. Kim
AU  - C. He
AU  - M. Urias
AU  - P. Gehlbach
AU  - G. D. Hager
AU  - I. Iordachita
AU  - M. Kobilarov
PY  - 2020
KW  - end effectors
KW  - eye
KW  - learning by example
KW  - medical robotics
KW  - navigation
KW  - position control
KW  - robot programming
KW  - surgery
KW  - visual servoing
KW  - retinal surgery
KW  - auditory feedback
KW  - autonomous navigation system
KW  - needle surgical tool navigation
KW  - learning from demonstration
KW  - haptic feedback
KW  - deep network training
KW  - visual servoing
KW  - steady hand eye robot
KW  - SHER surgical robot
KW  - end effector
KW  - Tools
KW  - Retina
KW  - Surgery
KW  - Navigation
KW  - Task analysis
KW  - Trajectory
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196537
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A fundamental challenge in retinal surgery is safely navigating a surgical tool to a desired goal position on the retinal surface while avoiding damage to surrounding tissues, a procedure that typically requires tens-of-microns accuracy. In practice, the surgeon relies on depth-estimation skills to localize the tool-tip with respect to the retina in order to perform the tool-navigation task, which can be prone to human error. To alleviate such uncertainty, prior work has introduced ways to assist the surgeon by estimating the tooltip distance to the retina and providing haptic or auditory feedback. However, automating the tool-navigation task itself remains unsolved and largely unexplored. Such a capability, if reliably automated, could serve as a building block to streamline complex procedures and reduce the chance for tissue damage. Towards this end, we propose to automate the tool-navigation task by learning to mimic expert demonstrations of the task. Specifically, a deep network is trained to imitate expert trajectories toward various locations on the retina based on recorded visual servoing to a given goal specified by the user. The proposed autonomous navigation system is evaluated in simulation and in physical experiments using a silicone eye phantom. We show that the network can reliably navigate a needle surgical tool to various desired locations within 137 Œºm accuracy in physical experiments and 94 Œºm in simulation on average, and generalizes well to unseen situations such as in the presence of auxiliary surgical tools, variable eye backgrounds, and brightness conditions.
ER  - 


TY  - CONF
TI  - Learn-to-Recover: Retrofitting UAVs with Reinforcement Learning-Assisted Flight Control Under Cyber-Physical Attacks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7358
EP  - 7364
AU  - F. Fei
AU  - Z. Tu
AU  - D. Xu
AU  - X. Deng
PY  - 2020
KW  - actuators
KW  - aerospace computing
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - fault diagnosis
KW  - fault tolerant control
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - position control
KW  - security of data
KW  - stability
KW  - fault-tolerant control policy
KW  - actuator
KW  - stabilizing controller
KW  - detection activation
KW  - sensor faults
KW  - position control
KW  - learn-to-recover
KW  - UAVs
KW  - reinforcement learning-assisted flight control
KW  - cyber-physical attacks
KW  - quadcopter unmanned aerial vehicles
KW  - sensor attack
KW  - Actuators
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Vehicle dynamics
KW  - Learning (artificial intelligence)
KW  - Training
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196611
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a generic fault-tolerant control (FTC) strategy via reinforcement learning (RL). We demonstrate the effectiveness of this method on quadcopter unmanned aerial vehicles (UAVs). The fault-tolerant control policy is trained to handle actuator and sensor fault/attack. Unlike traditional FTC, this policy does not require fault detection and diagnosis (FDD) nor tailoring the controller for specific attack scenarios. Instead, the policy is running simultaneously alongside the stabilizing controller without the need for on- detection activation. The effectiveness of the policy is compared with traditional active and passive FTC strategies against actuator and sensor faults. We compare their performance in position control tasks via simulation and experiments on quadcopters. The result shows that the strategy can effectively tolerate different types of attacks/faults and maintain the vehicle's position, outperforming the other two methods.
ER  - 

TY  - CONF
TI  - Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7373
EP  - 7379
AU  - T. Laidlow
AU  - J. Czarnowski
AU  - A. Nicastro
AU  - R. Clark
AU  - S. Leutenegger
PY  - 2020
KW  - geometry
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - probability
KW  - stereo image processing
KW  - probabilistic fusion
KW  - standard pipelines
KW  - deep learning
KW  - standard 3D reconstruction pipelines
KW  - open problem
KW  - deep neural network
KW  - error models
KW  - standard 3D reconstruction system
KW  - dense depth maps
KW  - discrete probability distributions
KW  - nonparametric probability distributions
KW  - multiview stereo approaches
KW  - geometry- based systems
KW  - learned single-view depth prior
KW  - Standards
KW  - Probability distribution
KW  - Fuses
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Uncertainty
KW  - Probability density function
DO  - 10.1109/ICRA40945.2020.9197001
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The best way to combine the results of deep learning with standard 3D reconstruction pipelines remains an open problem. While systems that pass the output of traditional multi-view stereo approaches to a network for regularisation or refinement currently seem to get the best results, it may be preferable to treat deep neural networks as separate components whose results can be probabilistically fused into geometry- based systems. Unfortunately, the error models required to do this type of fusion are not well understood, with many different approaches being put forward. Recently, a few systems have achieved good results by having their networks predict probability distributions rather than single values. We propose using this approach to fuse a learned single-view depth prior into a standard 3D reconstruction system. Our system is capable of incrementally producing dense depth maps for a set of keyframes. We train a deep neural network to predict discrete, nonparametric probability distributions for the depth of each pixel from a single image. We then fuse this "probability volume" with another probability volume based on the photometric consistency between subsequent frames and the keyframe image. We argue that combining the probability volumes from these two sources will result in a volume that is better conditioned. To extract depth maps from the volume, we minimise a cost function that includes a regularisation term based on network predicted surface normals and occlusion boundaries. Through a series of experiments, we demonstrate that each of these components improves the overall performance of the system.
ER  - 

TY  - CONF
TI  - Model Reference Adaptive Control of Multirotor for Missions with Dynamic Change of Payloads During Flight
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7433
EP  - 7439
AU  - T. Maki
AU  - M. Zhao
AU  - F. Shi
AU  - K. Okada
AU  - M. Inaba
PY  - 2020
KW  - aerospace robotics
KW  - aircraft control
KW  - attitude control
KW  - helicopters
KW  - MIMO systems
KW  - model reference adaptive control systems
KW  - nonlinear control systems
KW  - robust control
KW  - stability
KW  - payloads
KW  - multirotor aerial robot
KW  - flight controller
KW  - flight stability
KW  - attitude control
KW  - aerial robot system
KW  - model reference adaptive control
KW  - nonlinear multiple-input and multiple-output
KW  - MRAC
KW  - Attitude control
KW  - Adaptation models
KW  - Payloads
KW  - MIMO communication
KW  - Unmanned aerial vehicles
KW  - Adaptive control
KW  - Gravity
DO  - 10.1109/ICRA40945.2020.9196861
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Carrying payloads in air is a major mission for multirotor aerial robot. However, the presence of payloads on multirotor aerial robot has a risk of degrading the performance of the flight controller. This concern becomes obvious especially when carrying objects not securely attached to the body or performing aerial manipulation. Therefore, controller with the ability to adapt itself to the effects of payloads on flight stability is needed. This paper proposes a novel nonlinear multiple-input and multiple-output (MIMO) model reference adaptive control (MRAC) system for attitude control of multirotor aerial robots which can dynamically compensate change in the position of center of gravity and inertia caused by payloads. Stability and robustness of the controller are experimentally confirmed in quadrotor and transformable multirotor, and experiments modeling practical applications are conducted for each aerial robot system, proving the utility of the controller.
ER  - 

TY  - CONF
TI  - The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7448
EP  - 7454
AU  - Y. Mulgaonkar
AU  - W. Liu
AU  - D. Thakur
AU  - K. Daniilidis
AU  - C. J. Taylor
AU  - V. Kumar
PY  - 2020
KW  - cameras
KW  - collision avoidance
KW  - image sensors
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - space vehicles
KW  - autonomous microaerial vehicle
KW  - autonomous Tiercel robots
KW  - collision detector design
KW  - fisheye camera
KW  - reflective obstacles
KW  - transparent obstacles
KW  - collision-resilient robot
KW  - Tiercel MAV
KW  - autonomous navigation
KW  - autonomous flight
KW  - Collision avoidance
KW  - Cameras
KW  - Robot vision systems
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197269
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to localize using visual-inertial odometry (VIO) running on board the robot with a single downward facing fisheye camera and an IMU. We show how our collision detector design and experimental set up enable us to characterize the impact of collisions on VIO. We further develop a planning strategy to enable the Tiercel to fly autonomously in an unknown space, sustaining collisions and creating a 2D map of the environment. Finally we demonstrate a swarm of three autonomous Tiercel robots safely navigating and colliding through an obstacle field to reach their objectives.
ER  - 

TY  - CONF
TI  - Adaptive Control of Variable-Pitch Propellers: Pursuing Minimum-Effort Operation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7470
EP  - 7476
AU  - T. Henderson
AU  - N. Papanikolopoulos
PY  - 2020
KW  - adaptive control
KW  - aerospace propulsion
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - electric propulsion
KW  - pitch control (position)
KW  - propellers
KW  - state-space methods
KW  - adaptive control
KW  - minimum-effort operation
KW  - unmanned aerial vehicles
KW  - UAV
KW  - electric propulsion systems
KW  - disparate flight modes
KW  - forward-moving flight
KW  - flight mode dissimilarity
KW  - fixed-geometry propulsion systems
KW  - variable-geometry systems
KW  - variable pitch propeller
KW  - propulsion performance
KW  - VPP system control
KW  - operation state space
KW  - hovering
KW  - near-minimum-electrical-effort propulsion system behavior
KW  - Propellers
KW  - Mathematical model
KW  - Servomotors
KW  - Geometry
KW  - Brushless DC motors
KW  - Blades
DO  - 10.1109/ICRA40945.2020.9197208
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As Unmanned Aerial Vehicles (UAVs) become more commonly used in industry, their performance will continue to be challenged. A performance bottleneck that is crucial to overcome is the design of electric propulsion systems for UAVs that operate in disparate flight modes (e.g., hovering and forward-moving flight). While flight mode dissimilarity presents a fundamental design challenge for fixed-geometry propulsion systems, variable-geometry systems such as the Variable Pitch Propeller (VPP) ones are able to provide superior propulsion performance across a wide range of flight modes. This work builds on previous work by the authors and presents a VPP system control and estimation framework for safe, near-minimum-electrical-effort propulsion system behavior across the whole operation state space of any UAV. Multiple simulated validations are presented to support the feasibility of the approach.
ER  - 

TY  - CONF
TI  - On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7477
EP  - 7483
AU  - A. Pore
AU  - G. Aragon-Camarasa
PY  - 2020
KW  - grippers
KW  - learning (artificial intelligence)
KW  - neural net architecture
KW  - neurocontrollers
KW  - reactive neural networks
KW  - fully connected networks
KW  - reactive behaviours
KW  - actor-critic architecture
KW  - robot environment
KW  - end-to-end reinforcement learning
KW  - robotic learning
KW  - pick and place task
KW  - behaviour-based reinforcement learning
KW  - Brook subsumption architecture
KW  - pick and place robotic task
KW  - actor-critic policy
KW  - activation mechanisms
KW  - inhibition mechanisms
KW  - gripper
KW  - degree-of-freedom
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Computer architecture
KW  - Learning (artificial intelligence)
KW  - Grasping
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197262
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a behaviour-based reinforcement learning approach, inspired by Brook's subsumption architecture, in which simple fully connected networks are trained as reactive behaviours. Our working assumption is that a pick and place robotic task can be simplified by leveraging domain knowledge of a robotics developer to decompose and train reactive behaviours; namely, approach, grasp, and retract. Then the robot autonomously learns how to combine reactive behaviours via an Actor-Critic architecture. We use an Actor-Critic policy to determine the activation and inhibition mechanisms of the reactive behaviours in a particular temporal sequence. We validate our approach in a simulated robot environment where the task is about picking a block and taking it to a target position while orienting the gripper from a top grasp. The latter represents an extra degree-of-freedom of which current end-to-end reinforcement learning approaches fail to generalise. Our findings suggest that robotic learning can be more effective if each behaviour is learnt in isolation and then combined them to accomplish the task. That is, our approach learns the pick and place task in 8,000 episodes, which represents a drastic reduction in the number of training episodes required by an end-to-end approach ( 95,000 episodes) and existing state-of-the-art algorithms.
ER  - 

TY  - CONF
TI  - Predicting optimal value functions by interpolating reward functions in scalarized multi-objective reinforcement learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7484
EP  - 7490
AU  - A. Kusari
AU  - J. P. How
PY  - 2020
KW  - Gaussian processes
KW  - interpolation
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - smooth interpolation
KW  - reward function weights
KW  - optimal value function
KW  - multiobjective reinforcement learning problems
KW  - Gaussian process
KW  - value function transforms
KW  - MORL problems
KW  - Interpolation
KW  - Training
KW  - Learning (artificial intelligence)
KW  - Gaussian processes
KW  - Mathematical model
KW  - Random variables
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197456
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A common approach for defining a reward function for multi-objective reinforcement learning (MORL) problems is the weighted sum of the multiple objectives. The weights are then treated as design parameters dependent on the expertise (and preference) of the person performing the learning, with the typical result that a new solution is required for any change in these settings. This paper investigates the relationship between the reward function and the optimal value function for MORL; specifically addressing the question of how to approximate the optimal value function well beyond the set of weights for which the optimization problem was actually solved, thereby avoiding the need to recompute for any particular choice. We prove that the value function transforms smoothly given a transformation of weights of the reward function (and thus a smooth interpolation in the policy space). A Gaussian process is used to obtain a smooth interpolation over the reward function weights of the optimal value function for three well-known examples: Gridworld, Objectworld and Pendulum. The results show that the interpolation can provide robust values for sample states and actions in both discrete and continuous domain problems. Significant advantages arise from utilizing this interpolation technique in the domain of autonomous vehicles: easy, instant adaptation of user preferences while driving and true randomization of obstacle vehicle behavior preferences during training.
ER  - 

TY  - CONF
TI  - Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7491
EP  - 7497
AU  - L. He
AU  - N. Aouf
AU  - J. F. Whidborne
AU  - B. Song
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - control engineering computing
KW  - image motion analysis
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - SLAM (robots)
KW  - visual perception
KW  - deep reinforcement learning
KW  - UAV obstacle avoidance
KW  - learning-based reaction local planner
KW  - microUAVs
KW  - image moment
KW  - illuminance variation
KW  - mapless navigation
KW  - moment-based LGMD
KW  - bioinspired monocular vision perception method
KW  - Navigation
KW  - Collision avoidance
KW  - Robustness
KW  - Lighting
KW  - Robots
KW  - Optical imaging
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197152
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.
ER  - 

TY  - CONF
TI  - Interactive Reinforcement Learning with Inaccurate Feedback
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7498
EP  - 7504
AU  - T. A. Kessler Faulkner
AU  - E. Schaertl Short
AU  - A. L. Thomaz
PY  - 2020
KW  - feedback
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - interactive reinforcement learning
KW  - human teachers
KW  - sensor feedback
KW  - learning process
KW  - noninteractive RL
KW  - policy feedback
KW  - feedback source
KW  - interactive RL methods
KW  - revision estimation-from-partially incorrect resources
KW  - REPaIR
KW  - physical robot
KW  - Maintenance engineering
KW  - Robot sensing systems
KW  - Estimation
KW  - Task analysis
KW  - Learning (artificial intelligence)
KW  - Computer science
DO  - 10.1109/ICRA40945.2020.9197219
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Interactive Reinforcement Learning (RL) enables agents to learn from two sources: rewards taken from observations of the environment, and feedback or advice from a secondary critic source, such as human teachers or sensor feedback. The addition of information from a critic during the learning process allows the agents to learn more quickly than non-interactive RL. There are many methods that allow policy feedback or advice to be combined with RL. However, critics can often give imperfect information. In this work, we introduce a framework for characterizing Interactive RL methods with imperfect teachers and propose an algorithm, Revision Estimation from Partially Incorrect Resources (REPaIR), which can estimate corrections to imperfect feedback over time. We run experiments both in simulations and demonstrate performance on a physical robot, and find that when baseline algorithms do not have prior information on the exact quality of a feedback source, using REPaIR matches or improves the expected performance of these algorithms.
ER  - 

TY  - CONF
TI  - Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7505
EP  - 7512
AU  - M. A. Lee
AU  - C. Florensa
AU  - J. Tremblay
AU  - N. Ratliff
AU  - A. Garg
AU  - F. Ramos
AU  - D. Fox
PY  - 2020
KW  - learning systems
KW  - optimisation
KW  - robots
KW  - guided uncertainty-aware policy optimization
KW  - sample-efficient policy learning
KW  - robust perception system
KW  - reinforcement learning
KW  - model-based methods
KW  - learning-based methods
KW  - model-based strategies
KW  - peg insertion
KW  - GUAPO
KW  - model-based policy
KW  - Task analysis
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Learning (artificial intelligence)
KW  - Cameras
KW  - Switches
DO  - 10.1109/ICRA40945.2020.9197125
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Traditional robotic approaches rely on an accurate model of the environment, a detailed description of how to perform the task, and a robust perception system to keep track of the current state. On the other hand, reinforcement learning approaches can operate directly from raw sensory inputs with only a reward signal to describe the task, but are extremely sampleinefficient and brittle. In this work, we combine the strengths of model-based methods with the flexibility of learning-based methods to obtain a general method that is able to overcome inaccuracies in the robotics perception/actuation pipeline, while requiring minimal interactions with the environment. This is achieved by leveraging uncertainty estimates to divide the space in regions where the given model-based policy is reliable, and regions where it may have flaws or not be well defined. In these uncertain regions, we show that a locally learned-policy can be used directly with raw sensory inputs. We test our algorithm, Guided Uncertainty-Aware Policy Optimization (GUAPO), on a real-world robot performing peg insertion. Videos are available at: https://sites.google.com/view/guapo-rl.
ER  - 

TY  - CONF
TI  - Benchmark for Skill Learning from Demonstration: Impact of User Experience, Task Complexity, and Start Configuration on Performance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7561
EP  - 7567
AU  - M. A. Rana
AU  - D. Chen
AU  - J. Williams
AU  - V. Chu
AU  - S. R. Ahmadzadeh
AU  - S. Chernova
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - user experience
KW  - task reproductions
KW  - demonstration approaches
KW  - multiple motion-based learning
KW  - task complexity
KW  - user experience
KW  - skill learning
KW  - robot executions
KW  - task performance
KW  - starting configuration
KW  - human demonstrator
KW  - physical robot
KW  - task models
KW  - manipulation tasks
KW  - real-world tasks
KW  - relative strengths
KW  - Task analysis
KW  - Robots
KW  - Trajectory
KW  - Videos
KW  - Pressing
KW  - Benchmark testing
KW  - Complexity theory
DO  - 10.1109/ICRA40945.2020.9197470
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We contribute a study benchmarking the performance of multiple motion-based learning from demonstration approaches. Given the number and diversity of existing methods, it is critical that comprehensive empirical studies be performed comparing the relative strengths of these techniques. In particular, we evaluate four approaches based on properties an end user may desire for real-world tasks. To perform this evaluation, we collected data from nine participants, across four manipulation tasks. The resulting demonstrations were used to train 180 task models and evaluated on 720 task reproductions on a physical robot. Our results detail how i) complexity of the task, ii) the expertise of the human demonstrator, and iii) the starting configuration of the robot affect task performance. The collected dataset of demonstrations, robot executions, and evaluations are publicly available. Research insights and guidelines are also provided to guide future research and deployment choices about these approaches.
ER  - 

TY  - CONF
TI  - Robot Programming without Coding
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7576
EP  - 7582
AU  - G. Lentini
AU  - G. Grioli
AU  - M. G. Catalano
AU  - A. Bicchi
PY  - 2020
KW  - control engineering computing
KW  - end effectors
KW  - learning (artificial intelligence)
KW  - robot programming
KW  - teaching
KW  - telerobotics
KW  - robot programming
KW  - wearable consumer devices
KW  - programming tools
KW  - robot teleoperation
KW  - salient features
KW  - off-the-shelf soft-articulated robotic components
KW  - Dynamic Movement Primitives
KW  - human trajectories
KW  - impedance regulation skills
KW  - 7-DOF collaborative robots
KW  - anthropomorphic end-effectors
KW  - robot teaching
KW  - Robots
KW  - Task analysis
KW  - Education
KW  - Trajectory
KW  - Programming
KW  - Three-dimensional displays
KW  - Impedance
DO  - 10.1109/ICRA40945.2020.9196904
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An approach toward intuitive and easy robot programming, consists to transfer skills from humans to machines, through demonstration. A vast literature exists on learning from multiple demonstrations. This paper, on the other hand, tackles the problem of providing all needed information to execute a certain task by resorting to one single demonstration - hence, a problem closer to programming than to learning. We use wearable consumer devices - but no keyboard nor coding - as programming tools, to let the programmer tele-operate the robot, which in turn records the most salient features and affordances from the object, environment, robot, and human. To enable this goal we combine off-the-shelf soft-articulated robotic components with the framework of Dynamic Movement Primitives, which we contribute to extend to generalize human trajectories and impedance regulation skills. This framework enables to teach robot quickly and in a intuitive way without coding. Experimental tests have been performed on a dual-arm system composed by two 7-dofs collaborative robots equipped with anthropomorphic end-effectors. Experiments show the functionality of the framework and verify the effectiveness of the impedance extension.
ER  - 

TY  - CONF
TI  - Predictive Modeling of Periodic Behavior for Human-Robot Symbiotic Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7599
EP  - 7605
AU  - G. Clark
AU  - J. Campbell
AU  - S. M. Rezayat Sorkhabadi
AU  - W. Zhang
AU  - H. B. Amor
PY  - 2020
KW  - biomechanics
KW  - gait analysis
KW  - humanoid robots
KW  - human-robot interaction
KW  - intelligent robots
KW  - learning systems
KW  - man-machine systems
KW  - medical robotics
KW  - prosthetics
KW  - human-robot symbiotic walking
KW  - probabilistic framework
KW  - periodic behavior
KW  - periodic movement regimes
KW  - customized models
KW  - human walking
KW  - latent variables
KW  - biomechanical variables
KW  - robotic prosthesis
KW  - imitation learning approach
KW  - human participants
KW  - ankle angle control signals
KW  - robotic prosthetic ankle
KW  - predictive modeling
KW  - Periodic Interaction Primitives
KW  - Legged locomotion
KW  - Robot sensing systems
KW  - Biomechanics
KW  - Predictive models
KW  - Prosthetics
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196676
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose in this paper Periodic Interaction Primitives - a probabilistic framework that can be used to learn compact models of periodic behavior. Our approach extends existing formulations of Interaction Primitives to periodic movement regimes, i.e., walking. We show that this model is particularly well-suited for learning data-driven, customized models of human walking, which can then be used for generating predictions over future states or for inferring latent, biomechanical variables. We also demonstrate how the same framework can be used to learn controllers for a robotic prosthesis using an imitation learning approach. Results in experiments with human participants indicate that Periodic Interaction Primitives efficiently generate predictions and ankle angle control signals for a robotic prosthetic ankle, with MAE of 2.21¬∞ in 0.0008s per inference. Performance degrades gracefully in the presence of noise or sensor fall outs. Compared to alternatives, this algorithm functions 20 times faster and performed 4.5 times more accurately on test subjects.
ER  - 

TY  - CONF
TI  - Agile 3D-Navigation of a Helical Magnetic Swimmer
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7638
EP  - 7644
AU  - J. Leclerc
AU  - H. Zhao
AU  - D. Z. Bao
AU  - A. T. Becker
AU  - M. Ghosn
AU  - D. J. Shah
PY  - 2020
KW  - biomechanics
KW  - blood
KW  - blood vessels
KW  - cardiology
KW  - haemodynamics
KW  - patient diagnosis
KW  - complex 3D trajectory
KW  - blood-mimicking solution
KW  - millimeter-scale magnetic helical swimmer navigating
KW  - cardiac structures
KW  - blood vessels
KW  - blood flow
KW  - respiratory motions
KW  - pulmonary embolus
KW  - rotational movement
KW  - magnetic swimmers
KW  - helical magnetic swimmer
KW  - agile 3D-navigation
KW  - Navigation
KW  - Trajectory
KW  - Heart
KW  - Blood
KW  - Valves
KW  - Frequency measurement
KW  - Turning
DO  - 10.1109/ICRA40945.2020.9197323
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Rotating miniature magnetic swimmers are de-vices that could navigate within the bloodstream to access remote locations of the body and perform minimally invasive procedures. The rotational movement could be used, for example, to abrade a pulmonary embolus. Some regions, such as the heart, are challenging to navigate. Cardiac and respiratory motions of the heart combined with a fast and variable blood flow necessitate a highly agile swimmer. This swimmer should minimize contact with the walls of the blood vessels and the cardiac structures to mitigate the risk of complications. This paper presents experimental tests of a millimeter-scale magnetic helical swimmer navigating in a blood-mimicking solution and describes its turning capabilities. The step-out frequency and the position error were measured for different values of turn radius. The paper also introduces rapid movements that increase the swimmer's agility and demonstrates these experimentally on a complex 3D trajectory.
ER  - 

TY  - CONF
TI  - Inferring the Geometric Nullspace of Robot Skills from Human Demonstrations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7668
EP  - 7675
AU  - C. Cai
AU  - Y. S. Liang
AU  - N. Somani
AU  - W. Yan
PY  - 2020
KW  - geometry
KW  - humanoid robots
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - geometric nullspace
KW  - robot skills
KW  - human demonstrations
KW  - fit geometric nullspaces
KW  - geometric constraints
KW  - powerful mathematical model
KW  - geometric skill description
KW  - skill model
KW  - learnt skill
KW  - simulated industrial robot
KW  - iCub humanoid robot
KW  - geometric constraint models
KW  - Task analysis
KW  - Manifolds
KW  - Adaptation models
KW  - Service robots
KW  - Grasping
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197174
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper we present a framework to learn skills from human demonstrations in the form of geometric nullspaces, which can be executed using a robot. We collect data of human demonstrations, fit geometric nullspaces to them, and also infer their corresponding geometric constraint models. These geometric constraints provide a powerful mathematical model as well as an intuitive representation of the skill in terms of the involved objects. To execute the skill using a robot, we combine this geometric skill description with the robot's kinematics and other environmental constraints, from which poses can be sampled for the robot's execution. The result of our framework is a system that takes the human demonstrations as input, learns the underlying skill model, and executes the learnt skill with different robots in different dynamic environments. We evaluate our approach on a simulated industrial robot, and execute the final task on the iCub humanoid robot.
ER  - 

TY  - CONF
TI  - A Dynamical System Approach for Adaptive Grasping, Navigation and Co-Manipulation with Humanoid Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7676
EP  - 7682
AU  - N. Figueroa
AU  - S. Faraji
AU  - M. Koptev
AU  - A. Billard
PY  - 2020
KW  - compliance control
KW  - dexterous manipulators
KW  - humanoid robots
KW  - position control
KW  - dynamical system approach
KW  - adaptive grasping
KW  - humanoid robots
KW  - iCub humanoid robot
KW  - state-dependent dynamical systems
KW  - robots hands
KW  - intermediate virtual object
KW  - motion generators
KW  - object moves
KW  - whole-body compliant control strategy
KW  - manipulation tasks
KW  - body manipulation
KW  - iCub robots walk-to-grasp objects
KW  - Robot kinematics
KW  - Legged locomotion
KW  - Grasping
KW  - Task analysis
KW  - Humanoid robots
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197038
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an integrated approach that provides compliant control of an iCub humanoid robot and adaptive reaching, grasping, navigating and co-manipulating capabilities. We use state-dependent dynamical systems (DS) to (i) coordinate and drive the robots hands (in both position and orientation) to grasp an object using an intermediate virtual object, and (ii) drive the robot's base while walking/navigating. The use of DS as motion generators allows us to adapt smoothly as the object moves and to re-plan on-line motion of the arms and body to reach the object's new location. The desired motion generated by the DS are used in combination with a whole-body compliant control strategy that absorbs perturbations while walking and offers compliant behaviors for grasping and manipulation tasks. Further, the desired dynamics for the arm and body can be learned from demonstrations. By integrating these components, we achieve unprecedented adaptive behaviors for whole body manipulation. We showcase this in simulations and real-world experiments where iCub robots (i) walk-to-grasp objects, (ii) follow a human (or another iCub) through interaction and (iii) learn to navigate or comanipulate an object from human guided demonstrations; whilst being robust to changing targets and perturbations.
ER  - 

TY  - CONF
TI  - Subspace Projectors for State-Constrained Multi-Robot Consensus
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7705
EP  - 7711
AU  - F. Morbidi
PY  - 2020
KW  - distributed algorithms
KW  - distributed control
KW  - mobile robots
KW  - multi-robot systems
KW  - subspace projectors
KW  - state-constrained multirobot consensus
KW  - distributed algorithms
KW  - subspace projection methods
KW  - consensus value
KW  - constrained 2D rendezvous
KW  - single-integrator robots
KW  - discrete-time agreement protocol
KW  - Symmetric matrices
KW  - Eigenvalues and eigenfunctions
KW  - Protocols
KW  - Matrix decomposition
KW  - Laplace equations
KW  - Two dimensional displays
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196758
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we study the state-constrained consensus problem and introduce a new family of distributed algorithms based on subspace projection methods which are simple to implement and which preserve, under some suitable conditions, the consensus value of the original discrete-time agreement protocol. The proposed theory is supported by extensive numerical experiments for the constrained 2D rendezvous of single-integrator robots.
ER  - 

TY  - CONF
TI  - Multi-Agent Task Allocation using Cross-Entropy Temporal Logic Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7712
EP  - 7718
AU  - C. Banks
AU  - S. Wilson
AU  - S. Coogan
AU  - M. Egerstedt
PY  - 2020
KW  - discrete systems
KW  - entropy
KW  - graph theory
KW  - multi-agent systems
KW  - multi-robot systems
KW  - optimisation
KW  - search problems
KW  - stochastic programming
KW  - temporal logic
KW  - task specification
KW  - discrete transition systems
KW  - finite linear temporal logic specifications
KW  - stochastic optimization
KW  - graph based search
KW  - cross entropy temporal logic optimization
KW  - multiagent task allocation cross entropy algorithm
KW  - robot team
KW  - Task analysis
KW  - Automata
KW  - Cost function
KW  - Planning
KW  - Resource management
KW  - Switches
DO  - 10.1109/ICRA40945.2020.9197066
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a graph-based search method to optimally allocate tasks to a team of robots given a global task specification. In particular, we define these agents as discrete transition systems. In order to allocate tasks to the team of robots, we decompose finite linear temporal logic (LTL) specifications and consider agent specific cost functions. We propose to use the stochastic optimization technique, cross entropy, to optimize over this cost function. The multi-agent task allocation cross-entropy (MTAC-E) algorithm is developed to determine both when it is optimal to switch to a new agent to complete a task and minimize the costs associated with individual agent trajectories. The proposed algorithm is verified in simulation and experimental results are included.
ER  - 

TY  - CONF
TI  - Adaptive Task Allocation for Heterogeneous Multi-Robot Teams with Evolving and Unknown Robot Capabilities
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7719
EP  - 7725
AU  - Y. Emam
AU  - S. Mayya
AU  - G. Notomista
AU  - A. Bohannon
AU  - M. Egerstedt
PY  - 2020
KW  - adaptive systems
KW  - multi-robot systems
KW  - adaptive task allocation
KW  - task execution
KW  - robot capabilities
KW  - heterogeneous multirobot teams
KW  - Task analysis
KW  - Resource management
KW  - Cost function
KW  - Mobile robots
KW  - Real-time systems
KW  - Minimization
DO  - 10.1109/ICRA40945.2020.9197283
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For multi-robot teams with heterogeneous capabilities, typical task allocation methods assign tasks to robots based on the suitability of the robots to perform certain tasks as well as the requirements of the task itself. However, in real-world deployments of robot teams, the suitability of a robot might be unknown prior to deployment, or might vary due to changing environmental conditions. This paper presents an adaptive task allocation and task execution framework which allows individual robots to prioritize among tasks while explicitly taking into account their efficacy at performing the tasks-the parameters of which might be unknown before deployment and/or might vary over time. Such a specialization parameter-encoding the effectiveness of a given robot towards a task-is updated on-the-fly, allowing our algorithm to reassign tasks among robots with the aim of executing them. The developed framework requires no explicit model of the changing environment or of the unknown robot capabilities-it only takes into account the progress made by the robots at completing the tasks. Simulations and experiments demonstrate the efficacy of the proposed approach during variations in environmental conditions and when robot capabilities are unknown before deployment.
ER  - 

TY  - CONF
TI  - Mobile Wireless Network Infrastructure on Demand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7726
EP  - 7732
AU  - D. Mox
AU  - M. Calvo-Fullana
AU  - M. Gerasimenko
AU  - J. Fink
AU  - V. Kumar
AU  - A. Ribeiro
PY  - 2020
KW  - mobile ad hoc networks
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - optimisation
KW  - telecommunication network routing
KW  - mobile relay nodes
KW  - network team
KW  - wireless connectivity
KW  - task agents
KW  - Mobile wireless network infrastructure
KW  - multirobot teams
KW  - previous multiagent systems
KW  - communication infrastructure
KW  - end-to-end communication requirements
KW  - task team
KW  - arbitrary objective
KW  - joint optimization framework
KW  - optimal network routes
KW  - Task analysis
KW  - Ad hoc networks
KW  - Routing
KW  - Wireless networks
KW  - Hardware
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9197460
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we introduce Mobile Wireless Infrastructure on Demand: a framework for providing wireless connectivity to multi-robot teams via autonomously reconfiguring ad-hoc networks. In many cases, previous multi-agent systems either assumed the availability of existing communication infrastructure or were required to create a network in addition to completing their objective. Instead our system explicitly assumes the responsibility of creating and sustaining a wireless network capable of satisfying end-to-end communication requirements of a team of agents, called the task team, performing an arbitrary objective. To accomplish this goal, we propose a joint optimization framework that alternates between finding optimal network routes to support data flows between the task agents and improving the performance of the network by repositioning a collection of mobile relay nodes referred to as the network team. We demonstrate our approach with simulations and experiments wherein wireless connectivity is provided to patrolling task agents.
ER  - 

TY  - CONF
TI  - Monitoring Over the Long Term: Intermittent Deployment and Sensing Strategies for Multi-Robot Teams
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7733
EP  - 7739
AU  - J. Liu
AU  - R. K. Williams
PY  - 2020
KW  - combinatorial mathematics
KW  - Gaussian processes
KW  - greedy algorithms
KW  - matrix algebra
KW  - Monte Carlo methods
KW  - multi-robot systems
KW  - optimisation
KW  - multirobot team
KW  - intermittent deployment problem
KW  - heterogeneous robots
KW  - environmental process
KW  - spatiotemporal process
KW  - intermittent deployment strategy
KW  - spatiotemporal Gaussian process
KW  - Monte Carlo simulations
KW  - greedy algorithm
KW  - submodular optimization
KW  - matroids
KW  - Robot sensing systems
KW  - Monitoring
KW  - Mutual information
KW  - Spatiotemporal phenomena
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196826
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we formulate and solve the intermittent deployment problem, which yields strategies that couple when heterogeneous robots should sense an environmental process, with where a deployed team should sense in the environment. As a motivation, suppose that a spatiotemporal process is slowly evolving and must be monitored by a multi-robot team, e.g., unmanned aerial vehicles monitoring pasturelands in a precision agriculture context. In such a case, an intermittent deployment strategy is necessary as persistent deployment or monitoring is not cost-efficient for a slowly evolving process. At the same time, the problem of where to sense once deployed must be solved as process observations yield useful feedback for determining effective future deployment and monitoring decisions. In this context, we model the environmental process to be monitored as a spatiotemporal Gaussian process with mutual information as a criterion to measure our understanding of the environment. To make the sensing resource-efficient, we demonstrate how to use matroid constraints to impose a diverse set of homogeneous and heterogeneous constraints. In addition, to reflect the cost-sensitive nature of real-world applications, we apply budgets on the cost of deployed heterogeneous robot teams. To solve the resulting problem, we exploit the theories of submodular optimization and matroids and present a greedy algorithm with bounds on sub-optimality. Finally, Monte Carlo simulations demonstrate the correctness of the proposed method.
ER  - 

TY  - CONF
TI  - Multi-Robot Coordination for Estimation and Coverage of Unknown Spatial Fields
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7740
EP  - 7746
AU  - A. Benevento
AU  - M. Santos
AU  - G. Notarstefano
AU  - K. Paynabar
AU  - M. Bloch
AU  - M. Egerstedt
PY  - 2020
KW  - Bayes methods
KW  - computational geometry
KW  - Gaussian processes
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - sampling methods
KW  - multirobot coordination
KW  - unknown spatial fields
KW  - multirobot coverage
KW  - initially unknown spatial scalar field
KW  - Bayesian optimization
KW  - control law
KW  - centroidal Voronoi tessellation
KW  - adaptive sequential sampling method
KW  - surrogate function
KW  - density function
KW  - Gaussian processes
KW  - Density functional theory
KW  - Estimation
KW  - Gaussian processes
KW  - Robot sensing systems
KW  - Prediction algorithms
KW  - Bayes methods
DO  - 10.1109/ICRA40945.2020.9197487
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an algorithm for multi-robot coverage of an initially unknown spatial scalar field characterized by a density function, whereby a team of robots simultaneously estimates and optimizes its coverage of the density function over the domain. The proposed algorithm borrows powerful concepts from Bayesian Optimization with Gaussian Processes that, when combined with control laws to achieve centroidal Voronoi tessellation, give rise to an adaptive sequential sampling method to explore and cover the domain. The crux of the approach is to apply a control law using a surrogate function of the true density function, which is then successively refined as robots gather more samples for estimation. The performance of the algorithm is justified theoretically under slightly idealized assumptions, by demonstrating asymptotic no-regret with respect to the coverage obtained with a known density function. The performance is also evaluated in simulation and on the Robotarium with small teams of robots, confirming the good performance suggested by the theoretical analysis.
ER  - 

TY  - CONF
TI  - Learning Robotic Assembly Tasks with Lower Dimensional Systems by Leveraging Physical Softness and Environmental Constraints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7747
EP  - 7753
AU  - M. Hamaya
AU  - R. Lee
AU  - K. Tanaka
AU  - F. von Drigalski
AU  - C. Nakashima
AU  - Y. Shibata
AU  - Y. Ijiri
PY  - 2020
KW  - force control
KW  - force sensors
KW  - industrial manipulators
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robotic assembly
KW  - torque control
KW  - high frequency force-torque sensors
KW  - physical softness
KW  - data-driven approaches
KW  - hard robots
KW  - learning robotic assembly tasks
KW  - peg-in-hole tasks
KW  - model-based reinforcement learning method
KW  - lower dimensional systems
KW  - environmental constraints
KW  - soft robot
KW  - high frequency force-torque controllers
KW  - Task analysis
KW  - Soft robotics
KW  - Aerospace electronics
KW  - Force
KW  - Robotic assembly
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9197327
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, we present a novel control framework for assembly tasks with a soft robot. Typically, existing hard robots require high frequency controllers and precise force/torque sensors for assembly tasks. The resulting robot system is complex, entailing large amounts of engineering and maintenance. Physical softness allows the robot to interact with the environment easily. We expect soft robots to perform assembly tasks without the need for high frequency force/torque controllers and sensors. However, specific data-driven approaches are needed to deal with complex models involving nonlinearity and hysteresis. If we were to apply these approaches directly, we would be required to collect very large amounts of training data. To solve this problem, we argue that by leveraging softness and environmental constraints, a robot can complete tasks in lower dimensional state and action spaces, which could greatly facilitate the exploration of appropriate assembly skills. Then, we apply a highly efficient model-based reinforcement learning method to lower dimensional systems. To verify our method, we perform a simulation for peg-in-hole tasks. The results show that our method learns the appropriate skills faster than an approach that does not consider lower dimensional systems. Moreover, we demonstrate that our method works on a real robot equipped with a compliant module on the wrist.
ER  - 

TY  - CONF
TI  - Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7754
EP  - 7760
AU  - J. Austin
AU  - R. Corrales-Fatou
AU  - S. Wyetzner
AU  - H. Lipson
PY  - 2020
KW  - control engineering computing
KW  - graphics processing units
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - optimisation
KW  - parallel algorithms
KW  - parallel architectures
KW  - CUDA-based C++ robotics simulation library
KW  - multiagent robots
KW  - massively parallel integration scheme
KW  - reinforcement learning iterations
KW  - rapid topology optimization
KW  - simultaneous optimization
KW  - innovative GPU architecture design
KW  - robotics primitives
KW  - GPU-accelerated simulations
KW  - asynchronous computing model
KW  - GPU-accelerated interface
KW  - interacting bodies
KW  - multiagent robotics
KW  - intrinsically serial tasks
KW  - low-dimensional tasks
KW  - robotics simulation libraries
KW  - NVIDIA CUDA
KW  - soft-body robotics
KW  - parallel asynchronous library
KW  - Titan
KW  - Graphics processing units
KW  - Robots
KW  - Libraries
KW  - Springs
KW  - Computational modeling
KW  - Kernel
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9196808
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While most robotics simulation libraries are built for low-dimensional and intrinsically serial tasks, soft-body and multi-agent robotics have created a demand for simulation environments that can model many interacting bodies in parallel. Despite the increasing interest in these fields, no existing simulation library addresses the challenge of providing a unified, highly-parallelized, GPU-accelerated interface for simulating large robotic systems. Titan is a versatile CUDA-based C++ robotics simulation library that employs a novel asynchronous computing model for GPU-accelerated simulations of robotics primitives. The innovative GPU architecture design permits simultaneous optimization and control on the CPU while the GPU runs asynchronously, enabling rapid topology optimization and reinforcement learning iterations. Kinematics are solved with a massively parallel integration scheme that incorporates constraints and environmental forces. We report dramatically improved performance over CPU baselines, simulating as many as 300 million primitive updates per second, while allowing flexibility for a wide range of research applications. We present several applications of Titan to high-performance simulations of soft-body and multi-agent robots.
ER  - 

TY  - CONF
TI  - Motion Planning with Competency-Aware Transition Models for Underactuated Adaptive Hands
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7761
EP  - 7767
AU  - A. Sintov
AU  - A. Kimmel
AU  - K. E. Bekris
AU  - A. Boularias
PY  - 2020
KW  - dexterous manipulators
KW  - optimal control
KW  - path planning
KW  - competency-aware transition models
KW  - underactuated adaptive hands
KW  - in-hand manipulation
KW  - data-driven models
KW  - asymptotically optimal motion planner
KW  - motion planning
KW  - grasping tasks
KW  - dexterity
KW  - Data models
KW  - Planning
KW  - Predictive models
KW  - Adaptation models
KW  - Trajectory
KW  - Training
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196564
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Underactuated adaptive hands simplify grasping tasks but it is difficult to model their interactions with objects during in-hand manipulation. Learned data-driven models have been recently shown to be efficient in motion planning and control of such hands. Still, the accuracy of the models is limited even with the addition of more data. This becomes important for long horizon predictions, where errors are accumulated along the length of a path. Instead of throwing more data into learning the transition model, this work proposes to rather invest a portion of the training data in a critic model. The critic is trained to estimate the error of the transition model given a state and a sequence of future actions, along with information of past actions. The critic is used to reformulate the cost function of an asymptotically optimal motion planner. Given the critic, the planner directs planned paths to less erroneous regions in the state space. The approach is evaluated against standard motion planning on simulated and real hands. The results show that it outperforms an alternative where all the available data is used for training the transition model without a critic.
ER  - 

TY  - CONF
TI  - Human-like Planning for Reaching in Cluttered Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7784
EP  - 7790
AU  - M. Hasan
AU  - M. Warburton
AU  - W. C. Agboh
AU  - M. R. Dogar
AU  - M. Leonetti
AU  - H. Wang
AU  - F. Mushtaq
AU  - M. Mon-Williams
AU  - A. G. Cohn
PY  - 2020
KW  - collision avoidance
KW  - decision making
KW  - dexterous manipulators
KW  - grippers
KW  - learning (artificial intelligence)
KW  - planning (artificial intelligence)
KW  - robot programming
KW  - robot vision
KW  - trajectory control
KW  - virtual reality
KW  - decision making
KW  - decision classifiers
KW  - cluttered environments
KW  - robot planners
KW  - random sampling
KW  - object manipulation plans
KW  - virtual reality
KW  - trajectory optimisation
KW  - physics based robot simulation
KW  - human-like planning
KW  - depth camera
KW  - Robotiq two finger gripper
KW  - Task analysis
KW  - Planning
KW  - Robots
KW  - Testing
KW  - Feature extraction
KW  - Trajectory
KW  - Standards
DO  - 10.1109/ICRA40945.2020.9196665
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Humans, in comparison to robots, are remarkably adept at reaching for objects in cluttered environments. The best existing robot planners are based on random sampling of configuration space- which becomes excessively high-dimensional with large number of objects. Consequently, most planners often fail to efficiently find object manipulation plans in such environments. We addressed this problem by identifying high-level manipulation plans in humans, and transferring these skills to robot planners. We used virtual reality to capture human participants reaching for a target object on a tabletop cluttered with obstacles. From this, we devised a qualitative representation of the task space to abstract the decision making, irrespective of the number of obstacles. Based on this representation, human demonstrations were segmented and used to train decision classifiers. Using these classifiers, our planner produced a list of waypoints in task space. These waypoints provided a high-level plan, which could be transferred to an arbitrary robot model and used to initialise a local trajectory optimiser. We evaluated this approach through testing on unseen human VR data, a physics-based robot simulation, and a real robot (dataset and code are publicly available1). We found that the human-like planner outperformed a state-of-the-art standard trajectory optimisation algorithm, and was able to generate effective strategies for rapid planning- irrespective of the number of obstacles in the environment.
ER  - 

TY  - CONF
TI  - Where to relocate?: Object rearrangement inside cluttered and confined environments for robotic manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7791
EP  - 7797
AU  - S. Hun Cheong
AU  - B. Y. Cho
AU  - J. Lee
AU  - C. Kim
AU  - C. Nam
PY  - 2020
KW  - collision avoidance
KW  - computational complexity
KW  - graph theory
KW  - industrial robots
KW  - manipulators
KW  - mobile robots
KW  - optimisation
KW  - robot vision
KW  - warehousing
KW  - object rearrangement
KW  - cluttered confined environments
KW  - robotic manipulation
KW  - cluttered confined space
KW  - motion planning
KW  - manipulator
KW  - nonmonotone arrangement problems
KW  - pick-and-place actions
KW  - baseline methods
KW  - Planning
KW  - Manipulators
KW  - Collision avoidance
KW  - Task analysis
KW  - Clutter
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197485
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an algorithm determining where to relocate objects inside a cluttered and confined space while rearranging objects to retrieve a target object. Although methods that decide what to remove have been proposed, planning for the placement of removed objects inside a workspace has not received much attention. Rather, removed objects are often placed outside the workspace, which incurs additional laborious work (e.g., motion planning and execution of the manipulator and the mobile base, perception of other areas). Some other methods manipulate objects only inside the workspace but without a principle so the rearrangement becomes inefficient. In this work, we consider both monotone (each object is moved only once) and non-monotone arrangement problems which have shown to be NP-hard. Once the sequence of objects to be relocated is given by any existing algorithm, our method aims to minimize the number of pick-and-place actions to place the objects until the target becomes accessible. From extensive experiments, we show that our method reduces the number of pick-and-place actions and the total execution time (the reduction is up to 23.1% and 28.1% respectively) compared to baseline methods while achieving higher success rates.
ER  - 

TY  - CONF
TI  - Autonomous Modification of Unstructured Environments with Found Material
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7798
EP  - 7804
AU  - V. Thangavelu
AU  - M. S. da Silva
AU  - J. Choi
AU  - N. Napp
PY  - 2020
KW  - building materials
KW  - dexterous manipulators
KW  - friction
KW  - grippers
KW  - industrial manipulators
KW  - materials handling equipment
KW  - mechanical contact
KW  - path planning
KW  - sensors
KW  - unseen objects
KW  - adaptive ramp building algorithms
KW  - irregularly shaped stones
KW  - contact geometry
KW  - friction
KW  - high-level algorithm
KW  - physics-based planner
KW  - pickup
KW  - robotic system
KW  - complex grasp planning
KW  - autonomous modification
KW  - manipulation
KW  - found material
KW  - pickup
KW  - motion support structures
KW  - specialized construction algorithm
KW  - unstructured environments
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Physics
KW  - Shape
KW  - Buildings
KW  - physics simulation
KW  - autonomous construction
KW  - robotics
KW  - irregular building materials.
DO  - 10.1109/ICRA40945.2020.9197372
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to autonomously modify their environment dramatically increases the capability of robots to operate in unstructured environments. We develop a specialized construction algorithm and robotic system that can autonomously build motion support structures with previously unseen objects. The approach is based on our prior work on adaptive ramp building algorithms, but it eliminates the assumption of having specialized building materials that simplify manipulation and planning for stability. Utilizing irregularly shaped stones makes the problem significantly more challenging since the outcome of individual placements is sensitive to details of contact geometry and friction, which are difficult to observe. To reuse the same high-level algorithm, we develop a new physics-based planner that explicitly considers the uncertainty produced by incomplete in-situ sensing and imprecision during pickup and placement. We demonstrate the approach on a robotic system that uses a newly developed gripper to reliably pick up stones with minimal additional sensors or complex grasp planning. The resulting system can build structures with more than 70 stones, which in turn provide traversable paths to previously inaccessible locations.
ER  - 

TY  - CONF
TI  - LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7829
EP  - 7836
AU  - J. Zhang
AU  - M. S. Ramanagopal
AU  - R. Vasudevan
AU  - M. Johnson-Roberson
PY  - 2020
KW  - image matching
KW  - image resolution
KW  - optical radar
KW  - stereo image processing
KW  - dense depth maps
KW  - depth information
KW  - light detection and ranging
KW  - accurate depth map
KW  - stereo imagery
KW  - stereo systems
KW  - high-quality dense depth maps
KW  - stereo matching algorithms
KW  - sparse depth map
KW  - high-resolution LIDAR
KW  - Laser radar
KW  - Task analysis
KW  - Training
KW  - Feature extraction
KW  - Estimation
KW  - Convolution
KW  - Correlation
DO  - 10.1109/ICRA40945.2020.9196628
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate high-quality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.
ER  - 

TY  - CONF
TI  - Monocular Visual-Inertial Odometry in Low-Textured Environments with Smooth Gradients: A Fully Dense Direct Filtering Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7837
EP  - 7843
AU  - A. Hardt-Stremayr
AU  - S. Weiss
PY  - 2020
KW  - calibration
KW  - distance measurement
KW  - gradient methods
KW  - image filtering
KW  - image texture
KW  - inertial systems
KW  - interpolation
KW  - vectors
KW  - low-textured environments
KW  - fully dense direct filtering approach
KW  - visual texture
KW  - direct photometric approaches
KW  - image information
KW  - information propagation
KW  - complexity reduction approach
KW  - state vector
KW  - monocular visual-inertial odometry approaches
KW  - higher order covariance propagation
KW  - state handling improvement
KW  - Cameras
KW  - Uncertainty
KW  - Estimation
KW  - Mathematical model
KW  - Motion estimation
KW  - Integrated circuits
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196881
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State of the art visual-inertial odometry approaches suffer from the requirement of high gradients and sufficient visual texture. Even direct photometric approaches select a subset of the image with high-gradient areas and ignore smooth gradients or generally low-textured areas. In this work, we show that taking all image information (i.e. every single pixel) enables visual-inertial odometry even on areas with very low texture and smooth gradients, inherently interpolating and estimating the scene with no texture based on its informative surrounding. This information propagation is only possible as we estimate all states and their uncertainties (robot pose, extrinsic sensor calibration, and scene depth) jointly in a fully dense filter framework. Our complexity reduction approach enables real-time execution despite the large size of the state vector. Compared to our previous basic feasibility study on this topic, this work includes higher order covariance propagation and improved state handling for a significant performance gain, thorough comparisons to state-of-the-art algorithms, larger mapping components with uncertainty, self-calibration capability, and real-data tests.
ER  - 

TY  - CONF
TI  - Interaction Stability Analysis from the Input-Output Viewpoints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7878
EP  - 7884
AU  - Y. Huang
AU  - Q. Huang
PY  - 2020
KW  - bond graphs
KW  - haptic interfaces
KW  - Hilbert spaces
KW  - human-robot interaction
KW  - input-output stability
KW  - robotic assembly
KW  - mechanical impedance
KW  - linear spatial impedance representation
KW  - bond graph theory
KW  - ideal model
KW  - idealized interaction
KW  - port functions
KW  - ideal interaction model
KW  - collaborative interactions
KW  - competitive interactions
KW  - interaction models
KW  - passivity indices
KW  - interaction stability analysis
KW  - input-output viewpoints
KW  - robot applications
KW  - haptic devices
KW  - parts assembly
KW  - interaction behaviours
KW  - interaction dynamics
KW  - DAlembert principle
KW  - nonsmooth mechanics
KW  - kinematic constraints
KW  - bond graph methodology
KW  - Hilbert function space
KW  - continuous function
KW  - input-output stability condition
KW  - Impedance
KW  - Admittance
KW  - Robots
KW  - Force
KW  - Kinematics
KW  - Stability criteria
DO  - 10.1109/ICRA40945.2020.9196643
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Interaction with the environment is arguably one of the necessary actions for many robot applications such as haptic devices, manipulation, parts assembly, cooperation with humans, and the use of tools. Taxonomy of interaction behaviours is classified into three categories: cooperation, collaboration, and competition. In theory, interaction dynamics may be modelled by D'Alembert's principle or nonsmooth mechanics through seeking equality and/or inequality kinematic constraints. However, it is hard to gain these kinematic constraints in practice since they may be variable or be hardly described in a mathematical form. As a result, bond graph methodology is preferred in interaction dynamics modelling.In this paper, passivity and passivity indices with the differential operator are put forward by restricting its domain from the whole extended Hilbert function space to a set of all continuous function with finite derivative, and then the input-output stability condition, in this case, is derived. Next, mechanical impedance and admittance are defined, and a linear spatial impedance representation is given from the energetic point of view. Base on the bond graph theory, an ideal model is presented to model the idealized interaction, and invariance of port functions derived from the ideal interaction model is introduced; An interaction model is then proposed accounting for nonidealized factors and to describe cooperative, collaborative, and competitive interactions in a unified way. Finally, interaction stabilities are analyzed corresponding to different interaction models, and robustness of interaction stability is addressed based on the passivity indices.
ER  - 

TY  - CONF
TI  - Improving the contact instant detection of sensing antennae using a Super-Twisting algorithm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7885
EP  - 7890
AU  - D. Feliu-Talegon
AU  - R. Cortez-Vega
AU  - V. Feliu-Batlle
PY  - 2020
KW  - filtering theory
KW  - multi-robot systems
KW  - signal processing
KW  - variable structure systems
KW  - contact instant detection
KW  - antenna devices
KW  - mimic insect antennae
KW  - mammal whiskers
KW  - robotic systems
KW  - signal processing
KW  - flexible antenna
KW  - impact detection
KW  - impact instant estimation
KW  - super-twisting algorithm
KW  - time 5.0 ms
KW  - Antennas
KW  - Estimation
KW  - Vibrations
KW  - Robot sensing systems
KW  - Antenna measurements
KW  - Delays
DO  - 10.1109/ICRA40945.2020.9197107
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Sensing antenna devices, that mimic insect antennae or mammal whiskers, is an active field of research that still needs new developments in order to become efficient and reliable components of robotic systems. This work reports a new result in the area of signal processing of these devices that allows to detect the instant of the impact of a flexible antenna with an object faster than other reported methods. Previous methods require the use of filters that introduce delays in the impact detection. A method based on the Super-Twisting algorithm is proposed here that avoids the use of these filters and reduces such delays improving the impact instant estimation. Experiments show that these delays can be reduced in more than 50%, allowing reliable estimation of the impact instant with an error of less than 5 ms in many cases requiring a limited computational effort.
ER  - 

TY  - CONF
TI  - 6DFC: Efficiently Planning Soft Non-Planar Area Contact Grasps using 6D Friction Cones
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7891
EP  - 7897
AU  - J. Xu
AU  - M. Danielczuk
AU  - E. Steinbach
AU  - K. Goldberg
PY  - 2020
KW  - friction
KW  - grippers
KW  - manipulator dynamics
KW  - path planning
KW  - quadratic programming
KW  - robot vision
KW  - 6D friction cone
KW  - approximate compliant contacts
KW  - soft point contact models
KW  - area contact model
KW  - 6D friction limit surface
KW  - 6DFC algorithm
KW  - soft nonplanar area contact grasp planning
KW  - 3D friction cones
KW  - ABB YuMi robot
KW  - quadratic program
KW  - Friction
KW  - Grippers
KW  - Three-dimensional displays
KW  - Force
KW  - Solid modeling
KW  - Computational modeling
KW  - Ellipsoids
DO  - 10.1109/ICRA40945.2020.9197293
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Analytic grasp planning algorithms typically approximate compliant contacts with soft point contact models to compute grasp quality, but these models are overly conservative and do not capture the full range of grasps available. While area contact models can reduce the number of false negatives predicted by point contact models, they have been restricted to a 3D analysis of the wrench applied at the contact and so are still overly conservative. We extend traditional 3D friction cones and present an efficient algorithm for calculating the 6D friction cone (6DFC) for a non-planar area contact between a compliant gripper and a rigid object. We introduce a novel sampling algorithm to find the 6D friction limit surface for a non-planar area contact and a linearization method for these ellipsoids that reduces the computation of 6DFC constraints to a quadratic program. We show that constraining the wrench applied at the contact in this way increases recall, a metric inversely related to the number of false negative predictions, by 17% and precision, a metric inversely related to the number of false positive predictions, by 2% over soft point contact models on results from 1500 physical grasps on 12 3D printed nonplanar objects with an ABB YuMi robot. The 6DFC algorithm also achieves 6% higher recall with similar precision and 85x faster runtime than a previously proposed area contact model.
ER  - 

TY  - CONF
TI  - Long-Horizon Prediction and Uncertainty Propagation with Residual Point Contact Learners
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7898
EP  - 7904
AU  - N. Fazeli
AU  - A. Ajay
AU  - A. Rodriguez
PY  - 2020
KW  - computer simulation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - self-supervised approach
KW  - rigid-body simulators
KW  - contact models
KW  - predictive performance
KW  - horizon prediction
KW  - uncertainty propagation
KW  - residual point contact learners
KW  - robotic tasks
KW  - Predictive models
KW  - Analytical models
KW  - Uncertainty
KW  - Computational modeling
KW  - Trajectory
KW  - Dynamics
KW  - Stochastic processes
DO  - 10.1109/ICRA40945.2020.9196511
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to simulate and predict the outcome of contacts is paramount to the successful execution of many robotic tasks. Simulators are powerful tools for the design of robots and their behaviors, yet the discrepancy between their predictions and observed data limit their usability. In this paper, we propose a self-supervised approach to learning residual models for rigid-body simulators that exploits corrections of contact models to refine predictive performance and propagate uncertainty. We empirically evaluate the framework by predicting the outcomes of planar dice rolls and compare it's performance to state-of-the-art techniques.
ER  - 

TY  - CONF
TI  - Versatile Trajectory Optimization Using a LCP Wheel Model for Dynamic Vehicle Maneuvers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7905
EP  - 7911
AU  - G. Bellegarda
AU  - K. Byl
PY  - 2020
KW  - automobiles
KW  - friction
KW  - mechanical contact
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - robot dynamics
KW  - trajectory control
KW  - tyres
KW  - vehicle dynamics
KW  - wheels
KW  - dynamic drift parking
KW  - discontinuous friction model
KW  - tire dynamics model
KW  - cost function
KW  - wheel skidding
KW  - versatile trajectory optimization framework
KW  - vehicle motion
KW  - anisotropic Coulomb friction cone
KW  - multirigid-body contact problems
KW  - linear complementarity problem
KW  - robotics community
KW  - contact dynamics
KW  - real world contact behavior
KW  - empirical friction model
KW  - aggressive maneuvers
KW  - car models
KW  - dynamic vehicle maneuvers
KW  - LCP wheel model
KW  - executing dynamic drift parking
KW  - planning horizon
KW  - Vehicle dynamics
KW  - Wheels
KW  - Friction
KW  - Planning
KW  - Dynamics
KW  - Trajectory optimization
KW  - Tires
DO  - 10.1109/ICRA40945.2020.9197541
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Car models have been extensively studied at varying levels of abstraction, and planning and executing motions under ideal conditions is well researched and understood. For more aggressive maneuvers, for example when drifting or skidding, empirical and/or discontinuous friction models have been used to explain and approximate real world contact behavior. Separately, contact dynamics have been extensively studied by the robotics community, often times formulated as a linear complementarity problem (LCP) for dynamic multi-rigid-body contact problems with Coulomb friction cone approximations. In this work, we explore the validity of using such an anisotropic Coulomb friction cone to model tire dynamics to plan for vehicle motion, and present a versatile trajectory optimization framework using this model that can both avoid and/or exploit wheel skidding, depending on the cost function and planning horizon. Experimental evidence of planning and executing dynamic drift parking is shown on a 1/16 scale model car.
ER  - 

TY  - CONF
TI  - Highly Parallelizable Plane Extraction for Organized Point Clouds Using Spherical Convex Hulls
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7920
EP  - 7926
AU  - H. M√∂ls
AU  - K. Li
AU  - U. D. Hanebeck
PY  - 2020
KW  - geometry
KW  - image colour analysis
KW  - pixelwise plane extraction
KW  - highly parallelizable plane extraction
KW  - organized point clouds
KW  - spherical convex hull
KW  - region growing algorithm
KW  - explicit plane parameterization
KW  - geometric constraints
KW  - GPU
KW  - RGB-D camera
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Robot sensing systems
KW  - Clustering algorithms
KW  - Real-time systems
KW  - Data mining
DO  - 10.1109/ICRA40945.2020.9197139
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel region growing algorithm for plane extraction of organized point clouds using the spherical convex hull. Instead of explicit plane parameterization, our approach interprets potential underlying planes as a series of geometric constraints on the sphere that are refined during region growing. Unlike existing schemes relying on downsampling for sequential execution in real time, our approach enables pixelwise plane extraction that is highly parallelizable. We further test the proposed approach with a fully parallel implementation on a GPU. Evaluation based on public data sets has shown state-of-the-art extraction accuracy and superior speed compared to existing approaches, while guaranteeing real-time processing at full input resolution of a typical RGB-D camera.
ER  - 

TY  - CONF
TI  - View-Invariant Loop Closure with Oriented Semantic Landmarks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7943
EP  - 7949
AU  - J. Li
AU  - K. Koreitem
AU  - D. Meger
AU  - G. Dudek
PY  - 2020
KW  - geometry
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - view-invariant loop closure
KW  - oriented semantic landmarks
KW  - simultaneous localization and mapping
KW  - monocular semantic SLAM system
KW  - object identity
KW  - inter-object geometry
KW  - view-invariant loop detection
KW  - ORB-SLAM
KW  - local appearance-based features
KW  - indoor scenes
KW  - object orientation estimation
KW  - geometrical detailed semantic maps
KW  - object translation
KW  - object scale
KW  - Cameras
KW  - Simultaneous localization and mapping
KW  - Semantics
KW  - Trajectory
KW  - Layout
KW  - Robustness
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9196886
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent work on semantic simultaneous localization and mapping (SLAM) have shown the utility of natural objects as landmarks for improving localization accuracy and robustness. In this paper we present a monocular semantic SLAM system that uses object identity and inter-object geometry for view-invariant loop detection and drift correction. Our system's ability to recognize an area of the scene even under large changes in viewing direction allows it to surpass the mapping accuracy of ORB-SLAM, which uses only local appearance-based features that are not robust to large viewpoint changes. Experiments on real indoor scenes show that our method achieves mean drift reduction of 70% when compared directly to ORB-SLAM. Additionally, we propose a method for object orientation estimation, where we leverage the tracked pose of a moving camera under the SLAM setting to overcome ambiguities caused by object symmetry. This allows our SLAM system to produce geometrically detailed semantic maps with object orientation, translation, and scale.
ER  - 

TY  - CONF
TI  - Active Acoustic Contact Sensing for Soft Pneumatic Actuators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7966
EP  - 7972
AU  - G. Z√∂ller
AU  - V. Wall
AU  - O. Brock
PY  - 2020
KW  - elastic constants
KW  - manipulator dynamics
KW  - pneumatic actuators
KW  - soft pneumatic actuators
KW  - active acoustic sensor
KW  - contact sensors
KW  - soft actuator
KW  - embedded speaker
KW  - PneuFlex actuator
KW  - active sensors
KW  - active acoustic contact sensing
KW  - Panda robot arm
KW  - embedded microphone
KW  - Actuators
KW  - Robot sensing systems
KW  - Acoustics
KW  - Microphones
KW  - Acoustic measurements
DO  - 10.1109/ICRA40945.2020.9196916
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an active acoustic sensor that turns soft pneumatic actuators into contact sensors. The whole surface of the actuator becomes a sensor, rendering the question of where best to place a contact sensor unnecessary. At the same time, the compliance of the soft actuator remains unaffected. A small, embedded speaker emits a frequency sweep which travels through the actuator before it is recorded with an embedded microphone. The specific contact state of the actuator affects how the sound is modulated while traversing the structure. We learn to recognize these changes in the sound and map them to the corresponding contact locations. We demonstrate the method on the PneuFlex actuator. The active acoustic sensor achieves a classification rate of 93% and mean regression error of 3.7mm. It is robust against background noises and different objects. Finally, we test it on a Panda robot arm and show that it is unaffected by motor noises and other active sensors.
ER  - 

TY  - CONF
TI  - A Bidirectional 3D-printed Soft Pneumatic Actuator and Graphite-based Flex Sensor for Versatile Grasping*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 7979
EP  - 7985
AU  - J. H. Low
AU  - J. Y. Goh
AU  - N. Cheng
AU  - P. M. Khin
AU  - Q. Q. Han
AU  - C. H. Yeow
PY  - 2020
KW  - actuators
KW  - bending
KW  - data gloves
KW  - grippers
KW  - motion control
KW  - pneumatic actuators
KW  - sensors
KW  - solid modelling
KW  - soft pneumatic actuator
KW  - graphite-based flex sensor
KW  - versatile grasping
KW  - 3D-printing approach
KW  - fabrication complexity
KW  - actuator dimensions
KW  - bidirectional actuators
KW  - gripper system
KW  - functional grasping tasks
KW  - default grasping width
KW  - bidirectional bending characteristic
KW  - bidirectional 3D-printed soft pneumatic actuator
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196837
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - THIS paper presents a bidirectional 3D-printed soft pneumatic actuator that is capable of inward and outward bending. A direct 3D-printing approach is adopted to fabricate the actuator, which reduces fabrication complexity and allows for easy customization of actuator dimensions for various applications. To illustrate the applicability of the bidirectional actuators, four of these actuators were incorporated into a gripper system. A suite of various functional grasping tasks, such as packaging, assembly, and alignment tasks, were successfully conducted. It was observed that the unique bidirectional bending characteristic of the actuator allows the gripper to grasp objects with sizes up to 245% larger than its default grasping width. To complement the gripper system, a graphite-based flex sensor that is able to sense bending in two directions is developed to control the bidirectional actuators. A preliminary test was conducted successfully where the user controlled the gripper system to grasp, hold, and release an object using a glove with the sensors.
ER  - 

TY  - CONF
TI  - Simultaneous Learning from Human Pose and Object Cues for Real-Time Activity Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8006
EP  - 8012
AU  - B. Reily
AU  - Q. Zhu
AU  - C. Reardon
AU  - H. Zhang
PY  - 2020
KW  - feature extraction
KW  - human-robot interaction
KW  - image motion analysis
KW  - image recognition
KW  - image representation
KW  - learning (artificial intelligence)
KW  - optimisation
KW  - pose estimation
KW  - regression analysis
KW  - human activity categories
KW  - real-time human activity recognition
KW  - human pose
KW  - object cues
KW  - real-world human-centered robotics applications
KW  - assisted living
KW  - human-robot collaboration
KW  - frequency 104.0 Hz
KW  - Activity recognition
KW  - Real-time systems
KW  - Optimization
KW  - Object recognition
KW  - Feature extraction
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9196632
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Real-time human activity recognition plays an essential role in real-world human-centered robotics applications, such as assisted living and human-robot collaboration. Although previous methods based on skeletal data to encode human poses showed promising results on real-time activity recognition, they lacked the capability to consider the context provided by objects within the scene and in use by the humans, which can provide a further discriminant between human activity categories. In this paper, we propose a novel approach to real-time human activity recognition, through simultaneously learning from observations of both human poses and objects involved in the human activity. We formulate human activity recognition as a joint optimization problem under a unified mathematical framework, which uses a regression-like loss function to integrate human pose and object cues and defines structured sparsity-inducing norms to identify discriminative body joints and object attributes. To evaluate our method, we perform extensive experiments on two benchmark datasets and a physical robot in a home assistance setting. Experimental results have shown that our method outperforms previous methods and obtains real-time performance for human activity recognition with a processing speed of 104 Hz.
ER  - 

TY  - CONF
TI  - Demonstration of Hospital Receptionist Robot with Extended Hybrid Code Network to Select Responses and Gestures
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8013
EP  - 8018
AU  - E. J. Hwang
AU  - B. Kyu Ahn
AU  - B. A. Macdonald
AU  - H. Seok Ahn
PY  - 2020
KW  - gesture recognition
KW  - human-robot interaction
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - recurrent neural nets
KW  - service robots
KW  - hospital receptionist robot
KW  - task-oriented dialogue system
KW  - human-robot interaction
KW  - pipeline
KW  - dialogue states
KW  - end-to-end learning
KW  - recurrent neural networks
KW  - social robot system
KW  - end-to-end dialogue system
KW  - RNN based gesture selector
KW  - dialogue efficiency
KW  - gestures
KW  - extended hybrid code network
KW  - Task analysis
KW  - Robot sensing systems
KW  - Pipelines
KW  - Face detection
KW  - Recurrent neural networks
KW  - Speech recognition
DO  - 10.1109/ICRA40945.2020.9197160
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Task-oriented dialogue system has a vital role in Human-Robot Interaction (HRI). However, it has been developed based on conventional pipeline approach which has several drawbacks; expensive, time-consuming, and so on. Based on this approach, developers manually define a robot's behaviour such as gestures and facial expressions on the corresponding dialogue states. Recently, end-to-end learning of Recurrent Neural Networks (RNNs) is an attractive solution for the dialogue system. In this paper, we proposed a social robot system using end-to-end dialogue system in the context of hospital receptionist. We utilized Hybrid Code Network (HCN) as an end-to-end dialogue system and extended to select both response and gesture using RNN based gesture selector. We evaluate its performance with human users and compare the results with one of the conventional methods. Empirical result shows that the proposed method has benefits in terms of dialogue efficiency, which indicates how efficient users were in performing the given tasks with the help of the robot. Moreover, we achieved the same performance regarding the robot's gesture with the proposed method compared to manually defined gestures.
ER  - 

TY  - CONF
TI  - Can I Trust You? A User Study of Robot Mediation of a Support Group
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8019
EP  - 8026
AU  - C. Birmingham
AU  - Z. Hu
AU  - K. Mahajan
AU  - E. Reber
AU  - M. J. Matariƒá
PY  - 2020
KW  - human-robot interaction
KW  - robot mediation
KW  - socially assistive robots
KW  - group dynamics
KW  - social settings
KW  - trust dynamics
KW  - robot mediated support group
KW  - dyadic trust scale
KW  - general trust
KW  - average interpersonal trust
KW  - group interaction session
KW  - multiparty setting
KW  - Educational robots
KW  - Mediation
KW  - Sensitivity
KW  - Robot sensing systems
KW  - Atmospheric measurements
KW  - Particle measurements
DO  - 10.1109/ICRA40945.2020.9196875
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Socially assistive robots have the potential to improve group dynamics when interacting with groups of people in social settings. This work contributes to the understanding of those dynamics through a user study of trust dynamics in the novel context of a robot mediated support group. For this study, a novel framework for robot mediation of a support group was developed and validated. To evaluate interpersonal trust in the multi-party setting, a dyadic trust scale was implemented and found to be uni-factorial, validating it as an appropriate measure of general trust. The results of this study demonstrate a significant increase in average interpersonal trust after the group interaction session, and qualitative post-session interview data report that participants found the interaction helpful and successfully supported and learned from one other. The results of the study validate that a robot-mediated support group can improve trust among strangers and allow them to share and receive support for their academic stress.
ER  - 

TY  - CONF
TI  - Coronal Plane Spine Twisting Composes Shape To Adjust the Energy Landscape for Grounded Reorientation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8052
EP  - 8058
AU  - J. D. Caporale
AU  - B. W. McInroe
AU  - C. Ning
AU  - T. Libby
AU  - R. J. Full
AU  - D. E. Koditschek
PY  - 2020
KW  - biomechanics
KW  - bone
KW  - orthopaedics
KW  - CPST
KW  - coronal plane spine twisting composes shape
KW  - energy landscape
KW  - grounded reorientation
KW  - animal locomotion
KW  - legged robots
KW  - self-righting mechanics
KW  - freedom coronal plane representation
KW  - body shape affordance
KW  - cross-sectional geometries
KW  - kinematic model predictions
KW  - elliptical bodies
KW  - rectangular shaped bodies
KW  - quasistatic reorientation maneuvers
KW  - Shape
KW  - Kinematics
KW  - Potential energy
KW  - Hip
KW  - Torso
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197026
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Despite substantial evidence for the crucial role played by an active backbone or spine in animal locomotion, its adoption in legged robots remains limited because the added mechanical complexity and resulting dynamical challenges pose daunting obstacles to characterizing even a partial range of potential performance benefits. This paper takes a next step toward such a characterization by exploring the quasistatic terrestrial self-righting mechanics of a model system with coronal plane spine twisting (CPST). Reduction from a full 3D kinematic model of CPST to a two parameter, two degree of freedom coronal plane representation of body shape affordance predicts a substantial benefit to ground righting by lowering the barrier between stable potential energy basins. The reduced model predicts the most advantageous twist angle for several cross-sectional geometries, reducing the required righting torque by up to an order of magnitude depending on constituent shapes. Experiments with a three actuated degree of freedom physical mechanism corroborate the kinematic model predictions using two different quasistatic reorientation maneuvers for both elliptical and rectangular shaped bodies with a range of eccentricities or aspect ratios. More speculative experiments make intuitive use of the kinematic model in a highly dynamic maneuver to suggest still greater benefits of CPST achievable by coordinating kinetic as well as potential energy, for example as in a future multi-appendage system interacting with a contact-rich 3D environment.
ER  - 

TY  - CONF
TI  - Motion Design for a Snake Robot Negotiating Complicated Pipe Structures of a Constant Diameter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8073
EP  - 8079
AU  - M. Inazawa
AU  - T. Takemori
AU  - M. Tanaka
AU  - F. Matsuno
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - pipes
KW  - motion design
KW  - snake robot
KW  - constant diameter
KW  - multiple pipe structures
KW  - target form
KW  - rolling motion
KW  - complicated pipe structures
KW  - Snake robots
KW  - Windings
KW  - Shape
KW  - Robots
KW  - Junctions
KW  - Modeling
KW  - Pins
DO  - 10.1109/ICRA40945.2020.9197224
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A method for designing the motion of a snake robot negotiating complicated pipe structures having a constant diameter is presented. For such robots moving inside pipes, there are various "obstacles" such as junctions, bends, shears, and blockages. To surmount these obstacles, we propose a method that enables the robot to adapt to multiple pipe structures of a constant diameter. We designed the target form of the snake robot of two helices connected with an arbitrary shape. This method is applicable to various obstacles by designing a part of the target form conforming to the obstacle. The robot negotiates obstacles under shift control by employing a rolling motion. We demonstrated the effectiveness of the proposed method in various experiments.
ER  - 

TY  - CONF
TI  - Single Actuator Peristaltic Robot for Subsurface Exploration and Device Emplacement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8096
EP  - 8102
AU  - J. De la Fuente
AU  - R. Shor
AU  - S. Larter
PY  - 2020
KW  - actuators
KW  - cams (mechanical)
KW  - data acquisition
KW  - design engineering
KW  - geology
KW  - hydrocarbon reservoirs
KW  - mobile robots
KW  - oil reservoirs
KW  - autonomous robots
KW  - data acquisition
KW  - tool transportation
KW  - petroleum reservoirs
KW  - cam-follower configuration worm robot
KW  - peristaltic displacement
KW  - single actuator peristaltic robot
KW  - subsurface exploration
KW  - device emplacement
KW  - initial testing
KW  - single actuator peristaltic motion robot
KW  - subsurface geological exploration
KW  - design
KW  - nonconsolidated media
KW  - Robots
KW  - Hydrocarbons
KW  - Soil
KW  - Reservoirs
KW  - Actuators
KW  - Oils
KW  - Asphalt
DO  - 10.1109/ICRA40945.2020.9196823
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we present the concept, design, and initial testing of a single actuator peristaltic motion robot for subsurface geological exploration and device emplacement. We are researching unconventional methods, including robotics, for the production of energy from oil reservoirs that do not liberate carbon to the atmosphere. For such application, we are developing autonomous robots for data acquisition and tool transportation inside petroleum reservoirs. The mechanism described in this work is a cam-follower configuration worm robot that utilizes peristaltic displacement. We confirmed that the mechanism works on a plane surface and in non-consolidated media.
ER  - 

TY  - CONF
TI  - Dynamic modeling of robotic manipulators for accuracy evaluation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8144
EP  - 8150
AU  - S. A. Zimmermann
AU  - T. F. C. Berninger
AU  - J. Derkx
AU  - D. J. Rixen
PY  - 2020
KW  - finite element analysis
KW  - flexible manipulators
KW  - manipulator dynamics
KW  - robotic manipulators
KW  - industrial robots
KW  - mechanical stiffness
KW  - multibody models
KW  - finite element model
KW  - flexible link manipulator model
KW  - industrial robot
KW  - stiffness parameters
KW  - robot behavior
KW  - weight-reduced manipulator
KW  - Solid modeling
KW  - Manipulator dynamics
KW  - Mathematical model
KW  - Service robots
KW  - Finite element analysis
DO  - 10.1109/ICRA40945.2020.9197304
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to fulfill conflicting requirements in the development of industrial robots, such as increased accuracy of a weightreduced manipulator with lower mechanical stiffness, the robot's dynamical behavior must be evaluated early in the development process. This leads to the need of accurate multibody models of the manipulator under development.This paper deals with multibody models that include flexible bodies, which are exported from the corresponding Finite Element model of the structural parts. It is shown that such a flexible link manipulator model, which is purely based on development and datasheet data, is suitable for an accurate description of an industrial robot's dynamic behavior. No stiffness parameters need to be identified by experimental methods, making this approach especially relevant during the development of new manipulators. This paper presents results of experiments in time and frequency domain for analyzing the modeling approach and for validating the model performance against real robot behavior.
ER  - 

TY  - CONF
TI  - A Real-Robot Dataset for Assessing Transferability of Learned Dynamics Models
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8151
EP  - 8157
AU  - D. Agudelo-Espa√±a
AU  - A. Zadaianchuk
AU  - P. Wenk
AU  - A. Garg
AU  - J. Akpo
AU  - F. Grimminger
AU  - J. Viereck
AU  - M. Naveau
AU  - L. Righetti
AU  - G. Martius
AU  - A. Krause
AU  - B. Sch√∂lkopf
AU  - S. Bauer
AU  - M. W√ºthrich
PY  - 2020
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - optimal control
KW  - robot dynamics
KW  - robust control
KW  - trajectory control
KW  - learned dynamics models
KW  - model based reinforcement learning
KW  - robust current dynamics learning
KW  - real robot dataset
KW  - transferability assessment
KW  - 3 degrees of freedom robot trajectories
KW  - optimal control
KW  - robotic learning
KW  - Trajectory
KW  - Robots
KW  - Artificial neural networks
KW  - Heuristic algorithms
KW  - Mathematical model
KW  - Torque measurement
DO  - 10.1109/ICRA40945.2020.9197392
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the context of model-based reinforcement learning and control, a large number of methods for learning system dynamics have been proposed in recent years. The purpose of these learned models is to synthesize new control policies. An important open question is how robust current dynamics-learning methods are to shifts in the data distribution due to changes in the control policy. We present a real-robot dataset which allows to systematically investigate this question. This dataset contains trajectories of a 3 degrees-of-freedom (DOF) robot being controlled by a diverse set of policies. For comparison, we also provide a simulated version of the dataset. Finally, we benchmark a few widely-used dynamics-learning methods using the proposed dataset. Our results show that the iid test error of a learned model is not necessarily a good indicator of its accuracy under control policies different from the one which generated the training data. This suggests that it may be important to evaluate dynamics-learning methods in terms of their transfer performance, rather than only their iid error.
ER  - 

TY  - CONF
TI  - MagNet: Discovering Multi-agent Interaction Dynamics using Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8158
EP  - 8164
AU  - P. Saha
AU  - A. Ali
AU  - B. A. Mudassar
AU  - Y. Long
AU  - S. Mukhopadhyay
PY  - 2020
KW  - differential equations
KW  - learning (artificial intelligence)
KW  - multi-agent systems
KW  - neural nets
KW  - synchronisation
KW  - agents change
KW  - point-mass system
KW  - Kuramoto phase synchronization dynamics
KW  - predator-swarm interaction dynamics
KW  - multiagent interaction dynamics
KW  - neural network-based multiagent interaction model
KW  - governing dynamics
KW  - complex multiagent system
KW  - nonlinear network
KW  - generic ordinary differential equation based state evolution
KW  - neural network-based realization
KW  - time-discretized model
KW  - core dynamics
KW  - agent-specific parameters
KW  - MagNet
KW  - traditional deep learning models
KW  - Mathematical model
KW  - Magnetic cores
KW  - Multi-agent systems
KW  - Force
KW  - Training
KW  - Springs
KW  - Oscillators
DO  - 10.1109/ICRA40945.2020.9196846
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present the MagNet, a neural network-based multi-agent interaction model to discover the governing dynamics and predict evolution of a complex multi-agent system from observations. We formulate a multi-agent system as a coupled non-linear network with a generic ordinary differential equation (ODE) based state evolution, and develop a neural network-based realization of its time-discretized model. MagNet is trained to discover the core dynamics of a multi-agent system from observations, and tuned on-line to learn agent-specific parameters of the dynamics to ensure accurate prediction even when physical or relational attributes of agents, or number of agents change. We evaluate MagNet on a point-mass system in two-dimensional space, Kuramoto phase synchronization dynamics and predator-swarm interaction dynamics demonstrating orders of magnitude improvement in prediction accuracy over traditional deep learning models.
ER  - 

TY  - CONF
TI  - Development of a Robotic System for Automated Decaking of 3D-Printed Parts
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8202
EP  - 8208
AU  - H. Nguyen
AU  - N. Adrian
AU  - J. L. Xin Yan
AU  - J. M. Salfity
AU  - W. Allen
AU  - Q. -C. Pham
PY  - 2020
KW  - force control
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - path planning
KW  - production engineering computing
KW  - three-dimensional printing
KW  - industrial robots
KW  - robotic decaking
KW  - automated decaking
KW  - 3D printed parts
KW  - 3D printing based mass manufacturing
KW  - smart mechanical design
KW  - motion planning
KW  - force control
KW  - deep learning
KW  - Powders
KW  - Cleaning
KW  - Service robots
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Force control
KW  - deep learning
KW  - manipulation
KW  - system design
KW  - 3D-printing
KW  - decaking
DO  - 10.1109/ICRA40945.2020.9197110
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the rapid rise of 3D-printing as a competitive mass manufacturing method, manual "decaking" - i.e. removing the residual powder that sticks to a 3D-printed part - has become a significant bottleneck. Here, we introduce, for the first time to our knowledge, a robotic system for automated decaking of 3D-printed parts. Combining Deep Learning for 3D perception, smart mechanical design, motion planning, and force control for industrial robots, we developed a system that can automatically decake parts in a fast and efficient way. Through a series of decaking experiments performed on parts printed by a Multi Jet Fusion printer, we demonstrated the feasibility of robotic decaking for 3D-printing-based mass manufacturing.
ER  - 

TY  - CONF
TI  - A Novel Solar Tracker Driven by Waves: From Idea to Implementation*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8209
EP  - 8214
AU  - R. Xu
AU  - H. Liu
AU  - C. Liu
AU  - Z. Sun
AU  - T. L. Lam
AU  - H. Qian
PY  - 2020
KW  - attitude control
KW  - power generation control
KW  - solar cell arrays
KW  - solar power stations
KW  - solar tracker
KW  - solar panels
KW  - ocean environment
KW  - electromagnetic brakes
KW  - dynamic model
KW  - angular acceleration
KW  - control algorithm
KW  - real water surface
KW  - time 28.0 s
KW  - Solar panels
KW  - Brakes
KW  - Oceans
KW  - DC motors
KW  - Sun
KW  - Solar energy
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9196998
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Traditional solar trackers often adopt motors to automatically adjust the attitude of the solar panels towards the sun for maximum power efficiency. In this paper, a novel design of solar tracker for the ocean environment is introduced. Utilizing the fluctuations due to the waves, electromagnetic brakes are utilized instead of motors to adjust the attitude of the solar panels. Compared with the traditional solar trackers, the proposed one is simpler in hardware while the harvesting efficiency is similar. The desired attitude is calculated out of the local location and time. Then based on the dynamic model of the system, the angular acceleration of the solar panels is estimated and a control algorithm is proposed to decide the release and lock states of the brakes. In such a manner, the adjustment of the attitude of the solar panels can be achieved by using two brakes only. Experiments are conducted to validate the acceleration estimator and the dynamic model. At last, the feasibility of the proposed solar tracker is tested on the real water surface. The results show that the system is able to adjust 40¬∞ in two dimensions within 28 seconds.
ER  - 

TY  - CONF
TI  - Design and Implementation of Hydraulic-Cable driven Manipulator for Disaster Response Operation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8215
EP  - 8221
AU  - J. Kim
AU  - J. Seo
AU  - S. Park
AU  - S. Han
AU  - J. Cho
PY  - 2020
KW  - emergency management
KW  - hydraulic actuators
KW  - manipulators
KW  - motion control
KW  - rescue robots
KW  - hydraulic-cable driven actuation modules
KW  - 3DOF manipulator
KW  - hydraulic actuation system
KW  - disaster response mobile-manipulation
KW  - disaster response operation
KW  - hydraulic-cable driven manipulator
KW  - Manipulators
KW  - Blades
KW  - Actuators
KW  - Hydraulic systems
KW  - Torque
KW  - Wires
DO  - 10.1109/ICRA40945.2020.9196554
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a new hydraulic manipulator with hydraulic-cable driven actuation (HCA) modules for disaster response mobile-manipulation. The hydraulic actuation system has the potential to apply disaster-response application, because it has a higher power-to-weight ratio and robustness to external impacts than electric motor actuation. However, using a conventional hydraulic manipulators is inappropriate because the revolute joint uses conventional actuators, such as linear cylinders and vanes, which have some limitations: 1) linear cylinder: small range of motion, 2) vane: low torque-toweight ratio. To overcome these limitations, we propose new 3DOF manipulator which has a larger workspace than the conventional hydraulic manipulator and comparable payloadto-weight ratio. To this end, we use hydraulic-cable driven actuation modules from our previous research. Experimental results verify the basic performance of the actuator modules and manipulator and their capability to perform various disaster response tasks.
ER  - 

TY  - CONF
TI  - Designs for an Expressive Mechatronic Chordophone
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8222
EP  - 8228
AU  - J. P. Yepez Placencia
AU  - D. A. Carnegie
AU  - J. W. Murphy
PY  - 2020
KW  - hearing
KW  - mechatronics
KW  - music
KW  - musical instruments
KW  - technical exploration
KW  - timbral exploration
KW  - mechatronic chordophones
KW  - musical robotics
KW  - stand-alone instruments
KW  - sound art installations
KW  - expressive potential
KW  - plucked strings
KW  - expressive mechatronic mono-chord
KW  - polystring chordophone
KW  - expressive mechatronic chordophone
KW  - sound generation model
KW  - Mechatronics
KW  - Instruments
KW  - Music
KW  - Actuators
KW  - Robots
KW  - Prototypes
KW  - Solenoids
DO  - 10.1109/ICRA40945.2020.9197255
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Plucked strings are an exciting sound generation model for technical and timbral exploration. Mechatronic chordophones take advantage of this model and have been the focus of extensive research and exploration in musical robotics, often used as stand-alone instruments or as part of sound art installations. However, no existing chordophone designs have utilised the expressive potential of plucked strings to their full extent.In this paper, we introduce an expressive mechatronic mono-chord that serves as a prototyping platform for the construction of a polystring chordophone. This new chordophone has been developed to offer enhanced dynamic range, fast picking speeds, fast pitch shifter displacement, and additional expressive techniques compared to existing systems.
ER  - 

TY  - CONF
TI  - OmBURo: A Novel Unicycle Robot with Active Omnidirectional Wheel
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8237
EP  - 8243
AU  - J. Shen
AU  - D. Hong
PY  - 2020
KW  - mechanical stability
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - pendulums
KW  - robot dynamics
KW  - wheels
KW  - human environments
KW  - ideal locomotion mechanism
KW  - omnidirectional balancing unicycle robot
KW  - mobility mechanism
KW  - OmBURo
KW  - agile mobility
KW  - compact structure
KW  - active omnidirectional wheel
KW  - Wheels
KW  - Mobile robots
KW  - Gears
KW  - Friction
KW  - Mathematical model
KW  - Energy loss
DO  - 10.1109/ICRA40945.2020.9196927
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A mobility mechanism for robots to be used in tight spaces shared with people requires it to have a small footprint, to move omnidirectionally, as well as to be highly maneuverable. However, currently there exist few such mobility mechanisms that satisfy all these conditions well. Here we introduce Omnidirectional Balancing Unicycle Robot (OmBURo), a novel unicycle robot with active omnidirectional wheel. The effect is that the unicycle robot can drive in both longitudinal and lateral directions simultaneously. Thus, it can dynamically balance itself based on the principle of dual-axis wheeled inverted pendulum. This paper discloses the early development of this novel unicycle robot involving the overall design, modeling, and control, as well as presents some preliminary results including station keeping and path following. With its very compact structure and agile mobility, it might be the ideal locomotion mechanism for robots to be used in human environments in the future.
ER  - 

TY  - CONF
TI  - Recognition and Reconfiguration of Lattice-Based Cellular Structures by Simple Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8252
EP  - 8259
AU  - E. Niehs
AU  - A. Schmidt
AU  - C. Scheffer
AU  - D. E. Biediger
AU  - M. Yannuzzi
AU  - B. Jenett
AU  - A. Abdel-Rahman
AU  - K. C. Cheung
AU  - A. T. Becker
AU  - S. P. Fekete
PY  - 2020
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - flexibility
KW  - cellular components
KW  - robust robots
KW  - cellular building materials
KW  - arbitrary cellular structures
KW  - cellular materials
KW  - lattice-based cellular structures
KW  - Tiles
KW  - Robot sensing systems
KW  - Lattices
KW  - Shape
KW  - Autonomous robots
DO  - 10.1109/ICRA40945.2020.9196700
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider recognition and reconfiguration of lattice-based cellular structures by very simple robots with only basic functionality. The underlying motivation is the construction and modification of space facilities of enormous dimensions, where the combination of new materials with extremely simple robots promises structures of previously unthinkable size and flexibility; this is also closely related to the newly emerging field of programmable matter. Aiming for large-scale scalability, both in terms of the number of the cellular components of a structure, as well as the number of robots that are being deployed for construction requires simple yet robust robots and mechanisms, while also dealing with various basic constraints, such as connectivity of a structure during reconfiguration. To this end, we propose an approach that combines ultra-light, cellular building materials with extremely simple robots. We develop basic algorithmic methods that are able to detect and reconfigure arbitrary cellular structures, based on robots that have only constant-sized memory. As a proof of concept, we demonstrate the feasibility of this approach for specific cellular materials and robots that have been developed at NASA.
ER  - 

TY  - CONF
TI  - A Fast Configuration Space Algorithm for Variable Topology Truss Modular Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8260
EP  - 8266
AU  - C. Liu
AU  - S. Yu
AU  - M. Yim
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - path planning
KW  - topology
KW  - fast configuration space algorithm
KW  - VTT
KW  - self-reconfigurable robot
KW  - truss shape
KW  - motion planning
KW  - shape changing actions
KW  - topology reconfiguration
KW  - geometry reconfiguration actions
KW  - cell decomposition approach
KW  - collision-free space
KW  - simple shape-morphing method
KW  - variable topology truss modular robot
KW  - Topology
KW  - Robots
KW  - Planning
KW  - Geometry
KW  - Shape
KW  - Kinematics
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196880
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The Variable Topology Truss (VTT) is a new class of self-reconfigurable robot that can reconfigure its truss shape and topology depending on the task or environment requirements. Motion planning and avoiding self-collision are difficult as these systems usually have dozens of degrees-of-freedom with complex intersecting parallel actuation. There are two different types of shape changing actions for a VTT: geometry reconfiguration and topology reconfiguration. This paper focuses on the geometry reconfiguration actions. A new cell decomposition approach is presented based on a fast and complete method to compute the collision-free space of a node in a truss. A simple shape-morphing method is shown to quickly create motion paths for reconfiguration by moving one node at a time.
ER  - 

TY  - CONF
TI  - ModQuad-DoF: A Novel Yaw Actuation for Modular Quadrotors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8267
EP  - 8273
AU  - B. Gabrich
AU  - G. Li
AU  - M. Yim
PY  - 2020
KW  - actuators
KW  - aerospace robotics
KW  - attitude control
KW  - autonomous aerial vehicles
KW  - drag
KW  - helicopters
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - position control
KW  - vehicle dynamics
KW  - ModQuad-DoF
KW  - modular quadrotors
KW  - robotic structure
KW  - enhanced capabilities
KW  - module design
KW  - freedom relative motion
KW  - flying robot
KW  - cage
KW  - docking mechanism
KW  - structure control authority
KW  - structure yaw control
KW  - yaw actuation method
DO  - 10.1109/ICRA40945.2020.9196735
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we introduce ModQuad-DoF, a modular flying robotic structure with enhanced capabilities for yaw actuation. We propose a new module design that allows a one degree of freedom relative motion between the flying robot and the cage, with a docking mechanism allowing rigid connections between cages. A novel method of yaw actuation that increases the structure control authority is also presented. Our new method for the structure yaw control relies on the independent roll angles of each one of the modules, instead of the traditional drag moments from the propellers. In this paper, we propose a controller that allows the ModQuad-DoF to control its position and attitude. In our experiments, we tested a different number of modules flying in cooperation and validated the novel yaw actuation method.
ER  - 

TY  - CONF
TI  - An Actuation Fault Tolerance Approach to Reconfiguration Planning of Modular Self-folding Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8274
EP  - 8280
AU  - M. Yao
AU  - X. Xiao
AU  - Y. Tian
AU  - H. Cui
AU  - J. Paik
PY  - 2020
KW  - actuators
KW  - fault tolerance
KW  - motion control
KW  - robots
KW  - fault tolerant reconfiguration
KW  - self-folding robots
KW  - modular system
KW  - complete actuation failure
KW  - active modules
KW  - imprecise robotic motion
KW  - reconfiguration failure
KW  - intra-module connection
KW  - reconfiguration schemes
KW  - user-specified fault tolerant capability
KW  - arbitrary input initial pattern
KW  - robotic platform
KW  - modular origami robot
KW  - fault tolerant initial patterns
KW  - actuation fault tolerance approach
KW  - reconfiguration planning
KW  - modular self-folding
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Robots
KW  - Three-dimensional displays
KW  - Circuit faults
KW  - Shape
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196574
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel approach to fault tolerant reconfiguration of modular self-folding robots. Among various types of faults that probably occur in the modular system, we focus on the tolerance of complete actuation failure of active modules that might cause imprecise robotic motion and even reconfiguration failure. Our approach is to utilize the reconfigurability of modular self-folding robots and investigate intra-module connection to determine initial patterns that are inherently fault tolerant. We exploit the redundancy of actuation and distribute active modules in both layout-based and target-based scenarios, such that reconfiguration schemes with user-specified fault tolerant capability can be generated for an arbitrary input initial pattern or 3D configuration. Our methods are demonstrated in computer-aided simulation on the robotic platform of Mori, a modular origami robot. The simulation results validate that the proposed algorithms yield fault tolerant initial patterns and distribution schemes of active modules for several 2D and 3D configurations with Mori, while retaining generalizability for a large number of modular self-folding robots.
ER  - 

TY  - CONF
TI  - Parallel Permutation for Linear Full-resolution Reconfiguration of Heterogeneous Sliding-only Cubic Modular Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8281
EP  - 8287
AU  - H. Kawano
PY  - 2020
KW  - computational complexity
KW  - evolutionary computation
KW  - motion control
KW  - robots
KW  - studied cubic modules
KW  - convex motion primitives
KW  - rotating motion primitives
KW  - heterogeneous reconfiguration algorithm
KW  - parallel heterogeneous permutation method
KW  - full-resolution reconfiguration algorithm
KW  - heterogeneous operations
KW  - space saving
KW  - module hardware
KW  - sliding-only motion primitive
KW  - cubic modular robot
KW  - cubic module
KW  - sliding-only cubic modular robots
KW  - parallel permutation algorithm
KW  - heterogeneous sliding-only cubic modular
KW  - linear full-resolution reconfiguration
KW  - linear operating-time cost
KW  - sliding-only cubic modules
KW  - robot structure
KW  - linear operating time cost
KW  - Navigation
KW  - Hardware
KW  - Robot kinematics
KW  - Shape
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197033
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a parallel permutation algorithm that achieves linear full-resolution reconfiguration of sliding-only cubic modular robots. We assume the use of a cubic module that can only slide across other modules' surfaces. The idea of a cubic modular robot with sliding-only motion primitive is a new concept that has advantages in simplifying the mechanisms of module hardware and space saving in its heterogeneous operations compared with previously studied cubic modules, such as those with sliding and convex motion primitives, or rotating motion primitives. However, because of its limited mobility, there are difficulties in managing the connectivity and scalability of the heterogeneous reconfiguration algorithm for it. To overcome these disadvantages, we introduce a parallel heterogeneous permutation method with linear operating time cost that can be incorporated into our previous full-resolution reconfiguration algorithm. We prove the correctness and completeness of the proposed algorithm. Simulation results show that the full-resolution reconfiguration algorithm that incorporates the proposed permutation algorithm reconfigures the robot structure with sliding-only cubic modules in linear operating-time cost.
ER  - 

TY  - CONF
TI  - Determining and Improving the Localization Accuracy of AprilTag Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8288
EP  - 8294
AU  - J. Kallwies
AU  - B. Forkel
AU  - H. -J. Wuensche
PY  - 2020
KW  - edge detection
KW  - feature extraction
KW  - image colour analysis
KW  - image matching
KW  - location based services
KW  - object detection
KW  - robot vision
KW  - software libraries
KW  - localization accuracy
KW  - AprilTag detection
KW  - fiducial markers
KW  - freely available libraries
KW  - AprilTag 3
KW  - ArUco
KW  - OpenCV algorithm
KW  - AprilTags C++
KW  - robotics
KW  - edge refinement
KW  - grayscale camera image
KW  - template matching
KW  - Cameras
KW  - C++ languages
KW  - Libraries
KW  - Robot vision systems
KW  - Calibration
KW  - Image edge detection
DO  - 10.1109/ICRA40945.2020.9197427
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fiducial markers like AprilTags play an important role in robotics, e.g., for the calibration of cameras or the localization of robots. One of the most important properties of an algorithm for detecting such tags is its localization accuracy.In this paper, we present the results of an extensive comparison of four freely available libraries capable of detecting AprilTags, namely AprilTag 3, AprilTags C++, ArUco as standalone libraries, and the OpenCV algorithm based on ArUco. The focus of the comparison is on localization accuracy, but the processing time is also examined. Besides working with pure tags, their extension to checkerboard corners is investigated.In addition, we present two new post-processing techniques. Firstly, a method that can filter out very inaccurate detections resulting from partial border occlusion, and secondly a new highly accurate method for edge refinement. With this we achieve a median pixel error of 0.017 px, compared to 0.17 px for standard OpenCV corner refinement.The dataset used for the evaluation, as well as the developed post-processing techniques, are made publicly available to encourage further comparison and improvement of the detection libraries.
ER  - 

TY  - CONF
TI  - Change of Optimal Values: A Pre-calculated Metric
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8295
EP  - 8301
AU  - F. Bai
PY  - 2020
KW  - decision making
KW  - optimisation
KW  - linear least norm optimization problem
KW  - linear least distance optimization problems
KW  - nonlinear least distance optimization
KW  - optimal value
KW  - minimum norm optimization
KW  - Optimization
KW  - Measurement
KW  - Manifolds
KW  - Mathematical model
KW  - Robots
KW  - Covariance matrices
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9197163
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A variety of optimization problems takes the form of a minimum norm optimization. In this paper, we study the change of optimal values between two incrementally constructed least norm optimization problems, with new measurements included in the second one. We prove an exact equation to calculate the change of optimal values in the linear least norm optimization problem. With the result in this paper, the change of the optimal values can be pre-calculated as a metric to guide online decision makings, without solving the second optimization problem as long the solution and covariance of the first optimization problem are available. The result can be extended to linear least distance optimization problems, and nonlinear least distance optimization with (nonlinear) equality constraints through linearizations. This derivation in this paper provides a theoretically sound explanation to the empirical observations shown in [1]. As an additional contribution, we propose another optimization problem, i.e. aligning two trajectories at given poses, to further demonstrate how to use the metric. The accuracy of the metric is validated with numerical examples, which is quite satisfactory in general (see the experiments in [1] as well), unless in some extremely adverse scenarios. Last but not least, calculating the optimal value by the proposed metric is at least one magnitude faster than solving the corresponding optimization problems directly.
ER  - 

TY  - CONF
TI  - A Flexible Method for Performance Evaluation of Robot Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8302
EP  - 8308
AU  - S. Scheideman
AU  - N. Ray
AU  - H. Zhang
PY  - 2020
KW  - image motion analysis
KW  - mobile robots
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - robot localization
KW  - research issue
KW  - mobile robotics
KW  - performance assessment
KW  - robot SLAM algorithms
KW  - localization accuracy
KW  - SLAM algorithm
KW  - benchmark datasets
KW  - motion capture
KW  - environment-specific
KW  - spatial coverage
KW  - SLAM performance evaluation
KW  - distinctive markers
KW  - robot navigation environment
KW  - generative latent optimization problem
KW  - local robot-to-marker
KW  - global robot
KW  - Simultaneous localization and mapping
KW  - Navigation
KW  - Cameras
KW  - Performance evaluation
KW  - Robot localization
DO  - 10.1109/ICRA40945.2020.9197275
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An important research issue in mobile robotics is performance assessment of robot SLAM algorithms in terms of their localization accuracy. Typically, SLAM algorithms are evaluated with the help of benchmark datasets or expensive equipment such as motion capture. Benchmark datasets however, are environment-specific, and use of motion capture constrains spatial coverage and affordability. In this paper, we present a novel method for SLAM performance evaluation, which only uses distinctive markers (such as AR tags), randomly placed in the robot navigation environment at arbitrary locations, and observes these markers with a camera onboard of the robot. Formulated as a generative latent optimization (GLO) problem, our method uses the local robot-to-marker poses to evaluate the global robot pose estimates by a SLAM algorithm and therefore its performance. Through extensive experiments on two robots, three localization/SLAM algorithms and both LiDAR and RGB-D sensors, we demonstrate the feasibility and accuracy of our proposed method.
ER  - 

TY  - CONF
TI  - Quantifying Good Seamanship For Autonomous Surface Vessel Performance Evaluation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8309
EP  - 8315
AU  - P. Stankiewicz
AU  - M. Heistand
AU  - M. Kobilarov
PY  - 2020
KW  - collision avoidance
KW  - decision making
KW  - marine safety
KW  - marine vehicles
KW  - mobile robots
KW  - remotely operated vehicles
KW  - ships
KW  - performance metrics
KW  - ASV decision-making
KW  - collision risk
KW  - ASV planning strategies
KW  - International Regulations for Prevention of Collisions at Sea
KW  - quantified good seamanship
KW  - COLREGS compliance
KW  - vessel interactions
KW  - autonomous surface vehicle decision-making
KW  - autonomous surface vessel performance evaluation
KW  - seamanship performance criteria
KW  - Marine vehicles
KW  - Safety
KW  - Geometry
KW  - Decision making
KW  - Navigation
KW  - Risk management
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197572
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The current state-of-the-art for testing and evaluation of autonomous surface vehicle (ASV) decision-making is currently limited to one-versus-one vessel interactions by determining compliance with the International Regulations for Prevention of Collisions at Sea, referred to as COLREGS. Strict measurement of COLREGS compliance, however, loses value in multi-vessel encounters, as there can be conflicting rules which make determining compliance extremely subjective. This work proposes several performance metrics to evaluate ASV decision-making based on the concept of "good seamanship," a practice which generalizes to multi-vessel encounters. Methodology for quantifying good seamanship is presented based on the criteria of reducing the overall collision risk of the situation and taking early, appropriate actions. Case study simulation results are presented to showcase the seamanship performance criteria against different ASV planning strategies.
ER  - 

TY  - CONF
TI  - Action-conditioned Benchmarking of Robotic Video Prediction Models: a Comparative Study
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8316
EP  - 8322
AU  - M. S. Nunes
AU  - A. Dehban
AU  - P. Moreno
AU  - J. Santos-Victor
PY  - 2020
KW  - Bayes methods
KW  - image sequences
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - video coding
KW  - video signal processing
KW  - action-conditioned benchmarking
KW  - robotic video prediction models
KW  - intelligent systems
KW  - video prediction systems
KW  - robot actions
KW  - video prediction models
KW  - frame quality
KW  - robot performs
KW  - action inference system
KW  - robot planning systems
KW  - Predictive models
KW  - Measurement
KW  - Visualization
KW  - Stochastic processes
KW  - Planning
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9196839
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A defining characteristic of intelligent systems is the ability to make action decisions based on the anticipated outcomes. Video prediction systems have been demonstrated as a solution for predicting how the future will unfold visually, and thus, many models have been proposed that are capable of predicting future frames based on a history of observed frames (and sometimes robot actions). However, a comprehensive method for determining the fitness of different video prediction models at guiding the selection of actions is yet to be developed.Current metrics assess video prediction models based on human perception of frame quality. In contrast, we argue that if these systems are to be used to guide action, necessarily, the actions the robot performs should be encoded in the predicted frames. In this paper, we are proposing a new metric to compare different video prediction models based on this argument. More specifically, we propose an action inference system and quantitatively rank different models based on how well we can infer the robot actions from the predicted frames. Our extensive experiments show that models with high perceptual scores can perform poorly in the proposed action inference tests and thus, may not be suitable options to be used in robot planning systems.
ER  - 

TY  - CONF
TI  - LyRN (Lyapunov Reaching Network): A Real-Time Closed Loop approach from Monocular Vision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8331
EP  - 8337
AU  - Z. Zhuang
AU  - X. Yu
AU  - R. Mahony
PY  - 2020
KW  - cameras
KW  - closed loop systems
KW  - convolutional neural nets
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - Lyapunov methods
KW  - manipulator dynamics
KW  - neurocontrollers
KW  - pose estimation
KW  - robot vision
KW  - velocity control
KW  - visual servoing
KW  - GTX 1080Ti GPU
KW  - grasping mugs
KW  - multiinstance control
KW  - real-time closed loop
KW  - LyRN
KW  - single shot RGB 6D pose estimation
KW  - complex multiinstance task
KW  - reaching action
KW  - control Lyapunov function
KW  - learning principles
KW  - visually guided reaching
KW  - Lyapunov reaching network
KW  - pose-based-visual-servo grasping system
KW  - closed-loop control
KW  - over-the-shoulder monocular RGB camera
KW  - multiinstance capability
KW  - visual control
KW  - velocity control
KW  - deep convolution neural network
KW  - manipulator joint angles
KW  - monocular vision
KW  - reaching points
KW  - frequency 85.0 Hz
KW  - Lyapunov methods
KW  - Grasping
KW  - Feature extraction
KW  - Computer architecture
KW  - Task analysis
KW  - Pose estimation
KW  - Velocity control
DO  - 10.1109/ICRA40945.2020.9196781
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a closed-loop, multi-instance control algorithm for visually guided reaching based on novel learning principles. A control Lyapunov function methodology is used to design a reaching action for a complex multi-instance task in the case where full state information (poses of all potential reaching points) is available. The proposed algorithm uses monocular vision and manipulator joint angles as the input to a deep convolution neural network to predict the value of the control Lyapunov function (cLf) and corresponding velocity control. The resulting network output is used in real-time as visual control for the grasping task with the multi-instance capability emerging naturally from the design of the control Lyapunov function. We demonstrate the proposed algorithm grasping mugs (textureless and symmetric objects) on a table-top from an over-the-shoulder monocular RGB camera. The manipulator dynamically converges to the best-suited target among multiple identical instances from any random initial pose within the workspace. The system trained with only simulated data is able to achieve 90.3% grasp success rate in the real-world experiments with up to 85Hz closed-loop control on one GTX 1080Ti GPU and significantly outperforms a Pose-Based-Visual-Servo (PBVS) grasping system adapted from a state-of-the-art single shot RGB 6D pose estimation algorithm. A key contribution of the paper is the inclusion of a first-order differential constraint associated with the cLf as a regularisation term during learning, and we provide evidence that this leads to more robust and reliable reaching/grasping performance than vanilla regression on general control inputs.
ER  - 

TY  - CONF
TI  - Object Finding in Cluttered Scenes Using Interactive Perception
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8338
EP  - 8344
AU  - T. Novkovic
AU  - R. Pautrat
AU  - F. Furrer
AU  - M. Breyer
AU  - R. Siegwart
AU  - J. Nieto
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object recognition
KW  - robot vision
KW  - search problems
KW  - object finding
KW  - cases physical interaction
KW  - target object
KW  - complex environment
KW  - object search
KW  - cluttered scenes interactions
KW  - reinforcement learning based active perception system
KW  - reinforcement learning based interactive perception system
KW  - robotic manipulator
KW  - RGB
KW  - depth camera
KW  - Cameras
KW  - Task analysis
KW  - Robot sensing systems
KW  - Search problems
KW  - Clutter
KW  - Detectors
DO  - 10.1109/ICRA40945.2020.9197101
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Object finding in clutter is a skill that requires perception of the environment and in many cases physical interaction. In robotics, interactive perception defines a set of algorithms that leverage actions to improve the perception of the environment, and vice versa use perception to guide the next action. Scene interactions are difficult to model, therefore, most of the current systems use predefined heuristics. This limits their ability to efficiently search for the target object in a complex environment. In order to remove heuristics and the need for explicit models of the interactions, in this work we propose a reinforcement learning based active and interactive perception system for scene exploration and object search. We evaluate our work both in simulated and in real-world experiments using a robotic manipulator equipped with an RGB and a depth camera, and compare our system to two baselines. The results indicate that our approach, trained in simulation only, transfers smoothly to reality and can solve the object finding task efficiently and with more than 88% success rate.
ER  - 

TY  - CONF
TI  - CCAN: Constraint Co-Attention Network for Instance Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8353
EP  - 8359
AU  - J. Cai
AU  - X. Tao
AU  - H. Cheng
AU  - Z. Zhang
PY  - 2020
KW  - dexterous manipulators
KW  - feature extraction
KW  - grippers
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - query image
KW  - soft constraints
KW  - workspace image
KW  - grasp configuration
KW  - CCAN
KW  - instance grasping
KW  - learning-based method
KW  - constraint co-attention module
KW  - constraint co-attention network
KW  - robotic grasping task
KW  - end-to-end instance grasping method
KW  - grasp affordance predictor
KW  - Feature extraction
KW  - Grasping
KW  - Training
KW  - Task analysis
KW  - Robots
KW  - Data mining
KW  - Correlation
DO  - 10.1109/ICRA40945.2020.9197182
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Instance grasping is a challenging robotic grasping task when a robot aims to grasp a specified target object in cluttered scenes. In this paper, we propose a novel end-to-end instance grasping method using only monocular workspace and query images, where the workspace image includes several objects and the query image only contains the target object. To effectively extract discriminative features and facilitate the training process, a learning-based method, referred to as Constraint Co-Attention Network (CCAN), is proposed which consists of a constraint co-attention module and a grasp affordance predictor. An effective co-attention module is presented to construct the features of a workspace image from the extracted features of the query image. By introducing soft constraints into the co-attention module, it highlights the target object's features while trivializes other objects' features in the workspace image. Using the features extracted from the co-attention module, the cascaded grasp affordance interpreter network only predicts the grasp configuration for the target object. The training of the CCAN is totally based on simulated self-supervision. Extensive qualitative and quantitative experiments show the effectiveness of our method both in simulated and real-world environments even for totally unseen objects.
ER  - 

TY  - CONF
TI  - 3D Object Detection and Tracking Based on Streaming Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8376
EP  - 8382
AU  - X. Guo
AU  - J. Gu
AU  - S. Guo
AU  - Z. Xu
AU  - C. Yang
AU  - S. Liu
AU  - L. Cheng
AU  - K. Huang
PY  - 2020
KW  - data analysis
KW  - image motion analysis
KW  - interpolation
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - object tracking
KW  - data streaming
KW  - temporal information
KW  - 3D streaming based object detection
KW  - nonkey frames
KW  - motion based interpolation algorithm
KW  - frame-by-frame paradigm
KW  - KITTI object tracking benchmark
KW  - deep learning
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Proposals
KW  - Object detection
KW  - Prediction algorithms
KW  - Correlation
KW  - Agriculture
DO  - 10.1109/ICRA40945.2020.9197183
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recent approaches for 3D object detection have made tremendous progresses due to the development of deep learning. However, previous researches are mostly based on individual frames, leading to limited exploitation of information between frames. In this paper, we attempt to leverage the temporal information in streaming data and explore 3D streaming based object detection as well as tracking. Toward this goal, we set up a dual-way network for 3D object detection based on keyframes, and then propagate predictions to non-key frames through a motion based interpolation algorithm guided by temporal information. Our framework is not only shown to have significant improvements on object detection compared with frame-by-frame paradigm, but also proven to produce competitive results on KITTI Object Tracking Benchmark, with 76.68% in MOTA and 81.65% in MOTP respectively.
ER  - 

TY  - CONF
TI  - Object-Centric Stereo Matching for 3D Object Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8383
EP  - 8389
AU  - A. D. Pon
AU  - J. Ku
AU  - C. Li
AU  - S. L. Waslander
PY  - 2020
KW  - cameras
KW  - image matching
KW  - object detection
KW  - optical radar
KW  - pose estimation
KW  - stereo image processing
KW  - LiDAR-based 3D object detector
KW  - object point clouds
KW  - object-centric stereo matching method
KW  - 3D object detection
KW  - stereo cameras
KW  - LiDAR sensor
KW  - stereo 3D object detection
KW  - PSMNet stereo matching network
KW  - 6 DoF pose
KW  - 2D box association
KW  - Three-dimensional displays
KW  - Object detection
KW  - Two dimensional displays
KW  - Laser radar
KW  - Detectors
KW  - Cameras
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9196660
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Safe autonomous driving requires reliable 3D object detection-determining the 6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve this task is a cost-effective alternative to the widely used LiDAR sensor. The current state-of-the-art for stereo 3D object detection takes the existing PSMNet stereo matching network, with no modifications, and converts the estimated disparities into a 3D point cloud, and feeds this point cloud into a LiDAR-based 3D object detector. The issue with existing stereo matching networks is that they are designed for disparity estimation, not 3D object detection; the shape and accuracy of object point clouds are not the focus. Stereo matching networks commonly suffer from inaccurate depth estimates at object boundaries, which we define as streaking, because background and foreground points are jointly estimated. Existing networks also penalize disparity instead of the estimated position of object point clouds in their loss functions. We propose a novel 2D box association and object-centric stereo matching method that only estimates the disparities of the objects of interest to address these two issues. Our method achieves state-of-the-art results on the KITTI 3D and BEV benchmarks.
ER  - 

TY  - CONF
TI  - The Relative Confusion Matrix, a Tool to Assess Classifiablility in Large Scale Picking Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8390
EP  - 8396
AU  - A. Balasch
AU  - M. Beinhofer
AU  - G. Zauner
PY  - 2020
KW  - image classification
KW  - industrial robots
KW  - logistics
KW  - materials handling equipment
KW  - robot vision
KW  - warehousing
KW  - relative confusion matrix
KW  - mixed-product bin
KW  - robot workstation
KW  - manual picking station
KW  - bin picking robot
KW  - logistics installations
KW  - warehouse
KW  - image dataset
KW  - Robots
KW  - Measurement
KW  - Task analysis
KW  - Reliability
KW  - Tools
KW  - Logistics
KW  - Workstations
DO  - 10.1109/ICRA40945.2020.9197540
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For bin picking robots in real logistics installations, the certainty of picking the correct product out of a mixed-product bin is essential. This paper proposes an approach for the robot to efficiently decide whether it can robustly distinguish the product to pick from the others in the bin. If not, the pick has to be routed not to the robot workstation but to a manual picking station. For this, we introduce a modified version of the confusion matrix, which we call the relative confusion matrix. We show how this matrix can be used to make the required decision, taking into account that all other products in the warehouse can be logically ruled out as they are not contained in the bin. Considering only this subset of products would require a re-computation of the standard confusion matrix. With the relative confusion matrix, no such re-computation is needed, which makes our approach more efficient. We show the usefulness of our approach in extensive experiments with a real bin picking robot, on simulated data, and on a publicly available image dataset.
ER  - 

TY  - CONF
TI  - Pose-guided Auto-Encoder and Feature-Based Refinement for 6-DoF Object Pose Regression
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8397
EP  - 8403
AU  - Z. Li
AU  - X. Ji
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image colour analysis
KW  - neural nets
KW  - pose estimation
KW  - regression analysis
KW  - pose estimation performance
KW  - pose-irrelevant factors
KW  - encoding process
KW  - suitable pose representation
KW  - regression approaches
KW  - single RGB image
KW  - 6-DoF object Pose regression
KW  - Feature-based refinement
KW  - Pose-guided Auto-Encoder
KW  - direct regression-based approaches
KW  - PAE
KW  - Feature-based Pose Refiner
KW  - Feature extraction
KW  - Pose estimation
KW  - Three-dimensional displays
KW  - Training
KW  - Rendering (computer graphics)
KW  - Image reconstruction
KW  - Decoding
DO  - 10.1109/ICRA40945.2020.9196953
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurately estimating the 6-DoF object pose from a single RGB image is a challenging task in computer vision. Though pose regression approaches have achieved great progress, the performance is still limited. In this work, we propose Pose-guided Auto-Encoder (PAE), which can distill better pose-related features from the image by utilizing a suitable pose representation, 3D Location Field (3DLF), to guide the encoding process. The features from PAE show strong robustness to pose-irrelevant factors. Compared with traditional auto-encoder, PAE can not only improve the pose estimation performance but also handle the ambiguity viewpoints problem. Further, we propose Feature-based Pose Refiner (FPR), which refines the pose from the extracted features without rendering. Combining PAE with FPR, our approach achieved state-of-the-art performance on the widely used LINEMOD dataset. Our approach not only outperforms the direct regression-based approaches with a large margin but also thrillingly surpasses current state-of-the-art indirect PnP-based approach.
ER  - 

TY  - CONF
TI  - PrimiTect: Fast Continuous Hough Voting for Primitive Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8404
EP  - 8410
AU  - C. Sommer
AU  - Y. Sun
AU  - E. Bylow
AU  - D. Cremers
PY  - 2020
KW  - computational geometry
KW  - feature extraction
KW  - Hough transforms
KW  - object detection
KW  - data abstraction
KW  - geometric primitives
KW  - compact representation
KW  - PrimiTect
KW  - fast continuous Hough voting
KW  - primitive detection
KW  - 3D point sets
KW  - semiglobal Hough voting scheme
KW  - robotics applications
KW  - local low-dimensional parameterization
KW  - Three-dimensional displays
KW  - Feature extraction
KW  - Interpolation
KW  - Two dimensional displays
KW  - Transforms
KW  - Computational modeling
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9196988
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper tackles the problem of data abstraction in the context of 3D point sets. Our method classifies points into different geometric primitives, such as planes and cones, leading to a compact representation of the data. Being based on a semi-global Hough voting scheme, the method does not need initialization and is robust, accurate, and efficient. We use a local, low-dimensional parameterization of primitives to determine type, shape and pose of the object that a point belongs to. This makes our algorithm suitable to run on devices with low computational power, as often required in robotics applications. The evaluation shows that our method outperforms state-of-the-art methods both in terms of accuracy and robustness.
ER  - 

TY  - CONF
TI  - FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale Context Aggregation and Feature Space Super-resolution
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8411
EP  - 8417
AU  - Z. Zhang
AU  - K. Zhang
PY  - 2020
KW  - convolutional neural nets
KW  - feature extraction
KW  - image resolution
KW  - image sampling
KW  - image segmentation
KW  - object detection
KW  - real-time systems
KW  - robot vision
KW  - feature space superresolution
KW  - FarSee-Net
KW  - real time semantic segmentation
KW  - cascaded factorized atrous spatial pyramid pooling
KW  - feature maps
KW  - convolutional neural networks
KW  - multiscale context aggregation
KW  - object scale variations
KW  - robotic applications
KW  - subsampled image
KW  - Semantics
KW  - Convolution
KW  - Image segmentation
KW  - Feature extraction
KW  - Real-time systems
KW  - Spatial resolution
DO  - 10.1109/ICRA40945.2020.9196599
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Real-time semantic segmentation is desirable in many robotic applications with limited computation resources. One challenge of semantic segmentation is to deal with the object scale variations and leverage the context. How to perform multi-scale context aggregation within limited computation budget is important. In this paper, firstly, we introduce a novel and efficient module called Cascaded Factorized Atrous Spatial Pyramid Pooling (CF-ASPP). It is a lightweight cas-caded structure for Convolutional Neural Networks (CNNs) to efficiently leverage context information. On the other hand, for runtime efficiency, state-of-the-art methods will quickly decrease the spatial size of the inputs or feature maps in the early network stages. The final high-resolution result is usually obtained by non-parametric up-sampling operation (e.g. bilinear interpolation). Differently, we rethink this pipeline and treat it as a super-resolution process. We use optimized super-resolution operation in the up-sampling step and improve the accuracy, especially in sub-sampled input image scenario for real-time applications. By fusing the above two improvements, our methods provide better latency-accuracy trade-off than the other state-of-the-art methods. In particular, we achieve 68.4% mIoU at 84 fps on the Cityscapes test set with a single Nivida Titan X (Maxwell) GPU card. The proposed module can be plugged into any feature extraction CNN and benefits from the CNN structure development.
ER  - 

TY  - CONF
TI  - Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8418
EP  - 8424
AU  - C. Li
AU  - Y. Meng
AU  - S. H. Chan
AU  - Y. -T. Chen
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - driver information systems
KW  - feature extraction
KW  - interactive systems
KW  - learning (artificial intelligence)
KW  - road traffic
KW  - road vehicles
KW  - graph convolutional networks
KW  - intelligent automated driving systems
KW  - complicated driving situations
KW  - spatial-temporal interaction framework
KW  - graph convolution networks
KW  - GCN
KW  - interaction modeling
KW  - ego-stuff interaction
KW  - Honda research institute driving dataset
KW  - 3D-aware egocentric spatial-temporal interaction learning
KW  - tactical driver behavior annotations
KW  - ego-thing interactions
KW  - feature extraction
KW  - Feature extraction
KW  - Three-dimensional displays
KW  - Roads
KW  - Hidden Markov models
KW  - Vehicles
KW  - Two dimensional displays
KW  - Convolution
DO  - 10.1109/ICRA40945.2020.9197057
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To enable intelligent automated driving systems, a promising strategy is to understand how human drives and interacts with road users in complicated driving situations. In this paper, we propose a 3D-aware egocentric spatial-temporal interaction framework for automated driving applications. Graph convolution networks (GCN) is devised for interaction modeling. We introduce three novel concepts into GCN. First, we decompose egocentric interactions into ego-thing and ego- stuff interaction, modeled by two GCNs. In both GCNs, ego nodes are introduced to encode the interaction between thing objects (e.g., car and pedestrian), and interaction between stuff objects (e.g., lane marking and traffic light). Second, objects' 3D locations are explicitly incorporated into GCN to better model egocentric interactions. Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign operation to extract features for irregular objects.We validate the proposed framework on tactical driver behavior recognition. Extensive experiments are conducted using Honda Research Institute Driving Dataset, the largest dataset with diverse tactical driver behavior annotations. Our framework demonstrates substantial performance boost over baselines on the two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we visualize the learned affinity matrices, which encode ego-thing and ego-stuff interactions, to showcase the proposed framework can capture interactions effectively.
ER  - 

TY  - CONF
TI  - C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8425
EP  - 8432
AU  - T. Kim
AU  - J. -H. Lee
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - human-robot motion retargeting
KW  - kinematic configurations
KW  - kinematic independent general solution
KW  - three-phase optimization method
KW  - deep reinforcement learning
KW  - motion retargeting learning
KW  - motion retargeting policy
KW  - motion retargeting skill
KW  - human skeleton
KW  - cyclic-three-phase optimization
KW  - NAO robot
KW  - Pepper robot
KW  - Baxter robot
KW  - C-3PO robot
KW  - Skeleton
KW  - Robot motion
KW  - Robot kinematics
KW  - Kinematics
KW  - Zirconium
KW  - Torso
DO  - 10.1109/ICRA40945.2020.9196948
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion retargeting between heterogeneous polymorphs with different sizes and kinematic configurations requires a comprehensive knowledge of (inverse) kinematics. Moreover, it is non-trivial to provide a kinematic independent general solution. In this study, we developed a cyclic three-phase optimization method based on deep reinforcement learning for human-robot motion retargeting. The motion retargeting learning is performed using refined data in a latent space by the cyclic and filtering paths of our method. In addition, the human- in-the-loop based three-phase approach provides a framework for the improvement of the motion retargeting policy by both quantitative and qualitative manners. Using the proposed C- 3PO method, we were successfully able to learn the motion retargeting skill between the human skeleton and motion of the multiple robots such as NAO, Pepper, Baxter and C-3PO.
ER  - 

TY  - CONF
TI  - AP-MTL: Attention Pruned Multi-task Learning Model for Real-time Instrument Detection and Segmentation in Robot-assisted Surgery
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8433
EP  - 8439
AU  - M. Islam
AU  - V. S. Vibashan
AU  - H. Ren
PY  - 2020
KW  - endoscopes
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - robot vision
KW  - surgery
KW  - attention pruned multitask learning model
KW  - real-time instrument detection
KW  - robot-assisted surgery
KW  - image-guided robotic surgery
KW  - real-time robotic system
KW  - weight-shared encoder
KW  - task-aware detection
KW  - asynchronous task-aware optimization
KW  - robotic instrument segmentation dataset
KW  - end-to-end trainable realtime multitask learning model
KW  - global attention dynamic pruning
KW  - skip squeeze and excitation module
KW  - Task analysis
KW  - Instruments
KW  - Decoding
KW  - Real-time systems
KW  - Computational modeling
KW  - Optimization
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9196905
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical scene understanding and multi-tasking learning are crucial for image-guided robotic surgery. Training a real-time robotic system for the detection and segmentation of high-resolution images provides a challenging problem with the limited computational resource. The perception drawn can be applied in effective real-time feedback, surgical skill assessment, and human-robot collaborative surgeries to enhance surgical outcomes. For this purpose, we develop a novel end-to-end trainable real-time Multi-Task Learning (MTL) model with weight-shared encoder and task-aware detection and segmentation decoders. Optimization of multiple tasks at the same convergence point is vital and presents a complex problem. Thus, we propose an asynchronous task-aware optimization (ATO) technique to calculate task-oriented gradients and train the decoders independently. Moreover, MTL models are always computationally expensive, which hinder real-time applications. To address this challenge, we introduce a global attention dynamic pruning (GADP) by removing less significant and sparse parameters. We further design a skip squeeze and excitation (SE) module, which suppresses weak features, excites significant features and performs dynamic spatial and channel-wise feature re-calibration. Validating on the robotic instrument segmentation dataset of MICCAI endoscopic vision challenge, our model significantly outperforms state-of-the-art segmentation and detection models, including best-performed models in the challenge.
ER  - 

TY  - CONF
TI  - Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8440
EP  - 8446
AU  - X. Gao
AU  - Y. Jin
AU  - Q. Dou
AU  - P. -A. Heng
PY  - 2020
KW  - gesture recognition
KW  - image classification
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - neural nets
KW  - surgery
KW  - tree searching
KW  - video signal processing
KW  - video surveillance
KW  - joint surgical gesture segmentation
KW  - tree search algorithm
KW  - neural networks design
KW  - reinforcement learning framework
KW  - surgical robotic applications
KW  - surgical video classification
KW  - baseline methods
KW  - JIGSAWS dataset
KW  - surgery surveillance
KW  - automatic surgical gesture recognition
KW  - robot-assisted surgery
KW  - Surgery
KW  - Gesture recognition
KW  - Robots
KW  - Hidden Markov models
KW  - Learning (artificial intelligence)
KW  - Feature extraction
KW  - Task analysis
KW  - Surgical gesture recognition
KW  - Deep reinforcement learning in robotics
KW  - Tree search
KW  - Robotic surgery
DO  - 10.1109/ICRA40945.2020.9196674
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Automatic surgical gesture recognition is fundamental for improving intelligence in robot-assisted surgery, such as conducting complicated tasks of surgery surveillance and skill evaluation. However, current methods treat each frame individually and produce the outcomes without effective consideration on future information. In this paper, we propose a framework based on reinforcement learning and tree search for joint surgical gesture segmentation and classification. An agent is trained to segment and classify the surgical video in a human-like manner whose direct decisions are re-considered by tree search appropriately. Our proposed tree search algorithm unites the outputs from two designed neural networks, i.e., policy and value network. With the integration of complementary information from distinct models, our framework is able to achieve the better performance than baseline methods using either of the neural networks. For an overall evaluation, our developed approach consistently outperforms the existing methods on the suturing task of JIGSAWS dataset in terms of accuracy, edit score and F1 score. Our study highlights the utilization of tree search to refine actions in reinforcement learning framework for surgical robotic applications.
ER  - 

TY  - CONF
TI  - ACNN: a Full Resolution DCNN for Medical Image Segmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8455
EP  - 8461
AU  - X. -Y. Zhou
AU  - J. -Q. Zheng
AU  - P. Li
AU  - G. -Z. Yang
PY  - 2020
KW  - biomedical MRI
KW  - computerised tomography
KW  - convolutional neural nets
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - neural nets
KW  - surgery
KW  - medical image segmentation
KW  - robot-assisted Minimally Invasive Surgeries
KW  - current DCNNs
KW  - sampling layer
KW  - receptive field
KW  - spatial dimension
KW  - feature maps
KW  - atrous convolutional layers
KW  - ACNN
KW  - magnetic resonance imaging
KW  - computed tomography image segmentation
KW  - segmentation Intersection
KW  - Atrous convolutional neural network
KW  - full-resolution DCNN
KW  - U-Net
KW  - Deeplabv3+
KW  - Image segmentation
KW  - Convolution
KW  - Biomedical imaging
KW  - Image resolution
KW  - Convolutional neural networks
KW  - Three-dimensional displays
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197328
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep Convolutional Neural Networks (DCNNs) are used extensively in medical image segmentation and hence 3D navigation for robot-assisted Minimally Invasive Surgeries (MISs). However, current DCNNs usually use down sampling layers for increasing the receptive field and gaining abstract semantic information. These down sampling layers decrease the spatial dimension of feature maps, which can be detrimental to image segmentation. Atrous convolution is an alternative for the down sampling layer. It increases the receptive field whilst maintains the spatial dimension of feature maps. In this paper, a method for effective atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. Furthermore, a new and full resolution DCNN - Atrous Convolutional Neural Network (ACNN), which incorporates cascaded atrous II-blocks, residual learning and Instance Normalization (IN) is proposed. Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) image segmentation demonstrate that the proposed ACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Net and Deeplabv3+, but with reduced trainable parameters.
ER  - 

TY  - CONF
TI  - Hyperproperties for Robotics: Planning via HyperLTL
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8462
EP  - 8468
AU  - Y. Wang
AU  - S. Nalluri
AU  - M. Pajic
PY  - 2020
KW  - formal specification
KW  - path planning
KW  - robots
KW  - temporal logic
KW  - hyperproperties
KW  - formal methods
KW  - temporal logic objectives
KW  - hyper-temporal logics
KW  - multiple paths
KW  - HyperLTL specifications
KW  - planning strategies
KW  - robotic planning
KW  - discrete transition systems
KW  - Planning
KW  - Robustness
KW  - Privacy
KW  - Automata
KW  - Model checking
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196874
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - There is a growing interest on formal methods-based robotic planning for temporal logic objectives. In this work, we extend the scope of existing synthesis methods to hyper-temporal logics. We are motivated by the fact that important planning objectives, such as optimality, robustness, and privacy, (maybe implicitly) involve the interrelation between multiple paths. Such objectives are thus hyperproperties, and cannot be expressed with usual temporal logics like the linear temporal logic (LTL). We show that such hyperproperties can be expressed by HyperLTL, an extension of LTL to multiple paths. To handle the complexity of planning with HyperLTL specifications, we introduce a symbolic approach for synthesizing planning strategies on discrete transition systems. Our planning method is evaluated on several case studies.
ER  - 

TY  - CONF
TI  - Abstractions for computing all robotic sensors that suffice to solve a planning problem
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8469
EP  - 8475
AU  - Y. Zhang
AU  - D. A. Shell
PY  - 2020
KW  - computational complexity
KW  - control engineering computing
KW  - data structures
KW  - graph theory
KW  - planning (artificial intelligence)
KW  - robots
KW  - search problems
KW  - sensors
KW  - robotic sensors
KW  - planning problem
KW  - search algorithms
KW  - sensor designs
KW  - design trade-offs
KW  - sensor maps
KW  - potential sensors
KW  - outer limits
KW  - search space
KW  - data structures
KW  - single special representative
KW  - task domain knowledge
KW  - sensor technology
KW  - particular problem instances
KW  - sensor characterization pairs
KW  - yielding solutions
KW  - Robot sensing systems
KW  - Planning
KW  - Sensor phenomena and characterization
KW  - Uncertainty
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196812
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Whether a robot can perform some specific task depends on several aspects, including the robot's sensors and the plans it possesses. We are interested in search algorithms that treat plans and sensor designs jointly, yielding solutions-i.e., plan and sensor characterization pairs-if and only if they exist. Such algorithms can help roboticists explore the space of sensors to aid in making design trade-offs. Generalizing prior work where sensors are modeled abstractly as sensor maps on p-graphs, the present paper increases the potential sensors which can be sought significantly. But doing so enlarges a problem currently on the outer limits of being considered tractable. Toward taming this complexity, two contributions are made: (1) we show how to represent the search space for this more general problem and describe data structures that enable whole sets of sensors to be summarized via a single special representative; (2) we give a means by which other structure (either task domain knowledge, sensor technology or fabrication constraints) can be incorporated to reduce the sets to be enumerated. These lead to algorithms that we have implemented and which suffice to solve particular problem instances, albeit only of small scale. Nevertheless, the algorithm aids in helping understand what attributes sensors must possess and what information they must provide in order to ensure a robot can achieve its goals despite non-determinism.
ER  - 

TY  - CONF
TI  - T* : A Heuristic Search Based Path Planning Algorithm for Temporal Logic Specifications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8476
EP  - 8482
AU  - D. Khalidi
AU  - D. Gujarathi
AU  - I. Saha
PY  - 2020
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - navigation
KW  - search problems
KW  - temporal logic
KW  - trajectory control
KW  - obstacle avoidance
KW  - heuristic search based path planning algorithm
KW  - temporal logic path planning
KW  - graph search problem
KW  - point-to-point navigation
KW  - temporal logic specifications
KW  - temporal logic query
KW  - optimal trajectory
KW  - Dijkstra's shortest path algorithm
KW  - Automata
KW  - Heuristic algorithms
KW  - Trajectory
KW  - Task analysis
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196928
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The fundamental path planning problem for a mobile robot involves generating a trajectory for point-to-point navigation while avoiding obstacles. Heuristic-based search algorithms like A* have been shown to be efficient in solving such planning problems. Recently, there has been an increased interest in specifying complex path planning problem using temporal logic. In the state-of-the-art algorithm, the temporal logic path planning problem is reduced to a graph search problem, and Dijkstra's shortest path algorithm is used to compute the optimal trajectory satisfying the specification. The A* algorithm, when used with an appropriate heuristic for the distance from the destination, can generate an optimal path in a graph more efficiently than Dijkstra's shortest path algorithm. The primary challenge for using A* algorithm in temporal logic path planning is that there is no notion of a single destination state for the robot. We present a novel path planning algorithm T* that uses the A* search procedure opportunistically to generate an optimal trajectory satisfying a temporal logic query. Our experimental results demonstrate that T* achieves an order of magnitude improvement over the state-of-the-art algorithm to solve many temporal logic path planning problems in 2-D as well as 3-D workspaces.
ER  - 

TY  - CONF
TI  - Global/local motion planning based on Dynamic Trajectory Reconfiguration and Dynamical Systems for Autonomous Surgical Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8483
EP  - 8489
AU  - N. Sayols
AU  - A. Sozzi
AU  - N. Piccinelli
AU  - A. Hernansanz
AU  - A. Casals
AU  - M. Bonf√®
AU  - R. Muradore
PY  - 2020
KW  - collision avoidance
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - splines (mathematics)
KW  - surgery
KW  - robotic minimally invasive surgery
KW  - geometric constraints
KW  - desired task
KW  - final target
KW  - moving obstacles
KW  - developed motion planner
KW  - two-layer architecture
KW  - global level computes smooth spline-based trajectories
KW  - collision free connections
KW  - realistic surgical scenario
KW  - autonomous surgical
KW  - collision-free trajectories
KW  - autonomous execution
KW  - assistive tasks
KW  - dynamical systems based obstacle avoidance
KW  - Trajectory
KW  - Tools
KW  - Robots
KW  - Collision avoidance
KW  - Task analysis
KW  - Surgery
KW  - Splines (mathematics)
DO  - 10.1109/ICRA40945.2020.9197525
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the generation of collision-free trajectories for the autonomous execution of assistive tasks in Robotic Minimally Invasive Surgery (R-MIS). The proposed approach takes into account geometric constraints related to the desired task, like for example the direction to approach the final target and the presence of moving obstacles. The developed motion planner is structured as a two-layer architecture: a global level computes smooth spline-based trajectories that are continuously updated using virtual potential fields; a local level, exploiting Dynamical Systems based obstacle avoidance, ensures collision free connections among the spline control points. The proposed architecture is validated in a realistic surgical scenario.
ER  - 

TY  - CONF
TI  - Deep Imitative Reinforcement Learning for Temporal Logic Robot Motion Planning with Noisy Semantic Observations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8490
EP  - 8496
AU  - Q. Gao
AU  - M. Pajic
AU  - M. M. Zavlanos
PY  - 2020
KW  - Markov processes
KW  - mobile robots
KW  - motion control
KW  - neurocontrollers
KW  - path planning
KW  - probability
KW  - temporal logic
KW  - noisy semantic observations
KW  - mobile robots
KW  - linear temporal logic specifications
KW  - robot sensing error
KW  - probabilistic labels
KW  - labeled transition system
KW  - robot mobility
KW  - labeled Markov decision process
KW  - unknown transition probabilities
KW  - product-based model checkers
KW  - probabilistic labeling functions
KW  - Q-learning agent
KW  - deep imitative reinforcement learning
KW  - temporal logic robot motion planning
KW  - deep imitative Q-learning method
KW  - DIQL
KW  - control policies synthesis
KW  - LTL
KW  - LMDP
KW  - suboptimal instructions
KW  - Robot sensing systems
KW  - Labeling
KW  - Uncertainty
KW  - Semantics
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9197297
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a Deep Imitative Q-learning (DIQL) method to synthesize control policies for mobile robots that need to satisfy Linear Temporal Logic (LTL) specifications using noisy semantic observations of their surroundings. The robot sensing error is modeled using probabilistic labels defined over the states of a Labeled Transition System (LTS) and the robot mobility is modeled using a Labeled Markov Decision Process (LMDP) with unknown transition probabilities. We use existing product-based model checkers (PMCs) as experts to guide the Q-learning algorithm to convergence. To the best of our knowledge, this is the first approach that models noise in semantic observations using probabilistic labeling functions and employs existing model checkers to provide suboptimal instructions to the Q-learning agent.
ER  - 

TY  - CONF
TI  - Minimal 3D Dubins Path with Bounded Curvature and Pitch Angle
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8497
EP  - 8503
AU  - P. V√°≈àa
AU  - A. Alves Neto
AU  - J. Faigl
AU  - D. G. Macharet
PY  - 2020
KW  - optimisation
KW  - path planning
KW  - vehicles
KW  - minimal 3D Dubins path
KW  - bounded curvature
KW  - pitch angle
KW  - cost-efficient three-dimensional paths
KW  - two-dimensional Dubins curves
KW  - closed-form solutions
KW  - local optimization
KW  - cost-efficient solution
KW  - lower bound estimation
KW  - optimal path
KW  - vehicle
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Turning
KW  - Path planning
KW  - Atmospheric modeling
KW  - Space vehicles
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197084
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we address the problem of finding cost-efficient three-dimensional paths that satisfy the maximum allowed curvature and the pitch angle of the vehicle. For any given initial and final configurations, the problem is decoupled into finding the horizontal and vertical parts of the path separately. Although the individual paths are modeled as two-dimensional Dubins curves using closed-form solutions, the final 3D path is constructed using the proposed local optimization to find a cost-efficient solution. Moreover, based on the decoupled approach, we provide a lower bound estimation of the optimal path that enables us to determine the quality of the found heuristic solution. The proposed solution has been evaluated using existing benchmark instances and compared with state-of-the-art approaches. Based on the reported results and lower bounds, the proposed approach provides paths close to the optimal solution while the computational requirements are in hundreds of microseconds. Besides, the proposed method provides paths with fewer turns than others, which make them easier to be followed by the vehicle's controller.
ER  - 

TY  - CONF
TI  - AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8504
EP  - 8510
AU  - I. Bozcan
AU  - E. Kayacan
PY  - 2020
KW  - autonomous aerial vehicles
KW  - cameras
KW  - computer vision
KW  - image annotation
KW  - image capture
KW  - image colour analysis
KW  - object detection
KW  - video signal processing
KW  - video surveillance
KW  - multimodal unmanned aerial vehicle dataset
KW  - low altitude traffic surveillance
KW  - UAVs
KW  - mounted cameras
KW  - aerial image capture
KW  - aerial visual data
KW  - object detection algorithms
KW  - computer vision community
KW  - object annotations
KW  - flying-cameras
KW  - multipurpose aerial dataset
KW  - multimodal sensor data
KW  - AU-AIR dataset
KW  - meta-data
KW  - traffic-related object category
KW  - mobile object detectors
KW  - real-time object detection
KW  - robotics
KW  - real-world outdoor environments
KW  - bounding box annotation
KW  - RGB videos recording
KW  - YOLOv3-Tiny
KW  - MobileNetv2-SSDLite
KW  - on-board computers
KW  - bird-view image
KW  - data types recording
KW  - Object detection
KW  - Videos
KW  - Visualization
KW  - Cameras
KW  - Detectors
KW  - Surveillance
DO  - 10.1109/ICRA40945.2020.9196845
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https://bozcani.github.io/auairdataset.
ER  - 

TY  - CONF
TI  - Design and Autonomous Stabilization of a Ballistically-Launched Multirotor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8511
EP  - 8517
AU  - A. Bouman
AU  - P. Nadan
AU  - M. Anderson
AU  - D. Pastor
AU  - J. Izraelevitz
AU  - J. Burdick
AU  - B. Kennedy
PY  - 2020
KW  - aerodynamics
KW  - autonomous aerial vehicles
KW  - ballistics
KW  - helicopters
KW  - mobile robots
KW  - robot vision
KW  - aircraft
KW  - drones
KW  - emergency response
KW  - space exploration
KW  - critical situational data
KW  - onboard sensors
KW  - multirotor prototype
KW  - onboard sensor suite
KW  - autonomy pipeline
KW  - aerodynamic stability
KW  - active stabilization
KW  - ballistic launch
KW  - streamlined quick unfolding investigation drone
KW  - vision-based autonomous transition
KW  - SQUID
KW  - SQUIDs
KW  - Electron tubes
KW  - Aerodynamics
KW  - Drones
KW  - Prototypes
KW  - Thermal stability
KW  - Aircraft
DO  - 10.1109/ICRA40945.2020.9197542
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Aircraft that can launch ballistically and convert to autonomous, free-flying drones have applications in many areas such as emergency response, defense, and space exploration, where they can gather critical situational data using onboard sensors. This paper presents a ballistically-launched, autonomously-stabilizing multirotor prototype (SQUID - Streamlined Quick Unfolding Investigation Drone) with an onboard sensor suite, autonomy pipeline, and passive aerodynamic stability. We demonstrate autonomous transition from passive to vision-based, active stabilization, confirming the multirotor's ability to autonomously stabilize after a ballistic launch in a GPS-denied environment.
ER  - 

TY  - CONF
TI  - Asynchronous event-based clustering and tracking for intrusion monitoring in UAS
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8518
EP  - 8524
AU  - J. P. Rodr√≠guez-Gomez
AU  - A. G. Egu√≠luz
AU  - J. R. Mart√≠nez-de Dios
AU  - A. Ollero
PY  - 2020
KW  - autonomous aerial vehicles
KW  - cameras
KW  - image sensors
KW  - object tracking
KW  - pattern clustering
KW  - robot vision
KW  - surveillance
KW  - feature tracking
KW  - intrusion monitoring
KW  - UAS
KW  - unmanned aerial systems
KW  - perception systems
KW  - illumination conditions
KW  - event cameras
KW  - neuromorphic sensors
KW  - illumination changes
KW  - event based vision
KW  - event stream
KW  - intruder monitoring
KW  - event clustering
KW  - event-by-event processing
KW  - asynchronous event-based clustering
KW  - automatic surveillance
KW  - on-board hardware computational constraints
KW  - Cameras
KW  - Robot vision systems
KW  - Tracking
KW  - Surveillance
KW  - Robustness
KW  - event camera
KW  - asynchronous
KW  - intrusion monitoring
KW  - surveillance
KW  - UAS
KW  - clustering
KW  - feature tracking
DO  - 10.1109/ICRA40945.2020.9197341
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Automatic surveillance and monitoring using Unmanned Aerial Systems (UAS) require the development of perception systems that robustly work under different illumination conditions. Event cameras are neuromorphic sensors that capture the illumination changes in the scene with very low latency and high dynamic range. Although recent advances in eventbased vision have explored the use of event cameras onboard UAS, most techniques group events in frames and, therefore, do not fully exploit the sequential and asynchronous nature of the event stream. This paper proposes a fully asynchronous scheme for intruder monitoring using UAS. It employs efficient event clustering and feature tracking modules and includes a sampling mechanism to cope with the computational cost of event-by-event processing adapting to on-board hardware computational constraints. The proposed scheme was tested on a real multirotor in challenging scenarios showing significant accuracy and robustness to lighting conditions.
ER  - 

TY  - CONF
TI  - SHIFT: Selective Heading Image for Translation An onboard monocular optical flow estimator for fast constantly rotating UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8525
EP  - 8531
AU  - M. Ng
AU  - E. Tang
AU  - G. S. Soh
AU  - S. Foong
PY  - 2020
KW  - aerospace control
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - cameras
KW  - image sensors
KW  - image sequences
KW  - Kalman filters
KW  - motion estimation
KW  - nonlinear filters
KW  - pose estimation
KW  - remotely operated vehicles
KW  - onboard monocular optical flow estimator
KW  - UAV
KW  - pose estimation
KW  - flight control
KW  - unmanned aerial vehicles
KW  - autonomous operations
KW  - onboard sensors
KW  - monocular camera
KW  - free rotors
KW  - flight dynamics
KW  - falling samara seed
KW  - constantly rotating body frame
KW  - optical flow sensing
KW  - optimal images
KW  - rotation vectors
KW  - optical axis
KW  - translation vectors
KW  - flow field
KW  - SHIFT estimation
KW  - selective heading image for translation
KW  - Cameras
KW  - Optical imaging
KW  - Optical sensors
KW  - Adaptive optics
KW  - Optical filters
KW  - Tracking
KW  - Integrated optics
DO  - 10.1109/ICRA40945.2020.9197073
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pose estimation is of paramount importance for flight control as well as localization and navigation of Unmanned Aerial Vehicles (UAVs) to enable autonomous operations. In environments without GPS, such estimation can only be determined using onboard sensors; optical flow using a monocular camera is a popular approach. Monocopters are a class of nature inspired UAVs known as free rotors where their design and flight dynamics are inspired by the falling samara seed. With a constantly rotating body frame, free rotors introduces some unique challenges for visual perception required during optical flow sensing. This paper addresses these problems with the introduction of SHIFT (Selective Heading Image for Translation) that selects optimal images for determining translation with optical flow. It achieves this by decoupling rotation vectors about the optical axis from translation vectors in a flow field through the separate tracking of orientation and position using an Unscented Kalman Filter with phase correlation in the log-polar and spatial domain. The experiments show that SHIFT's estimation in orientation is stable even under sinusoidal excitation with a median absolute percentage errors of less than 1%. It is able to track position and orientation of a UAV accurately.
ER  - 

TY  - CONF
TI  - Flydar: Magnetometer-based High Angular Rate Estimation during Gyro Saturation for SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8532
EP  - 8537
AU  - C. H. Tan
AU  - D. Sufiyan bin Shaiful
AU  - E. Tang
AU  - J. -Y. Khaw
AU  - G. S. Soh
AU  - S. Foong
PY  - 2020
KW  - gyroscopes
KW  - Kalman filters
KW  - magnetometers
KW  - mobile robots
KW  - nonlinear filters
KW  - optical radar
KW  - SLAM (robots)
KW  - Flydar
KW  - magnetometer-based high angular rate estimation
KW  - SLAM
KW  - simultaneous localisation and mapping
KW  - Flying Li-DAR
KW  - EKF-based algorithm
KW  - sinusoidal magnetometer measurement
KW  - continuously rotating airframe
KW  - IMU sensors
KW  - gyro measurement
KW  - gyro bias
KW  - gyro saturation condition
KW  - rotating locomotion
KW  - robot hovering angular velocity
KW  - Robots
KW  - Magnetometers
KW  - Sensors
KW  - Estimation
KW  - Frequency measurement
KW  - Saturation magnetization
KW  - Angular velocity
DO  - 10.1109/ICRA40945.2020.9197486
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, the high angular rate estimation for simultaneous localisation and mapping (SLAM) of a Flying Li-DAR (Flydar) is presented. The proposed EKF-based algorithm exploits the sinusoidal magnetometer measurement generated by the continuously rotating airframe for estimation of the robot hovering angular velocity. Significantly, the proposed method does not rely on additional sensors other than existing IMU sensors already being used for flight stabilization. The gyro measurement and the gyro bias are incorporated as a control input and a filter state respectively to enable estimation even under gyro saturation condition. Additionally, this work proposes leveraging on the inherently rotating locomotion to generate a planar lidar scan using only a single-point laser for possible lightweight autonomy. The proposed estimation method was experimentally evaluated on a ground rotating rig up to twice the gyro saturation limit with an effective rms error of 0.0045Hz; and on the proposed aerial platform - Flydar - hovering beyond the saturation limit with a rms error of 0.0056Hz. Lastly, the proposed method for SLAM using the rotating dynamics of Flydar was demonstrated with a localisation accuracy of 0.11m.
ER  - 

TY  - CONF
TI  - Nonlinear MPC with Motor Failure Identification and Recovery for Safe and Aggressive Multicopter Flight
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8538
EP  - 8544
AU  - D. Tzoumanikas
AU  - Q. Yan
AU  - S. Leutenegger
PY  - 2020
KW  - actuators
KW  - autonomous aerial vehicles
KW  - helicopters
KW  - Kalman filters
KW  - nonlinear control systems
KW  - nonlinear filters
KW  - predictive control
KW  - precise reference tracking
KW  - crucial characteristic
KW  - microaerial vehicles
KW  - MAV
KW  - external disturbances
KW  - cluttered environments
KW  - nonlinear model predictive control
KW  - NMPC
KW  - fully physics
KW  - nonlinear dynamics
KW  - control inputs
KW  - feasible actuator commands
KW  - safe operation
KW  - potential loss
KW  - flight experiments
KW  - motor failures
KW  - nonlinear MPC
KW  - aggressive multicopter flight
KW  - extended Kalman filter based motor failure identification algorithm
KW  - Actuators
KW  - Resource management
KW  - Propellers
KW  - Aerodynamics
KW  - Quaternions
KW  - Angular velocity
KW  - Vehicle dynamics
DO  - 10.1109/ICRA40945.2020.9196690
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Safe and precise reference tracking is a crucial characteristic of Micro Aerial Vehicles (MAVs) that have to operate under the influence of external disturbances in cluttered environments. In this paper, we present a Nonlinear Model Predictive Control (NMPC) that exploits the fully physics based non-linear dynamics of the system. We furthermore show how the moment and thrust control inputs can be transformed into feasible actuator commands. In order to guarantee safe operation despite potential loss of a motor under which we show our system keeps operating safely, we developed an Extended Kalman Filter (EKF) based motor failure identification algorithm. We verify the effectiveness of the developed pipeline in flight experiments with and without motor failures.
ER  - 

TY  - CONF
TI  - Temporal information integration for video semantic segmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8545
EP  - 8551
AU  - G. Guarino
AU  - T. Chateau
AU  - C. Teuli√®re
AU  - V. Antoine
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - image segmentation
KW  - image sequences
KW  - neural nets
KW  - probability
KW  - video signal processing
KW  - temporal information integration
KW  - video semantic segmentation
KW  - temporal Bayesian filter
KW  - video sequence
KW  - discrete probabilistic distribution function
KW  - possible semantic classes
KW  - Bayesian filtering
KW  - prediction model
KW  - observation model
KW  - datadriven prediction function
KW  - dense optical flow
KW  - deep neural network
KW  - observation function
KW  - semantic segmentation network
KW  - temporal filtering
KW  - Cityscapes
KW  - Optical imaging
KW  - Image segmentation
KW  - Semantics
KW  - Adaptive optics
KW  - Optical filters
KW  - Bayes methods
KW  - Optical fiber networks
DO  - 10.1109/ICRA40945.2020.9197204
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a temporal Bayesian filter for semantic segmentation of a video sequence. Each pixel is a random variable following a discrete probabilistic distribution function representing possible semantic classes. Bayesian filtering consists in two main steps: 1) a prediction model and 2) an observation model (likelihood). We propose to use a datadriven prediction function derived from a dense optical flow between images t and t + 1 achieved by a deep neural network [1]. Moreover, the observation function uses a semantic segmentation network. The resulting approach is evaluated on the public dataset Cityscapes. We show that using the temporal filtering increases the accuracy of the semantic segmentation.
ER  - 

TY  - CONF
TI  - Map-Predictive Motion Planning in Unknown Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8552
EP  - 8558
AU  - A. Elhafsi
AU  - B. Ivanovic
AU  - L. Janson
AU  - M. Pavone
PY  - 2020
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - predictive control
KW  - trajectory control
KW  - map prediction
KW  - data-driven method
KW  - autonomous navigation
KW  - hallway environments
KW  - na√Øve frontier pursuit method
KW  - heuristic methods
KW  - map-predictive motion planning
KW  - dynamically-constrained robots
KW  - trajectory planning
KW  - robot position
KW  - frontier selection heuristics
KW  - Robots
KW  - Trajectory
KW  - Planning
KW  - Navigation
KW  - Collision avoidance
KW  - Safety
KW  - Cognition
DO  - 10.1109/ICRA40945.2020.9197522
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Algorithms for motion planning in unknown environments are generally limited in their ability to reason about the structure of the unobserved environment. As such, current methods generally navigate unknown environments by relying on heuristic methods to choose intermediate objectives along frontiers. We present a unified method that combines map prediction and motion planning for safe, time-efficient au-tonomous navigation of unknown environments by dynamically-constrained robots. We propose a data-driven method for predicting the map of the unobserved environment, using the robot's observations of its surroundings as context. These map predictions are then used to plan trajectories from the robot's position to the goal without requiring frontier selection. We applied this map-predictive motion planning strategy to randomly generated winding hallway environments, yielding substantial improvement in trajectory duration over a na√Øve frontier pursuit method. We also experimentally demonstrate similar performance to methods using more sophisticated fron-tier selection heuristics while significantly reducing computation time.
ER  - 

TY  - CONF
TI  - Using multiple short hops for multicopter navigation with only inertial sensors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8559
EP  - 8565
AU  - X. Wu
AU  - M. W. Mueller
PY  - 2020
KW  - accelerometers
KW  - closed loop systems
KW  - gyroscopes
KW  - helicopters
KW  - inertial navigation
KW  - Kalman filters
KW  - nonlinear filters
KW  - position control
KW  - state estimation
KW  - closed-loop control
KW  - mean absolute position estimation error
KW  - total flight distance
KW  - standard inertial navigation method
KW  - trajectory tracking error
KW  - multiple short hops
KW  - multicopter navigation
KW  - GPS systems
KW  - multicopter localization
KW  - direct integration
KW  - inertial navigation sensors
KW  - accelerometer
KW  - rate gyroscope
KW  - rapid error accumulation
KW  - motion strategy
KW  - inertial navigation state estimation error
KW  - long duration flight
KW  - multiple short duration hops
KW  - zero-velocity pseudomeasurements
KW  - extended Kalman filter
KW  - LiDAR
KW  - real-world environment
KW  - distance 5.0 m
KW  - distance 10.0 m
KW  - State estimation
KW  - Gyroscopes
KW  - Sensors
KW  - Inertial navigation
KW  - Accelerometers
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196610
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In certain challenging environments, such as inside buildings on fire, the main sensors (e.g. cameras, LiDARs and GPS systems) used for multicopter localization can become unavailable. Direct integration of the inertial navigation sensors (the accelerometer and rate gyroscope), is however unaffected by external disturbances, but the rapid error accumulation quickly makes a naive application of such a strategy feasible only for very short durations. In this work we propose a motion strategy for reducing the inertial navigation state estimation error of multicopters. The proposed strategy breaks a long duration flight into multiple short duration hops between which the vehicle remains stationary on the ground. When the vehicle is stationary, zero-velocity pseudo-measurements are introduced to an extended Kalman Filter to reduce the state estimation error. We perform experiments for closed-loop control of a multicopter for evaluation. The mean absolute position estimation error was 3.4% over a total flight distance of 5m in the experiments. The results showed a 80% reduction compared to the standard inertial navigation method without using this strategy. In addition, an additional experiment with total flight distance of 10m is conducted to demonstrate the ability of this method to navigate a multicopter in real-world environment. The final trajectory tracking error was 3% of the total flight distance.
ER  - 

TY  - CONF
TI  - An Efficient and Continuous Approach to Information-Theoretic Exploration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8566
EP  - 8572
AU  - T. Henderson
AU  - V. Sze
AU  - S. Karaman
PY  - 2020
KW  - computational complexity
KW  - information theory
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - information-theoretic exploration
KW  - continuous occupancy map framework
KW  - |Œò| measurement beams
KW  - recursive structure
KW  - robotics applications
KW  - autonomous navigation task
KW  - Robot sensing systems
KW  - Mutual information
KW  - Distortion measurement
KW  - Gain measurement
KW  - Time measurement
DO  - 10.1109/ICRA40945.2020.9196592
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration of unknown environments is embedded and essential in many robotics applications. Traditional algorithms, that decide where to explore by computing the expected information gain of an incomplete map from future sensor measurements, are limited to very powerful computational platforms. In this paper, we describe a novel approach for computing this expected information gain efficiently, as principally derived via mutual information. The key idea behind the proposed approach is a continuous occupancy map framework and the recursive structure it reveals. This structure makes it possible to compute the expected information gain of sensor measurements across an entire map much faster than computing each measurements' expected gain independently. Specifically, for an occupancy map composed of |M| cells and a range sensor that emits |Œò| measurement beams, the algorithm (titled FCMI) computes the information gain corresponding to measurements made at each cell in O(|Œò||M|) steps. To the best of our knowledge, this complexity bound is better than all existing methods for computing information gain. In our experiments, we observe that this novel, continuous approach is two orders of magnitude faster than the state-of-the-art FSMI algorithm.
ER  - 

TY  - CONF
TI  - A Feature-Based Underwater Path Planning Approach using Multiple Perspective Prior Maps
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8573
EP  - 8579
AU  - D. Cagara
AU  - M. Dunbabin
AU  - P. Rigby
PY  - 2020
KW  - autonomous underwater vehicles
KW  - bathymetry
KW  - maximum likelihood estimation
KW  - mobile robots
KW  - navigation
KW  - path planning
KW  - remotely operated vehicles
KW  - multiple perspective prior maps
KW  - path planning methodology
KW  - Autonomous Underwater Vehicles
KW  - AUV
KW  - shallow complex environments
KW  - coral reefs
KW  - aerial photographic survey
KW  - bathymetric information
KW  - prior map
KW  - navigation graph
KW  - test points
KW  - shortest paths
KW  - destination points
KW  - maximum likelihood function
KW  - misclassified objects
KW  - photo-realistic simulated environment
KW  - Navigation
KW  - Cameras
KW  - Sensors
KW  - Uncertainty
KW  - Image segmentation
KW  - Feature extraction
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196680
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a path planning methodology which enables Autonomous Underwater Vehicles (AUVs) to navigate in shallow complex environments such as coral reefs. The approach leverages prior information from an aerial photographic survey, and derived bathymetric information of the corresponding area. From these prior maps, a set of features is obtained which define an expected arrangement of objects and bathymetry likely to be perceived by the AUV when underwater. A navigation graph is then constructed by predicting the arrangement of features visible from a set of test points within the prior, which allows the calculation of the shortest paths from any pair of start and destination points. A maximum likelihood function is defined which allows the AUV to match its observations to the navigation graph as it undertakes its mission. To improve robustness, the history of observed features are retained to facilitate possible recovery from non-detectable or misclassified objects. The approach is evaluated using a photo-realistic simulated environment, and results illustrate the merits of the approach even when only a relatively small number of features can be identified from the prior map.
ER  - 

TY  - CONF
TI  - Automatic LiDAR-Camera Calibration of Extrinsic Parameters Using a Spherical Target
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8580
EP  - 8586
AU  - T. T√≥th
AU  - Z. Pusztai
AU  - L. Hajder
PY  - 2020
KW  - calibration
KW  - computer vision
KW  - optical radar
KW  - radar imaging
KW  - surface topography measurement
KW  - computer vision applications
KW  - fully automatic extrinsic calibration
KW  - LiDAR extrinsic parameters
KW  - automatic LiDAR-camera calibration
KW  - spherical target
KW  - LiDAR-camera imaging system
KW  - point clouds
KW  - Cameras
KW  - Calibration
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Estimation
KW  - Mathematical model
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197316
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper investigates a novel calibration process of devices with different modalities, which is a critical step of computer vision applications. We propose a fully automatic extrinsic calibration of a LiDAR-camera system. Our approach applies sphere as their surfaces and contours can be accurately detected on point clouds and camera images, respectively. Experiments on synthetic and real data exhibit that our automatic algorithm is fast and robust and it yields accurate camera and LiDAR extrinsic parameters.
ER  - 

TY  - CONF
TI  - Motion Estimation in Occupancy Grid Maps in Stationary Settings Using Recurrent Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8587
EP  - 8593
AU  - M. Schreiber
AU  - V. Belagiannis
AU  - C. Gl√§ser
AU  - K. Dietmayer
PY  - 2020
KW  - image filtering
KW  - image motion analysis
KW  - motion estimation
KW  - optical radar
KW  - path planning
KW  - probability
KW  - radar imaging
KW  - recurrent neural nets
KW  - traffic engineering computing
KW  - occupancy grid maps
KW  - recurrent neural networks
KW  - grid cell
KW  - occupancy probability
KW  - measurement grid maps
KW  - occupancy probabilities
KW  - filtered occupancy
KW  - network architecture
KW  - Computer architecture
KW  - Microprocessors
KW  - Vehicle dynamics
KW  - Measurement by laser beam
KW  - Time measurement
KW  - Recurrent neural networks
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196702
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we tackle the problem of modeling the vehicle environment as dynamic occupancy grid map in complex urban scenarios using recurrent neural networks. Dynamic occupancy grid maps represent the scene in a bird's eye view, where each grid cell contains the occupancy prob-ability and the two dimensional velocity. As input data, our approach relies on measurement grid maps, which contain occupancy probabilities, generated with lidar measurements. Given this configuration, we propose a recurrent neural net-work architecture to predict a dynamic occupancy grid map, i.e. filtered occupancy and velocity of each cell, by using a sequence of measurement grid maps. Our network architecture contains convolutional long-short term memories in order to sequentially process the input, makes use of spatial context, and captures motion. In the evaluation, we quantify improvements in estimating the velocity of braking and turning vehicles compared to the state-of-the-art. Additionally, we demonstrate that our approach provides more consistent velocity estimates for dynamic objects, as well as, less erroneous velocity estimates in static area.
ER  - 

TY  - CONF
TI  - A Divide and Conquer Method for 3D Registration of Inhomogeneous, Partially Overlapping Scans with Fourier Mellin SOFT (FMS)
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8594
EP  - 8601
AU  - H. B√ºlow
AU  - C. A. Mueller
AU  - A. Gomez Chavez
AU  - F. Buda
AU  - A. Birk
PY  - 2020
KW  - divide and conquer methods
KW  - image registration
KW  - laser ranging
KW  - high-end laser range-finders
KW  - divide-and-conquer method
KW  - 3D registration method
KW  - Fourier-Mellin-SOFT
KW  - FMS
KW  - partial overlaps
KW  - large-scale cultural heritage site
KW  - Three-dimensional displays
KW  - Frequency modulation
KW  - Robustness
KW  - Underwater vehicles
KW  - Two dimensional displays
KW  - Cultural differences
KW  - Nonhomogeneous media
DO  - 10.1109/ICRA40945.2020.9197453
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - High-end laser range-finders provide accurate 3D data over long ranges. But their scans are inhomogeneous, i.e., the environment is non-uniformly sampled, as there is denser data in the near range than in the far range. Furthermore, the generation of a scan is time-consuming. Thus, it is desirable to cover an area by as few scans as possible, i.e., scanning is more time-efficient if the overlap between scans is as small as possible. However, these factors pose significant challenges for state-of-the-art registration algorithms. In this work, we present a divide-and-conquer method that uses an efficient strategy to check for possible registrations between partitions of two scans. As underlying registration method, Fourier-Mellin-SOFT (FMS) is used. FMS is quite robust against partial overlaps but its performance is significantly boosted by the presented partitioning method. As concrete use case, results from the digitization of a WWII submarine bunker as a large-scale cultural heritage site are presented.
ER  - 

TY  - CONF
TI  - Estimating Motion Uncertainty with Bayesian ICP
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8602
EP  - 8608
AU  - F. A. Maken
AU  - F. Ramos
AU  - L. Ott
PY  - 2020
KW  - Bayes methods
KW  - image fusion
KW  - iterative methods
KW  - Markov processes
KW  - Monte Carlo methods
KW  - motion estimation
KW  - pose estimation
KW  - motion uncertainty
KW  - accurate uncertainty estimation
KW  - pose transformation
KW  - autonomous navigation
KW  - data fusion
KW  - iterative closest point
KW  - point cloud pairs
KW  - motion estimation
KW  - deterministic algorithm
KW  - probabilistic manner
KW  - data association errors
KW  - sensor noise
KW  - overconfident transformation estimates
KW  - pose uncertainty
KW  - Markov Chain Monte Carlo algorithm
KW  - scalable Bayesian sampling
KW  - stochastic gradient Langevin dynamics
KW  - data association uncertainty
KW  - 3D Kinect data
KW  - Bayesian ICP
KW  - Iterative closest point algorithm
KW  - Uncertainty
KW  - Bayes methods
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Standards
KW  - Stochastic processes
DO  - 10.1109/ICRA40945.2020.9197085
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate uncertainty estimation associated with the pose transformation between two 3D point clouds is critical for autonomous navigation, grasping, and data fusion. Iterative closest point (ICP) is widely used to estimate the transformation between point cloud pairs by iteratively performing data association and motion estimation. Despite its success and popularity, ICP is effectively a deterministic algorithm, and attempts to reformulate it in a probabilistic manner generally do not capture all sources of uncertainty, such as data association errors and sensor noise. This leads to overconfident transformation estimates, potentially compromising the robustness of systems relying on them. In this paper we propose a novel method to estimate pose uncertainty in ICP with a Markov Chain Monte Carlo (MCMC) algorithm. Our method combines recent developments in optimization for scalable Bayesian sampling such as stochastic gradient Langevin dynamics (SGLD) to infer a full posterior distribution of the pose transformation between two point clouds. We evaluate our method, called Bayesian ICP, in experiments using 3D Kinect data demonstrating that our method is capable of both quickly and accuractely estimating pose uncertainty, taking into account data association uncertainty as reflected by the shape of the objects.
ER  - 

TY  - CONF
TI  - Actively Mapping Industrial Structures with Information Gain-Based Planning on a Quadruped Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8609
EP  - 8615
AU  - Y. Wang
AU  - M. Ramezani
AU  - M. Fallon
PY  - 2020
KW  - collision avoidance
KW  - image representation
KW  - legged locomotion
KW  - robot vision
KW  - industrial structure
KW  - mapping industrial structures
KW  - information gain-based planning
KW  - quadruped robot
KW  - online active mapping system
KW  - voxel representation
KW  - NBV
KW  - expected information gain
KW  - terrain map
KW  - ANYbotics ANYmal robot
KW  - Robot sensing systems
KW  - Service robots
KW  - Solid modeling
KW  - Planning
KW  - Laser radar
DO  - 10.1109/ICRA40945.2020.9197153
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we develop an online active mapping system to enable a quadruped robot to autonomously survey large physical structures. We describe the perception, planning and control modules needed to scan and reconstruct an object of interest, without requiring a prior model. The system builds a voxel representation of the object, and iteratively determines the Next-Best-View (NBV) to extend the representation, according to both the reconstruction itself and to avoid collisions with the environment. By computing the expected information gain of a set of candidate scan locations sampled on the as-sensed terrain map, as well as the cost of reaching these candidates, the robot decides the NBV for further exploration. The robot plans an optimal path towards the NBV, avoiding obstacles and un-traversable terrain. Experimental results on both simulated and real-world environments show the capability and efficiency of our system. Finally we present a full system demonstration on the real robot, the ANYbotics ANYmal, autonomously reconstructing a building facade and an industrial structure.
ER  - 

TY  - CONF
TI  - Efficient Covisibility-based Image Matching for Large-Scale SfM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8616
EP  - 8622
AU  - Z. Ye
AU  - G. Zhang
AU  - H. Bao
PY  - 2020
KW  - feature extraction
KW  - image matching
KW  - image motion analysis
KW  - iterative methods
KW  - large-scale SfM
KW  - feature matches
KW  - large-scale structure-from-motion
KW  - unordered image collections
KW  - traditional feature matching method
KW  - region covisibility
KW  - overlapping image pairs
KW  - iterative matching strategy
KW  - unordered image datasets
KW  - robust SfM
KW  - efficient image matching method
KW  - covisibility-based image matching
KW  - Three-dimensional displays
KW  - Vocabulary
KW  - Image reconstruction
KW  - Image retrieval
KW  - Feature extraction
KW  - Robustness
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197383
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Obtaining accurate and sufficient feature matches is crucial for robust large-scale Structure-from-Motion. For unordered image collections, a traditional feature matching method with geometric verification requires a huge cost to find sufficient feature matches. Although several methods have been proposed to speed up this stage, none of them makes full use of existing matches. In this paper, we propose a novel efficient image matching method by using the transitivity of region covisibility. The overlapping image pairs can be efficiently found in an iterative matching strategy even only with few inlier feauture matches. The experimental results on unordered image datasets demonstrate that the proposed method is three times faster than the state-of-the-art and the matching result is high-quality enough for robust SfM.
ER  - 


TY  - CONF
TI  - Probabilistic TSDF Fusion Using Bayesian Deep Learning for Dense 3D Reconstruction with a Single RGB Camera
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8623
EP  - 8629
AU  - H. Kim
AU  - B. Lee
PY  - 2020
KW  - Bayes methods
KW  - belief networks
KW  - cameras
KW  - image colour analysis
KW  - image fusion
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - Monte Carlo methods
KW  - neural nets
KW  - depth prediction
KW  - robust 3D reconstruction
KW  - Bayesian deep learning framework
KW  - Conventional Bayesian deep learning
KW  - probabilistic TSDF fusion
KW  - dense 3D reconstruction
KW  - global TSDF
KW  - single RGB camera
KW  - 3D reconstruction problem
KW  - single RGB image
KW  - training environment
KW  - test environment
KW  - lightweight Bayesian neural network
KW  - Uncertainty
KW  - Three-dimensional displays
KW  - Probabilistic logic
KW  - Bayes methods
KW  - Predictive models
KW  - Cameras
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9196663
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we address a 3D reconstruction problem using depth prediction from a single RGB image. With the recent advances in deep learning, depth prediction shows high performance. However, due to the discrepancy between training environment and test environment, 3D reconstruction can be vulnerable to the uncertainty of depth prediction. To consider the uncertainty of depth prediction for robust 3D reconstruction, we adopt Bayesian deep learning framework. Conventional Bayesian deep learning requires a large amount of time and GPU memory to perform Monte Carlo sampling. To address this problem, we propose a lightweight Bayesian neural network consisting of U-net structure and summation-based skip connections, which is performed in real-time. Estimated uncertainty is utilized in probabilistic TSDF fusion for dense 3D reconstruction by maximizing the posterior of TSDF value per voxel. As a result, global TSDF robust to erroneous depth values can be obtained and then dense 3D reconstruction from the global TSDF is achievable more accurately. To evaluate the performance of depth prediction and 3D reconstruction using our method, we utilized two official datasets and demonstrated the outperformance of the proposed method over other conventional methods.
ER  - 

TY  - CONF
TI  - IF-Net: An Illumination-invariant Feature Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8630
EP  - 8636
AU  - P. -H. Chen
AU  - Z. -X. Luo
AU  - Z. -K. Huang
AU  - C. Yang
AU  - K. -W. Chen
PY  - 2020
KW  - computer vision
KW  - feature extraction
KW  - image matching
KW  - image retrieval
KW  - learning (artificial intelligence)
KW  - object detection
KW  - illumination-invariant feature network
KW  - feature descriptor matching
KW  - computer vision applications
KW  - image stitching
KW  - image retrieval
KW  - visual localization
KW  - illumination variations
KW  - descriptor learning
KW  - robust descriptor
KW  - generic descriptor
KW  - training data
KW  - dataset scheduling methods
KW  - ROI loss
KW  - hard-positive mining strategy
KW  - illumination change conditions
KW  - IF-Net
KW  - Lighting
KW  - Training
KW  - Measurement
KW  - Benchmark testing
KW  - Training data
KW  - Schedules
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196893
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Feature descriptor matching is a critical step is many computer vision applications such as image stitching, image retrieval and visual localization. However, it is often affected by many practical factors which will degrade its performance. Among these factors, illumination variations are the most influential one, and especially no previous descriptor learning works focus on dealing with this problem. In this paper, we propose IF-Net, aimed to generate a robust and generic descriptor under crucial illumination changes conditions. We find out not only the kind of training data important but also the order it is presented. To this end, we investigate several dataset scheduling methods and propose a separation training scheme to improve the matching accuracy. Further, we propose a ROI loss and hard-positive mining strategy along with the training scheme, which can strengthen the ability of generated descriptor dealing with large illumination change conditions. We evaluate our approach on public patch matching benchmark and achieve the best results compared with several state-of-the-arts methods. To show the practicality, we further evaluate IF-Net on the task of visual localization under large illumination changes scenes, and achieves the best localization accuracy.
ER  - 

TY  - CONF
TI  - Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8637
EP  - 8643
AU  - Y. Hu
AU  - W. Zhen
AU  - S. Scherer
PY  - 2020
KW  - cameras
KW  - image matching
KW  - image reconstruction
KW  - image resolution
KW  - learning (artificial intelligence)
KW  - stereo image processing
KW  - Middlebury dataset
KW  - nonlearning method
KW  - infrastructure inspection
KW  - downstream process
KW  - stereo reconstruction methods
KW  - semiglobal block matching method
KW  - 3D reconstruction error
KW  - infrastructure inspection experiments
KW  - customized binocular stereo camera
KW  - high-resolution stereo images
KW  - deep-learning assisted method
KW  - predicted disparity
KW  - perpixel searching range
KW  - down-sampled stereo image pair
KW  - initial disparity prediction
KW  - deep-learning model
KW  - accurate stereo reconstruction
KW  - learning-based model
KW  - resource demanding nonlearning method
KW  - task-specific training data
KW  - generalization issue
KW  - learning-based methods
KW  - high-resolution data
KW  - computational resource
KW  - infrastructure inspections
KW  - dense stereo reconstruction
KW  - assisted high-resolution binocular stereo depth reconstruction
KW  - Uncertainty
KW  - Predictive models
KW  - Image reconstruction
KW  - Proposals
KW  - Computational modeling
KW  - Image resolution
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196655
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents dense stereo reconstruction using high-resolution images for infrastructure inspections. The state-of-the-art stereo reconstruction methods, both learning and non-learning ones, consume too much computational resource on high-resolution data. Recent learning-based methods achieve top ranks on most benchmarks. However, they suffer from the generalization issue due to lack of task-specific training data. We propose to use a less resource demanding non-learning method, guided by a learning-based model, to handle high-resolution images and achieve accurate stereo reconstruction. The deep-learning model produces an initial disparity prediction with uncertainty for each pixel of the down-sampled stereo image pair. The uncertainty serves as a self-measurement of its generalization ability and the perpixel searching range around the initially predicted disparity. The downstream process performs a modified version of the Semi-Global Block Matching method with the up-sampled perpixel searching range. The proposed deep-learning assisted method is evaluated on the Middlebury dataset and high-resolution stereo images collected by our customized binocular stereo camera. The combination of learning and non-learning methods achieves better performance on 12 out of 15 cases of the Middlebury dataset. In our infrastructure inspection experiments, the average 3D reconstruction error is less than 0.004m.
ER  - 

TY  - CONF
TI  - Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8644
EP  - 8650
AU  - L. Hajder
AU  - D. Barath
PY  - 2020
KW  - calibration
KW  - computer vision
KW  - image motion analysis
KW  - least squares approximations
KW  - optimisation
KW  - polynomials
KW  - closed-form solver
KW  - point correspondences
KW  - camera movement
KW  - motion parameters
KW  - vehicle-mounted cameras
KW  - least-squares optimal relative planar motion
KW  - 6th degree polynomial
KW  - Cameras
KW  - Transmission line matrix methods
KW  - Robot vision systems
KW  - Estimation
KW  - Automobiles
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196755
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A new closed-form solver is proposed minimizing the algebraic error optimally, in the least squares sense, to estimate the relative planar motion of two calibrated cameras. The main objective is to solve the over-determined case, i.e., when a larger-than-minimal sample of point correspondences is given - thus, estimating the motion from at least three correspondences. The algorithm requires the camera movement to be constrained to a plane, e.g. mounted to a vehicle, and the image plane to be orthogonal to the ground.1 The solver obtains the motion parameters as the roots of a 6th degree polynomial. It is validated both in synthetic experiments and on publicly available real-world datasets that using the proposed solver leads to results superior to the state-of-the-art in terms of geometric accuracy with no noticeable deterioration in the processing time.
ER  - 

TY  - CONF
TI  - Relative planar motion for vehicle-mounted cameras from a single affine correspondence
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8651
EP  - 8657
AU  - L. Hajder
AU  - D. Barath
PY  - 2020
KW  - calibration
KW  - cameras
KW  - computer vision
KW  - image motion analysis
KW  - image sensors
KW  - relative planar motion
KW  - vehicle-mounted cameras
KW  - single affine correspondence
KW  - extrinsic camera parameters
KW  - general planar motion
KW  - camera movement
KW  - image plane
KW  - minimal solver
KW  - semicalibrated case
KW  - common focal length
KW  - fully calibrated case
KW  - Cameras
KW  - Transmission line matrix methods
KW  - Mathematical model
KW  - Estimation
KW  - Geometry
KW  - Robot vision systems
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197438
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Two solvers are proposed for estimating the extrinsic camera parameters from a single affine correspondence assuming general planar motion. In this case, the camera movement is constrained to a plane and the image plane is orthogonal to the ground. The algorithms do not assume other constraints, e.g. the non-holonomic one, to hold. A new minimal solver is proposed for the semi-calibrated case, i.e. the camera parameters are known except a common focal length. Another method is proposed for the fully calibrated case. Due to requiring a single correspondence, robust estimation, e.g. histogram voting, leads to a fast and accurate procedure. The proposed methods are tested in our synthetic environment and on publicly available real datasets consisting of videos through tens of kilometers. They are superior to the state-of-the-art both in terms of accuracy and processing time.
ER  - 

TY  - CONF
TI  - Moving object detection for visual odometry in a dynamic environment based on occlusion accumulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8658
EP  - 8664
AU  - H. Kim
AU  - P. Kim
AU  - H. J. Kim
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - image colour analysis
KW  - image segmentation
KW  - image sensors
KW  - mobile robots
KW  - object detection
KW  - pose estimation
KW  - regression analysis
KW  - robot vision
KW  - dynamic environment
KW  - simple moving object detection algorithm
KW  - dense visual odometry
KW  - VO algorithms
KW  - occlusion accumulation
KW  - color images
KW  - robotic navigation
KW  - real-time RGBD data
KW  - depth information
KW  - obstacle recognition
KW  - camera pose estimate
KW  - bi-square regression weight
KW  - segmentation accuracy
KW  - public datasets
KW  - Cameras
KW  - Heuristic algorithms
KW  - Object detection
KW  - Robustness
KW  - Trajectory
KW  - Visual odometry
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196767
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Detection of moving objects is an essential capability in dealing with dynamic environments. Most moving object detection algorithms have been designed for color images without depth. For robotic navigation where real-time RGBD data is often readily available, utilization of the depth information would be beneficial for obstacle recognition. Here, we propose a simple moving object detection algorithm that uses RGB-D images. The proposed algorithm does not require estimating a background model. Instead, it uses an occlusion model which enables us to estimate the camera pose on a background confused with moving objects that dominate the scene. The proposed algorithm allows to separate the moving object detection and visual odometry (VO) so that an arbitrary robust VO method can be employed in a dynamic situation with a combination of moving object detection, whereas other VO algorithms for a dynamic environment are inseparable. In this paper, we use dense visual odometry (DVO) as a VO method with a bi-square regression weight. Experimental results show the segmentation accuracy and the performance improvement of DVO in the situations. We validate our algorithm in public datasets and our dataset which also publicly accessible.
ER  - 

TY  - CONF
TI  - A Low-Rank Matrix Approximation Approach to Multiway Matching with Applications in Multi-Sensory Data Association
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8665
EP  - 8671
AU  - S. Leonardos
AU  - X. Zhou
AU  - K. Daniilidis
PY  - 2020
KW  - approximation theory
KW  - computational complexity
KW  - concave programming
KW  - image matching
KW  - matrix algebra
KW  - sensor fusion
KW  - stochastic processes
KW  - multisensory data association
KW  - multiple visual sensors
KW  - consistent visual perception
KW  - noisy pairwise correspondences
KW  - multiway matching problem
KW  - low-rank matrix approximation problem problem
KW  - alternating direction method of multipliers
KW  - stochastic matrices
KW  - Fisher information metric
KW  - computational complexity
KW  - ADMM
KW  - Convex functions
KW  - Optimization
KW  - Manifolds
KW  - Xenon
KW  - Approximation algorithms
KW  - Clustering algorithms
KW  - Noise measurement
DO  - 10.1109/ICRA40945.2020.9196583
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Consider the case of multiple visual sensors perceiving the same scene from different viewpoints. In order to achieve consistent visual perception, the problem of data association, in this case establishing correspondences between observed features, must be first solved. In this work, we consider multiway matching which is a specific instance of multi-sensory data association. Multiway matching refers to the problem of establishing correspondences among a set of images from noisy pairwise correspondences, typically by exploiting cycle- consistency. We propose a novel optimization-based formulation of multiway matching problem as a nonconvex low-rank matrix approximation problem. We propose two novel algorithms for numerically solving the problem at hand. The first one is an algorithm based on the Alternating Direction Method of Multipliers (ADMM). The second one is a Riemannian trust- region method on the multinomial manifold, the manifold of strictly positive stochastic matrices, equipped with the Fisher information metric. Experimental results demonstrate that the proposed methods have the state of the art performance in multiway matching while reducing the computational complexity compared to the state of the art.
ER  - 

TY  - CONF
TI  - A Parametric Grasping Methodology for Multi-Manual Interactions in Real-Time Dynamic Simulations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8712
EP  - 8718
AU  - A. Munawar
AU  - N. Srishankar
AU  - L. Fichera
AU  - G. S. Fischer
PY  - 2020
KW  - biomechanics
KW  - control engineering computing
KW  - data visualisation
KW  - kinematics
KW  - manipulators
KW  - medical robotics
KW  - rendering (computer graphics)
KW  - surgery
KW  - telerobotics
KW  - parametric grasping methodology
KW  - multimanual interactions
KW  - interactive simulators
KW  - training simulators
KW  - teleoperated robotic laparoscopic surgery
KW  - stateof-art simulators
KW  - realistic visuals
KW  - accurate dynamics
KW  - kinematic simplification techniques
KW  - truly multimanual manipulation
KW  - actual task
KW  - realistic grasping
KW  - rigid-body dynamics
KW  - collision computation techniques
KW  - state-of-the-art physics libraries
KW  - parametric approach
KW  - multimanual grasping
KW  - real-time dynamic simulation
KW  - accomplishing multimanual tasks
KW  - screwdriver task
KW  - Friction
KW  - Grasping
KW  - Force
KW  - Sensors
KW  - Computational modeling
KW  - Mathematical model
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197099
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Interactive simulators are used in several important applications which include the training simulators for teleoperated robotic laparoscopic surgery. While stateof-art simulators are capable of rendering realistic visuals and accurate dynamics, grasping is often implemented using kinematic simplification techniques that prevent truly multimanual manipulation, which is often an important requirement of the actual task. Realistic grasping and manipulation in simulation is a challenging problem due to the constraints imposed by the implementation of rigid-body dynamics and collision computation techniques in state-of-the-art physics libraries. We present a penalty based parametric approach to achieve multi-manual grasping and manipulation of complex objects at arbitrary postures in a real-time dynamic simulation. This approach is demonstrated by accomplishing multi-manual tasks modeled after realistic scenarios, which include the grasping and manipulation of a two-handed screwdriver task and the manipulation of a deformable thread.
ER  - 

TY  - CONF
TI  - A methodology for the incorporation of arbitrarily-shaped feet in passive bipedal walking dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8719
EP  - 8725
AU  - A. Smyrli
AU  - E. Papadopoulos
PY  - 2020
KW  - computational geometry
KW  - legged locomotion
KW  - motion control
KW  - pose estimation
KW  - public domain software
KW  - robot dynamics
KW  - stability
KW  - ankle trajectory
KW  - robot dynamics
KW  - shape dependent foot kinetics
KW  - OpenPose
KW  - open source pose estimation system
KW  - rigid foot passive robot
KW  - walking robot stability
KW  - foot shape optimization
KW  - exact foot geometry
KW  - dynamic model
KW  - biped robot
KW  - passive bipedal walking dynamics
KW  - Foot
KW  - Legged locomotion
KW  - Shape
KW  - Geometry
KW  - Mathematical model
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196617
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A methodology for implementing arbitrary foot shapes in the passive walking dynamics of biped robots is developed. The dynamic model of a walking robot is defined in a way that allows shape-dependent foot kinetics to contribute to the robot's dynamics, for all convex foot shapes regardless of the exact foot geometry: for the developed method, only the set of points describing the foot profile curve is needed. The method is mathematically derived and then showcased with an application. The open-source pose estimation system OpenPose is used to determine the foot profile that enables the rigid-foot passive robot to reproduce the ankle trajectory of the actively powered, multi-DOF human foot complex. The passive gait of the biped robot walking on the specified foot shape is simulated and analyzed, and a stable walking cycle is found and evaluated. The proposed model enables the study of the effects of foot shape on the walking dynamics of biped robots, eliminating the necessity of solely using simple, and analytically defined geometric shapes as the walking robots' feet. The method can be used for foot shape optimization towards achieving any desired walking pattern in walking robots.
ER  - 

TY  - CONF
TI  - Experimental Analysis of Structural Vibration Problems of a Biped Walking Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8726
EP  - 8731
AU  - T. F. C. Berninger
AU  - F. Sygulla
AU  - S. Fuderer
AU  - D. J. Rixen
PY  - 2020
KW  - closed loop systems
KW  - humanoid robots
KW  - legged locomotion
KW  - modal analysis
KW  - position control
KW  - robot dynamics
KW  - vibrations
KW  - biped using Experimental Modal Analysis
KW  - structural design
KW  - low level position control
KW  - walking control
KW  - control design
KW  - structural dynamics
KW  - LOLA's mechanical structure
KW  - biped walking robot
KW  - control algorithms
KW  - structural vibration problems
KW  - structural resonances
KW  - control loop resonances
KW  - closed-loop identification method
KW  - structural modes
KW  - Legged locomotion
KW  - Robot sensing systems
KW  - Foot
KW  - Gears
KW  - Harmonic analysis
DO  - 10.1109/ICRA40945.2020.9197282
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Over the past decade we have been able to vastly improve the control algorithms of our biped walking robot LOLA. Further enhancements, however, are limited by vibration problems caused by the dynamics of LOLA's mechanical structure. In this work, we present small examples how structural dynamics limit our control design for walking control as well as low level position control of the joints. We also provide a procedure to identify weaknesses in the structural design of our biped using Experimental Modal Analysis. Using this method, we could successfully identify the structural modes of the system. Furthermore, we were able to use a closed-loop identification method to show a connection between the control loop resonances and the structural resonances of our robot.
ER  - 

TY  - CONF
TI  - Dynamic Coupling as an Indicator of Gait Robustness for Underactuated Biped Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8732
EP  - 8738
AU  - M. Fevre
AU  - J. P. Schmiedeler
PY  - 2020
KW  - legged locomotion
KW  - robot dynamics
KW  - robust control
KW  - trajectory control
KW  - dynamic coupling
KW  - gait robustness
KW  - velocity decomposition
KW  - underactuated mechanical systems
KW  - two link biped model
KW  - underactuated biped robots
KW  - trajectory optimization
KW  - Couplings
KW  - Mathematical model
KW  - Legged locomotion
KW  - Aerodynamics
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197203
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper employs velocity decomposition of underactuated mechanical systems to determine the degree of dynamic coupling in the gaits of a two-link biped model. The degree of coupling between controlled and uncontrolled directions quantifies the control authority the system has over its unactuated degree of freedom. This paper shows that the amount of coupling is directly correlated to gait robustness, as seen through the size of the gait's region of attraction. The analytical measure of coupling is applied in the context of trajectory optimization to generate two-link gaits that maximize or minimize coupling. Simulation studies show that gaits maximizing coupling exhibit significantly superior robustness, as measured by 1) stochastic performance on uneven terrain, 2) ability to maintain desired walking speed under non-vanishing disturbances, 3) size of the region of attraction, and 4) robustness to model uncertainties.
ER  - 

TY  - CONF
TI  - ZMP Constraint Restriction for Robust Gait Generation in Humanoids
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8739
EP  - 8745
AU  - F. M. Smaldone
AU  - N. Scianca
AU  - V. Modugno
AU  - L. Lanari
AU  - G. Oriolo
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - predictive control
KW  - robot dynamics
KW  - robust control
KW  - stability
KW  - range amplitude
KW  - internal stability
KW  - IS-MPC method
KW  - constraint modification
KW  - ZMP constraint restriction
KW  - robust gait generation
KW  - humanoids
KW  - humanoid gait generation
KW  - robust performance
KW  - considered disturbance signals
KW  - mid-range value
KW  - sampling time
KW  - stability constraint
KW  - current mid-range disturbance
KW  - appropriate restriction
KW  - control horizon
KW  - Humanoid robots
KW  - Lips
KW  - Robustness
KW  - Stability criteria
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197171
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an extension of our previously proposed IS-MPC method for humanoid gait generation aimed at obtaining robust performance in the presence of disturbances. The considered disturbance signals vary in a range of known amplitude around a mid-range value that can change at each sampling time, but whose current value is assumed to be available. The method consists in modifying the stability constraint that is at the core of IS-MPC by incorporating the current mid-range disturbance, and performing an appropriate restriction of the ZMP constraint in the control horizon on the basis of the range amplitude of the disturbance. We derive explicit conditions for recursive feasibility and internal stability of the IS-MPC method with constraint modification. Finally, we illustrate its superior performance with respect to the nominal version by performing dynamic simulations on the NAO robot.
ER  - 

TY  - CONF
TI  - Hybrid Zero Dynamics Inspired Feedback Control Policy Design for 3D Bipedal Locomotion using Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8746
EP  - 8752
AU  - G. A. Castillo
AU  - B. Weng
AU  - W. Zhang
AU  - A. Hereid
PY  - 2020
KW  - feedback
KW  - humanoid robots
KW  - learning systems
KW  - legged locomotion
KW  - robot dynamics
KW  - hybrid zero dynamics inspired feedback control policy design
KW  - 3D bipedal locomotion
KW  - model-free reinforcement learning framework
KW  - feedback control policies
KW  - 3D bipedal walking
KW  - RL algorithms
KW  - reference joint trajectories
KW  - policy structure
KW  - hybrid nature
KW  - walking dynamics
KW  - RL framework
KW  - lightweight network structure
KW  - short training time
KW  - 3D bipedal robot
KW  - stable limit walking cycles
KW  - walking speed
KW  - Legged locomotion
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Robustness
KW  - Torso
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197175
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel model-free reinforcement learning (RL) framework to design feedback control policies for 3D bipedal walking. Existing RL algorithms are often trained in an end-to-end manner or rely on prior knowledge of some reference joint trajectories. Different from these studies, we propose a novel policy structure that appropriately incorporates physical insights gained from the hybrid nature of the walking dynamics and the well-established hybrid zero dynamics approach for 3D bipedal walking. As a result, the overall RL framework has several key advantages, including lightweight network structure, short training time, and less dependence on prior knowledge. We demonstrate the effectiveness of the proposed method on Cassie, a challenging 3D bipedal robot. The proposed solution produces stable limit walking cycles that can track various walking speed in different directions. Surprisingly, without specifically trained with disturbances to achieve robustness, it also performs robustly against various adversarial forces applied to the torso towards both the forward and the backward directions.
ER  - 

TY  - CONF
TI  - Optimal Reduced-order Modeling of Bipedal Locomotion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8753
EP  - 8760
AU  - Y. -M. Chen
AU  - M. Posa
PY  - 2020
KW  - legged locomotion
KW  - nonlinear control systems
KW  - pendulums
KW  - reduced order systems
KW  - robot dynamics
KW  - springs (mechanical)
KW  - five-link model
KW  - Cassie bipedal robot
KW  - optimal reduced-order modeling
KW  - bipedal locomotion
KW  - legged locomotion
KW  - LIP
KW  - spring-loaded inverted pendulum
KW  - SLIP
KW  - agile maneuvers
KW  - high-dimensional system
KW  - ground inclines
KW  - Task analysis
KW  - Legged locomotion
KW  - Reduced order systems
KW  - Lips
KW  - Trajectory optimization
DO  - 10.1109/ICRA40945.2020.9197004
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State-of-the-art approaches to legged locomotion are widely dependent on the use of models like the linear inverted pendulum (LIP) and the spring-loaded inverted pendulum (SLIP), popular because their simplicity enables a wide array of tools for planning, control, and analysis. However, they inevitably limit the ability to execute complex tasks or agile maneuvers. In this work, we aim to automatically synthesize models that remain low-dimensional but retain the capabilities of the high-dimensional system. For example, if one were to restore a small degree of complexity to LIP, SLIP, or a similar model, our approach discovers the form of that additional complexity which optimizes performance. In this paper, we define a class of reduced-order models and provide an algorithm for optimization within this class. To demonstrate our method, we optimize models for walking at a range of speeds and ground inclines, for both a five-link model and the Cassie bipedal robot.
ER  - 

TY  - CONF
TI  - CAPRICORN: Communication Aware Place Recognition using Interpretable Constellations of Objects in Robot Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8761
EP  - 8768
AU  - B. Ramtoula
AU  - R. de Azambuja
AU  - G. Beltrame
PY  - 2020
KW  - feature extraction
KW  - image colour analysis
KW  - image matching
KW  - image representation
KW  - mobile robots
KW  - multi-robot systems
KW  - object detection
KW  - robot vision
KW  - SLAM (robots)
KW  - particular communication bandwidth
KW  - limited communication bandwidth
KW  - relative object positions
KW  - 2step decentralized loop closure verification
KW  - compact semantic descriptors
KW  - bandwidth requirements
KW  - communication aware place recognition
KW  - interpretable constellations
KW  - robot networks
KW  - multiple robots
KW  - mapping environments
KW  - CAPRICORN
KW  - exploring environments
KW  - 3D points
KW  - compact spatial descriptors
KW  - matching robots
KW  - geometric information
KW  - global image descriptors
KW  - TUM RGB-D SLAM sequence
KW  - Semantics
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Visualization
KW  - Bandwidth
DO  - 10.1109/ICRA40945.2020.9197270
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Using multiple robots for exploring and mapping environments can provide improved robustness and performance, but it can be difficult to implement. In particular, limited communication bandwidth is a considerable constraint when a robot needs to determine if it has visited a location that was previously explored by another robot, as it requires for robots to share descriptions of places they have visited. One way to compress this description is to use constellations, groups of 3D points that correspond to the estimate of a set of relative object positions. Constellations maintain the same pattern from different viewpoints and can be robust to illumination changes or dynamic elements. We present a method to extract from these constellations compact spatial and semantic descriptors of the objects in a scene. We use this representation in a 2step decentralized loop closure verification: first, we distribute the compact semantic descriptors to determine which other robots might have seen scenes with similar objects; then we query matching robots with the full constellation to validate the match using geometric information. The proposed method requires less memory, is more interpretable than global image descriptors, and could be useful for other tasks and interactions with the environment. We validate our system's performance on a TUM RGB-D SLAM sequence and show its benefits in terms of bandwidth requirements.
ER  - 

TY  - CONF
TI  - Online Planning for Quadrotor Teams in 3-D Workspaces via Reachability Analysis On Invariant Geometric Trees
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8769
EP  - 8775
AU  - A. Desai
AU  - N. Michael
PY  - 2020
KW  - aerospace control
KW  - helicopters
KW  - multi-robot systems
KW  - path planning
KW  - position control
KW  - reachability analysis
KW  - robot dynamics
KW  - trees (mathematics)
KW  - collision-free geometric solution guarantees
KW  - online planning
KW  - aerial robots
KW  - quadrotor teams
KW  - cluttered 3D workspaces
KW  - reachability analysis
KW  - kinodynamic multirobot planning problem
KW  - position invariant geometric trees
KW  - kinodynamically feasible trajectories
KW  - multirobot team
KW  - nonstationary initial states
KW  - Collision avoidance
KW  - Planning
KW  - Robot kinematics
KW  - Vegetation
KW  - Trajectory
KW  - Reachability analysis
DO  - 10.1109/ICRA40945.2020.9197195
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the kinodynamic multi-robot planning problem in cluttered 3-D workspaces. Reachability analysis on position invariant geometric trees is leveraged to find kino- dynamically feasible trajectories for the multi-robot team from potentially non-stationary initial states. The key contribution of our approach is that a collision-free geometric solution guarantees a kinodynamically feasible, safe solution without additional refinement. Simulation results with up-to 40 robots and hardware results with 5 robots suggest the viability of the proposed approach for online planning and replanning for large teams of aerial robots in cluttered 3-D workspaces.
ER  - 

TY  - CONF
TI  - Decentralized Visual-Inertial-UWB Fusion for Relative State Estimation of Aerial Swarm
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8776
EP  - 8782
AU  - H. Xu
AU  - L. Wang
AU  - Y. Zhang
AU  - K. Qiu
AU  - S. Shen
PY  - 2020
KW  - autonomous aerial vehicles
KW  - decentralised control
KW  - mobile robots
KW  - robot vision
KW  - state estimation
KW  - decentralized visual-inertial-UWB fusion
KW  - unmanned aerial vehicles
KW  - multiple UAVs
KW  - visual-inertial-UWB fusion framework
KW  - extensive aerial swarm flight experiments
KW  - motion capture system
KW  - vision based method
KW  - Global Positioning System
KW  - estimation consistency
KW  - relative state estimation framework
KW  - aerial swarm applications
KW  - decentralized relative state estimation method
KW  - Drones
KW  - State estimation
KW  - Cameras
KW  - Sensors
KW  - Global Positioning System
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196944
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The collaboration of unmanned aerial vehicles (UAVs) has become a popular research topic for its practicability in multiple scenarios. The collaboration of multiple UAVs, which is also known as aerial swarm is a highly complex system, which still lacks a state-of-art decentralized relative state estimation method. In this paper, we present a novel fully decentralized visual-inertial-UWB fusion framework for relative state estimation and demonstrate the practicability by performing extensive aerial swarm flight experiments. The comparison result with ground truth data from the motion capture system shows the centimeter-level precision which outperforms all the Ultra-WideBand (UWB) and even vision based method. The system is not limited by the field of view (FoV) of the camera or Global Positioning System (GPS), meanwhile on account of its estimation consistency, we believe that the proposed relative state estimation framework has the potential to be prevalently adopted by aerial swarm applications in different scenarios in multiple scales.
ER  - 

TY  - CONF
TI  - DC-CAPT: Concurrent Assignment and Planning of Trajectories for Dubins Cars
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8791
EP  - 8797
AU  - M. Whitzer
AU  - D. Shishika
AU  - D. Thakur
AU  - V. Kumar
AU  - A. Prorok
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - path planning
KW  - robot kinematics
KW  - trajectory control
KW  - Dubins curves
KW  - holonomic robots
KW  - separation distance
KW  - trajectory planning
KW  - collision-free trajectories
KW  - Dubins cars
KW  - concurrent assignment
KW  - DC-CAPT
KW  - Automobiles
KW  - Collision avoidance
KW  - Robots
KW  - Trajectory
KW  - Planning
KW  - Turning
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9196799
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an algorithm for the concurrent assignment and planning of collision-free trajectories (DC-CAPT) for robots whose kinematics can be modeled as Dubins cars, i.e., robots constrained in terms of their initial orientation and their minimum turning radius. Coupling the assignment and trajectory planning subproblems allows for a computationally tractable solution. This solution is guaranteed to be collision- free through the use of a single constraint: the start and goal locations have separation distance greater than some threshold. We derive this separation distance by extending a prior work that assumed holonomic robots. We demonstrate the validity of our approach, and show its efficacy through simulations and experiments where groups of robots executing Dubins curves travel to their assigned goal locations without collisions.
ER  - 

TY  - CONF
TI  - Anti-Jackknifing Control of Tractor-Trailer Vehicles via Intrinsically Stable MPC
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8806
EP  - 8812
AU  - M. Beglini
AU  - L. Lanari
AU  - G. Oriolo
PY  - 2020
KW  - feedback
KW  - linearisation techniques
KW  - optimal control
KW  - predictive control
KW  - road vehicles
KW  - stability
KW  - trajectory control
KW  - corrective term
KW  - tracking term
KW  - input-output linearization
KW  - nonminimum-phase systems
KW  - IS-MPC
KW  - antijackknifing control
KW  - feedback control law
KW  - reference Cartesian trajectory
KW  - trailer hitch angle
KW  - tractor-trailer vehicles
KW  - intrinsically stable MPC scheme
KW  - Trajectory
KW  - Agricultural machinery
KW  - Computational modeling
KW  - Vehicle dynamics
KW  - Tracking
KW  - Dynamics
KW  - Linear approximation
DO  - 10.1109/ICRA40945.2020.9197012
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is common knowledge that tractor-trailer vehicles are affected by jackknifing, a phenomenon that consists in the divergence of the trailer hitch angle and ultimately causes the vehicle to fold up. For the case of backwards motion, in which jackknifing can also occur at low speeds, we present a control method that drives the vehicle along a reference Cartesian trajectory while avoiding the divergence of the hitch angle. In particular, a feedback control law is obtained by combining two actions: a tracking term, computed using input-output linearization, and a corrective term, generated via IS-MPC, an intrinsically stable MPC scheme which is effective for stable inversion of nonminimum-phase systems. The proposed method has been verified in simulation and experimentally validated on a purposely built prototype.
ER  - 

TY  - CONF
TI  - On sensing-aware model predictive path-following control for a reversing general 2-trailer with a car-like tractor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8813
EP  - 8819
AU  - O. Ljungqvist
AU  - D. Axehill
AU  - H. Pettersson
PY  - 2020
KW  - control system synthesis
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - position control
KW  - predictive control
KW  - road traffic control
KW  - road vehicles
KW  - stability
KW  - vehicle dynamics
KW  - sensing-aware model predictive path
KW  - car-like tractor
KW  - controller-design problem
KW  - joint-angle kinematics
KW  - backward motion
KW  - vehicle segments
KW  - jackknife state
KW  - joint-angle estimation problem
KW  - path-following controller
KW  - Agricultural machinery
KW  - Axles
KW  - Sensors
KW  - Kinematics
KW  - Reliability
KW  - Estimation
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197346
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The design of reliable path-following controllers is a key ingredient for successful deployment of self-driving vehicles. This controller-design problem is especially challenging for a general 2-trailer with a car-like tractor due to the vehicle's structurally unstable joint-angle kinematics in backward motion and the car-like tractor's curvature limitations which can cause the vehicle segments to fold and enter a jackknife state. Furthermore, advanced sensors with a limited field of view have been proposed to solve the joint-angle estimation problem online, which introduce additional restrictions on which vehicle states that can be reliably estimated. To incorporate these restrictions at the level of control, a model predictive path-following controller is proposed. By taking the vehicle's physical and sensing limitations into account, it is shown in real-world experiments that the performance of the proposed path-following controller in terms of suppressing disturbances and recovering from non-trivial initial states is significantly improved compared to a previously proposed solution where the constraints have been neglected.
ER  - 

TY  - CONF
TI  - Offline Practising and Runtime Training Framework for Autonomous Motion Control of Snake Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8820
EP  - 8826
AU  - L. Cheng
AU  - J. Huang
AU  - L. Liu
AU  - Z. Jian
AU  - Y. Huang
AU  - K. Huang
PY  - 2020
KW  - adaptive control
KW  - biomimetics
KW  - feedback
KW  - mobile robots
KW  - motion control
KW  - regression analysis
KW  - robot dynamics
KW  - biomorphic hyperredundant robots
KW  - locomotion gait
KW  - autonomous motion control
KW  - runtime training framework
KW  - offline practising
KW  - snake robot
KW  - linear regression
KW  - dynamic feedback
KW  - Robots
KW  - Snake robots
KW  - Runtime
KW  - Training
KW  - Entropy
KW  - Motion control
KW  - Linear regression
DO  - 10.1109/ICRA40945.2020.9196637
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes an offline and runtime combined framework for the autonomous motion of snake robots. With the dynamic feedback of its state during runtime, the robot utilizes the linear regression to update its control parameters for better performance and thus adaptively reacts to the environment. To reduce interference from infeasible samples and improve efficiency, the data set for runtime training is chosen from one in several clusters categorized from samples collected in offline practice. Moreover, only the most sensitive control parameter is updated at one iteration for better robustness and efficiency. The effectiveness and efficiency of our approach are evaluated by a set of case studies of pole climbing. Experimental results demonstrate that with the proposed framework, the snake robot can adapt its locomotion gait to poles with different unknown diameters.
ER  - 

TY  - CONF
TI  - Control of a differentially driven nonholonomic robot subject to a restricted wheels rotation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8827
EP  - 8832
AU  - D. Pazderski
AU  - K. Koz≈Çowski
PY  - 2020
KW  - controllability
KW  - feedback
KW  - geometry
KW  - position control
KW  - robot kinematics
KW  - motion task scenarios
KW  - virtual geometry constraint
KW  - transverse function
KW  - four-dimensional configuration manifold
KW  - small time local controllability
KW  - two-wheeled nonholonomic robot
KW  - nonstandard motion tasks
KW  - restricted wheels rotation
KW  - Wheels
KW  - Mobile robots
KW  - Kinematics
KW  - Task analysis
KW  - Manifolds
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196519
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper deals with non-standard motion tasks specified for a two-wheeled nonholonomic robot. It is assumed that wheels cannot fully rotate which reduces a set of feasible movements significantly. In spite of these constraints, it is expected that position of the robot can be changed without violating nonholonomic constraints. Such a possibility comes from the small time local controllability (STLC) of the kinematics described on four-dimensional configuration manifold. In order to solve these specific tasks a feedback taking advantage of the transverse function approach is designed. Consequently, the system can be virtually released from non-holonomic constraints. The transverse function also defines a virtual geometry constraint which makes it possible to limit wheels rotation.Properties of the designed controller are illustrated by results of numerical simulations in various motion task scenarios.
ER  - 

TY  - CONF
TI  - Inferring Task-Space Central Pattern Generator Parameters for Closed-loop Control of Underactuated Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8833
EP  - 8839
AU  - N. D. Kent
AU  - R. M. Bhirangi
AU  - M. Travers
AU  - T. M. Howard
PY  - 2020
KW  - closed loop systems
KW  - graph theory
KW  - legged locomotion
KW  - motion control
KW  - neurocontrollers
KW  - optimisation
KW  - path planning
KW  - real-time systems
KW  - sampling methods
KW  - optimal behaviors
KW  - gradient free optimization
KW  - closed loop control
KW  - underactuated robots
KW  - legged robot control
KW  - real time applications
KW  - probabilistic graphical model
KW  - locomotive behaviors
KW  - task space central pattern generator
KW  - sampling based motion planner
KW  - neural oscillator network
KW  - Mathematical model
KW  - Robot kinematics
KW  - Adaptation models
KW  - Probability distribution
KW  - Oscillators
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9196957
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The complexity associated with the control of highly-articulated legged robots scales quickly as the number of joints increases. Traditional approaches to the control of these robots are often impractical for many real-time applications. This work thus presents a novel sampling-based planning approach for highly-articulated robots that utilizes a probabilistic graphical model (PGM) to infer in real-time how to optimally modify goal-driven, locomotive behaviors for use in closed-loop control. Locomotive behaviors are quantified in terms of the parameters associated with a network of neural oscillators, or rather a central pattern generator (CPG). For the first time, we show that the PGM can be used to optimally modulate different behaviors in real-time (i.e., to select of optimal choice of parameter values across the CPG model) in response to changes both in the local environment and in the desired control signal. The PGM is trained offline using a library of optimal behaviors that are generated using a gradient-free optimization framework.
ER  - 

TY  - CONF
TI  - In-Hand Manipulation of Objects with Unknown Shapes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8848
EP  - 8854
AU  - S. Cruciani
AU  - H. Yin
AU  - D. Kragic
PY  - 2020
KW  - dexterous manipulators
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - unknown shape
KW  - grasp configurations
KW  - deep generative models
KW  - object shapes
KW  - partial visual sensing
KW  - object shape uncertainty
KW  - manipulation actions
KW  - in-hand manipulation tasks
KW  - unknown objects
KW  - dexterous manipulation graph method
KW  - Shape
KW  - Grippers
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Planning
KW  - Robots
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197273
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work addresses the problem of changing grasp configurations on objects with an unknown shape through in-hand manipulation. Our approach leverages shape priors, learned as deep generative models, to infer novel object shapes from partial visual sensing. The Dexterous Manipulation Graph method is extended to build incrementally and account for object shape uncertainty when planning a sequence of manipulation actions. We show that our approach successfully solves in-hand manipulation tasks with unknown objects, and demonstrate the validity of these solutions with robot experiments.
ER  - 

TY  - CONF
TI  - Learning Hierarchical Control for Robust In-Hand Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8855
EP  - 8862
AU  - T. Li
AU  - K. Srinivasan
AU  - M. Q. -H. Meng
AU  - W. Yuan
AU  - J. Bohg
PY  - 2020
KW  - computational complexity
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - hierarchical control
KW  - robotic in-hand manipulation
KW  - finger motion
KW  - complex manipulation sequences
KW  - low-level controllers
KW  - model-free deep reinforcement learning
KW  - hierarchical method
KW  - traditional model-based controllers
KW  - manipulation primitives
KW  - elongated objects
KW  - object models
KW  - Robustness
KW  - Task analysis
KW  - Force
KW  - Robot kinematics
KW  - Torque
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197343
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic in-hand manipulation has been a longstanding challenge due to the complexity of modelling hand and object in contact and of coordinating finger motion for complex manipulation sequences. To address these challenges, the majority of prior work has either focused on model-based, low-level controllers or on model-free deep reinforcement learning that each have their own limitations. We propose a hierarchical method that relies on traditional, model-based controllers on the low-level and learned policies on the mid-level. The low-level controllers can robustly execute different manipulation primitives (reposing, sliding, flipping). The mid-level policy orchestrates these primitives. We extensively evaluate our approach in simulation with a 3-fingered hand that controls three degrees of freedom of elongated objects. We show that our approach can move objects between almost all the possible poses in the workspace while keeping them firmly grasped. We also show that our approach is robust to inaccuracies in the object models and to observation noise. Finally, we show how our approach generalizes to objects of other shapes.
ER  - 

TY  - CONF
TI  - Tactile Dexterity: Manipulation Primitives with Tactile Feedback
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8863
EP  - 8869
AU  - F. R. Hogan
AU  - J. Ballester
AU  - S. Dong
AU  - A. Rodriguez
PY  - 2020
KW  - closed loop systems
KW  - dexterous manipulators
KW  - end effectors
KW  - force control
KW  - manipulator dynamics
KW  - path planning
KW  - perturbation techniques
KW  - robot vision
KW  - tactile sensors
KW  - robot trajectories
KW  - manipulation primitives
KW  - ABB YuMi dual-arm robot
KW  - tactile dexterity
KW  - tactile feedback
KW  - closed-loop tactile controllers
KW  - dexterous robotic manipulation
KW  - dual-palm robotic system
KW  - tactile control
KW  - tactile-based tracking
KW  - end-effector
KW  - Tactile sensors
KW  - Trajectory
KW  - Force
KW  - Friction
KW  - Perturbation methods
DO  - 10.1109/ICRA40945.2020.9196976
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper develops closed-loop tactile controllers for dexterous robotic manipulation with a dual-palm robotic system. Tactile dexterity is an approach to dexterous manipulation that plans for robot/object interactions that render interpretable tactile information for control. We divide the role of tactile control into two goals: 1) control the contact state between the end-effector and the object (contact/no-contact, stick/slip) by regulating the stability of planned contact configurations and monitoring undesired slip events; and 2) control the object state by tactile-based tracking and iterative replanning of the object and robot trajectories. Key to this formulation is the decomposition of manipulation plans into sequences of manipulation primitives with simple mechanics and efficient planners. We consider the scenario of manipulating an object from an initial pose to a target pose on a flat surface while correcting for external perturbations and uncertainty in the initial pose of the object. We experimentally validate the approach with an ABB YuMi dual-arm robot and demonstrate the ability of the tactile controller to react to external perturbations.
ER  - 

TY  - CONF
TI  - Design of a Roller-Based Dexterous Hand for Object Grasping and Within-Hand Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8870
EP  - 8876
AU  - S. Yuan
AU  - A. D. Epps
AU  - J. B. Nowak
AU  - J. K. Salisbury
PY  - 2020
KW  - control system synthesis
KW  - dexterous manipulators
KW  - grippers
KW  - manipulator kinematics
KW  - motion control
KW  - continuous rotation capability
KW  - fingertips
KW  - object manipulation
KW  - roller-based dexterous hand design
KW  - two-finger manipulation
KW  - nonholonomic spatial motion
KW  - robotic hands
KW  - three-finger manipulation
KW  - actively driven rollers
KW  - nonanthropomorphic robot hand
KW  - within-hand manipulation
KW  - object grasping
KW  - Grasping
KW  - Prototypes
KW  - Kinematics
KW  - Task analysis
KW  - Robot sensing systems
KW  - Thumb
DO  - 10.1109/ICRA40945.2020.9197146
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes the development of a novel non-anthropomorphic robot hand with the ability to manipulate objects by means of articulated, actively driven rollers located at the fingertips. An analysis is conducted and systems of equations for two-finger and three-finger manipulation of a sphere are formulated to demonstrate full six degree of freedom nonholonomic spatial motion capability. A prototype version of the hand was constructed and used to grasp and manipulate a variety of objects. Tests conducted with the prototype confirmed the validity of the mathematical analysis. Unlike conventional approaches to within-hand manipulation using legacy robotic hands, the continuous rotation capability of our rolling fingertips allows for unbounded rotation of a grasped object without the need for finger gaiting.
ER  - 

TY  - CONF
TI  - High-Resolution Optical Fiber Shape Sensing of Continuum Robots: A Comparative Study *
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8877
EP  - 8883
AU  - F. Monet
AU  - S. Sefati
AU  - P. Lorre
AU  - A. Poiffaut
AU  - S. Kadoury
AU  - M. Armand
AU  - I. Iordachita
AU  - R. Kashyap
PY  - 2020
KW  - bending
KW  - Bragg gratings
KW  - dexterous manipulators
KW  - fibre optic sensors
KW  - optical fibres
KW  - reflectometry
KW  - flexible medical instruments
KW  - continuum dexterous manipulators
KW  - minimally invasive surgery
KW  - accurate CDM shape reconstruction
KW  - fiber Bragg grating sensors
KW  - sensing locations
KW  - basic shapes
KW  - optical frequency domain reflectometry
KW  - higher spatial resolution
KW  - complex shapes
KW  - ultraviolet laser exposure
KW  - orthopedic surgeries
KW  - maximum tip position error
KW  - OFDR reconstruction
KW  - FBG reconstruction
KW  - more accurate alternative
KW  - FBG sensors
KW  - complex CDM shapes
KW  - continuum robots
KW  - high-resolution optical fiber shape sensing
KW  - random optical gratings
KW  - size 35.0 mm
KW  - size 3.4 mm
KW  - Shape
KW  - Fiber gratings
KW  - Optical sensors
KW  - Robot sensing systems
KW  - Spatial resolution
DO  - 10.1109/ICRA40945.2020.9197454
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Flexible medical instruments, such as Continuum Dexterous Manipulators (CDM), constitute an important class of tools for minimally invasive surgery. Accurate CDM shape reconstruction during surgery is of great importance, yet a challenging task. Fiber Bragg grating (FBG) sensors have demonstrated great potential in shape sensing and consequently tip position estimation of CDMs. However, due to the limited number of sensing locations, these sensors can only accurately recover basic shapes, and become unreliable in the presence of obstacles or many inflection points such as s-bends. Optical Frequency Domain Reflectometry (OFDR), on the other hand, can achieve much higher spatial resolution, and can therefore accurately reconstruct more complex shapes. Additionally, Random Optical Gratings by Ultraviolet laser Exposure (ROGUEs) can be written in the fibers to increase signal to noise ratio of the sensors. In this comparison study, the tip position error is used as a metric to compare both FBG and OFDR shape reconstructions for a 35 mm long CDM developed for orthopedic surgeries, using a pair of stereo cameras as ground truth. Three sets of experiments were conducted to measure the accuracy of each technique in various surgical scenarios. The tip position error for the OFDR (and FBG) technique was found to be 0.32 (0.83) mm in free-bending environment, 0.41 (0.80) mm when interacting with obstacles, and 0.45 (2.27) mm in s-bending. Moreover, the maximum tip position error remains sub-millimeter for the OFDR reconstruction, while it reaches 3.40 mm for FBG reconstruction. These results propose a cost-effective, robust and more accurate alternative to FBG sensors for reconstructing complex CDM shapes.
ER  - 

TY  - CONF
TI  - Local Trajectory Stabilization for Dexterous Manipulation via Piecewise Affine Approximations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8884
EP  - 8891
AU  - W. Han
AU  - R. Tedrake
PY  - 2020
KW  - approximation theory
KW  - dexterous manipulators
KW  - feedback
KW  - linear programming
KW  - linearisation techniques
KW  - manipulator dynamics
KW  - nonlinear control systems
KW  - stability
KW  - piecewise affine approximations
KW  - dexterous robotic manipulation
KW  - nonsmooth nonlinear system
KW  - trajectory optimization
KW  - local multicontact dynamics
KW  - piecewise affine system
KW  - linearization
KW  - feedback controller
KW  - linear programs
KW  - local trajectory stabilization
KW  - dexterous manipulation
KW  - feedback policy design
KW  - Trajectory optimization
KW  - Manipulator dynamics
KW  - Task analysis
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196824
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a model-based approach to design feedback policies for dexterous robotic manipulation. The manipulation problem is formulated as reaching the target region from an initial state for some non-smooth nonlinear system. First, we use trajectory optimization to find a feasible trajectory. Next, we characterize the local multi-contact dynamics around the trajectory as a piecewise affine system, and build a funnel around the linearization of the nominal trajectory using polytopes. We prove that the feedback controller at the vicinity of the linearization is guaranteed to drive the nonlinear system to the target region. During online execution, we solve linear programs to track the system trajectory. We validate the algorithm on hardware, showing that even under large external disturbances, the controller is able to accomplish the task.
ER  - 

TY  - CONF
TI  - Monocular Direct Sparse Localization in a Prior 3D Surfel Map
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8892
EP  - 8898
AU  - H. Ye
AU  - H. Huang
AU  - M. Liu
PY  - 2020
KW  - cameras
KW  - geophysical image processing
KW  - object tracking
KW  - optimisation
KW  - photometry
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - solid modelling
KW  - monocular direct sparse localization
KW  - prior 3d surfel map
KW  - monocular camera
KW  - prior surfel map
KW  - vertex
KW  - normal maps
KW  - global planar information
KW  - sparse tracked points
KW  - image frame
KW  - direct photometric errors
KW  - camera localization
KW  - pose tracking
KW  - rendering
KW  - optimization
KW  - global 6-DoF camera poses
KW  - Cameras
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Visualization
KW  - Rendering (computer graphics)
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197022
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we introduce an approach to tracking the pose of a monocular camera in a prior surfel map. By rendering vertex and normal maps from the prior surfel map, the global planar information for the sparse tracked points in the image frame is obtained. The tracked points with and without the global planar information involve both global and local constraints of frames to the system. Our approach formulates all constraints in the form of direct photometric errors within a local window of the frames. The final optimization utilizes these constraints to provide the accurate estimation of global 6-DoF camera poses with the absolute scale. The extensive simulation and real-world experiments demonstrate that our monocular method can provide accurate camera localization results under various conditions.
ER  - 

TY  - CONF
TI  - LINS: A Lidar-Inertial State Estimator for Robust and Efficient Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8899
EP  - 8906
AU  - C. Qin
AU  - H. Ye
AU  - C. E. Pranata
AU  - J. Han
AU  - S. Zhang
AU  - M. Liu
PY  - 2020
KW  - distance measurement
KW  - inertial navigation
KW  - iterative methods
KW  - Kalman filters
KW  - motion estimation
KW  - optical radar
KW  - state estimation
KW  - ground vehicles
KW  - 6-axis IMU
KW  - iterated error-state Kalman filter
KW  - feature correspondences
KW  - filter divergence
KW  - LINS
KW  - state-of-the-art lidar-inertial odometry
KW  - lightweight lidar-inertial state estimator
KW  - real-time ego-motion estimation
KW  - robust navigation
KW  - 3D lidar
KW  - robocentric formulation
KW  - Feature extraction
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Kalman filters
KW  - Real-time systems
KW  - Optimization
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197567
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present LINS, a lightweight lidar-inertial state estimator, for real-time ego-motion estimation. The proposed method enables robust and efficient navigation for ground vehicles in challenging environments, such as feature-less scenes, via fusing a 6-axis IMU and a 3D lidar in a tightly-coupled scheme. An iterated error-state Kalman filter (ESKF) is designed to correct the estimated state recursively by generating new feature correspondences in each iteration, and to keep the system computationally tractable. Moreover, we use a robocentric formulation that represents the state in a moving local frame in order to prevent filter divergence in a long run. To validate robustness and generalizability, extensive experiments are performed in various scenarios. Experimental results indicate that LINS offers comparable performance with the state-of-the-art lidar-inertial odometry in terms of stability and accuracy and has order-of-magnitude improvement in speed.
ER  - 

TY  - CONF
TI  - Automated Eye-in-Hand Robot-3D Scanner Calibration for Low Stitching Errors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8906
EP  - 8912
AU  - H. Madhusudanan
AU  - X. Liu
AU  - W. Chen
AU  - D. Li
AU  - L. Du
AU  - J. Li
AU  - J. Ge
AU  - Y. Sun
PY  - 2020
KW  - calibration
KW  - industrial manipulators
KW  - robot kinematics
KW  - robot vision
KW  - DH parameters
KW  - high stitching errors
KW  - long-term routine industrial use
KW  - robot-scanner calibration approach
KW  - low data stitching error
KW  - long-term continuous measurement
KW  - 2D standard calibration board
KW  - low stitching error
KW  - virtual arm-based robot-scanner kinematic model
KW  - trajectory-based robot-world transformation calculation
KW  - cumbersome marker-based method
KW  - lower system downtime
KW  - automated eye-in-hand robot-3D scanner calibration
KW  - industrial robot
KW  - complete measurement
KW  - data stitching process
KW  - single coordinate system
KW  - marker-free stitching
KW  - cumbersome traditional fiducial marker-based method
KW  - align multiple FOV
KW  - Calibration
KW  - DH-HEMTs
KW  - Robot kinematics
KW  - Three-dimensional displays
KW  - Kinematics
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196748
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A 3D measurement system consisting of a 3D scanner and an industrial robot (eye-in-hand) is commonly used to scan large object under test (OUT) from multiple fieldof-views (FOVs) for complete measurement. A data stitching process is required to align multiple FOVs into a single coordinate system. Marker-free stitching assisted by robot's accurate positioning becomes increasingly attractive since it bypasses the cumbersome traditional fiducial marker-based method. Most existing methods directly use initial Denavit-Hartenberg (DH) parameters and hand-eye calibration to calculate the transformations between multiple FOVs. Since accuracy of DH parameters deteriorates over time, such methods suffer from high stitching errors (e.g., 0.2 mm) in long-term routine industrial use. This paper reports a new robot-scanner calibration approach to realize such measurement with low data stitching errors. During long-term continuous measurement, the robot periodically moves towards a 2D standard calibration board to optimize kinematic model's parameters to maintain a low stitching error. This capability is enabled by several techniques including virtual arm-based robot-scanner kinematic model, trajectory-based robot-world transformation calculation, nonlinear optimization. Experimental results demonstrated a low data stitching error (<; 0.1 mm) similar to the cumbersome marker-based method and a lower system downtime (<; 60 seconds vs. 10-15 minutes by traditional DH and hand-eye calibration).
ER  - 

TY  - CONF
TI  - Monocular Visual Odometry using Learned Repeatability and Description
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8913
EP  - 8919
AU  - H. Huang
AU  - H. Ye
AU  - Y. Sun
AU  - M. Liu
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - stereo image processing
KW  - monocular visual odometry
KW  - hybrid scheme
KW  - camera pose estimation
KW  - predicted repeatability maps
KW  - patch-wise 3D-2D association
KW  - local feature parameterization
KW  - adapted mapping module
KW  - local reconstruction accuracy
KW  - monocular VO system
KW  - learned repeatability
KW  - learned description
KW  - public datasets
KW  - robust backend
KW  - lightweight backend
KW  - Cameras
KW  - Feature extraction
KW  - Two dimensional displays
KW  - Robustness
KW  - Pose estimation
KW  - Three-dimensional displays
KW  - Visual odometry
DO  - 10.1109/ICRA40945.2020.9197406
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robustness and accuracy for monocular visual odometry (VO) under challenging environments are widely concerned. In this paper, we present a monocular VO system leveraging learned repeatability and description. In a hybrid scheme, the camera pose is initially tracked on the predicted repeatability maps in a direct manner and then refined with the patch-wise 3D-2D association. The local feature parameterization and the adapted mapping module further boost different functionalities in the system. Extensive evaluations on challenging public datasets are performed. The competitive performance on camera pose estimation demonstrates the effectiveness of our method. Additional studies on the local reconstruction accuracy and running time exhibit that our system is capable of maintaining a robust and lightweight backend.
ER  - 

TY  - CONF
TI  - Interaction Graphs for Object Importance Estimation in On-road Driving Videos
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8920
EP  - 8927
AU  - Z. Zhang
AU  - A. Tawari
AU  - S. Martin
AU  - D. Crandall
PY  - 2020
KW  - decision making
KW  - driver information systems
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - road traffic
KW  - road vehicles
KW  - traffic engineering computing
KW  - interaction graph
KW  - object importance estimation
KW  - on-road driving videos
KW  - human driving behavior
KW  - autonomous driving systems
KW  - ego-vehicle
KW  - Feature extraction
KW  - Automobiles
KW  - Videos
KW  - Convolution
KW  - Estimation
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197104
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A vehicle driving along the road is surrounded by many objects, but only a small subset of them influence the driver's decisions and actions. Learning to estimate the importance of each object on the driver's real-time decision-making may help better understand human driving behavior and lead to more reliable autonomous driving systems. Solving this problem requires models that understand the interactions between the ego-vehicle and the surrounding objects. However, interactions among other objects in the scene can potentially also be very helpful, e.g., a pedestrian beginning to cross the road between the ego-vehicle and the car in front will make the car in front less important. We propose a novel framework for object importance estimation using an interaction graph, in which the features of each object node are updated by interacting with others through graph convolution. Experiments show that our model outperforms state-of-the-art baselines with much less input and pre-processing.
ER  - 

TY  - CONF
TI  - A Robotics Inspection System for Detecting Defects on Semi-specular Painted Automotive Surfaces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8928
EP  - 8934
AU  - S. Akhtar
AU  - A. Tandiya
AU  - M. Moussa
AU  - C. Tarry
PY  - 2020
KW  - automobile industry
KW  - flaw detection
KW  - inspection
KW  - mobile robots
KW  - painting
KW  - quality control
KW  - vibrations
KW  - semispecular painted automotive surfaces
KW  - real-time robotics system
KW  - tolerate varying lighting conditions
KW  - inspected surface
KW  - defect tracking mechanism
KW  - robotics inspection system
KW  - detecting defects
KW  - painted surface defect detection
KW  - small inherent vibrations
KW  - manufacturing operations
KW  - topographical information
KW  - spectral analysis
KW  - quality control
KW  - Inspection
KW  - Surface treatment
KW  - Cameras
KW  - Surface topography
KW  - Robots
KW  - Automobiles
KW  - Surface reconstruction
DO  - 10.1109/ICRA40945.2020.9196980
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper describes the design and implementation of a real-time robotics system for semi-specular/painted surface defect detection. The system can be used on moving parts, tolerate varying lighting conditions, and can accommodate small inherent vibrations of the inspected surface that is common in manufacturing operations. Topographical information of the inspected surface is first obtained by the analysis of reflections of a known pattern from this surface. Spectral analysis is then applied to identify defects through novelty detection. Finally, a defect tracking mechanism eliminates spurious defects. The proposed system operates continuously at 90 fps. The paper presents field testing results that show the system can be used as a consistent and cost-effective way of quality control.
ER  - 

TY  - CONF
TI  - A Novel Underactuated End-Effector for Planar Sequential Grasping of Multiple Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8935
EP  - 8941
AU  - C. Mucchiani
AU  - M. Yim
PY  - 2020
KW  - actuators
KW  - control system synthesis
KW  - dexterous manipulators
KW  - end effectors
KW  - human-robot interaction
KW  - motion control
KW  - torque control
KW  - underactuated end-effector
KW  - planar sequential grasping
KW  - multiple objects
KW  - underactuated end-effector design
KW  - autonomous grasp
KW  - circular objects
KW  - sequential grasps
KW  - human-robot hand-off interactions
KW  - torque control
KW  - Force
KW  - Grasping
KW  - Sensors
KW  - Estimation
KW  - Shape
KW  - Fasteners
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197380
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a serpentine type tendon driven underactuated end-effector design with a closing mechanism that is triggered upon contact with an object. This end-effector can grasp objects without knowing the size a priori and is able to grasp a new object while securing another one previously grasped, and so grasp multiple objects sequentially with a single DOF actuation. Design parameters based on the object dimensions are proposed. A low-cost prototype demonstrates two implementations (radius estimation and autonomous grasp of circular objects by torque control, and sequential grasps of multiple objects) of the end-effector through several experiments. A method for estimating applied internal forces is also proposed. This end-effector can benefit robotic manipulation in tasks such as fetching applications, industrial pick-and-place of single or multiple objects and human-robot hand-off interactions.
ER  - 

TY  - CONF
TI  - Design and Analysis of a Synergy-Inspired Three-Fingered Hand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8942
EP  - 8948
AU  - W. Chen
AU  - Z. Xiao
AU  - J. Lu
AU  - Z. Zhao
AU  - Y. Wang
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - manipulator kinematics
KW  - motion control
KW  - synergy-inspired hands
KW  - biomechanical characteristics
KW  - human hand synergy
KW  - robot hands
KW  - synergy characteristics
KW  - anthropomorphic hands
KW  - synergy-inspired design
KW  - Thumb
KW  - Robots
KW  - Joints
KW  - Muscles
KW  - Grasping
KW  - Electronics packaging
DO  - 10.1109/ICRA40945.2020.9196901
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hand synergy from neuroscience provides an effective tool for anthropomorphic hands to realize versatile grasping with simple planning and control. This paper aims to extend the synergy-inspired design from anthropomorphic hands to multi-fingered robot hands. The synergy-inspired hands are not necessarily humanoid in morphology but perform primary characteristics and functions similar to the human hand. At first, the biomechanics of hand synergy is investigated. Three biomechanical characteristics of the human hand synergy are explored as a basis for the mechanical simplification of the robot hands. Secondly, according to the synergy characteristics, a three-fingered hand is designed, and its kinematic model is developed for the analysis of some typical grasping and manipulation functions. Finally, a prototype is developed and preliminary grasping experiments validate the effectiveness of the design and analysis.
ER  - 

TY  - CONF
TI  - Multiplexed Manipulation: Versatile Multimodal Grasping via a Hybrid Soft Gripper
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8949
EP  - 8955
AU  - L. Chin
AU  - F. Barscevicius
AU  - J. Lipton
AU  - D. Rus
PY  - 2020
KW  - compliant mechanisms
KW  - dexterous manipulators
KW  - grippers
KW  - motion control
KW  - multiplexed manipulation
KW  - hybrid soft gripper
KW  - hybrid suction
KW  - parallel jaw grippers
KW  - multimodal grippers
KW  - soft robotic manipulators
KW  - soft fingers
KW  - multimodal grasping
KW  - Amazon Robotics/Picking Challenge
KW  - complaint handed shearing auxetics actuators
KW  - Grasping
KW  - Grippers
KW  - Multiplexing
KW  - Force
KW  - Belts
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9196626
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The success of hybrid suction + parallel-jaw grippers in the Amazon Robotics/Picking Challenge have demonstrated the effectiveness of multimodal grasping approaches. However, existing multimodal grippers combine grasping modes in isolation and do not incorporate the benefits of compliance found in soft robotic manipulators. In this paper, we present a gripper that integrates three modes of grasping: suction, parallel jaw, and soft fingers. Using complaint handed shearing auxetics actuators as the foundation, this gripper is able to multiplex manipulation by creating unique grasping primitives through permutations of these grasping techniques. This gripper is able to grasp 88% of tested objects, 14% of which could only be grasped using a combination of grasping modes. The gripper is also able to perform in-hand object re-orientation of flat objects without the need for pre-grasp manipulation.
ER  - 

TY  - CONF
TI  - Underactuated Gecko Adhesive Gripper for Simple and Versatile Grasp
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8964
EP  - 8969
AU  - D. Hirano
AU  - N. Tanishima
AU  - A. Bylard
AU  - T. G. Chen
PY  - 2020
KW  - actuators
KW  - adhesion
KW  - adhesives
KW  - grippers
KW  - controllable activation
KW  - minimal disturbance
KW  - form closure
KW  - robotic grasping
KW  - versatile grasp
KW  - underactuated gecko adhesive gripper
KW  - resulting gripper grasp force
KW  - adhesive contact area
KW  - simple tendon-driven mechanism
KW  - underactuated gecko-inspired adhesive gripper
KW  - multiple activation steps
KW  - complex activation mechanism
KW  - grippers
KW  - Grippers
KW  - Force
KW  - Grasping
KW  - Pulleys
KW  - Adhesives
KW  - Actuators
KW  - Tendons
DO  - 10.1109/ICRA40945.2020.9196806
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gecko-inspired adhesives have several desirable characteristics in robotic grasping: controllable activation and deactivation of adhesion, ability to grasp and release with minimal disturbance, and grasping without the need of form closure. Previously proposed grippers with this technology either require a complex activation mechanism or multiple activation steps. In this paper, we present an underactuated gecko-inspired adhesive gripper that can grasp a wide range of curved surfaces using a single actuator through a simple tendon-driven mechanism that attaches and adheres in one step. We derive a theoretical model of the adhesive contact area and resulting gripper grasp force, which is verified experimentally. The actual performance of the proposed mechanism is demonstrated by successfully grasping several surfaces with different curvature diameters.
ER  - 

TY  - CONF
TI  - Active Deformation through Visual Servoing of Soft Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8978
EP  - 8984
AU  - R. Lagneau
AU  - A. Krupa
AU  - M. Marchal
PY  - 2020
KW  - control engineering computing
KW  - deformation
KW  - eigenvalues and eigenfunctions
KW  - end effectors
KW  - Jacobian matrices
KW  - least squares approximations
KW  - robot vision
KW  - visual servoing
KW  - soft objects
KW  - online estimation
KW  - deformation Jacobian
KW  - robot end-effector
KW  - deformation behavior
KW  - ADVISEd method
KW  - model-free methods
KW  - marker-based active shaping task
KW  - shape preservation tasks
KW  - active deformation through visual servoing method
KW  - model-free deformation servoing method
KW  - weighted least-squares minimization
KW  - sliding window
KW  - eigenvalue-based confidence criterion
KW  - marker-less active shaping
KW  - model-based methods
KW  - Strain
KW  - Jacobian matrices
KW  - Deformable models
KW  - Shape
KW  - Task analysis
KW  - Adaptation models
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197506
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose the ADVISEd (Active Deformation through VIsual SErvoing) method, a novel model-free deformation servoing method able to deform a soft object towards a desired shape. ADVISEd relies on an online estimation of the deformation Jacobian that relates the motion of the robot end-effector to the deformation behavior of the object. The estimation is based on a weighted least-squares minimization with a sliding window. The robustness of the method to observation noise is ensured using an eigenvalue-based confidence criterion. The ADVISEd method is validated through comparisons with a model-based and a model-free state-of-the-art methods. Two experimental setups are proposed to compare the methods, one to perform a marker-based active shaping task and one to perform several marker-less active shaping and shape preservation tasks. Experiments showed that our approach can interactively control the deformations of an object in different tasks while ensuring better robustness to external perturbations than the state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Visual Geometric Skill Inference by Watching Human Demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 8985
EP  - 8991
AU  - J. Jin
AU  - L. Petrich
AU  - Z. Zhang
AU  - M. Dehghan
AU  - M. Jagersand
PY  - 2020
KW  - control engineering computing
KW  - data visualisation
KW  - entropy
KW  - feature selection
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - regression analysis
KW  - video signal processing
KW  - visual geometric skill inference
KW  - manipulation skills
KW  - human demonstration video
KW  - association relationships
KW  - eye-hand coordination tasks
KW  - geometric control error
KW  - graph based kernel regression method
KW  - association constraints
KW  - human readable task definition
KW  - control errors
KW  - feature-based visual ser-voing
KW  - incremental maximum entropy inverse reinforcement learning
KW  - feature selection
KW  - robust feature trackers
KW  - Task analysis
KW  - Kernel
KW  - Robustness
KW  - Feature extraction
KW  - Robot kinematics
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196570
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We study the problem of learning manipulation skills from human demonstration video by inferring the association relationships between geometric features. Motivation for this work stems from the observation that humans perform eye-hand coordination tasks by using geometric primitives to define a task while a geometric control error drives the task through execution. We propose a graph based kernel regression method to directly infer the underlying association constraints from human demonstration video using Incremental Maximum Entropy Inverse Reinforcement Learning (InMaxEnt IRL). The learned skill inference provides human readable task definition and outputs control errors that can be directly plugged into traditional controllers. Our method removes the need for tedious feature selection and robust feature trackers required in traditional approaches (e.g. feature-based visual ser-voing). Experiments show our method infers correct geometric associations even with only one human demonstration video and can generalize well under variance.
ER  - 

TY  - CONF
TI  - DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9000
EP  - 9006
AU  - Y. V. S. Harish
AU  - H. Pandya
AU  - A. Gaud
AU  - S. Terupally
AU  - S. Shankar
AU  - K. M. Krishna
PY  - 2020
KW  - cameras
KW  - feature extraction
KW  - image sensors
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - visual servoing
KW  - optical flow
KW  - visual features
KW  - deep neural network
KW  - diverse scenes
KW  - visual servoing approaches
KW  - robust servoing performance
KW  - camera transformations
KW  - deep flow guided scene agnostic image
KW  - deep learning
KW  - relative camera pose
KW  - photo-realistic 3D simulation
KW  - aerial robot
KW  - DFVS
KW  - interaction matrix
KW  - Visual servoing
KW  - Cameras
KW  - Convergence
KW  - Visualization
KW  - Adaptive optics
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196753
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing deep learning based visual servoing approaches regress the relative camera pose between a pair of images. Therefore, they require a huge amount of training data and sometimes fine-tuning for adaptation to a novel scene. Furthermore, current approaches do not consider underlying geometry of the scene and rely on direct estimation of camera pose. Thus, inaccuracies in prediction of the camera pose, especially for distant goals, lead to a degradation in the servoing performance. In this paper, we propose a two-fold solution: (i) We consider optical flow as our visual features, which are predicted using a deep neural network. (ii) These flow features are then systematically integrated with depth estimates provided by another neural network using interaction matrix. We further present an extensive benchmark in a photo-realistic 3D simulation across diverse scenes to study the convergence and generalisation of visual servoing approaches. We show convergence for over 3m and 40 degrees while maintaining precise positioning of under 2cm and 1 degree on our challenging benchmark where the existing approaches that are unable to converge for majority of scenarios for over 1.5m and 20 degrees. Furthermore, we also evaluate our approach for a real scenario on an aerial robot. Our approach generalizes to novel scenarios producing precise and robust servoing performance for 6 degrees of freedom positioning tasks with even large camera transformations without any retraining or fine-tuning.
ER  - 

TY  - CONF
TI  - Photometric Path Planning for Vision-Based Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9007
EP  - 9013
AU  - E. A. Rodr√≠guez Mart√≠nez
AU  - G. Caron
AU  - C. P√©gard
AU  - D. L. Alabazares
PY  - 2020
KW  - cameras
KW  - manipulators
KW  - navigation
KW  - path planning
KW  - robot vision
KW  - photometric path planning
KW  - vision-based navigation system
KW  - visual memory
KW  - topological map
KW  - virtual camera
KW  - navigability
KW  - visual path
KW  - navigation stage
KW  - onboard camera
KW  - top view image
KW  - learning stage
KW  - urban scene
KW  - Visualization
KW  - Navigation
KW  - Cameras
KW  - Visual servoing
DO  - 10.1109/ICRA40945.2020.9197091
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a vision-based navigation system that uses a visual memory to navigate. Such memory corresponds to a topological map of key images created from moving a virtual camera over a model of the real scene. The advantage of our approach is that it provides a useful insight into the navigability of a visual path without relying on a traditional learning stage. During the navigation stage, the robot is controlled by sequentially comparing the images stored in the memory with the images acquired by the onboard camera.The evaluation is conducted on a robotic arm equipped with a camera and the model of the environment corresponds to a top view image of an urban scene.
ER  - 

TY  - CONF
TI  - A memory of motion for visual predictive control tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9014
EP  - 9020
AU  - A. Paolillo
AU  - T. S. Lembono
AU  - S. Calinon
PY  - 2020
KW  - manipulators
KW  - optimal control
KW  - optimisation
KW  - predictive control
KW  - regression analysis
KW  - robot vision
KW  - visual servoing
KW  - visual predictive control tasks
KW  - regression techniques
KW  - control optimization process
KW  - 7-axis manipulator
KW  - image-based visual servoing
KW  - Visualization
KW  - Microsoft Windows
KW  - Trajectory
KW  - Optimization
KW  - Task analysis
KW  - Computational modeling
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197216
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper addresses the problem of efficiently achieving visual predictive control tasks. To this end, a memory of motion, containing a set of trajectories built off-line, is used for leveraging precomputation and dealing with difficult visual tasks. Standard regression techniques, such as k-nearest neighbors and Gaussian process regression, are used to query the memory and provide on-line a warm-start and a way point to the control optimization process. The proposed technique allows the control scheme to achieve high performance and, at the same time, keep the computational time limited. Simulation and experimental results, carried out with a 7-axis manipulator, show the effectiveness of the approach.
ER  - 

TY  - CONF
TI  - Design and Workspace Characterisation of Malleable Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9021
EP  - 9027
AU  - A. B. Clark
AU  - N. Rojas
PY  - 2020
KW  - actuators
KW  - design engineering
KW  - elasticity
KW  - end effectors
KW  - manipulator dynamics
KW  - manipulator kinematics
KW  - position control
KW  - bin picking
KW  - variable stiffness link
KW  - low DOF serial robot
KW  - 2-DOF malleable robot
KW  - workspace categories
KW  - serial robot arms
KW  - End effectors
KW  - Robot kinematics
KW  - Task analysis
KW  - Mathematical model
KW  - Topology
DO  - 10.1109/ICRA40945.2020.9197439
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For the majority of tasks performed by traditional serial robot arms, such as bin picking or pick and place, only two or three degrees of freedom (DOF) are required for motion; however, by augmenting the number of degrees of freedom, further dexterity of robot arms for multiple tasks can be achieved. Instead of increasing the number of joints of a robot to improve flexibility and adaptation, which increases control complexity, weight, and cost of the overall system, malleable robots utilise a variable stiffness link between joints allowing the relative positioning of the revolute pairs at each end of the link to vary, thus enabling a low DOF serial robot to adapt across tasks by varying its workspace. In this paper, we present the design and prototyping of a 2-DOF malleable robot, calculate the general equation of its workspace using a parameterisation based on distance geometry-suitable for robot arms of variable topology, and characterise the workspace categories that the end effector of the robot can trace via reconfiguration. Through the design and construction of the malleable robot we explore design considerations, and demonstrate the viability of the overall concept. By using motion tracking on the physical robot, we show examples of the infinite number of workspaces that the introduced 2-DOF malleable robot can achieve.
ER  - 

TY  - CONF
TI  - A Tri-Stable Soft Robotic Finger Capable of Pinch and Wrap Grasps
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9028
EP  - 9034
AU  - A. K. Nguyen
AU  - A. Russell
AU  - N. Naclerio
AU  - V. Vuong
AU  - H. Huang
AU  - K. Chui
AU  - E. W. Hawkes
PY  - 2020
KW  - bending
KW  - dexterous manipulators
KW  - elastomers
KW  - force control
KW  - grippers
KW  - pneumatic actuators
KW  - position control
KW  - springs (mechanical)
KW  - preprogrammed grasp
KW  - constant-curvature wrap
KW  - finger-sized round objects
KW  - flat objects
KW  - small objects
KW  - adaptable tri-stable
KW  - grasped object
KW  - bi-stable springs
KW  - stable positions
KW  - finger bending
KW  - grasping performance
KW  - control gripper
KW  - soft grippers
KW  - wrap grasps
KW  - soft robotic pneumatic grippers
KW  - delicate objects
KW  - fluidic elastomer grippers
KW  - inextensible gripping surface
KW  - extensible pneumatic chambers
KW  - extensibility results
KW  - finger curling
KW  - simple fingers
KW  - tri-stable soft robotic finger
KW  - pinch grasps
KW  - Grippers
KW  - Springs
KW  - Shape
KW  - Soft robotics
KW  - Grasping
KW  - Force
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196818
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robotic pneumatic grippers have been shown to be versatile, robust to impacts, and safe for use on delicate objects. One type, fluidic elastomer grippers, are characterized by fingers with an inextensible gripping surface backed by extensible pneumatic chambers; when inflated, this mismatch in extensibility results in the finger curling. However, one drawback of these simple fingers is that they have one preprogrammed grasp, usually a simple constant-curvature wrap. While well-suited for finger-sized round objects, they do not grasp flat or small objects well. Here, we present an adaptable tri-stable soft robotic finger that can form either a pinch or wrap grasp based on the shape of the grasped object. We enable this by incorporating two bi-stable springs into the inextensible layer. The three stable positions are: i) open (unpressurized), ii) pinch (with only the proximal section bending), and iii) wrap (with the entire finger bending). We present a simple model of the behavior of our finger and experimental results verifying the model. Further, we apply forces and moments to grasped objects, and show that the tri-stable finger increases the grasping performance when compared to a control gripper with equal gripping force. Our work presents a novel design modification that is unobtrusive, simple, and passive. Our introduction of inexpensive programmable hardware advances the versatility and adaptability of soft grippers.
ER  - 

TY  - CONF
TI  - A Dexterous Tip-extending Robot with Variable-length Shape-locking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9035
EP  - 9041
AU  - S. Wang
AU  - R. Zhang
AU  - D. A. Haggerty
AU  - N. D. Naclerio
AU  - E. W. Hawkes
PY  - 2020
KW  - biomechanics
KW  - dexterous manipulators
KW  - mobile robots
KW  - position control
KW  - dexterous tip-extending robot
KW  - variable-length shape-locking
KW  - tip-extending vine robots
KW  - distal end
KW  - inextensible tip-extending
KW  - pressurized tip-extending
KW  - robot body
KW  - locked sections
KW  - free sections
KW  - shape-locking mechanism
KW  - shape-locking concept
KW  - soft robotics
KW  - dexterous workspace
KW  - Electron tubes
KW  - Tendons
KW  - Shape
KW  - Educational robots
KW  - Manipulators
KW  - Pneumatic systems
DO  - 10.1109/ICRA40945.2020.9197311
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft, tip-extending "vine" robots offer a unique mode of inspection and manipulation in highly constrained environments. For practicality, it is desirable that the distal end of the robot can be manipulated freely, while the body remains stationary. However, in previous vine robots, either the shape of the body was fixed after growth with no ability to manipulate the distal end, or the whole body moved together with the tip. Here, we present a concept for shape-locking that enables a vine robot to move only its distal tip, while the body is locked in place. This is achieved using two inextensible, pressurized, tip-extending, chambers that "grow" along the sides of the robot body, preserving curvature in the section where they have been deployed. The length of the locked and free sections can be varied by controlling the extension and retraction of these chambers. We present models describing this shape-locking mechanism and workspace of the robot in both free and constrained environments. We experimentally validate these models, showing an increased dexterous workspace compared to previous vine robots. Our shape-locking concept allows improved performance for vine robots, advancing the field of soft robotics for inspection and manipulation in highly constrained environments.
ER  - 

TY  - CONF
TI  - Compliant Electromagnetic Actuator Architecture for Soft Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9042
EP  - 9049
AU  - N. Kohls
AU  - B. Dias
AU  - Y. Mensah
AU  - B. P. Ruddy
AU  - Y. C. Mazumdar
PY  - 2020
KW  - dexterous manipulators
KW  - electromagnetic actuators
KW  - grippers
KW  - liquid metal ion sources
KW  - pneumatic actuators
KW  - compliant electromagnetic actuator architecture
KW  - soft robotics
KW  - soft materials
KW  - compliant actuation concepts
KW  - soft robotic systems
KW  - electromagnetic actuators
KW  - gallium-indium liquid metal conductors
KW  - soft actuator
KW  - Xenia soft corals
KW  - compliant permanent magnetic tips
KW  - robotic actuator
KW  - frequency 7.0 Hz
KW  - size 6.0 mm
KW  - Iron
KW  - Actuators
KW  - Magnetic cores
KW  - Powders
KW  - Electromagnetics
KW  - Magnetic liquids
DO  - 10.1109/ICRA40945.2020.9197442
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft materials and compliant actuation concepts have generated new design and control approaches in areas from robotics to wearable devices. Despite the potential of soft robotic systems, most designs currently use hard pumps, valves, and electromagnetic actuators. In this work, we take a step towards fully soft robots by developing a new compliant electromagnetic actuator architecture using gallium-indium liquid metal conductors, as well as compliant permanent magnetic and compliant iron composites. Properties of the new materials are first characterized and then co-fabricated to create an exemplary biologically-inspired soft actuator with pulsing or grasping motions, similar to Xenia soft corals. As current is applied to the liquid metal coil, the compliant permanent magnetic tips on passive silicone arms are attracted or repelled. The dynamics of the robotic actuator are characterized using stochastic system identification techniques and then operated at the resonant frequency of 7 Hz to generate high-stroke (>6 mm) motions.
ER  - 

TY  - CONF
TI  - Dynamically Reconfigurable Discrete Distributed Stiffness for Inflated Beam Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9050
EP  - 9056
AU  - B. H. Do
AU  - V. Banashek
AU  - A. M. Okamura
PY  - 2020
KW  - beams (structures)
KW  - buckling
KW  - cantilevers
KW  - electromagnets
KW  - electromechanical actuators
KW  - jamming
KW  - motion control
KW  - rigidity
KW  - robot kinematics
KW  - valves
KW  - tip-everting robots
KW  - tendonsteering
KW  - robot kinematics
KW  - cantilevered loads
KW  - electromechanical device
KW  - electromagnet
KW  - passive valves
KW  - pressure layer jamming
KW  - buckle point locations
KW  - compressive loads
KW  - actuators
KW  - motion control
KW  - discrete distributed stiffness control
KW  - inflated beam robot body
KW  - inflated continuum robots
KW  - Valves
KW  - Jamming
KW  - Actuators
KW  - Laser beams
KW  - Soft robotics
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197237
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Inflated continuum robots are promising for a variety of navigation tasks, but controlling their motion with a small number of actuators is challenging. These inflated beam robots tend to buckle under compressive loads, producing extremely tight local curvature at difficult-to-control buckle point locations. In this paper, we present an inflated beam robot that uses distributed stiffness changing sections enabled by positive pressure layer jamming to control or prevent buckling. Passive valves are actuated by an electromagnet carried by an electromechanical device that travels inside the main inflated beam robot body. The valves themselves require no external connections or wiring, allowing the distributed stiffness control to be scaled to long beam lengths. Multiple layer jamming elements are stiffened simultaneously to achieve global stiffening, allowing the robot to support greater cantilevered loads and longer unsupported lengths. Local stiffening, achieved by leaving certain layer jamming elements unstiffened, allows the robot to produce "virtual joints" that dynamically change the robot kinematics. Implementing these stiffening strategies is compatible with growth through tip eversion and tendonsteering, and enables a number of new capabilities for inflated beam robots and tip-everting robots.
ER  - 

TY  - CONF
TI  - Data-Driven Reinforcement Learning for Walking Assistance Control of a Lower Limb Exoskeleton with Hemiplegic Patients
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9065
EP  - 9071
AU  - Z. Peng
AU  - R. Luo
AU  - R. Huang
AU  - J. Hu
AU  - K. Shi
AU  - H. Cheng
AU  - B. K. Ghosh
PY  - 2020
KW  - adaptive control
KW  - artificial limbs
KW  - handicapped aids
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - medical robotics
KW  - neural nets
KW  - optimal control
KW  - patient rehabilitation
KW  - wearable robots
KW  - Data-driven reinforcement learning
KW  - lower limb exoskeleton
KW  - hemiplegic patient
KW  - rehabilitation scenario
KW  - affected leg
KW  - unaffected leg
KW  - exoskeleton system
KW  - DDRL strategy
KW  - optimal control
KW  - policy iteration algorithm
KW  - online adaptation control
KW  - walking assistance control
KW  - walking assistance scenario
KW  - strength augmentation scenario
KW  - Actor-Critic Neural Network
KW  - ACNN
KW  - Legged locomotion
KW  - Exoskeletons
KW  - Adaptation models
KW  - Learning (artificial intelligence)
KW  - Trajectory
KW  - Extremities
KW  - Optimal control
KW  - Data-driven Control
KW  - Reinforcement Learning
KW  - Leader-Follower Multi-Agent System
KW  - Lower Limb Exoskeleton
KW  - Hemiplegic Patients
KW  - Actor-Critic Neural Network
DO  - 10.1109/ICRA40945.2020.9197229
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Lower limb exoskeleton (LLE) has received considerable interests in strength augmentation, rehabilitation and walking assistance scenarios. For walking assistance, the LLE is expected to have the capability of controlling the affected leg to track the unaffected leg's motion naturally. An important issue in this scenario is that the exoskeleton system needs to deal with unpredictable disturbance from the patient, which requires the controller of exoskeleton system to have the ability to adapt to different wearers. This paper proposes a novel Data-Driven Reinforcement Learning (DDRL) control strategy to adapt different hemiplegic patients with unpredictable disturbances. In the proposed DDRL strategy, the interaction between two lower limbs of LLE and the legs of hemiplegic patient are modeled in the context of leader-follower framework. The walking assistance control problem is transformed into a optimal control problem. Then, a policy iteration (PI) algorithm is introduced to learn optimal controller. To achieve online adaptation control for different patients, based on PI algorithm, an Actor-Critic Neural Network (ACNN) technology of the reinforcement learning (RL) is employed in the proposed DDRL. We conduct experiments both on a simulation environment and a real LLE system. Experimental results demonstrate that the proposed control strategy has strong robustness against disturbances and adaptability to different pilots.
ER  - 

TY  - CONF
TI  - On the Effects of Visual Anticipation of Floor Compliance Changes on Human Gait: Towards Model-based Robot-Assisted Rehabilitation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9072
EP  - 9078
AU  - M. Drolet
AU  - E. Q. Yumbla
AU  - B. Hobbs
AU  - P. Artemiadis
PY  - 2020
KW  - biomechanics
KW  - feedback
KW  - gait analysis
KW  - mechanoception
KW  - medical robotics
KW  - muscle
KW  - neurophysiology
KW  - patient rehabilitation
KW  - patient treatment
KW  - virtual reality
KW  - poststroke gait rehabilitation
KW  - variable stiffness treadmill
KW  - robot-assisted gait therapies
KW  - feedback mechanisms
KW  - visual feedback
KW  - surface stiffness changes
KW  - repeatable muscle activation patterns
KW  - predictable muscle activation patterns
KW  - surface changes
KW  - proprioceptive feedback
KW  - manipulated visual feedback
KW  - virtual environment
KW  - real-world compliant surfaces
KW  - walking surface stiffness
KW  - sensorimotor mechanisms
KW  - robotic rehabilitation device
KW  - virtual reality experience
KW  - robot-assisted interventions
KW  - rehabilitation method
KW  - robot assistance
KW  - model-based robot-assisted rehabilitation
KW  - human gait
KW  - floor compliance changes
KW  - visual anticipation
KW  - Legged locomotion
KW  - Visualization
KW  - Muscles
KW  - Robot sensing systems
KW  - Perturbation methods
KW  - Electromyography
DO  - 10.1109/ICRA40945.2020.9197536
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The role of various types of robot assistance in post-stroke gait rehabilitation has gained much attention in recent years. Furthermore, there is increased popularity to use more than one rehabilitation method in order to utilize the different advantages of each. Naturally, this results in the need to study how the different robot-assisted interventions affect the various underlying sensorimotor mechanisms involved in rehabilitation. To answer this important question, this paper combines a virtual reality experience with a unique robotic rehabilitation device, the Variable Stiffness Treadmill (VST), as a way of understanding interactions across different sensorimotor mechanisms involved in gait. The VST changes the walking surface stiffness in order to simulate real-world compliant surfaces while seamlessly interacting with a virtual environment. Through the manipulated visual and proprioceptive feedback, this paper focuses on the muscle activation patterns before, during, and after surface changes that are both visually informed and uninformed. The results show that there are predictable and repeatable muscle activation patterns both before and after surface stiffness changes, and these patterns are affected by the perceived visual and proprioceptive feedback. The interaction of feedback mechanisms and their effect on evoked muscular activation can be used in future robot-assisted gait therapies, where the intended muscle responses are informed by deterministic models and are tailored to a specific patient's needs.
ER  - 

TY  - CONF
TI  - A Visual Positioning System for Indoor Blind Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9079
EP  - 9085
AU  - H. Zhang
AU  - C. Ye
PY  - 2020
KW  - cameras
KW  - collision avoidance
KW  - distance measurement
KW  - graph theory
KW  - handicapped aids
KW  - mobile robots
KW  - navigation
KW  - particle filtering (numerical methods)
KW  - pose estimation
KW  - robot vision
KW  - visual positioning system
KW  - indoor blind navigation
KW  - VPS
KW  - robotic navigation aid
KW  - RNA
KW  - assistive navigation
KW  - depth-enhanced visual-inertial odometry
KW  - RGB-D camera
KW  - inertial measurement unit
KW  - DVIO method
KW  - geometric feature
KW  - floor plane
KW  - measurement residuals
KW  - inertial data
KW  - graph optimization framework
KW  - Sampson error
KW  - near-range visual features
KW  - known depth
KW  - far-range visual features
KW  - estimation accuracy
KW  - particle filter localization method
KW  - PFL
KW  - visually impaired person
KW  - heading error
KW  - accurate pose estimation
KW  - Cameras
KW  - RNA
KW  - Feature extraction
KW  - Visualization
KW  - Pose estimation
KW  - Navigation
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196782
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a visual positioning system (VPS) for real-time pose estimation of a robotic navigation aid (RNA) for assistive navigation. The core of the VPS is a new method called depth-enhanced visual-inertial odometry (DVIO) that uses an RGB-D camera and an inertial measurement unit (IMU) to estimate the RNA's pose. The DVIO method extracts the geometric feature (the floor plane) from the camera's depth data and integrates its measurement residuals with that of the visual features and the inertial data in a graph optimization framework for pose estimation. A new measure based on the Sampson error is introduced to describe the measurement residuals of the near-range visual features with a known depth and that of the far-range visual features whose depths are unknown. The measure allows for the incorporation of both types of visual features into graph optimization. The use of the geometric feature and the Sampson error improves pose estimation accuracy and precision. The DVIO method is paired with a particle filter localization (PFL) method to locate the RNA in a 2D floor plan and the information is used to guide a visually impaired person. The PFL reduces the RNA's position and heading error by aligning the camera's depth data with the floor plan map. Together, the DVIO and the PFL allow for accurate pose estimation for wayfinding and 3D mapping for obstacle avoidance. Experimental results demonstrate the usefulness of the RNA in assistive navigation in indoor spaces.
ER  - 

TY  - CONF
TI  - An Outsole-Embedded Optoelectronic Sensor to Measure Shear Ground Reaction Forces During Locomotion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9086
EP  - 9092
AU  - T. T. H. Duong
AU  - D. R. Whittaker
AU  - D. Zanotto
PY  - 2020
KW  - closed loop systems
KW  - footwear
KW  - force measurement
KW  - force sensors
KW  - gait analysis
KW  - medical robotics
KW  - muscle
KW  - patient rehabilitation
KW  - foot-mounted sensors
KW  - outsole-embedded optoelectronic sensor configuration
KW  - biaxial shear GRFs
KW  - traditional strain-gauge based solutions
KW  - optoelectronic sensors
KW  - footwear structure
KW  - outsole-embedded sensor
KW  - shear ground reaction forces
KW  - online estimation
KW  - 3D ground reaction forces
KW  - closed-loop control
KW  - lower-extremity robotic exoskeletons
KW  - in-verse dynamics
KW  - optimization models
KW  - net joint torques
KW  - muscle forces
KW  - instrumented footwear
KW  - vertical GRFs
KW  - Robot sensing systems
KW  - Footwear
KW  - Force
KW  - Force measurement
KW  - Instruments
KW  - Legged locomotion
KW  - wearable technology
KW  - optoelectronics
KW  - shear force sensor
KW  - instrumented footwear
DO  - 10.1109/ICRA40945.2020.9196962
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Online estimation of 3D ground reaction forces (GRFs) is becoming increasingly important for closed-loop control of lower-extremity robotic exoskeletons. Through in-verse dynamics and optimization models, 3D GRFs can be used to estimate net joint torques and approximate muscle forces. Although instrumented footwear to measure vertical GRFs in out-of-the-lab environments is available, accurately measuring shear GRFs with foot-mounted sensors still remains a challenging task. In this paper, a new outsole-embedded optoelectronic sensor configuration that is able to measure biaxial shear GRFs is proposed. Compared with traditional strain-gauge based solutions, optoelectronic sensors allow for a more affordable design. To mitigate the risk of altering the wearer's natural gait, the proposed solution does not involve external modifications to the footwear structure. A preliminary validation of the outsole-embedded sensor was conducted against validated laboratory equipment. The test involved two sessions of treadmill walking at different speeds. Experimental results suggest that the proposed design may be a promising solution for measuring shear GRFs in unconstrained environments.
ER  - 

TY  - CONF
TI  - Bump‚Äôem: an Open-Source, Bump-Emulation System for Studying Human Balance and Gait
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9093
EP  - 9099
AU  - G. R. Tan
AU  - M. Raitor
AU  - S. H. Collins
PY  - 2020
KW  - closed loop systems
KW  - force control
KW  - gait analysis
KW  - geriatrics
KW  - injuries
KW  - mechanoception
KW  - medical computing
KW  - medical control systems
KW  - open-source bump-emulation system
KW  - robotic rope-driven system
KW  - human gait
KW  - fall-inducing perturbations
KW  - laboratory-based perturbation systems
KW  - aging population
KW  - fall-related injury
KW  - human balance
KW  - open-loop system
KW  - closed-loop force control
KW  - open-loop force control
KW  - transverse plane
KW  - force-fields
KW  - Brushless motors
KW  - Perturbation methods
KW  - Force
KW  - Force sensors
KW  - Shafts
KW  - Force control
KW  - Legged locomotion
DO  - 10.1109/ICRA40945.2020.9197105
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fall-related injury is a significant health problem on a global scale and is expected to grow with the aging population. Laboratory-based perturbation systems have the capability of simulating various modes of fall-inducing perturbations in a repeatable way. These systems enable fundamental research on human gait and balance and facilitate the development of devices to assist human balance. We present a robotic, rope-driven system capable of rendering bumps and force-fields at a person's pelvis in any direction in the transverse plane with forces up to 200 N, and a 90% rise time of as little as 44 ms, which is faster than a human's ability to sense and respond to the force. These capabilities enable experiments that require stabilizing or destabilizing subjects as they stand or walk on a treadmill. To facilitate use by researchers from all backgrounds, we designed both a configuration with simpler open-loop force control, and another with higher-performance, closed-loop force control. Both configurations are modular, and the open-loop system is made entirely from 3D-printed and catalog components. The design files and assembly instructions for both are freely available in an online repository.
ER  - 

TY  - CONF
TI  - A Hybrid, Soft Exoskeleton Glove Equipped with a Telescopic Extra Thumb and Abduction Capabilities
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9100
EP  - 9106
AU  - L. Gerez
AU  - A. Dwivedi
AU  - M. Liarokapis
PY  - 2020
KW  - dexterous manipulators
KW  - diseases
KW  - force feedback
KW  - grippers
KW  - handicapped aids
KW  - motion control
KW  - patient rehabilitation
KW  - quality assessment experiments
KW  - inflatable thumb
KW  - grasp stability
KW  - hybrid assistive glove
KW  - grasping capabilities
KW  - hand exoskeletons
KW  - neurological diseases
KW  - musculoskeletal diseases
KW  - wearable gloves
KW  - exoskeleton glove
KW  - pneumatic telescopic extra thumb
KW  - force exertion experiments
KW  - activities of daily living
KW  - Actuators
KW  - Thumb
KW  - Exoskeletons
KW  - Tendons
KW  - Robots
KW  - Pneumatic systems
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9197473
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Over the last years, hand exoskeletons have become a popular and efficient technical solution for assisting people that suffer from neurological and musculoskeletal diseases and enhance the capabilities of healthy individuals. These devices can vary from rigid and complex structures to soft, lightweight, wearable gloves. Despite the significant progress in the field, most existing solutions do not provide the same dexterity as the healthy human hand. In this paper, we focus on the development of a hybrid (tendon-driven and pneumatic), lightweight, affordable, wearable exoskeleton glove equipped with abduction/adduction capabilities and a pneumatic telescopic extra thumb that increases grasp stability. The efficiency of the proposed device is experimentally validated through three different types of experiments: i) abduction/adduction tests, ii) force exertion experiments that capture the maximum forces that can be applied by the proposed device, and iii) grasp quality assessment experiments that focus on the effect of the inflatable thumb on enhancing grasp stability. The hybrid assistive glove considerably improves the grasping capabilities of the user, being able to exert the forces required to assist people in the execution of activities of daily living.
ER  - 

TY  - CONF
TI  - Controlling an upper-limb exoskeleton by EMG signal while carrying unknown load
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9107
EP  - 9113
AU  - B. Treussart
AU  - F. Geffard
AU  - N. Vignais
AU  - F. Marin
PY  - 2020
KW  - biomechanics
KW  - electromyography
KW  - force sensors
KW  - human-robot interaction
KW  - medical robotics
KW  - medical signal processing
KW  - muscle
KW  - wireless EMG
KW  - movement direction
KW  - intensity estimation
KW  - gravity compensation
KW  - EMG armband
KW  - EMG signal
KW  - traditional gravity compensation
KW  - intuitive control law
KW  - human-robot collaboration
KW  - force sensors
KW  - freedom upper-limb exoskeleton
KW  - Exoskeletons
KW  - Electromyography
KW  - Torque
KW  - Robots
KW  - Muscles
KW  - Gravity
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197087
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Implementing an intuitive control law for an upper-limb exoskeleton dedicated to force augmentation is a challenging issue in the field of human-robot collaboration. The aim of this study is to design an innovative approach to assist carrying an unknown load without using force sensors or specific handle. The method is based on user's intentions estimated through a wireless EMG armband allowing movement direction and intensity estimation along 1 Degree of Freedom. This control law aimed to behave like a gravity compensation except that the mass of the load does not need to be known. The proposed approach was tested on 10 participants during a lifting task with a single Degree of Freedom upper-limb exoskeleton. Participants performed it in three different conditions : without assistance, with an exact gravity compensation and with the proposed method based on EMG armband. The evaluation of the efficiency of the assistance was based on EMG signals captured on seven muscles (objective indicator) and a questionnaire (subjective indicator). Results showed a statically significant reduction of mean activity of the biceps, erector spinae and deltoid by 20%¬±14%, 18%¬±12% and 25% ¬± 16% respectively while comparing the proposed method with no assistance. In addition, similar muscle activities were found both in the proposed method and the traditional gravity compensation. Subjective evaluation showed better precision, efficiency and responsiveness of the proposed method compared to the traditional one.
ER  - 

TY  - CONF
TI  - Learning Grasping Points for Garment Manipulation in Robot-Assisted Dressing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9114
EP  - 9120
AU  - F. Zhang
AU  - Y. Demiris
PY  - 2020
KW  - assisted living
KW  - clothing
KW  - collision avoidance
KW  - dexterous manipulators
KW  - end effectors
KW  - grippers
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - neurocontrollers
KW  - position control
KW  - robot vision
KW  - robot-assisted dressing system
KW  - dressing activities
KW  - grasping point estimations
KW  - Baxter robot
KW  - robot-garment collision avoidance
KW  - orientation computation
KW  - grasping point prediction
KW  - depth images
KW  - supervised deep neural network
KW  - robotic manipulation
KW  - robot end-effector
KW  - robot configuration
KW  - elderly people
KW  - disabled people
KW  - assistive robots
KW  - garment manipulation
KW  - Clothing
KW  - Robots
KW  - Grasping
KW  - Collision avoidance
KW  - Rails
KW  - Neural networks
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196994
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Assistive robots have the potential to provide tremendous support for disabled and elderly people in their daily dressing activities. Recent studies on robot-assisted dressing usually simplify the setup of the initial robot configuration by manually attaching the garments on the robot end-effector and positioning them close to the user's arm. A fundamental challenge in automating such a process for robots is computing suitable grasping points on garments that facilitate robotic manipulation. In this paper, we address this problem by introducing a supervised deep neural network to locate a predefined grasping point on the garment, using depth images for their invariance to color and texture. To reduce the amount of real data required, which is costly to collect, we leverage the power of simulation to produce large amounts of labeled data. The network is jointly trained with synthetic datasets of depth images and a limited amount of real data. We introduce a robot-assisted dressing system that combines the grasping point prediction method, with a grasping and manipulation strategy which takes grasping orientation computation and robot-garment collision avoidance into account. The experimental results demonstrate that our method is capable of yielding accurate grasping point estimations. The proposed dressing system enables the Baxter robot to autonomously grasp a hospital gown hung on a rail, bring it close to the user and successfully dress the upper-body.
ER  - 

TY  - CONF
TI  - TACTO-Selector: Enhanced Hierarchical Fusion of PBVS with Reactive Skin Control for Physical Human-Robot Interaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9121
EP  - 9127
AU  - A. E. H. Martin
AU  - E. Dean-Leon
AU  - G. Cheng
PY  - 2020
KW  - haptic interfaces
KW  - human-robot interaction
KW  - industrial robots
KW  - mobile robots
KW  - motion control
KW  - position control
KW  - robot vision
KW  - visual servoing
KW  - reactive skin control
KW  - 6 DOF industrial robot
KW  - physical human-robot interaction
KW  - industrial scenarios
KW  - hierarchical task approaches
KW  - low priority tasks
KW  - standard hierarchical fusion
KW  - tactile interaction
KW  - safety task
KW  - 6 DOF position-based visual servoing task
KW  - interactive task-reconfiguring approach
KW  - TACTO-selector
KW  - PBVS
KW  - Task analysis
KW  - Collision avoidance
KW  - Skin
KW  - Robot sensing systems
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196979
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In a physical Human-Robot Interaction for industrial scenarios is paramount to guarantee the safety of the user while keeping the robot's performance. Hierarchical task approaches are not sufficient since they tend to sacrifice the low priority tasks in order to guarantee the consistency of the main task. To handle this problem, we enhance the standard hierarchical fusion by introducing a novel interactive task-reconfiguring approach (TACTO-Selector) that uses the information of the tactile interaction to adapt the dimension of the tasks, therefore guaranteeing the execution of the safety task while performing the other task as good as possible. In this work, we hierarchically combine a 6 DOF Position-Based Visual Servoing (PBVS) task with a reactive skin control. This approach was evaluated on a 6 DOF industrial robot showing an improvement of 36.37% on average in tracking error reduction compared with a standard approach.
ER  - 

TY  - CONF
TI  - Towards an Intelligent Collaborative Robotic System for Mixed Case Palletizing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9128
EP  - 9134
AU  - E. Lamon
AU  - M. Leonori
AU  - W. Kim
AU  - A. Ajoudani
PY  - 2020
KW  - graphical user interfaces
KW  - groupware
KW  - human-robot interaction
KW  - industrial robots
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - palletising
KW  - production engineering computing
KW  - visual perception
KW  - collaborative palletizing tasks
KW  - intelligent collaborative robotic system
KW  - mixed case palletizing
KW  - visual perception algorithms
KW  - high-level optimisation
KW  - graphical user interface
KW  - Mobile COllaborative robotic Assistant
KW  - human-robot collaborative framework
KW  - MOCA
KW  - packing density maximisation
KW  - Pallets
KW  - Task analysis
KW  - Collaboration
KW  - Robots
KW  - Impedance
KW  - Torque
KW  - Resource management
DO  - 10.1109/ICRA40945.2020.9196850
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, a novel human-robot collaborative framework for mixed case palletizing is presented. The framework addresses several challenges associated with the detection and localisation of boxes and pallets through visual perception algorithms, high-level optimisation of the collaborative effort through effective role-allocation principles, and maximisation of packing density. A graphical user interface (GUI) is additionally developed to ensure an intuitive allocation of roles and the optimal placement of the boxes on target pallets. The framework is evaluated in two conditions where humans operate with and without the support of a Mobile COllaborative robotic Assistant (MOCA). The results show that the optimised placement can improve up to the 20% with respect to a manual execution of the same task, and reveal the high potential of MOCA in increasing the performance of collaborative palletizing tasks.
ER  - 

TY  - CONF
TI  - Treadmill Based Three Tether Parallel Robot for Evaluating Auditory Warnings While Running
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9135
EP  - 9142
AU  - N. G. Luttmer
AU  - T. E. Truong
AU  - A. M. Boynton
AU  - D. Carrier
AU  - M. A. Minor
PY  - 2020
KW  - force control
KW  - gait analysis
KW  - human-robot interaction
KW  - medical robotics
KW  - patient rehabilitation
KW  - statistical testing
KW  - three-term control
KW  - virtual reality
KW  - walking running subjects
KW  - treadmill
KW  - T-test
KW  - auditory warnings
KW  - 3 DoF parallel cable system
KW  - Utah's Treadport Active Wind Tunnel
KW  - gait algorithms
KW  - PID force controller
KW  - three tether parallel robot
KW  - Nexus VICON motion capture
KW  - sports related concussions
KW  - hemiparetic rehabilitation
KW  - immersive virtual reality locomotion system
KW  - Perturbation methods
KW  - Force
KW  - Legged locomotion
KW  - Muscles
KW  - Pulleys
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9196600
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We design and test a 3 DoF parallel cable system capable of applying precise and accurate impulses to walking and running subjects for the University of Utah's Treadport Active Wind Tunnel (TPAWT). Using Nexus VICON motion capture and gait algorithms, perturbations can be applied at different points in the subject's gait. The use of a PID force controller allow the system to create omnidirectional perturbations with walking and running subjects while having the capability to vary amplitude and direction of perturbations. Analysis is presented of the workspace of the large treadmill to test whether the workspace available to activate these perturbations is safe. This paper reports the efficacy of the system and evaluates how warning a runner before impact may affect their displacement. Participants experienced 48 perturbations while running applied with a random combination of a front/back/left/right impact at either toe-off or mid-stance with or without warning. A two sample T-test reveals that warning a runner before impact significantly reduced the magnitude they were displaced for both toe-off (t(46) = 4.98 p<; .001) and mid-stance (t(46) = 3.44, p = .001).
ER  - 

TY  - CONF
TI  - Evaluation of Human-Robot Object Co-manipulation Under Robot Impedance Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9143
EP  - 9149
AU  - M. Mujica
AU  - M. Benoussaad
AU  - J. -Y. Fourquet
PY  - 2020
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - trajectory control
KW  - object dynamical properties
KW  - robot impedance control
KW  - human-robot collaboration
KW  - human-robot object co-manipulation
KW  - pHRI
KW  - 7-dof Kuka LBR iiwa 14 R820 robot
KW  - human trajectory
KW  - robot control law
KW  - human forces
KW  - interaction quality metrics
KW  - interaction comfort
KW  - human safety
KW  - physical human-robot interaction
KW  - continuous physical interaction
KW  - Collaboration
KW  - Task analysis
KW  - Impedance
KW  - Manipulators
KW  - Service robots
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197329
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The human-robot collaboration is a promising and challenging field of robotics research. One of the main collaboration tasks is the object co-manipulation where the human and robot are in a continuous physical interaction and forces exerted must be handled. This involves some issues known in robotics as physical Human-Robot Interaction (pHRI), where human safety and interaction comfort are required. Moreover, a definition of interaction quality metrics would be relevant. In the current work, the assessment of Human-Robot object co-manipulation task was explored through the proposed metrics of interaction quality, based on human forces throughout the movement. This analysis is based on co-manipulation of objects with different dynamical properties (weight and inertia), with and without including these properties knowledge in the robot control law. Here, the human is a leader of task and the robot the follower without any information of the human trajectory and movement profile. For the robot control law, a well-known impedance control was applied on a 7-dof Kuka LBR iiwa 14 R820 robot. Results show that the consideration of object dynamical properties in the robot control law is crucial for a good and more comfortable interaction. Besides, human efforts are more significant with a higher no-considered weight, whereas it remains stable when these weights were considered.
ER  - 

TY  - CONF
TI  - Whole-Body Bilateral Teleoperation of a Redundant Aerial Manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9150
EP  - 9156
AU  - A. Coelho
AU  - H. Singh
AU  - K. Kondak
AU  - C. Ott
PY  - 2020
KW  - autonomous aerial vehicles
KW  - delays
KW  - end effectors
KW  - force feedback
KW  - haptic interfaces
KW  - manipulator dynamics
KW  - mobile robots
KW  - position control
KW  - redundant manipulators
KW  - robot vision
KW  - telerobotics
KW  - video cameras
KW  - redundant aerial manipulator
KW  - robotic manipulator
KW  - flying base
KW  - reachability
KW  - manipulation task
KW  - human capabilities
KW  - telemanipulation tasks
KW  - visual feedback
KW  - task-dependent
KW  - video camera
KW  - end-effector motion
KW  - base position
KW  - stable bilateral teleoperation
KW  - time-delayed telemanipulation
KW  - whole-body bilateral teleoperation
KW  - null-space wall
KW  - haptic concept
KW  - kinematic structure
KW  - task-dependent optimal pose
KW  - Task analysis
KW  - Manipulators
KW  - Haptic interfaces
KW  - Cameras
KW  - Null space
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197028
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Attaching a robotic manipulator to a flying base allows for significant improvements in the reachability and versatility of manipulation tasks. In order to explore such systems while taking advantage of human capabilities in terms of perception and cognition, bilateral teleoperation arises as a reasonable solution. However, since most telemanipulation tasks require visual feedback in addition to the haptic one, real-time (task-dependent) positioning of a video camera, which is usually attached to the flying base, becomes an additional objective to be fulfilled. Since the flying base is part of the kinematic structure of the robot, if proper care is not taken, moving the video camera could undesirably disturb the end-effector motion. For that reason, the necessity of controlling the base position in the null space of the manipulation task arises. In order to provide the operator with meaningful information about the limits of the allowed motions in the null space, this paper presents a novel haptic concept called Null-Space Wall. In addition, a framework to allow stable bilateral teleoperation of both tasks is presented. Numerical simulation data confirm that the proposed framework is able to keep the system passive while allowing the operator to perform time-delayed telemanipulation and command the base to a task-dependent optimal pose.
ER  - 

TY  - CONF
TI  - Shared Autonomous Interface for Reducing Physical Effort in Robot Teleoperation via Human Motion Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9157
EP  - 9163
AU  - T. -C. Lin
AU  - A. Unni Krishnan
AU  - Z. Li
PY  - 2020
KW  - humanoid robots
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - telerobotics
KW  - mobile humanoid robot
KW  - general-purpose assistive tasks
KW  - motion mapping
KW  - human motion
KW  - robot teleoperation
KW  - autonomous interface
KW  - teleoperation interfaces
KW  - task completion time
KW  - assistance function
KW  - teleoperator
KW  - autonomous grasping function
KW  - Task analysis
KW  - Fatigue
KW  - Muscles
KW  - Grasping
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197220
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion mapping is an intuitive method of teleoperation with a low learning curve. Our previous study investigates the physical fatigue caused by teleoperating a robot to perform general-purpose assistive tasks and this fatigue affects the operator's performance. The results from that study indicate that physical fatigue happens more in the tasks which involve more precise manipulation and steady posture maintenance. In this paper, we investigate how teleoperation assistance in terms of shared autonomy can reduce the physical workload in robot teleoperation via motion mapping. Specifically, we conduct a user study to compare the muscle effort in teleoperating a mobile humanoid robot to (1) reach and grasp an individual object and (2) collect objects in a cluttered workspace with and without an autonomous grasping function that can be triggered manually by the teleoperator. We also compare the participants' task performance, subjective user experience, and change in attitude towards the usage of teleoperation assistance in the future based on their experience using the assistance function. Our results show that: (1) teleoperation assistance like autonomous grasping can effectively reduce the physical effort, task completion time and number of errors; (2) based on their experience performing the tasks with and without assistance, the teleoperators reported that they would prefer to use automated functions for future teleoperation interfaces.
ER  - 

TY  - CONF
TI  - DexPilot: Vision-Based Teleoperation of Dexterous Robotic Hand-Arm System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9164
EP  - 9170
AU  - A. Handa
AU  - K. Van Wyk
AU  - W. Yang
AU  - J. Liang
AU  - Y. -W. Chao
AU  - Q. Wan
AU  - S. Birchfield
AU  - N. Ratliff
AU  - D. Fox
PY  - 2020
KW  - dexterous manipulators
KW  - robot vision
KW  - telerobotics
KW  - vision-based teleoperation
KW  - dexterous robotic hand-arm system
KW  - robotic systems
KW  - reasoning skills
KW  - depth-based teleoperation system
KW  - DoA robotic system
KW  - DexPilot
KW  - degree-of-actuation
KW  - multifingered robots
KW  - pick-and-place operations
KW  - Tracking
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Robot sensing systems
KW  - Cameras
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197124
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Teleoperation offers the possibility of imparting robotic systems with sophisticated reasoning skills, intuition, and creativity to perform tasks. However, teleoperation solutions for high degree-of-actuation (DoA), multi-fingered robots are generally cost-prohibitive, while low-cost offerings usually offer reduced degrees of control. Herein, a low-cost, depth-based teleoperation system, DexPilot, was developed that allows for complete control over the full 23 DoA robotic system by merely observing the bare human hand. DexPilot enabled operators to solve a variety of complex manipulation tasks that go beyond simple pick-and-place operations and performance was measured through speed and reliability metrics. DexPilot cost-effectively enables the production of high dimensional, multi-modality, state-action data that can be leveraged in the future to learn sensorimotor policies for challenging manipulation tasks. The videos of the experiments can be found at https://sites.google.com/view/dex-pilot.
ER  - 

TY  - CONF
TI  - Distributed Winner-Take-All Teleoperation of A Multi-Robot System
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9171
EP  - 9177
AU  - Y. Yang
AU  - D. Constantinescu
AU  - Y. Shi
PY  - 2020
KW  - decision making
KW  - Lyapunov methods
KW  - multi-robot systems
KW  - protocols
KW  - stability
KW  - telerobotics
KW  - team cohesion
KW  - dynamic decision-making protocol
KW  - decision variable
KW  - slave robots
KW  - decision-making algorithm
KW  - 3-masters-11-slaves teleoperation
KW  - distributed winner-take-all teleoperation
KW  - multirobot system
KW  - multimaster-multislave teleoperation system
KW  - Lyapunov stability analysis
KW  - Robots
KW  - Decision making
KW  - Protocols
KW  - Heuristic algorithms
KW  - Indexes
KW  - Force
KW  - Multi-robot systems
DO  - 10.1109/ICRA40945.2020.9197535
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In a distributed multi-master-multi-slave teleoperation system, the human users may compete against each other for the control of the team of slave robots. To win the competition, one operator would send the largest command to the slave group. For the sake of team cohesion, the slave group should follow the command of the winning operator and ignore the commands of the other users. To enable (i) the slave team to identify the winning operator, and (ii) each slave to determine whether to admit or discard the command it receives from its operator, this paper proposes a dynamic decision-making protocol that distinguishes the decision variable of the slave commanded by the winner from the decision variables of all other slave robots. The protocol only requires the slaves to exchange and evaluate their decision variables locally. Lyapunov stability analysis proves the theoretical convergence of the proposed decision-making algorithm. An experimental distributed winner-take-all teleoperation in a 3-masters-11-slaves teleoperation testbed validates its practical efficacy.
ER  - 

TY  - CONF
TI  - Enhanced Teleoperation Using Autocomplete
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9178
EP  - 9184
AU  - M. K. Zein
AU  - A. Sidaoui
AU  - D. Asmar
AU  - I. H. Elhajj
PY  - 2020
KW  - autonomous aerial vehicles
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - telerobotics
KW  - enhanced teleoperation
KW  - Autocomplete
KW  - remote location
KW  - skilled teleoperators
KW  - training time
KW  - novice teleoperators
KW  - human input
KW  - desired motion
KW  - machine learning
KW  - motion primitives
KW  - unmanned aerial vehicle
KW  - Task analysis
KW  - Trajectory
KW  - Robots
KW  - Training
KW  - Support vector machines
KW  - Manuals
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9197140
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Controlling and manning robots from a remote location is difficult because of the limitations one faces in perception and available degrees of actuation. Although humans can become skilled teleoperators, the amount of training time required to acquire such skills is typically very high. In this paper, we propose a novel solution (named Autocomplete) to aid novice teleoperators in manning robots adroitly. At the input side, Autocomplete relies on machine learning to detect and categorize human inputs as one from a group of motion primitives. Once a desired motion is recognized, at the actuation side an automated command replaces the human input in performing the desired action. So far, Autocomplete can recognize and synthesize lines, arcs, full circles, 3-D helices, and sine trajectories. Autocomplete was tested in simulation on the teleoperation of an unmanned aerial vehicle, and results demonstrate the advantages of the proposed solution versus manual steering.
ER  - 

TY  - CONF
TI  - Contact-based Bounding Volume Hierarchy for Assembly Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9185
EP  - 9190
AU  - E. Shellshear
AU  - Y. Li
AU  - R. Bohlin
AU  - J. S. Carlson
PY  - 2020
KW  - assembling
KW  - collision avoidance
KW  - computational geometry
KW  - industrial control
KW  - path planning
KW  - distance computation algorithms
KW  - path planning
KW  - contact-based assembly tasks
KW  - collision queries
KW  - distance queries
KW  - bounding volume hierarchy
KW  - broad phase proximity query algorithm
KW  - contact-based hierarchy for assembly tasks
KW  - CHAT tasks
KW  - Path planning
KW  - Task analysis
KW  - Geometry
KW  - Buildings
KW  - Planning
KW  - Measurement
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196573
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning of an object which is allowed to be in contact with other objects during assembly process is a significant challenge due to the variety of permitted or forbidden collisions between the distinct parts of the objects to be assembled. In order to put objects together in real-life scenarios, parts of assembled objects may be required to flex, whereas other parts may have to fit exactly. Consequently, existing collision checking and distance computation algorithms have to be modified to enable path planning of objects that can be in contact during the assembly process. In this paper, we analyze an improved broad phase proximity query algorithm to enable such contact-based assembly tasks we call CHAT (Contact-based Hierarchy for Assembly Tasks). We demonstrate that, compared to existing approaches, our proposed method is more than an order of magnitude faster for collision queries and up to three times faster for distance queries when the two objects contain a large number of parts (with some parts containing thousands or tens of thousands of triangles). Due to the nature of the algorithm, we expect the performance improvements to increase as the number of parts in an object becomes larger.
ER  - 

TY  - CONF
TI  - Construction of Bounding Volume Hierarchies for Triangle Meshes with Mixed Face Sizes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9191
EP  - 9195
AU  - Y. Li
AU  - E. Shellshear
AU  - R. Bohlin
AU  - J. S. Carlson
PY  - 2020
KW  - collision avoidance
KW  - computational geometry
KW  - computer graphics
KW  - manufacturing industries
KW  - mesh generation
KW  - trees (mathematics)
KW  - BVH node
KW  - IPS CDC
KW  - split axis
KW  - split position
KW  - IPS Path Planner
KW  - collision-free disassembly paths
KW  - bounding volume hierarchies
KW  - mixed face sizes
KW  - BVH
KW  - complex 3D geometries
KW  - triangle meshes
KW  - collision and distance computation module
KW  - manufacturing industries
KW  - tighter-fitting bounding volumes
KW  - IP networks
KW  - Covariance matrices
KW  - Three-dimensional displays
KW  - Tensile stress
KW  - Cost function
KW  - Path planning
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9197113
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of creating tighter-fitting bounding volumes (more specifically rectangular swept spheres) when constructing bounding volume hierarchies (BVHs) for complex 3D geometries given in the form of unstructured triangle meshes/soups with the aim of speeding up our IPS Path Planner for rigid bodies, where the triangles often have very different sizes. Currently, the underlying collision and distance computation module (IPS CDC) does not take into account the sizes of the triangles when it constructs BVHs using a top-down strategy. To split triangles in a BVH node into two BVH nodes, IPS CDC has to compute both the split axis and the split position. In this work, we use the principal axes of the tensor of inertia as the potential split axes and the center of mass as the split position, where the computations of both the tensor of inertia and the center of mass require knowledge of the areas of the triangles. We show that our method improves performance (up to 20 % faster) of our IPS Path Planner when it is used to plan collision-free disassembly paths for three different test cases taken from manufacturing industries.
ER  - 

TY  - CONF
TI  - Strategy for automated dense parking: how to navigate in narrow lanes*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9196
EP  - 9202
AU  - P. Polack
AU  - L. -M. Dallen
AU  - A. Cord
PY  - 2020
KW  - automobiles
KW  - Global Positioning System
KW  - Lyapunov methods
KW  - mobile robots
KW  - motion control
KW  - navigation
KW  - path planning
KW  - road traffic control
KW  - narrow lanes
KW  - high- density parking solution
KW  - car-like robots
KW  - hard constraints
KW  - robot motion
KW  - robot localization
KW  - navigation
KW  - configuration space formulation
KW  - Stanley Robotics robots
KW  - automated dense parking
KW  - Lyapunov- based control strategy
KW  - GPS orientation
KW  - Robots
KW  - Collision avoidance
KW  - Aerospace electronics
KW  - Navigation
KW  - Mathematical model
KW  - Kinematics
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9197088
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the architecture of a high- density parking solution based on car-like robots specifically designed to move cars. The main difficulty is to park the vehicles close to one another which implies hard constraints on the robot motion and localization. In particular, this paper focuses on navigation in narrow lanes. We propose a Lyapunov- based control strategy that has been derived after expressing the problem in a Configuration Space formulation. The current solution has been implemented and tested on Stanley Robotics' robots and has been running in production for several months. Thanks to the Configuration Space formulation, we are able to guarantee the obstacles' integrity. Moreover, a method for calibrating the GPS orientation with a high-precision is derived from the present control strategy.
ER  - 

TY  - CONF
TI  - Multimodal Trajectory Predictions for Urban Environments Using Geometric Relationships between a Vehicle and Lanes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9203
EP  - 9209
AU  - A. Kawasaki
AU  - A. Seki
PY  - 2020
KW  - feature extraction
KW  - object detection
KW  - road traffic
KW  - road vehicles
KW  - traffic engineering computing
KW  - autonomous driving systems
KW  - traffic behavior
KW  - urban environments
KW  - road geometries
KW  - lane-based multimodal prediction network
KW  - arbitrary shapes
KW  - traffic lanes
KW  - future trajectory
KW  - lane geometry
KW  - lane feature
KW  - generalized geometric relationships
KW  - vehicle state
KW  - vehicle motion model constraint
KW  - prediction method
KW  - multimodal trajectory predictions
KW  - safe driving systems
KW  - LAMP-Net
KW  - Trajectory
KW  - Predictive models
KW  - Hidden Markov models
KW  - Acceleration
KW  - Geometry
KW  - Shape
KW  - Urban areas
DO  - 10.1109/ICRA40945.2020.9196738
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Implementation of safe and efficient autonomous driving systems requires accurate prediction of the long-term trajectories of surrounding vehicles. High uncertainty in traffic behavior makes it difficult to predict trajectories in urban environments, which have various road geometries. To over-come this problem, we propose a method called lane-based multimodal prediction network (LAMP-Net), which can handle arbitrary shapes and numbers of traffic lanes and predict both the future trajectory along each lane and the probability of each lane being selected. A vector map is used to define the lane geometry and a novel lane feature is introduced to represent the generalized geometric relationships between the vehicle state and lanes. Our network takes this feature as the input and is trained to be versatile for arbitrarily shaped lanes. Moreover, we introduce a vehicle motion model constraint to our network. Our prediction method combined with the constraint significantly enhances prediction accuracy. We evaluate the prediction performance on two datasets which contain a wide variety of real-world traffic scenarios. Experimental results show that our proposed LAMP-Net outperforms state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Online optimal motion generation with guaranteed safety in shared workspace
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9210
EP  - 9215
AU  - P. Zheng
AU  - P. -B. Wieber
AU  - O. Aycard
PY  - 2020
KW  - collision avoidance
KW  - industrial manipulators
KW  - motion control
KW  - occupational safety
KW  - path planning
KW  - predictive control
KW  - online optimal motion generation
KW  - guaranteed safety
KW  - shared workspace
KW  - safer manipulator robots
KW  - serious injury
KW  - equip robots
KW  - online motion generation
KW  - partially unknown dynamic environment
KW  - industrial manipulator robots
KW  - model predictive control scheme
KW  - Collision avoidance
KW  - Trajectory
KW  - Safety
KW  - Manipulators
KW  - Robot sensing systems
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197018
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With new, safer manipulator robots, the probability of serious injury due to collisions with humans remains low (5%), even at speeds as high as 2 m.s-1. Collisions would better be avoided nevertheless, because they disrupt the tasks of both the robot and the human. We propose in this paper to equip robots with exteroceptive sensors and online motion generation so that the robot is able to perceive and react to the motion of the human in order to reduce the occurrence of collisions. It's impossible to guarantee that no collision will ever take place in a partially unknown dynamic environment such as a shared workspace, but we can guarantee instead that, if a collision takes place, the robot is at rest at the time of collision, so that it doesn't inject its own kinetic energy in the collision. To do so, we adapt a Model Predictive Control scheme which has been demonstrated previously with two industrial manipulator robots avoiding collisions while sharing their workspace. The proposed control scheme is validated in simulation.
ER  - 

TY  - CONF
TI  - Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9216
EP  - 9222
AU  - C. Folkestad
AU  - D. Pastor
AU  - J. W. Burdick
PY  - 2020
KW  - aircraft control
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - nonlinear control systems
KW  - nonlinear dynamical systems
KW  - optimal control
KW  - predictive control
KW  - robot dynamics
KW  - model predictive control
KW  - nonlinear diffeomorphism
KW  - nonlinear dynamical systems
KW  - optimal control
KW  - multirotor landing
KW  - nonlinear robot dynamics
KW  - episodic Koopman learning
KW  - Eigenvalues and eigenfunctions
KW  - Nonlinear dynamical systems
KW  - Robots
KW  - Aerospace electronics
KW  - Heuristic algorithms
KW  - Vehicle dynamics
DO  - 10.1109/ICRA40945.2020.9197510
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.
ER  - 

TY  - CONF
TI  - Eye-in-Hand 3D Visual Servoing of Helical Swimmers Using Parallel Mobile Coils
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9223
EP  - 9229
AU  - Z. Yang
AU  - L. Yang
AU  - L. Zhang
PY  - 2020
KW  - marine systems
KW  - microrobots
KW  - mobile robots
KW  - robot vision
KW  - visual servoing
KW  - refraction-rectified location algorithm
KW  - coil module
KW  - motor module
KW  - eye-in-hand stereo-vision module
KW  - medical applications
KW  - spacial movement
KW  - control methods
KW  - magnetic actuation systems
KW  - narrow space
KW  - magnetic field
KW  - magnetic helical microswimmers
KW  - parallel mobile coils
KW  - eye-in-hand 3D visual servoing
KW  - cylindrical workspace
KW  - prototype system
KW  - long-distance 3D path
KW  - triple-loop stereo visual servoing strategy
KW  - dynamic magnetic fields
KW  - mobile-coil system
KW  - Coils
KW  - Magnetic resonance imaging
KW  - Three-dimensional displays
KW  - Cameras
KW  - Magnetic devices
KW  - Visual servoing
KW  - Magnetic separation
DO  - 10.1109/ICRA40945.2020.9197276
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Magnetic helical microswimmers can be propelled by rotating magnetic field and are adept at passing through narrow space. To date, various magnetic actuation systems and control methods have been developed to drive these microswimmers. However, steering their spacial movement in a large workspace is still challenging, which could be significant for potential medical applications. In this regard, this paper designs an eye-in-hand stereo-vision module and corresponding refraction-rectified location algorithm. Combined with the motor module and the coil module, the mobile-coil system is capable of generating dynamic magnetic fields in a large 3D workspace. Based on the system, a robust triple-loop stereo visual servoing strategy is proposed that operates simultaneous tracking, locating, and steering, through which the helical swimmer is able to follow a long-distance 3D path. A scaled-up magnetic helical swimmer is employed in the path following experiment. Our prototype system reaches a cylindrical workspace with a diameter more than 200 mm, and the mean error of path tracking is less than 2 mm.
ER  - 

TY  - CONF
TI  - A Mobile Paramagnetic Nanoparticle Swarm with Automatic Shape Deformation Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9230
EP  - 9236
AU  - L. Yang
AU  - J. Yu
AU  - L. Zhang
PY  - 2020
KW  - control nonlinearities
KW  - deformation
KW  - fuzzy control
KW  - microrobots
KW  - mobile robots
KW  - multi-robot systems
KW  - nanoparticles
KW  - robot dynamics
KW  - EPNS
KW  - mobile paramagnetic nanoparticle swarm
KW  - automatic shape deformation control
KW  - swarm control
KW  - active shape deformation
KW  - elliptical rotating magnetic fields
KW  - swarm pattern
KW  - elliptical paramagnetic nanoparticle swarm
KW  - strength ratio
KW  - elliptical field
KW  - shape ratio
KW  - length ratio
KW  - deformation dynamics
KW  - fuzzy logic-based control
KW  - nanorobot
KW  - microrobotics
KW  - field ratio
KW  - nonlinearity
KW  - planar rotational locomotion
KW  - planar translational locomotion
KW  - Shape
KW  - Strain
KW  - Magnetic resonance imaging
KW  - Nanoparticles
KW  - Virtual private networks
KW  - Micromagnetics
KW  - Magnetic anisotropy
DO  - 10.1109/ICRA40945.2020.9197010
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, swarm control of micro-/nanorobots has drawn much attention in the field of microrobotics. This paper reports a mobile paramagnetic nanoparticle swarm with the capability of active shape deformation that can improve its environment adaptability. We show that, by applying elliptical rotating magnetic fields, a swarm pattern called the elliptical paramagnetic nanoparticle swarm (EPNS) would be formed. When changing the field ratio-Œ± (i.e. the strength ratio between the minor axis and major axis of the elliptical field), the shape ratio-Œ≤ of the EPNS (i.e. the length ratio between the major axis and minor axis) will change accordingly. However, automatically control this shape deformation process has difficulties because the deformation dynamics has strong nonlinearity, model variation and long time requirement. To solve this problem, we propose a fuzzy logic-based control scheme that utilizes the knowledge and control experience from skilled human operators. Experiments show that the proposed control scheme can stably maneuver the shape deformation of the EPNS with small overshoot, which cannot be achieved by conventional PI control. Moreover, experimental results show that, with the automatic shape deformation control, shape of the EPNS is controlled with high reversibility and also can be well maintained during the planar rotational and translational locomotion of the EPNS.
ER  - 

TY  - CONF
TI  - Magnetic miniature swimmers with multiple rigid flagella
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9237
EP  - 9243
AU  - J. Quispe
AU  - S. R√©gnier
PY  - 2020
KW  - biomechanics
KW  - microrobots
KW  - mobile robots
KW  - position control
KW  - propulsion
KW  - magnetic miniature swimmers
KW  - multiple rigid flagella
KW  - multiple rigid tails
KW  - rotating magnetic field
KW  - robot rotation
KW  - tail distribution
KW  - tail height
KW  - multitailed swimmer robots
KW  - spherical helices
KW  - 2-tailed swimmer
KW  - angular position
KW  - Robots
KW  - Propulsion
KW  - Magnetosphere
KW  - Prototypes
KW  - Microorganisms
KW  - Force
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196531
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we introduce novel miniature swimmers with multiple rigid tails based on spherical helices. The tail distribution of these prototypes enhances its swimming features as well as allowing to carry objects with it. The proposed swimmers are actuated by a rotating magnetic field, generating the robot rotation and thus producing a considerable thrust to start self-propelling. These prototypes achieved propulsion speeds up to 6 mm/s at 3.5 Hz for a 6-mm in size prototypes. We study the efficiency of different tail distribution for a 2-tailed swimmer by varying the angular position between both tails. Moreover, it is demonstrated that these swimmers experience great sensibility when changing their tail height. Besides, these swimmers demonstrate to be effective for cargo carrying tasks since they can displace objects up to 3.5 times their weight. Finally, wall effect is studied with multi-tailed swimmer robots considering 2 containers with 20 and 50-mm in width. Results showed speeds' increments up to 59% when swimmers are actuated in the smaller container.
ER  - 

TY  - CONF
TI  - Design and Control of a Large-Range Nil-Stiffness Electro-Magnetic Active Force Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9244
EP  - 9250
AU  - J. Cailliez
AU  - A. Weill-Duflos
AU  - M. Boudaoud
AU  - S. R√©gnier
AU  - S. Haliyo
PY  - 2020
KW  - calibration
KW  - closed loop systems
KW  - electromagnetic actuators
KW  - force measurement
KW  - force sensors
KW  - microsensors
KW  - MicroElectro Mechanical Systems
KW  - force measurements
KW  - meso-scale robotics
KW  - meso-scale active force sensor
KW  - novel meso-scale sensor
KW  - nil-stiffness guidance
KW  - loop control
KW  - nil-stiffness characteristic
KW  - infinite stiffness
KW  - sensor architecture
KW  - low frequency forces
KW  - cutoff frequency
KW  - large-range nil-stiffness electro-magnetic active force sensor
KW  - active force sensors
KW  - measurement range
KW  - conventional passive force sensors
KW  - quasiinfinite stiffness
KW  - frequency 73.9 Hz
KW  - Probes
KW  - Force
KW  - Force measurement
KW  - Optical sensors
KW  - Force sensors
KW  - Sensor phenomena and characterization
DO  - 10.1109/ICRA40945.2020.9197096
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Active force sensors are key instruments to get around the tradeoff between the sensitivity and the measurement range of conventional passive force sensors. Thanks to their quasi-infinite stiffness in closed loop, active sensors can be applied for force measurements on samples with a wide range of stiffness without interference with the mechanical parameters of the sensor. MEMS (Micro-Electro Mechanical Systems) active force sensors have been wildly developed in the literature but they are ill adapted for force measurements at the Newton level needed in meso-scale robotics. In this article, a novel structure for a meso-scale active force sensor is proposed for the measurement of forces from the milli-newton to the newton.This novel meso-scale sensor is based on a nil-stiffness guidance and an electromagnetic actuation. This paper deals with its design, identification, calibration and closed loop control. The sensor exhibits nil-stiffness characteristic in open loop and an almost infinite stiffness in closed loop. This allows measuring forces with a large range of gradients. First experiments shows the ability of this new sensor architecture to measure low frequency forces up to 0.8N with a precision of 0.03 N and a closed loop -20 dB cutoff frequency of 73.9Hz.
ER  - 

TY  - CONF
TI  - Modeling Electromagnetic Navigation Systems for Medical Applications using Random Forests and Artificial Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9251
EP  - 9256
AU  - R. Yu
AU  - S. L. Charreyron
AU  - Q. Boehler
AU  - C. Weibel
AU  - C. Chautems
AU  - C. C. Y. Poon
AU  - B. J. Nelson
PY  - 2020
KW  - electromagnetic devices
KW  - learning (artificial intelligence)
KW  - mean square error methods
KW  - medical computing
KW  - neural nets
KW  - surgery
KW  - nonlinear regions
KW  - higher magnetic fields
KW  - random forest
KW  - RF
KW  - artificial neural network
KW  - eMNS
KW  - state-of-the-art linear multipole electromagnet model
KW  - MPEM
KW  - ANN model
KW  - field magnitude
KW  - current range
KW  - high current regions
KW  - field-magnitude RMSE improvement
KW  - error reduction
KW  - machine learning
KW  - medical applications
KW  - complex nonlinear behavior
KW  - accurate field
KW  - magnetic navigation
KW  - modeling electromagnetic Navigation Systems
KW  - multiscale devices
KW  - human body
KW  - remote surgery
KW  - electromagnets
KW  - linear behavior
KW  - significant modeling errors
KW  - Saturation magnetization
KW  - Electromagnets
KW  - Current measurement
KW  - Magnetic cores
KW  - Magnetostatics
KW  - Magnetic resonance imaging
KW  - Magnetic separation
DO  - 10.1109/ICRA40945.2020.9197212
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electromagnetic Navigation Systems (eMNS) can be used to control a variety of multiscale devices within the human body for remote surgery. Accurate modeling of the magnetic fields generated by the electromagnets of an eMNS is crucial for the precise control of these devices. Existing methods assume a linear behavior of these systems, leading to significant modeling errors within nonlinear regions exhibited at higher magnetic fields, preventing these systems from operating at full capacity. In this paper, we use a random forest (RF) and an artificial neural network (ANN) to model the nonlinear behavior of the magnetic fields generated by an eMNS. Both machine learning methods outperformed the state-of-the-art linear multipole electromagnet model (MPEM). The RF and the ANN model reduced the root mean squared error (RMSE) of the MPEM when predicting the field magnitude by approximately 40% and 87%, respectively, over the entire current range of the eMNS. At high current regions, especially between 30 and 35 A, the field-magnitude RMSE improvement of the ANN model over the MPEM was 37 mT, equivalent to 90% error reduction. This study demonstrates the feasibility of using machine learning to model an eMNS for medical applications, and its ability to account for complex nonlinear behavior at high currents. The use of machine learning thus shows promise in developing accurate field predicting models, and ultimately improving surgical procedures that use magnetic navigation.
ER  - 

TY  - CONF
TI  - Automated Tracking System with Head and Tail Recognition for Time-Lapse Observation of Free-Moving C. elegans
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9257
EP  - 9262
AU  - S. Dong
AU  - X. Liu
AU  - P. Li
AU  - X. Tang
AU  - D. Liu
AU  - M. Kojima
AU  - Q. Huang
AU  - T. Arai
PY  - 2020
KW  - biology computing
KW  - CCD image sensors
KW  - feature extraction
KW  - image motion analysis
KW  - object tracking
KW  - automated tracking system
KW  - time-lapse observation
KW  - tail recognition
KW  - C. elegans
KW  - automated platform
KW  - behavioral analysis
KW  - long-term tracking
KW  - response speed
KW  - tracking time
KW  - Fitting
KW  - Tracking
KW  - Head
KW  - Microscopy
KW  - Mathematical model
KW  - Real-time systems
KW  - Curve fitting
DO  - 10.1109/ICRA40945.2020.9197546
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, an automated tracking system with head and tail recognition for time-lapse observation of free-moving C. elegans is presented. In microscale field, active C. elegans can move out of the view easily without an automated tracking system because of the narrow field of view and rapid speed of C. elegans. In our previous works, we constructed an automated platform with 3D freedom to track centroid region of the nematode successfully. However, tracking time was not long enough to support a full time-lapse observation. Our proposed system in this study integrate the detection method in horizontal plane with depth evaluation more tightly. Tracking time and response speed have been greatly improved. Besides, we make full use of curvature calculation to make the system recognize the head and tail of C. elegans and the recognition rate can be up to 95%. The results demonstrate that the system can fully achieve automated long-term tracking of a free-living nematode and will be a nice tool for C. elegans behavioral analysis.
ER  - 

TY  - CONF
TI  - Towards Adaptive Benthic Habitat Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9263
EP  - 9270
AU  - J. Shields
AU  - O. Pizarro
AU  - S. B. Williams
PY  - 2020
KW  - bathymetry
KW  - geophysical image processing
KW  - neural nets
KW  - oceanographic techniques
KW  - remotely operated vehicles
KW  - seafloor phenomena
KW  - sonar
KW  - underwater vehicles
KW  - habitat model
KW  - AUV systems
KW  - seafloor imagery
KW  - efficient AUV surveys
KW  - visually-derived habitat classes
KW  - broad-scale bathymetric data
KW  - fewer samples
KW  - benthic surveys
KW  - adaptive benthic habitat
KW  - autonomous underwater vehicles
KW  - benthic habitat mapping
KW  - broadscale bathymetric data
KW  - remotely-sensed acoustic data
KW  - Feature extraction
KW  - Uncertainty
KW  - Biological system modeling
KW  - Bayes methods
KW  - Data models
KW  - Neural networks
KW  - Backscatter
DO  - 10.1109/ICRA40945.2020.9196811
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous Underwater Vehicles (AUVs) are increasingly being used to support scientific research and monitoring studies. One such application is in benthic habitat mapping where these vehicles collect seafloor imagery that complements broadscale bathymetric data collected using sonar. Using these two data sources, the relationship between remotely-sensed acoustic data and the sampled imagery can be learned, creating a habitat model. As the areas to be mapped are often very large and AUV systems collecting seafloor imagery can only sample from a small portion of the survey area, the information gathered should be maximised for each deployment. This paper illustrates how the habitat models themselves can be used to plan more efficient AUV surveys by identifying where to collect further samples in order to most improve the habitat model. A Bayesian neural network is used to predict visually-derived habitat classes when given broad-scale bathymetric data. This network can also estimate the uncertainty associated with a prediction, which can be deconstructed into its aleatoric (data) and epistemic (model) components. We demonstrate how these structured uncertainty estimates can be utilised to improve the model with fewer samples. Such adaptive approaches to benthic surveys have the potential to reduce costs by prioritizing further sampling efforts. We illustrate the effectiveness of the proposed approach using data collected by an AUV on offshore reefs in Tasmania, Australia.
ER  - 

TY  - CONF
TI  - Multispectral Domain Invariant Image for Retrieval-based Place Recognition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9271
EP  - 9277
AU  - D. Han
AU  - Y. Hwang
AU  - N. Kim
AU  - Y. Choi
PY  - 2020
KW  - image colour analysis
KW  - image recognition
KW  - image retrieval
KW  - image segmentation
KW  - infrared imaging
KW  - spectral analysis
KW  - multispectral place recognition task
KW  - multispectral semantic segmentation
KW  - multispectral domain invariant image
KW  - retrieval-based place recognition
KW  - multispectral recognition
KW  - thermal image
KW  - RGB domain-based tasks
KW  - multispectral domain invariant framework
KW  - unpaired image translation method
KW  - semantic image
KW  - discriminative invariant image
KW  - Image recognition
KW  - Robot sensing systems
KW  - Task analysis
KW  - Semantics
KW  - Thermal sensors
KW  - Feature extraction
KW  - Imaging
DO  - 10.1109/ICRA40945.2020.9197514
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multispectral recognition has attracted increasing attention from the research community due to its potential competence for many applications from day to night. However, due to the domain shift between RGB and thermal image, it has still many challenges to apply and to use RGB domain-based tasks. To reduce the domain gap, we propose multispectral domain invariant framework, which leverages the unpaired image translation method to generate a semantic and strongly discriminative invariant image by enforcing novel constraints in the objective function. We demonstrate the efficacy of the proposed method on mainly multispectral place recognition task and achieve significant improvement compared to previous works. Furthermore, we test on multispectral semantic segmentation and unsupervised domain adaptations to prove the scalability and generality of the proposed method. We will open our source code and dataset.
ER  - 

TY  - CONF
TI  - Probabilistic Effect Prediction through Semantic Augmentation and Physical Simulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9278
EP  - 9284
AU  - A. S. Bauer
AU  - P. Schmaus
AU  - F. Stulp
AU  - D. Leidner
PY  - 2020
KW  - failure analysis
KW  - humanoid robots
KW  - mobile robots
KW  - planning (artificial intelligence)
KW  - probability
KW  - probabilistic effect prediction
KW  - semantic augmentation
KW  - exact outcome
KW  - failure situations
KW  - failure tolerance
KW  - robot actions
KW  - augmenting collected experience
KW  - semantic knowledge
KW  - realistic physics simulations
KW  - outcome probabilities
KW  - unknown tasks
KW  - simulated experience
KW  - action success probabilities
KW  - world experiments
KW  - humanoid robot
KW  - planning trials
KW  - Rollin Justin
KW  - Robots
KW  - Planning
KW  - Probabilistic logic
KW  - Cognition
KW  - Semantics
KW  - Task analysis
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9197477
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Nowadays, robots are mechanically able to perform highly demanding tasks, where AI-based planning methods are used to schedule a sequence of actions that result in the desired effect. However, it is not always possible to know the exact outcome of an action in advance, as failure situations may occur at any time. To enhance failure tolerance, we propose to predict the effects of robot actions by augmenting collected experience with semantic knowledge and leveraging realistic physics simulations. That is, we consider semantic similarity of actions in order to predict outcome probabilities for previously unknown tasks. Furthermore, physical simulation is used to gather simulated experience that makes the approach robust even in extreme cases. We show how this concept is used to predict action success probabilities and how this information can be exploited throughout future planning trials. The concept is evaluated in a series of real world experiments conducted with the humanoid robot Rollin' Justin.
ER  - 

TY  - CONF
TI  - Anytime Integrated Task and Motion Policies for Stochastic Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9285
EP  - 9291
AU  - N. Shah
AU  - D. Kala Vasudevan
AU  - K. Kumar
AU  - P. Kamojjhala
AU  - S. Srivastava
PY  - 2020
KW  - intelligent robots
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - path planning
KW  - planning (artificial intelligence)
KW  - stochastic processes
KW  - multiple execution-time contingencies
KW  - motion policies
KW  - stochastic settings
KW  - stochastic situations
KW  - abstract models
KW  - motion planning
KW  - abstract planning
KW  - intelligent robots
KW  - stochastic environments
KW  - anytime integrated task
KW  - Robots
KW  - Planning
KW  - Task analysis
KW  - Computational modeling
KW  - Collision avoidance
KW  - Stochastic processes
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197574
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be unexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about, and plan for multiple contingencies. We present a new approach for integrated task and motion planning in stochastic settings. In contrast to prior work in this direction, we show that our approach can effectively compute integrated task and motion policies whose branching structures encoding agent behaviors handling multiple execution-time contingencies. We prove that our algorithm is probabilistically complete and can compute feasible solution policies in an anytime fashion so that the probability of encountering an unresolved contingency decreases over time. Empirical results on a set of challenging problems show the utility and scope of our methods.
ER  - 

TY  - CONF
TI  - CCRobot-III: a Split-type Wire-driven Cable Climbing Robot for Cable-stayed Bridge Inspection*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9308
EP  - 9314
AU  - N. Ding
AU  - Z. Zheng
AU  - J. Song
AU  - Z. Sun
AU  - T. L. Lam
AU  - H. Qian
PY  - 2020
KW  - bridges (structures)
KW  - cables (mechanical)
KW  - grippers
KW  - inspection
KW  - mobile robots
KW  - robot dynamics
KW  - wires
KW  - palm-based gripper
KW  - CCRobot-III
KW  - cable climbing robot
KW  - cable-stayed bridge inspection
KW  - mainbody frame
KW  - steel wires
KW  - climbing precursor
KW  - Split-type Wire-driven design
KW  - mass 40.0 kg
KW  - size 90.0 mm to 110.0 mm
KW  - Bridges
KW  - Payloads
KW  - Force
KW  - Wheels
KW  - Winches
KW  - Robots
KW  - Inspection
DO  - 10.1109/ICRA40945.2020.9196772
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel Cable Climbing Robot CCRobot-III, which is the third version designed for bridge cable inspection tasks, aiming at surpassing previous versions in terms of climbing speed and payload capacity. Benefiting from Split-type Wire-driven design, CCRobot-III can climb along a 90-110mm diameter bridge cable in inchworm-like gait at a speed of up to 12m/min, and carrying more than 40kg payload at the same time. CCRobot-III consists of a climbing precursor and a main-body frame. The two parts are connected and driven by steel wires. The climbing precursor, acting as a mobile anchor, moves quickly on a bridge cable. The mainbody frame, acting as a mobile winch, carries payload and pulls itself to a certain position with steel wires. Both parts have one or two pairs of palm-based gripper, which is the key component for providing strong adhesion to support the robot climbing. Experimental results have shown that CCRobotIII possesses outstanding climbing performance, high payload capacity, and good adaptability to complex conditions of cable surface. Moreover, it has potential engineering applications on the cable-stayed bridge for fieldwork.
ER  - 

TY  - CONF
TI  - Omnidirectional Tractable Three Module Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9316
EP  - 9321
AU  - K. Suryavanshi
AU  - R. Vadapalli
AU  - R. Vucha
AU  - A. Sarkar
AU  - K. M. Krishna
PY  - 2020
KW  - control engineering computing
KW  - mobile robots
KW  - motion control
KW  - pipes
KW  - robot kinematics
KW  - omnidirectional tractable three module robot
KW  - omnidirectional modules
KW  - holonomic motion
KW  - motion singularity region
KW  - motion capabilities
KW  - closed-form kinematic model
KW  - MSC ADAMS
KW  - Robots
KW  - Angular velocity
KW  - Turning
KW  - Shafts
KW  - Elbow
KW  - Crawlers
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197210
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces the Omnidirectional Tractable Three Module Robot for traversing inside complex pipe networks. The robot consists of three omnidirectional modules fixed 120¬∞ apart circumferentially which can rotate about their own axis allowing holonomic motion of the robot. The holonomic motion enables the robot to overcome motion singularity when negotiating T-junctions and further allows the robot to arrive in a preferred orientation while taking turns inside a pipe. We have developed a closed-form kinematic model for the robot in the paper and propose the `Motion Singularity Region' that the robot needs to avoid while negotiating T-junction. The design and motion capabilities of the robot are demonstrated both by conducting simulations in MSC ADAMS on a simplified lumped-model of the robot and with experiments on its physical embodiment.
ER  - 

TY  - CONF
TI  - A Practical Climbing Robot for Steel Bridge Inspection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9322
EP  - 9328
AU  - S. T. Nguyen
AU  - A. Q. Pham
AU  - C. Motley
AU  - H. M. La
PY  - 2020
KW  - bridges (structures)
KW  - design engineering
KW  - inspection
KW  - maintenance engineering
KW  - mobile robots
KW  - steel
KW  - structural engineering
KW  - ARA lab robot
KW  - steel structures
KW  - practical climbing robot
KW  - steel bridge inspection
KW  - cutting edge steel inspection robots
KW  - unified design
KW  - advanced robotic and automation lab
KW  - Bridges
KW  - Steel
KW  - Mobile robots
KW  - Inspection
KW  - Adhesives
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196892
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The advanced robotic and automation (ARA) lab has developed and successfully implemented a design inspired by many of the various cutting edge steel inspection robots to date. The combination of these robots concepts into a unified design came with its own set of challenges since the parameters for these features sometimes conflicted. An extensive amount of design and analysis work was performed by the ARA lab in order to find a carefully tuned balance between the implemented features on the ARA robot and general functionality. Having successfully managed to implement this conglomerate of features represents a breakthrough to the industry of steel inspection robots as the ARA lab robot is capable of traversing most complex geometries found on steel structures while still maintaining its ability to efficiently travel along these structures; a feat yet to be done until now.
ER  - 

TY  - CONF
TI  - Development of a Wheeled Wall-Climbing Robot with a Shape-Adaptive Magnetic Adhesion Mechanism
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9329
EP  - 9335
AU  - H. Eto
AU  - H. H. Asada
PY  - 2020
KW  - adhesion
KW  - elasticity
KW  - industrial manipulators
KW  - manipulator kinematics
KW  - mobile robots
KW  - permanent magnets
KW  - robotic welding
KW  - steel
KW  - wheels
KW  - nonelastic suspension mechanism
KW  - arbitrary curved shape
KW  - wheeled wall-climbing robot
KW  - curved ferromagnetic surfaces
KW  - magnetic force direction
KW  - magnetic wheels
KW  - spherical shape
KW  - 2 DOF rotational magnetic adhesion
KW  - shape-adaptive magnetic adhesion
KW  - Wheels
KW  - Mobile robots
KW  - Magnetic levitation
KW  - Magnetic forces
KW  - Welding
KW  - Adhesives
DO  - 10.1109/ICRA40945.2020.9196919
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a wheeled wall-climbing robot with a shape-adaptive magnetic adhesion mechanism for large steel structures. To travel up and down various curved ferromagnetic surfaces, we developed a 2 DOF rotational magnetic adhesion mechanism installed on each wheel that can change the orientation of the magnets to keep the magnetic force direction always normal to the contact surface. These magnetic wheels have a spherical shape and can move relative to the main body by a non-elastic suspension mechanism so that the robot can climb up small obstacles on the ground and find contact points for each wheel on a wall with an arbitrary curved shape. Being geometrically stable is important for the robot because this robot is intended to be a mobile base for a welding manipulator. The detailed design of the mechanism and the results of climbing tests are presented.
ER  - 

TY  - CONF
TI  - Algebraic Fault Detection and Identification for Rigid Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9352
EP  - 9358
AU  - A. Lomakin
AU  - J. Deutscher
PY  - 2020
KW  - fault diagnosis
KW  - manipulators
KW  - nonlinear control systems
KW  - polynomial approximation
KW  - SCARA
KW  - polynomial approximation
KW  - orthonormal Jacobi polynomials
KW  - nonlinear mechanical systems
KW  - rigid robots
KW  - algebraic fault detection
KW  - Fault detection
KW  - Jacobian matrices
KW  - Mathematical model
KW  - Kernel
KW  - Time measurement
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197561
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method for algebraic fault detection and identification of nonlinear mechanical systems, describing rigid robots, by using an approximation with orthonormal Jacobi polynomials. An explicit expression is derived for the fault from the equation of motion, which is decoupled from disturbances and only depends on measurable signals and their time derivatives. Fault detection and identification is then achieved by polynomial approximation of the determined fault term. The results are illustrated by a simulation for a faulty SCARA.
ER  - 

TY  - CONF
TI  - Fault tolerance analysis of a hexarotor with reconfigurable tilted rotors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9359
EP  - 9365
AU  - C. Pose
AU  - J. Giribet
AU  - I. Mas
PY  - 2020
KW  - aircraft control
KW  - attitude control
KW  - fault tolerance
KW  - helicopters
KW  - fault tolerance analysis
KW  - reconfigurable tilted rotor
KW  - multirotor vehicles
KW  - yaw maneuverability
KW  - attitude control
KW  - hexarotor vehicle
KW  - hexagon-shaped multirotor
KW  - altitude control
KW  - tilt angle
KW  - rotor reconfiguration
KW  - Rotors
KW  - Torque
KW  - Force
KW  - Servomotors
KW  - Fault tolerance
KW  - Fault tolerant systems
KW  - Attitude control
DO  - 10.1109/ICRA40945.2020.9196552
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tilted rotors in multirotor vehicles have shown to be useful for different practical reasons. For instance, increasing yaw maneuverability or enabling full position and attitude control of hexarotor vehicles. It has also been proven that a hexagon-shaped multirotor is capable of complete attitude and altitude control under failures of one of its rotors. However, when a rotor fails, the torque that can be reached in the worst- case direction decreases considerably.This work proposes to actively change the tilt angle of the rotors when a failure occurs. This rotor reconfiguration increases the maximum torque that can be achieved in the most stressful direction, reducing maneuverability limitations. Experimental validations are shown, where the proposed reconfigurable tilted rotor is used in order to control a hexarotor vehicle when a failure appears mid-flight. The impact of the delay in the reconfiguration when a failure occurs is also addressed.
ER  - 

TY  - CONF
TI  - Detecting Execution Anomalies As an Oracle for Autonomy Software Robustness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9366
EP  - 9373
AU  - D. S. Katz
AU  - C. Hutchison
AU  - M. Zizyte
AU  - C. L. Goues
PY  - 2020
KW  - fault diagnosis
KW  - pattern clustering
KW  - robot programming
KW  - system monitoring
KW  - execution anomaly detection
KW  - autonomy software robustness
KW  - system monitoring
KW  - clustering algorithm
KW  - autonomy systems
KW  - robotics systems
KW  - real-world industrial system
KW  - autonomous systems
KW  - fault identification
KW  - Testing
KW  - Tools
KW  - Instruments
KW  - Clustering algorithms
KW  - Service robots
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197060
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a method for detecting execution anomalies in robotics and autonomy software. The algorithm uses system monitoring techniques to obtain profiles of executions. It uses a clustering algorithm to create clusters of those executions, representing nominal execution. A distance metric determines whether additional execution profiles belong to the existing clusters or should be considered anomalies. The method is suitable for identifying faults in robotics and autonomy systems. We evaluate the technique in simulation on two robotics systems, one of which is a real-world industrial system. We find that our technique works well to detect possibly unsafe behavior in autonomous systems.
ER  - 

TY  - CONF
TI  - Reliability Validation of Learning Enabled Vehicle Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9390
EP  - 9396
AU  - Y. Sun
AU  - Y. Zhou
AU  - S. Maskell
AU  - J. Sharp
AU  - X. Huang
PY  - 2020
KW  - image filtering
KW  - image motion analysis
KW  - Kalman filters
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object tracking
KW  - real-world learning-enabled system
KW  - dynamic vehicle tracking
KW  - high-resolution wide-area motion imagery input
KW  - symbolic components
KW  - Kalman filter
KW  - neural networks
KW  - system-level reliability
KW  - coverage-guided neural network testing tool
KW  - vehicle tracking system
KW  - adversarial examples
KW  - deep learning components
KW  - validation methods
KW  - learning-enabled systems
KW  - neural network components
KW  - DeepConcolic tool
KW  - Testing
KW  - Tools
KW  - Tracking
KW  - Neurons
KW  - Cameras
KW  - Feature extraction
KW  - Reliability
DO  - 10.1109/ICRA40945.2020.9196932
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components - to process the imagery inputs - and multiple symbolic (Kalman filter) components - to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which cannot be determined by analysing the deep learning components only. This research suggests the need for novel verification and validation methods for learning-enabled systems.
ER  - 

TY  - CONF
TI  - Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Network with Rotation Ensemble Module
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9397
EP  - 9403
AU  - D. Park
AU  - Y. Seo
AU  - S. Y. Chun
PY  - 2020
KW  - convolutional neural nets
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object detection
KW  - robot vision
KW  - highly accurate robotic grasp detection
KW  - fully convolutional neural network
KW  - rotation ensemble module
KW  - rotation invariance
KW  - computer vision tasks
KW  - rotation anchor box
KW  - multiple objects
KW  - 4-axis robot arm
KW  - Cornell dataset
KW  - REM
KW  - Proposals
KW  - Grasping
KW  - Task analysis
KW  - Robot sensing systems
KW  - Feature extraction
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9197002
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Rotation invariance has been an important topic in computer vision tasks. Ideally, robot grasp detection should be rotation-invariant. However, rotation-invariance in robotic grasp detection has been only recently studied by using rotation anchor box that are often time-consuming and unreliable for multiple objects. In this paper, we propose a rotation ensemble module (REM) for robotic grasp detection using convolutions that rotates network weights. Our proposed REM was able to outperform current state-of-the-art methods by achieving up to 99.2% (image-wise), 98.6% (object-wise) accuracies on the Cornell dataset with real-time computation (50 frames per second). Our proposed method was also able to yield reliable grasps for multiple objects and up to 93.8% success rate for the real-time robotic grasping task with a 4-axis robot arm for small novel objects that was significantly higher than the baseline methods by 11-56%.
ER  - 

TY  - CONF
TI  - Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9404
EP  - 9410
AU  - K. Zakka
AU  - A. Zeng
AU  - J. Lee
AU  - S. Song
PY  - 2020
KW  - CAD
KW  - learning (artificial intelligence)
KW  - object recognition
KW  - pose estimation
KW  - robotic assembly
KW  - kit assembly task
KW  - shape matching problem
KW  - shape descriptor
KW  - object surfaces
KW  - self-supervised data-collection pipeline
KW  - robotic assembly
KW  - 3D CAD models
KW  - task-specific training data
KW  - Task analysis
KW  - Shape
KW  - Visualization
KW  - Training data
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196733
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Is it possible to learn policies for robotic assembly that can generalize to new objects? We explore this idea in the context of the kit assembly task. Since classic methods rely heavily on object pose estimation, they often struggle to generalize to new objects without 3D CAD models or task-specific training data. In this work, we propose to formulate the kit assembly task as a shape matching problem, where the goal is to learn a shape descriptor that establishes geometric correspondences between object surfaces and their target placement locations from visual input. This formulation enables the model to acquire a broader understanding of how shapes and surfaces fit together for assembly - allowing it to generalize to new objects and kits. To obtain training data for our model, we present a self-supervised data-collection pipeline that obtains ground truth object-to-placement correspondences by disassembling complete kits. Our resulting real-world system, Form2Fit, learns effective pick and place strategies for assembling objects into a variety of kits - achieving 90% average success rates under different initial conditions (e.g. varying object and kit poses), 94% success under new configurations of multiple kits, and over 86% success with completely new objects and kits. Code, videos, and supplemental material are available at https://form2fit.github.io.
ER  - 

TY  - CONF
TI  - Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9411
EP  - 9418
AU  - P. Sundaresan
AU  - J. Grannen
AU  - B. Thananjeyan
AU  - A. Balakrishna
AU  - M. Laskey
AU  - K. Stone
AU  - J. E. Gonzalez
AU  - K. Goldberg
PY  - 2020
KW  - intelligent robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - robot vision
KW  - ropes
KW  - rope manipulation policies
KW  - dense object descriptors
KW  - synthetic depth data
KW  - robotic manipulation
KW  - deformable 1D objects
KW  - high-fidelity analytic models
KW  - configuration spaces
KW  - end-to-end manipulation policies
KW  - physical interaction
KW  - interpretable deep visual representations
KW  - robot manipulation
KW  - interpretable policies
KW  - transferable geometric policies
KW  - visual reasoning
KW  - point-pair correspondences
KW  - initial goal rope configurations
KW  - geometric structure
KW  - synthetic depth images
KW  - dense depth object descriptors
KW  - ABB YuMi Robot
KW  - interpretable geometric policies
KW  - Task analysis
KW  - Visualization
KW  - Robots
KW  - Strain
KW  - Training
KW  - Videos
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197121
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation - dense depth object descriptors (DDODs) - can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See https://tinyurl.com/rope-learning for supplementary material and videos.
ER  - 

TY  - CONF
TI  - Efficient two step optimization for large embedded deformation graph based SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9419
EP  - 9425
AU  - J. Song
AU  - F. Bai
AU  - L. Zhao
AU  - S. Huang
AU  - R. Xiong
PY  - 2020
KW  - computational complexity
KW  - embedded systems
KW  - graph theory
KW  - Hessian matrices
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - parameter estimation
KW  - computation complexity
KW  - two step optimization
KW  - deformable geometry
KW  - stereo camera
KW  - SLAM applications
KW  - large scale embedded deformation graph
KW  - Hessian matrix
KW  - Simultaneous localization and mapping
KW  - Strain
KW  - Jacobian matrices
KW  - Optimization
KW  - Cameras
KW  - Deformable models
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196930
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Embedded deformation graph is a widely used technique in deformable geometry and graphical problems. Although the technique has been transmitted to stereo (or RGB-D) camera based SLAM applications, it remains challenging to compromise the computational cost as the model grows. In practice, the processing time grows rapidly in accordance with the expansion of maps. In this paper, we propose an approach to decouple the nodes of deformation graph in large scale dense deformable SLAM and keep the estimation time to be constant. We observe that only partial deformable nodes in the graph are connected to visible points. Based on this fact, the sparsity of the original Hessian matrix is utilized to split the parameter estimation into two independent steps. With this new technique, we achieve faster parameter estimation with amortized computation complexity reduced from O(n2) to almost O(1). As a result, the computational cost barely increases as the map keeps growing. Based on our strategy, the computational bottleneck in large scale embedded deformation graph based applications will be greatly mitigated. The effectiveness is validated by experiments, featuring large scale deformation scenarios.
ER  - 

TY  - CONF
TI  - Camera-to-Robot Pose Estimation from a Single Image
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9426
EP  - 9432
AU  - T. E. Lee
AU  - J. Tremblay
AU  - T. To
AU  - J. Cheng
AU  - T. Mosier
AU  - O. Kroemer
AU  - D. Fox
AU  - S. Birchfield
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - image sensors
KW  - manipulators
KW  - neural nets
KW  - pose estimation
KW  - robot vision
KW  - camera-to-robot pose estimation
KW  - single RGB image
KW  - deep neural network
KW  - perspective-n-point
KW  - robot manipulator
KW  - classic hand-eye calibration systems
KW  - camera sensors
KW  - classic off-line hand-eye calibration
KW  - robot sensors
KW  - Cameras
KW  - Robot vision systems
KW  - Robot kinematics
KW  - Calibration
KW  - Two dimensional displays
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196596
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present an approach for estimating the pose of an external camera with respect to a robot using a single RGB image of the robot. The image is processed by a deep neural network to detect 2D projections of keypoints (such as joints) associated with the robot. The network is trained entirely on simulated data using domain randomization to bridge the reality gap. Perspective-n-point (PnP) is then used to recover the camera extrinsics, assuming that the camera intrinsics and joint configuration of the robot manipulator are known. Unlike classic hand-eye calibration systems, our method does not require an off-line calibration step. Rather, it is capable of computing the camera extrinsics from a single frame, thus opening the possibility of on-line calibration. We show experimental results for three different robots and camera sensors, demonstrating that our approach is able to achieve accuracy with a single frame that is comparable to that of classic off-line hand-eye calibration using multiple frames. With additional frames from a static pose, accuracy improves even further. Code, datasets, and pretrained models for three widely-used robot manipulators are made available.
ER  - 

TY  - CONF
TI  - PST900: RGB-Thermal Calibration, Dataset and Segmentation Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9441
EP  - 9447
AU  - S. S. Shivakumar
AU  - N. Rodrigues
AU  - A. Zhou
AU  - I. D. Miller
AU  - V. Kumar
AU  - C. J. Taylor
PY  - 2020
KW  - calibration
KW  - control engineering computing
KW  - convolutional neural nets
KW  - image colour analysis
KW  - image segmentation
KW  - infrared imaging
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - PST900
KW  - segmentation network
KW  - long wave infrared imagery
KW  - RGB-thermal camera calibration
KW  - thermal image pairs
KW  - DARPA Subterranean Challenge
KW  - RGB imagery
KW  - semantic segmentation
KW  - CNN architecture
KW  - Cameras
KW  - Calibration
KW  - Image segmentation
KW  - Semantics
KW  - Heating systems
KW  - Aluminum
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196831
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we propose long wave infrared (LWIR) imagery as a viable supporting modality for semantic segmentation using learning-based techniques. We first address the problem of RGB-thermal camera calibration by proposing a passive calibration target and procedure that is both portable and easy to use. Second, we present PST900, a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge. Lastly, we propose a CNN architecture for fast semantic segmentation that combines both RGB and Thermal imagery in a way that leverages RGB imagery independently. We compare our method against the state-of-the-art and show that our method outperforms them in our dataset.
ER  - 

TY  - CONF
TI  - Instance Segmentation of LiDAR Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9448
EP  - 9455
AU  - F. Zhang
AU  - C. Guan
AU  - J. Fang
AU  - S. Bai
AU  - R. Yang
AU  - P. H. S. Torr
AU  - V. Prisacariu
PY  - 2020
KW  - feature extraction
KW  - image segmentation
KW  - optical radar
KW  - radar imaging
KW  - stereo image processing
KW  - single-shot instance prediction
KW  - LiDAR instance segmentation
KW  - LiDAR point cloud dataset
KW  - point-wise labels
KW  - robust baseline method
KW  - large-scale outdoor LiDAR point clouds
KW  - dense feature encoding technique
KW  - precise 3D bounding box
KW  - Three-dimensional displays
KW  - Laser radar
KW  - Image segmentation
KW  - Two dimensional displays
KW  - Encoding
KW  - Semantics
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196622
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a robust baseline method for instance segmentation which are specially designed for large-scale outdoor LiDAR point clouds. Our method includes a novel dense feature encoding technique, allowing the localization and segmentation of small, far-away objects, a simple but effective solution for single-shot instance prediction and effective strategies for handling severe class imbalances. Since there is no public dataset for the study of LiDAR instance segmentation, we also build a new publicly available LiDAR point cloud dataset to include both precise 3D bounding box and point-wise labels for instance segmentation, while still being about 3~20 times as large as other existing LiDAR datasets. The dataset will be published at https://github.com/feihuzhang/LiDARSeg.
ER  - 

TY  - CONF
TI  - Generation of Object Candidates Through Simply Looking Around
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9456
EP  - 9462
AU  - D. Patar
AU  - H. I. Bozma
PY  - 2020
KW  - cameras
KW  - image segmentation
KW  - image sequences
KW  - mobile robots
KW  - object detection
KW  - object recognition
KW  - robot vision
KW  - video signal processing
KW  - mobile robot
KW  - pan-tilt monocular camera
KW  - camera movements
KW  - robot operating indoors
KW  - object candidates
KW  - Cameras
KW  - Robot vision systems
KW  - Image segmentation
KW  - Visualization
KW  - Coherence
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9197482
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider the generation of generic object candidates by a mobile robot that is endowed with a pan-tilt monocular camera. This is an important problem because these candidates serve as basis for the robot to categorize and/or recognize the objects in its surroundings. The previously proposed methods either do not have a means of enabling the robot to look around through moving its camera or do not take advantage of the temporal coherence of the video data. We present a novel approach that enables the robot to achieve both of these capabilities simultaneously. In this approach, the robot's camera movements are governed by a family of controllers whose constructions depend on the set of object candidates that have been hitherto generated, but not directly looked at. In parallel, the robot discovers the object candidates through tracking segments and determining spatio-temporally coherent ones. The advantage of the proposed approach is that while the robot can explore its surroundings by simply looking around prior to more sophisticated exploration behavior involving possibly bodily locomotion the generated object candidates turn out to be consolidated across the visual stream in comparison to single-shot methods. This is demonstrated in extensive experimental results with a robot operating indoors varying in clutter as well as outdoors.
ER  - 

TY  - CONF
TI  - Dilated Point Convolutions: On the Receptive Field Size of Point Convolutions on 3D Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9463
EP  - 9469
AU  - F. Engelmann
AU  - T. Kontogianni
AU  - B. Leibe
PY  - 2020
KW  - convolutional neural nets
KW  - image classification
KW  - image representation
KW  - image segmentation
KW  - neural net architecture
KW  - solid modelling
KW  - dilated point convolutions
KW  - receptive field size
KW  - 3D point cloud
KW  - point convolutional networks
KW  - semantic segmentation
KW  - object classification
KW  - 3D data representations
KW  - network architectures
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Semantics
KW  - Network architecture
KW  - Image segmentation
KW  - Two dimensional displays
KW  - Conferences
DO  - 10.1109/ICRA40945.2020.9197503
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work, we propose Dilated Point Convolutions (DPC). In a thorough ablation study, we show that the receptive field size is directly related to the performance of 3D point cloud processing tasks, including semantic segmentation and object classification. Point convolutions are widely used to efficiently process 3D data representations such as point clouds or graphs. However, we observe that the receptive field size of recent point convolutional networks is inherently limited. Our dilated point convolutions alleviate this issue, they significantly increase the receptive field size of point convolutions. Importantly, our dilation mechanism can easily be integrated into most existing point convolutional networks. To evaluate the resulting network architectures, we visualize the receptive field and report competitive scores on popular point cloud benchmarks.
ER  - 

TY  - CONF
TI  - A water-obstacle separation and refinement network for unmanned surface vehicles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9470
EP  - 9476
AU  - B. Bovcon
AU  - M. Kristan
PY  - 2020
KW  - collision avoidance
KW  - edge detection
KW  - feature extraction
KW  - image fusion
KW  - image segmentation
KW  - inertial navigation
KW  - marine vehicles
KW  - mobile robots
KW  - neural net architecture
KW  - remotely operated vehicles
KW  - robot vision
KW  - refinement network
KW  - unmanned surface vehicles
KW  - obstacle detection
KW  - water reflections
KW  - wakes
KW  - inertial information fusion
KW  - visual features
KW  - deep encoder decoder architecture
KW  - water obstacle separation
KW  - semantic segmentation
KW  - autonomous navigation
KW  - water edge estimation
KW  - inertial measurement unit
KW  - loss function
KW  - water features
KW  - Decoding
KW  - Visualization
KW  - Feature extraction
KW  - Image segmentation
KW  - Semantics
KW  - Image edge detection
KW  - Task analysis
KW  - obstacle detection
KW  - semantic segmentation
KW  - sensor fusion
KW  - unmanned surface vehicles
KW  - separation function
DO  - 10.1109/ICRA40945.2020.9197194
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Obstacle detection by semantic segmentation shows a great promise for autonomous navigation in unmanned surface vehicles (USV). However, existing methods suffer from poor estimation of the water edge in presence of visual ambiguities, poor detection of small obstacles and high false-positive rate on water reflections and wakes. We propose a new deep encoder-decoder architecture, a water-obstacle separation and refinement network (WaSR), to address these issues. Detection and water edge accuracy are improved by a novel decoder that gradually fuses inertial information from inertial measurement unit (IMU) with the visual features from the encoder. In addition, a novel loss function is designed to increase the separation between water and obstacle features early on in the network. Subsequently, the capacity of the remaining layers in the decoder is better utilised, leading to a significant reduction in false positives and increased true positives. Experimental results show that WaSR outperforms the current state-of-the-art by a large margin, yielding a 14% increase in F-measure over the second-best method.
ER  - 


TY  - CONF
TI  - Dynamic Anchor Selection for Improving Object Localization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9477
EP  - 9483
AU  - P. Shyam
AU  - K. -J. Yoon
AU  - K. -S. Kim
PY  - 2020
KW  - computer vision
KW  - neural nets
KW  - object detection
KW  - DANet
KW  - single-stage object detectors
KW  - dynamic anchor selection
KW  - anchor boxes
KW  - object localization candidates
KW  - MS COCO dataset
KW  - Feature extraction
KW  - Detectors
KW  - Computer architecture
KW  - Task analysis
KW  - Spatial resolution
KW  - Head
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9197076
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101. The code and models will be available at https://github.com/PS06/AnchorNet.
ER  - 

TY  - CONF
TI  - Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9484
EP  - 9490
AU  - D. Barnes
AU  - I. Posner
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - image colour analysis
KW  - image sensors
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - object recognition
KW  - object tracking
KW  - radar computing
KW  - robot vision
KW  - SLAM (robots)
KW  - supervised learning
KW  - predict robust keypoints
KW  - odometry estimation
KW  - metric localisation
KW  - self-supervised framework
KW  - differentiable point-based motion estimator
KW  - localisation error
KW  - Oxford Radar RobotCar Dataset
KW  - point-based radar odometry
KW  - Radar
KW  - Measurement
KW  - Task analysis
KW  - Estimation
KW  - Robot sensing systems
KW  - Computer architecture
DO  - 10.1109/ICRA40945.2020.9196835
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.
ER  - 

TY  - CONF
TI  - SpAGNN: Spatially-Aware Graph Neural Networks for Relational Behavior Forecasting from Sensor Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9491
EP  - 9497
AU  - S. Casas
AU  - C. Gulino
AU  - R. Liao
AU  - R. Urtasun
PY  - 2020
KW  - belief propagation
KW  - convolutional neural nets
KW  - Gaussian processes
KW  - graph theory
KW  - intelligent transportation systems
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - message passing
KW  - object detection
KW  - probability
KW  - self-driving
KW  - probabilistic predictions
KW  - iterative actor state update
KW  - spatially-aware graph neural network
KW  - convolutional neural network
KW  - sensor data
KW  - relational behavior forecasting
KW  - SpAGNN
KW  - model uncertainty
KW  - Gaussian belief propagation
KW  - message passing
KW  - Forecasting
KW  - Laser radar
KW  - Three-dimensional displays
KW  - Neural networks
KW  - Object detection
KW  - Predictive models
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196697
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we tackle the problem of relational behavior forecasting from sensor data. Towards this goal, we propose a novel spatially-aware graph neural network (SpAGNN) that models the interactions between agents in the scene. Specifically, we exploit a convolutional neural network to detect the actors and compute their initial states. A graph neural network then iteratively updates the actor states via a message passing process. Inspired by Gaussian belief propagation, we design the messages to be spatially-transformed parameters of the output distributions from neighboring agents. Our model is fully differentiable, thus enabling end-to-end training. Importantly, our probabilistic predictions can model uncertainty at the trajectory level. We demonstrate the effectiveness of our approach by achieving significant improvements over the state-of-the-art on two real-world self-driving datasets: ATG4D and nuScenes.
ER  - 

TY  - CONF
TI  - Any Motion Detector: Learning Class-agnostic Scene Dynamics from a Sequence of LiDAR Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9498
EP  - 9504
AU  - A. Filatov
AU  - A. Rykov
AU  - V. Murashkin
PY  - 2020
KW  - computational geometry
KW  - computer vision
KW  - feature extraction
KW  - image sequences
KW  - intelligent transportation systems
KW  - learning (artificial intelligence)
KW  - motion compensation
KW  - motion estimation
KW  - object detection
KW  - optical radar
KW  - parameter estimation
KW  - real-time systems
KW  - road safety
KW  - traffic engineering computing
KW  - LiDAR point clouds
KW  - object detection
KW  - urban environment
KW  - motion detection
KW  - 3D point cloud sequence
KW  - object categories
KW  - temporal context aggregation
KW  - class-agnostic scene dynamics
KW  - motion parameters estimation
KW  - self driving vehicle navigation safety
KW  - motion parameter estimation
KW  - real time inference
KW  - road participant motion
KW  - neural network
KW  - camera images
KW  - Three-dimensional displays
KW  - Tensile stress
KW  - Vehicle dynamics
KW  - Transforms
KW  - Laser radar
KW  - Estimation
KW  - Object detection
DO  - 10.1109/ICRA40945.2020.9196716
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Object detection and motion parameters estimation are crucial tasks for self-driving vehicle safe navigation in a complex urban environment. In this work we propose a novel real-time approach of temporal context aggregation for motion detection and motion parameters estimation based on 3D point cloud sequence. We introduce an ego-motion compensation layer to achieve real-time inference with performance comparable to a naive odometric transform of the original point cloud sequence. Not only is the proposed architecture capable of estimating the motion of common road participants like vehicles or pedestrians but also generalizes to other object categories which are not present in training data. We also conduct an in-deep analysis of different temporal context aggregation strategies such as recurrent cells and 3D convolutions. Finally, we provide comparison results of our state-of-the-art model with existing solutions on KITTI Scene Flow dataset.
ER  - 

TY  - CONF
TI  - Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub‚Äôs Point-Of-View
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9521
EP  - 9527
AU  - M. Monforte
AU  - A. Arriandiaga
AU  - A. Glover
AU  - C. Bartolozzi
PY  - 2020
KW  - cameras
KW  - control engineering computing
KW  - human-robot interaction
KW  - image resolution
KW  - recurrent neural nets
KW  - robot vision
KW  - spatiotemporal phenomena
KW  - iCub robot
KW  - event-based spatiotemporal trajectory prediction
KW  - nonlinear trajectories
KW  - frame-based cameras
KW  - asynchronous information stream
KW  - low latency information stream
KW  - high temporal resolution
KW  - long short-term memory networks
KW  - event-cameras spatial sampling
KW  - encoder-decoder architecture
KW  - spatial trajectory points
KW  - human-to-robot handover trajectories
KW  - Trajectory
KW  - Robots
KW  - Predictive models
KW  - Cameras
KW  - Target tracking
KW  - Data models
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197373
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fast, non-linear trajectories have been shown to be more accurately visually measured, and hence predicted, when sampled spatially (that is when the target position changes) rather than temporally, i.e. at a fixed-rate as in traditional frame-based cameras. Event-cameras, with their asynchronous, low latency information stream, allow for spatial sampling with very high temporal resolution, improving the quality of the data and the accuracy of post-processing operations. This paper investigates the use of Long Short-Term Memory (LSTM) networks with event-cameras spatial sampling for trajectory prediction. We show the benefit of using an Encoder-Decoder architecture over parameterised models for regression on event-based human-to-robot handover trajectories. In particular, we exploit the temporal information associated to the events stream to predict not only the incoming spatial trajectory points, but also when these will occur in time. After having studied the proper LSTM input/output sequence length, the network performance are compared to other regression models. Then, prediction behavior and computational time are analysed for the proposed method. We carry out the experiment using an iCub robot equipped with event-cameras, addressing the problem from the robot perspective.
ER  - 

TY  - CONF
TI  - A Data-driven Planning Framework for Robotic Texture Painting on 3D Surfaces
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9528
EP  - 9534
AU  - A. S. Vempati
AU  - R. Siegwart
AU  - J. Nieto
PY  - 2020
KW  - geometry
KW  - image texture
KW  - industrial robots
KW  - mixing
KW  - painting
KW  - path planning
KW  - recurrent neural nets
KW  - robot vision
KW  - supervised learning
KW  - robotic texture painting
KW  - painting textures
KW  - surface geometry
KW  - paint mixing
KW  - self-supervised learning framework
KW  - painting process
KW  - paint simulation environment
KW  - robot executes
KW  - data-driven planning framework
KW  - paint delivery tool flow rate
KW  - 3D surfaces
KW  - recurrent neural network
KW  - RNN
KW  - Painting
KW  - Paints
KW  - Robots
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Two dimensional displays
DO  - 10.1109/ICRA40945.2020.9196693
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Painting textures on 3D surfaces requires an understanding of the surface geometry, paint flow and paint mixing. This work formulates automated painting as a planning problem and proposes a solution based on a self-supervised learning framework that enables a robot to paint monochromatic non-uniform textures on 3D surfaces. We developed a method that iteratively decides the actions to take based on constant feedback of the painting process. Inspired by recent results, we formulate our solution using a recurrent neural network (RNN) to decide where and what to paint on the surface at each time instant. Specifically, the paint delivery tool's flow rate, orientation and position relative to the surface at each time instant are evaluated. This data can then be processed by a robot's planner of choice for generating a painting mission that can achieve the desired end result. We evaluate the proposed approach by providing qualitative and quantitative results of the different components. Furthermore, we validate the effectiveness of the approach for the application by providing renderings from a paint simulation environment and show how a robot executes the planned painting mission on a generic 3D surface.
ER  - 

TY  - CONF
TI  - Learned Critical Probabilistic Roadmaps for Robotic Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9535
EP  - 9541
AU  - B. Ichter
AU  - E. Schmerling
AU  - T. -W. E. Lee
AU  - A. Faust
PY  - 2020
KW  - graph theory
KW  - mobile robots
KW  - motion control
KW  - path planning
KW  - sampling methods
KW  - implicit graph representation
KW  - state space
KW  - solution trajectories
KW  - graph-theoretic techniques
KW  - hierarchical graph
KW  - uniform sampling
KW  - sampling-based motion planning
KW  - robotic motion planning
KW  - motion planning techniques
KW  - learned critical probabilistic roadmaps
KW  - critical PRM
KW  - critical probabilistic roadmaps
KW  - Planning
KW  - Robots
KW  - Trajectory
KW  - Probabilistic logic
KW  - Training
KW  - Complexity theory
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197106
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Sampling-based motion planning techniques have emerged as an efficient algorithmic paradigm for solving complex motion planning problems. These approaches use a set of probing samples to construct an implicit graph representation of the robot's state space, allowing arbitrarily accurate representations as the number of samples increases to infinity. In practice, however, solution trajectories only rely on a few critical states, often defined by structure in the state space (e.g., doorways). In this work we propose a general method to identify these critical states via graph-theoretic techniques (betweenness centrality) and learn to predict criticality from only local environment features. These states are then leveraged more heavily via global connections within a hierarchical graph, termed Critical Probabilistic Roadmaps. Critical PRMs are demonstrated to achieve up to three orders of magnitude improvement over uniform sampling, while preserving the guarantees and complexity of sampling-based motion planning. A video is available at https://youtu.be/AYoD-pGd9ms.
ER  - 

TY  - CONF
TI  - Learning Heuristic A*: Efficient Graph Search using Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9542
EP  - 9547
AU  - S. Kim
AU  - B. An
PY  - 2020
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optimisation
KW  - path planning
KW  - search problems
KW  - path planning problem
KW  - computation load
KW  - neural network
KW  - optimal paths
KW  - optimal cost
KW  - global optimality
KW  - admissible heuristic function
KW  - efficient graph search
KW  - learning heuristic A*
KW  - LHA*
KW  - suboptimality bound
KW  - maze-like map
KW  - Heuristic algorithms
KW  - Training
KW  - Path planning
KW  - Biological neural networks
KW  - Robots
KW  - Navigation
DO  - 10.1109/ICRA40945.2020.9197015
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we consider the path planning problem on a graph. To reduce computation load by efficiently exploring the graph, we model the heuristic function as a neural network, which is trained by a training set derived from optimal paths to estimate the optimal cost between a pair of vertices on the graph. As such heuristic function cannot be proved to be an admissible heuristic to guarantee the global optimality of the path, we adapt an admissible heuristic function for the terminating criteria. Thus, proposed Learning Heuristic A* (LHA*) guarantees the bounded suboptimality of the path. The performance of LHA* was demonstrated by simulations in a maze-like map and compared with the performance of weighted A* with the same suboptimality bound.
ER  - 

TY  - CONF
TI  - 3D-CNN Based Heuristic Guided Task-Space Planner for Faster Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9548
EP  - 9554
AU  - R. Terasawa
AU  - Y. Ariki
AU  - T. Narihira
AU  - T. Tsuboi
AU  - K. Nagasaka
PY  - 2020
KW  - collision avoidance
KW  - convolutional neural nets
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - sampling methods
KW  - stereo image processing
KW  - trees (mathematics)
KW  - TS-RRT
KW  - task-space rapidly-exploring random trees
KW  - 3D-CNN
KW  - heuristic guided task-space planner
KW  - fully convolutional neural networks
KW  - heuristic map
KW  - Random Trees
KW  - sampling-based planner
KW  - collision-free path
KW  - robotic manipulation
KW  - motion planning
KW  - Planning
KW  - Task analysis
KW  - Robots
KW  - Collision avoidance
KW  - Heuristic algorithms
KW  - Feature extraction
KW  - Convolution
DO  - 10.1109/ICRA40945.2020.9196883
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motion planning is important in a wide variety of applications such as robotic manipulation. However, it is still challenging to reliably find a collision-free path within a reasonable time. To address the issue, this paper proposes a novel framework which combines a sampling-based planner and deep learning for faster motion planning, focusing on heuristics. The proposed method extends Task-Space Rapidly-exploring Random Trees (TS-RRT) to guide the trees with a "heuristic map" where every voxel has a cost-to-go value toward the goal. It also utilizes fully convolutional neural networks (CNNs) for producing more appropriate heuristic maps, rather than manually-designed heuristics. To verify the effectiveness of the proposed method, experiments for motion planning using a real environment and mobile manipulator are carried out. The results indicate that it outperforms the existing planners, especially in terms of the average planning time with smaller variance.
ER  - 

TY  - CONF
TI  - Learned Sampling Distributions for Efficient Planning in Hybrid Geometric and Object-Level Representations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9555
EP  - 9562
AU  - K. Liu
AU  - M. Stadler
AU  - N. Roy
PY  - 2020
KW  - geometry
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - path planning
KW  - robot vision
KW  - linear regression
KW  - efficiency planning
KW  - sampling distribution learning
KW  - sampling-based planners
KW  - object-level semantics
KW  - myopic behavior
KW  - geometric information
KW  - navigation
KW  - robotic agent
KW  - object-level representations
KW  - hybrid geometric
KW  - Navigation
KW  - Planning
KW  - Semantics
KW  - Trajectory
KW  - Mathematical model
KW  - Optimization
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196771
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We would like to enable a robotic agent to quickly and intelligently find promising trajectories through structured, unknown environments. Many approaches to navigation in unknown environments are limited to considering geometric information only, which leads to myopic behavior. In this work, we show that learning a sampling distribution that incorporates both geometric information and explicit, object-level semantics for sampling-based planners enables efficient planning at longer horizons in partially-known environments. We demonstrate that our learned planner is up to 2.7 times more likely to find a plan than the baseline, and can result in up to a 16% reduction in traversal costs as calculated by linear regression. We also show promising qualitative results on real-world data.
ER  - 

TY  - CONF
TI  - Deep Visual Heuristics: Learning Feasibility of Mixed-Integer Programs for Manipulation Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9563
EP  - 9569
AU  - D. Driess
AU  - O. Oguz
AU  - J. -S. Ha
AU  - M. Toussaint
PY  - 2020
KW  - integer programming
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - neurocontrollers
KW  - path planning
KW  - robot vision
KW  - deep visual heuristics
KW  - mixed-integer program
KW  - deep neural network
KW  - visual input
KW  - robot manipulation planning
KW  - motion planning
KW  - learning algorithm
KW  - goal encoding
KW  - optimization problems
KW  - Planning
KW  - Task analysis
KW  - Robot sensing systems
KW  - Neural networks
KW  - Grasping
KW  - Search problems
DO  - 10.1109/ICRA40945.2020.9197291
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.Our experiments show that our proposed network generalizes to scenes with multiple objects, although during training only two objects are present at the same time. By using the learned network as a heuristic to guide the search over the discrete variables of the mixed-integer program, the number of optimization problems that have to be solved to find a feasible solution or to detect infeasibility can greatly be reduced.
ER  - 

TY  - CONF
TI  - Fast Frontier-based Information-driven Autonomous Exploration with an MAV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9570
EP  - 9576
AU  - A. Dai
AU  - S. Papatheodorou
AU  - N. Funk
AU  - D. Tzoumanikas
AU  - S. Leutenegger
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - entropy
KW  - microrobots
KW  - mobile robots
KW  - navigation
KW  - octrees
KW  - probability
KW  - robot vision
KW  - MAV
KW  - collision-free navigation
KW  - autonomous robots
KW  - microaerial vehicles
KW  - map entropy
KW  - occupancy probabilities
KW  - utility function
KW  - frontier extraction
KW  - frontier-based exploration
KW  - frontier voxels
KW  - map frontiers
KW  - frontier-based information-driven autonomous exploration
KW  - exploration planner
KW  - octree map representation
KW  - visual-based navigation
KW  - motion planning
KW  - octree-based occupancy mapping
KW  - sampling-based exploration
KW  - Planning
KW  - Octrees
KW  - Entropy
KW  - Robot sensing systems
KW  - Measurement
KW  - Task analysis
KW  - Aerial Systems: Perception and Autonomy
KW  - Visual-Based Navigation
DO  - 10.1109/ICRA40945.2020.9196707
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration and collision-free navigation through an unknown environment is a fundamental task for autonomous robots. In this paper, a novel exploration strategy for Micro Aerial Vehicles (MAVs) is presented. The goal of the exploration strategy is the reduction of map entropy regarding occupancy probabilities, which is reflected in a utility function to be maximised. We achieve fast and efficient exploration performance with tight integration between our octree-based occupancy mapping approach, frontier extraction, and motion planning-as a hybrid between frontier-based and sampling-based exploration methods. The computationally expensive frontier clustering employed in classic frontier-based exploration is avoided by exploiting the implicit grouping of frontier voxels in the underlying octree map representation. Candidate next-views are sampled from the map frontiers and are evaluated using a utility function combining map entropy and travel time, where the former is computed efficiently using sparse raycasting. These optimisations along with the targeted exploration of frontier-based methods result in a fast and computationally efficient exploration planner. The proposed method is evaluated using both simulated and real-world experiments, demonstrating clear advantages over state-of-the-art approaches.
ER  - 

TY  - CONF
TI  - Dynamic Landing of an Autonomous Quadrotor on a Moving Platform in Turbulent Wind Conditions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9577
EP  - 9583
AU  - A. Paris
AU  - B. T. Lopez
AU  - J. P. How
PY  - 2020
KW  - aircraft landing guidance
KW  - autonomous aerial vehicles
KW  - Global Positioning System
KW  - helicopters
KW  - Kalman filters
KW  - mobile robots
KW  - nonlinear filters
KW  - robot dynamics
KW  - robot vision
KW  - robust control
KW  - turbulence
KW  - variable structure systems
KW  - vehicle dynamics
KW  - dynamic landing
KW  - autonomous quadrotor
KW  - moving platform
KW  - turbulent wind conditions
KW  - autonomous landing
KW  - fast trajectory planning
KW  - wind disturbance
KW  - fully autonomous vision-based system
KW  - quadrotor-platform distance
KW  - landing trajectory
KW  - receding horizon control
KW  - boundary layer sliding controller
KW  - extended Kalman filter
KW  - GPS measurements
KW  - robust control
KW  - precise control
KW  - Trajectory
KW  - Cameras
KW  - Vehicle dynamics
KW  - Planning
KW  - Global Positioning System
KW  - Visualization
KW  - Acceleration
KW  - Unmanned aerial vehicles
KW  - autonomous vehicles
KW  - landing on a moving platform
KW  - disturbance compensation
DO  - 10.1109/ICRA40945.2020.9197081
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous landing on a moving platform presents unique challenges for multirotor vehicles, including the need to accurately localize the platform, fast trajectory planning, and precise/robust control. Previous works studied this problem but most lack explicit consideration of the wind disturbance, which typically leads to slow descents onto the platform. This work presents a fully autonomous vision-based system that addresses these limitations by tightly coupling the localization, planning, and control, thereby enabling fast and accurate landing on a moving platform. The platform's position, orientation, and velocity are estimated by an extended Kalman filter using simulated GPS measurements when the quadrotor-platform distance is large, and by a visual fiducial system when the platform is nearby. The landing trajectory is computed online using receding horizon control and is followed by a boundary layer sliding controller that provides tracking performance guarantees in the presence of unknown, but bounded, disturbances. To improve the performance, the characteristics of the turbulent conditions are accounted for in the controller. The landing trajectory is fast, direct, and does not require hovering over the platform, as is typical of most stateof-the-art approaches. Simulations and hardware experiments are presented to validate the robustness of the approach.
ER  - 

TY  - CONF
TI  - Direct NMPC for Post-Stall Motion Planning with Fixed-Wing UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9592
EP  - 9598
AU  - M. Basescu
AU  - J. Moore
PY  - 2020
KW  - aerodynamics
KW  - aerospace components
KW  - autonomous aerial vehicles
KW  - feedback
KW  - mobile robots
KW  - motion control
KW  - nonlinear control systems
KW  - path planning
KW  - position control
KW  - predictive control
KW  - vehicle dynamics
KW  - rotary-wing UAVs
KW  - nonlinear control approach
KW  - fixed-wing UAVs
KW  - full-state direct trajectory optimization
KW  - representative aircraft model
KW  - nonlinear aircraft model
KW  - fixed-wing trajectories
KW  - randomized motion planning
KW  - local-linear feedback
KW  - direct NMPC
KW  - post-stall motion planning
KW  - fixed-wing unmanned aerial vehicles
KW  - frequency 5.0 Hz
KW  - Aerodynamics
KW  - Trajectory
KW  - Atmospheric modeling
KW  - Real-time systems
KW  - Computational modeling
KW  - Planning
KW  - Splines (mathematics)
DO  - 10.1109/ICRA40945.2020.9196724
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fixed-wing unmanned aerial vehicles (UAVs) offer significant performance advantages over rotary-wing UAVs in terms of speed, endurance, and efficiency. However, these vehicles have traditionally been severely limited with regards to maneuverability. In this paper, we present a nonlinear control approach for enabling aerobatic fixed-wing UAVs to maneuver in constrained spaces. Our approach utilizes full-state direct trajectory optimization and a minimalistic, but representative, nonlinear aircraft model to plan aggressive fixed-wing trajectories in real-time at 5 Hz across high angles-of-attack. Randomized motion planning is used to avoid local minima and local-linear feedback is used to compensate for model inaccuracies between updates. We demonstrate our method in hardware and show that both local-linear feedback and re-planning are necessary for successful navigation of a complex environment in the presence of model uncertainty.
ER  - 

TY  - CONF
TI  - A Flight Envelope Determination and Protection System for Fixed-Wing UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9599
EP  - 9605
AU  - G. Zogopoulos-Papaliakos
AU  - K. J. Kyriakopoulos
PY  - 2020
KW  - aerospace components
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - convex programming
KW  - nonlinear control systems
KW  - predictive control
KW  - remotely operated vehicles
KW  - flight envelope determination
KW  - protection system
KW  - fixed-wing UAV
KW  - generic model
KW  - nonlinear numerical model
KW  - model predictive controller
KW  - flight envelope constraints
KW  - trim flight envelope
KW  - MPC
KW  - Iron
KW  - Aircraft
KW  - Numerical models
KW  - Atmospheric modeling
KW  - Approximation algorithms
KW  - Mathematical model
KW  - Aerospace control
DO  - 10.1109/ICRA40945.2020.9197433
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a novel, approximate, efficient algorithm for determining the Trim Flight Envelope of a fixed-wing UAV, based on a generic, nonlinear numerical model. The resulting Flight Envelope is expressed as a convex intersection of half-spaces. Subsequently, a Model Predictive Controller (MPC) is designed which takes into account the Flight Envelope constraints, to avoid Loss-of-Control. The overall system is shown to operate in real-time in a simulation environment.
ER  - 

TY  - CONF
TI  - Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9638
EP  - 9644
AU  - J. Mercat
AU  - T. Gilles
AU  - N. El Zoghby
AU  - G. Sandou
AU  - D. Beauvois
AU  - G. P. Gil
PY  - 2020
KW  - position control
KW  - probability
KW  - road vehicles
KW  - tracking
KW  - joint forecast
KW  - multimodal probability density functions
KW  - vehicle position tracks
KW  - multimodal joint vehicle motion forecasting
KW  - maneuver definitions
KW  - road scene
KW  - long short-term memory layers
KW  - spatial grid
KW  - Forecasting
KW  - Predictive models
KW  - Roads
KW  - Uncertainty
KW  - Computer architecture
KW  - Tensile stress
KW  - Probability density function
DO  - 10.1109/ICRA40945.2020.9197340
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel vehicle motion forecasting method based on multi-head attention. It produces joint forecasts for all vehicles on a road scene as sequences of multi-modal probability density functions of their positions. Its architecture uses multi-head attention to account for interactions between all vehicles, and long short-term memory layers for encoding and forecasting. It relies solely on vehicle position tracks, does not need maneuver definitions, and does not rasterize the scene as a spatial grid. This allows it to be more versatile than similar model while combining many forecasting capabilities, namely joint forecast with interactions, uncertainty estimation, and multi-modality. The resulting prediction likelihood outperforms state-of-the-art models on the same dataset.
ER  - 

TY  - CONF
TI  - A Volumetric Albedo Framework for 3D Imaging Sonar Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9645
EP  - 9651
AU  - E. Westman
AU  - I. Gkioulekas
AU  - M. Kaess
PY  - 2020
KW  - autonomous underwater vehicles
KW  - image reconstruction
KW  - optimisation
KW  - robot vision
KW  - solid modelling
KW  - sonar
KW  - sonar imaging
KW  - stereo image processing
KW  - 3D imaging sonar reconstruction
KW  - object-level 3D underwater reconstruction
KW  - imaging sonar sensors
KW  - nonline-of-sight reconstruction
KW  - convex linear optimization problem
KW  - alternating direction method of multipliers
KW  - ADMM
KW  - sonar elevation apertures
KW  - autonomous underwater vehicles
KW  - volumetric Albedo framework
KW  - Sonar
KW  - Image reconstruction
KW  - Imaging
KW  - Three-dimensional displays
KW  - Sensors
KW  - Nonlinear optics
KW  - Surface reconstruction
DO  - 10.1109/ICRA40945.2020.9197042
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel framework for object-level 3D underwater reconstruction using imaging sonar sensors. We demonstrate that imaging sonar reconstruction is analogous to the problem of confocal non-line-of-sight (NLOS) reconstruction. Drawing upon this connection, we formulate the problem as one of solving for volumetric albedo, where the scene of interest is modeled as a directionless albedo field. After discretization, reconstruction reduces to a convex linear optimization problem, which we can augment with a variety of priors and regularization terms. We show how to solve the resulting regularized problems using the alternating direction method of multipliers (ADMM) algorithm. We demonstrate the effectiveness of the proposed approach in simulation and on real-world datasets collected in a controlled, test tank environment with several different sonar elevation apertures.
ER  - 

TY  - CONF
TI  - Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9652
EP  - 9658
AU  - S. F. G. Ehlers
AU  - M. Stuede
AU  - K. Nuelle
AU  - T. Ortmaier
PY  - 2020
KW  - image registration
KW  - iterative methods
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - link-points
KW  - multiple indoor areas
KW  - outdoor areas
KW  - map quality
KW  - single map approaches
KW  - semantic map management approach
KW  - multiple maps
KW  - modular map structure
KW  - utilized SLAM method
KW  - laser scan data
KW  - appropriate SLAM configuration
KW  - single independent maps
KW  - appearance-based method
KW  - iterative closest point registration
KW  - point clouds
KW  - simultaneous localization and mapping configurations
KW  - Simultaneous localization and mapping
KW  - Lasers
KW  - Navigation
KW  - Feature extraction
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196997
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents a semantic map management approach for various environments by triggering multiple maps with different simultaneous localization and mapping (SLAM) configurations. A modular map structure allows to add, modify or delete maps without influencing other maps of different areas. The hierarchy level of our algorithm is above the utilized SLAM method. Evaluating laser scan data (e.g. the detection of passing a doorway) triggers a new map, automatically choosing the appropriate SLAM configuration from a manually predefined list. Single independent maps are connected by link-points, which are located in an overlapping zone of both maps, enabling global navigation over several maps. Loop- closures between maps are detected by an appearance-based method, using feature matching and iterative closest point (ICP) registration between point clouds. Based on the arrangement of maps and link-points, a topological graph is extracted for navigation purpose and tracking the global robot's position over several maps. Our approach is evaluated by mapping a university campus with multiple indoor and outdoor areas and abstracting a metrical-topological graph. It is compared to a single map running with different SLAM configurations. Our approach enhances the overall map quality compared to the single map approaches by automatically choosing predefined SLAM configurations for different environmental setups.
ER  - 

TY  - CONF
TI  - A Hierarchical Framework for Collaborative Probabilistic Semantic Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9659
EP  - 9665
AU  - Y. Yue
AU  - C. Zhao
AU  - R. Li
AU  - C. Yang
AU  - J. Zhang
AU  - M. Wen
AU  - Y. Wang
AU  - D. Wang
PY  - 2020
KW  - Bayes methods
KW  - expectation-maximisation algorithm
KW  - geometry
KW  - mobile robots
KW  - multi-robot systems
KW  - robot vision
KW  - single robot semantic mapping
KW  - collaborative geometry mapping
KW  - semantic point cloud
KW  - heterogeneous sensor fusion model
KW  - collaborative robots level
KW  - 3D semantic map fusion algorithm
KW  - hierarchical collaborative probabilistic semantic mapping framework
KW  - Bayesian rule
KW  - probability
KW  - expectation-maximization
KW  - mathematical modeling
KW  - Semantics
KW  - Collaboration
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Geometry
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197261
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Performing collaborative semantic mapping is a critical challenge for cooperative robots to maintain a comprehensive contextual understanding of the surroundings. Most of the existing work either focus on single robot semantic mapping or collaborative geometry mapping. In this paper, a novel hierarchical collaborative probabilistic semantic mapping framework is proposed, where the problem is formulated in a distributed setting. The key novelty of this work is the mathematical modeling of the overall collaborative semantic mapping problem and the derivation of its probability decomposition. In the single robot level, the semantic point cloud is obtained based on heterogeneous sensor fusion model and is used to generate local semantic maps. Since the voxel correspondence is unknown in collaborative robots level, an Expectation-Maximization approach is proposed to estimate the hidden data association, where Bayesian rule is applied to perform semantic and occupancy probability update. The experimental results show the high quality global semantic map, demonstrating the accuracy and utility of 3D semantic map fusion algorithm in real missions.
ER  - 

TY  - CONF
TI  - Autonomous Navigation in Unknown Environments using Sparse Kernel-based Occupancy Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9666
EP  - 9672
AU  - T. Duong
AU  - N. Das
AU  - M. Yip
AU  - N. Atanasov
PY  - 2020
KW  - collision avoidance
KW  - image classification
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - trajectory control
KW  - decision boundary
KW  - kernel perceptron classifier
KW  - online training algorithm
KW  - piecewise-polynomial robot trajectories
KW  - autonomous navigation
KW  - Ackermann-drive robot
KW  - sparse kernel-based occupancy
KW  - real-time occupancy mapping
KW  - autonomous robot
KW  - map representation
KW  - piecewise-polynomial robot trajectory
KW  - piecewise-linear robot trajectory
KW  - collision checking algorithm
KW  - Support vector machines
KW  - Kernel
KW  - Training
KW  - Robot sensing systems
KW  - Collision avoidance
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197412
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on real-time occupancy mapping and collision checking onboard an autonomous robot navigating in an unknown environment. We propose a new map representation, in which occupied and free space are separated by the decision boundary of a kernel perceptron classifier. We develop an online training algorithm that maintains a very sparse set of support vectors to represent obstacle boundaries in configuration space. We also derive conditions that allow complete (without sampling) collision-checking for piecewise-linear and piecewise-polynomial robot trajectories. We demonstrate the effectiveness of our mapping and collision checking algorithms for autonomous navigation of an Ackermann-drive robot in unknown environments.
ER  - 

TY  - CONF
TI  - Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9673
EP  - 9679
AU  - C. Gomez
AU  - M. Fehr
AU  - A. Millane
AU  - A. C. Hernandez
AU  - J. Nieto
AU  - R. Barber
AU  - R. Siegwart
PY  - 2020
KW  - image representation
KW  - indoor navigation
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - topology
KW  - indoor environments
KW  - topological global representations
KW  - 3D dense submaps
KW  - hybrid global map
KW  - autonomous exploration
KW  - autonomous navigation
KW  - path planning
KW  - dense 3D maps
KW  - 3D dense representations
KW  - 3D dense mapping systems
KW  - hybrid topological mapping
KW  - metric 3D maps
KW  - standard CPU
KW  - Three-dimensional displays
KW  - Measurement
KW  - Robots
KW  - Semantics
KW  - Two dimensional displays
KW  - Indoor environments
KW  - Path planning
DO  - 10.1109/ICRA40945.2020.9197226
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots require a detailed understanding of the 3D structure of the environment for autonomous navigation and path planning. A popular approach is to represent the environment using metric, dense 3D maps such as 3D occupancy grids. However, in large environments the computational power required for most state-of-the-art 3D dense mapping systems is compromising precision and real-time capability. In this work, we propose a novel mapping method that is able to build and maintain 3D dense representations for large indoor environments using standard CPUs. Topological global representations and 3D dense submaps are maintained as hybrid global map. Submaps are generated for every new visited place. A place (room) is identified as an isolated part of the environment connected to other parts through transit areas (doors). This semantic partitioning of the environment allows for a more efficient mapping and path-planning. We also propose a method for autonomous exploration that directly builds the hybrid representation in real time.We validate the real-time performance of our hybrid system on simulated and real environments regarding mapping and path-planning. The improvement in execution time and memory requirements upholds the contribution of the proposed work.
ER  - 

TY  - CONF
TI  - Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9680
EP  - 9686
AU  - S. -F. Ch‚Äông
AU  - N. Sogi
AU  - P. Purkait
AU  - T. -J. Chin
AU  - K. Fukui
PY  - 2020
KW  - computer vision
KW  - object detection
KW  - optimisation
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - SLAM (robots)
KW  - lifted algorithm
KW  - combinatorial complexity
KW  - heuristic criterion
KW  - planar pose estimation
KW  - marker-based mapping
KW  - highly ambiguous inputs
KW  - PPE ambiguities
KW  - possible marker orientation solutions
KW  - rotation averaging formulation
KW  - marker corners
KW  - computer vision
KW  - planar markers
KW  - clique constraints
KW  - robust rotation averaging
KW  - marker pose ambiguity
KW  - Cameras
KW  - Pipelines
KW  - Machine-to-machine communications
KW  - Image edge detection
KW  - Pose estimation
KW  - Simultaneous localization and mapping
KW  - Histograms
DO  - 10.1109/ICRA40945.2020.9196902
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criterion, or simply ignore ambiguous markers.We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.
ER  - 

TY  - CONF
TI  - Anticipating the Start of User Interaction for Service Robot in the Wild
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9687
EP  - 9693
AU  - K. Ito
AU  - Q. Kong
AU  - S. Horiguchi
AU  - T. Sumiyoshi
AU  - K. Nagamatsu
PY  - 2020
KW  - cameras
KW  - face recognition
KW  - feature extraction
KW  - image colour analysis
KW  - image motion analysis
KW  - image sensors
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - pose estimation
KW  - service robots
KW  - user interaction
KW  - service robot
KW  - proactive service
KW  - potential visitors
KW  - visitor
KW  - early anticipation
KW  - human pose information
KW  - publicly available JPL interaction dataset
KW  - accurate anticipation performance
KW  - CNN-LSTM-based model
KW  - human verification
DO  - 10.1109/ICRA40945.2020.9196548
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A service robot is expected to provide proactive service for visitors who require its help. In contrast to passive service, e.g., providing service only after being spoken to, proactive service initiates an interaction at an early stage, e.g., talking to potential visitors who need the robot's help in advance. This paper addresses how to anticipate the start of user interaction. We propose an approach using only a single RGB camera that anticipates whether a visitor will come to the robot for interaction or just pass it by. In the proposed approach, we (i) utilize the visitor's pose information from captured images incorporating facial information, (ii) train a CNN-LSTM-based model in an end-to-end manner with an exponential loss for early anticipation, and (iii) during the training, the network branch for facial keypoints acquired as the part of the human pose information is taught to mimic the branch trained with the face image from a specialized face detector with a human verification. By virtue of (iii), at the inference, we can run our model in an embedded system processing only the pose information without an additional face detector and typical accuracy drop. We evaluated the proposed approach on our collected real world data with a real service robot and publicly available JPL interaction dataset and found that it achieved accurate anticipation performance.
ER  - 

TY  - CONF
TI  - Spin Detection in Robotic Table Tennis*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9694
EP  - 9700
AU  - J. Tebbe
AU  - L. Klamt
AU  - Y. Gao
AU  - A. Zell
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - sport
KW  - spin detection
KW  - robotic table tennis
KW  - rotation
KW  - table tennis match
KW  - human player
KW  - return stroke
KW  - high-speed camera
KW  - frame rate
KW  - circular brand logo
KW  - background difference
KW  - spin types
KW  - table tennis rally
KW  - frequency 380.0 Hz
KW  - Three-dimensional displays
KW  - Cameras
KW  - Robot vision systems
KW  - Trajectory
KW  - Quaternions
DO  - 10.1109/ICRA40945.2020.9196536
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In table tennis, the rotation (spin) of the ball plays a crucial role. A table tennis match will feature a variety of strokes. Each generates different amounts and types of spin. To develop a robot that can compete with a human player, the robot needs to detect spin, so it can plan an appropriate return stroke. In this paper we compare three methods to estimate spin. The first two approaches use a high-speed camera that captures the ball in flight at a frame rate of 380 Hz. This camera allows the movement of the circular brand logo printed on the ball to be seen. The first approach uses background difference to determine the position of the logo. In a second alternative, we train a CNN to predict the orientation of the logo. The third method evaluates the trajectory of the ball and derives the rotation from the effect of the Magnus force. This method gives the highest accuracy and is used for a demonstration. Our robot successfully copes with different spin types in a real table tennis rally against a human opponent.
ER  - 

TY  - CONF
TI  - Look, Listen, and Act: Towards Audio-Visual Embodied Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9701
EP  - 9707
AU  - C. Gan
AU  - Y. Zhang
AU  - J. Wu
AU  - B. Gong
AU  - J. B. Tenenbaum
PY  - 2020
KW  - acoustic signal processing
KW  - audio signal processing
KW  - audio-visual systems
KW  - human computer interaction
KW  - mobile agents
KW  - navigation
KW  - path planning
KW  - audio-visual embodied navigation
KW  - mobile intelligent agents
KW  - multiple sensory inputs
KW  - sound source
KW  - indoor environment
KW  - raw egocentric visual data
KW  - audio sensory data
KW  - audio signal
KW  - visual environment
KW  - visual pieces
KW  - audio pieces
KW  - visual perception mapper module
KW  - sound perception module
KW  - audio-visual observations
KW  - simulated multimodal environment
KW  - visual-audio-room dataset
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Robot sensing systems
KW  - Visual perception
KW  - Acoustics
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197008
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e., relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.
ER  - 

TY  - CONF
TI  - Autonomous Tool Construction with Gated Graph Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9708
EP  - 9714
AU  - C. Yang
AU  - X. Lan
AU  - H. Zhang
AU  - N. Zheng
PY  - 2020
KW  - convolutional neural nets
KW  - graph theory
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - tools
KW  - gated graph neural network
KW  - autonomous tool construction
KW  - reference tool
KW  - robotics
KW  - GGNN
KW  - RCNN-like structure
KW  - TC-GRCNN
KW  - large-scale training data generation
KW  - large-scale testing data generation
KW  - tool construction graph RCNN
KW  - region based convolutional neural network
KW  - candidate part pairs
KW  - Tools
KW  - Data models
KW  - Robots
KW  - Task analysis
KW  - Training
KW  - Solid modeling
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197285
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous tool construction is a significant but challenging task in robotics. This task can be interpreted as when given a reference tool, selecting some available candidate parts to reconstruct it. Most of the existing works perform tool construction in the form of action part and grasp part, which is only a specific construction pattern and limits its application to some extent. In general scenarios, a tool can be constructed in various patterns with different part pairs. Therefore, whether a part pair is most suitable for constructing the tool depends not only on itself, but on other parts in the same scene. To solve this problem, we construct a Gated Graph Neural Network (GGNN) to model the relations between all part pairs, so that we can select the candidate parts in consideration of the global information. Afterwards, we embed the constructed GGNN into a RCNN-like structure to finally accomplish tool construction. The whole model will be named Tool Construction Graph RCNN (TC-GRCNN). In addition, we develop a mechanism that can generate large-scale training and testing data in simulation environments, by which we can save the time of data collection and annotation. Finally, the proposed model is deployed on the physical robot. The experiment results show that TC-GRCNN can perform well in the general scenarios of tool construction.
ER  - 

TY  - CONF
TI  - Training-Set Distillation for Real-Time UAV Object Tracking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9715
EP  - 9721
AU  - F. Li
AU  - C. Fu
AU  - F. Lin
AU  - Y. Li
AU  - P. Lu
PY  - 2020
KW  - autonomous aerial vehicles
KW  - correlation methods
KW  - image filtering
KW  - image motion analysis
KW  - minimisation
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - training-set distillation
KW  - UAV object tracking
KW  - correlation filter
KW  - visual object tracking
KW  - unmanned aerial vehicle
KW  - energy minimization function
KW  - scoring process
KW  - time slot-based distillation approach
KW  - Training
KW  - Unmanned aerial vehicles
KW  - Correlation
KW  - Reliability
KW  - Real-time systems
KW  - Optimization
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197252
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Correlation filter (CF) has recently exhibited promising performance in visual object tracking for unmanned aerial vehicle (UAV). Such online learning method heavily depends on the quality of the training-set, yet complicated aerial scenarios like occlusion or out of view can reduce its reliability. In this work, a novel time slot-based distillation approach is proposed to efficiently and effectively optimize the training-set's quality on the fly. A cooperative energy minimization function is established to score the historical samples adaptively. To accelerate the scoring process, frames with high confident tracking results are employed as the keyframes to divide the tracking process into multiple time slots. After the establishment of a new slot, the weighted fusion of the previous samples generates one key-sample, in order to reduce the number of samples to be scored. Besides, when the current time slot exceeds the maximum frame number, which can be scored, the sample with the lowest score will be discarded. Consequently, the training-set can be efficiently and reliably distilled. Comprehensive tests on two well-known UAV benchmarks prove the effectiveness of our method with real-time speed on single CPU.
ER  - 

TY  - CONF
TI  - CNN-Based Simultaneous Dehazing and Depth Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9722
EP  - 9728
AU  - B. -U. Lee
AU  - K. Lee
AU  - J. Oh
AU  - I. S. Kweon
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - correlation methods
KW  - image coding
KW  - image colour analysis
KW  - image denoising
KW  - image representation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - spatial variables measurement
KW  - single hazy RGB input
KW  - single dense encoder
KW  - encoded image representation
KW  - dehazing image depth estimation
KW  - single image depth estimation
KW  - image dehazing
KW  - computer vision
KW  - convolutional neural networks
KW  - CNN
KW  - dehazing depth estimation algorithms
KW  - traditional haze modeling
KW  - depth estimation network
KW  - fully scaled depth map
KW  - depth-transmission consistency loss
KW  - separate decoders
KW  - Decoding
KW  - Propagation losses
KW  - Estimation
KW  - Training
KW  - Image reconstruction
KW  - Task analysis
KW  - Scattering
DO  - 10.1109/ICRA40945.2020.9197358
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is difficult for both cameras and depth sensors to obtain reliable information in hazy scenes. Therefore, image dehazing is still one of the most challenging problems to solve in computer vision and robotics. With the development of convolutional neural networks (CNNs), lots of dehazing and depth estimation algorithms using CNNs have emerged. However, very few of those try to solve these two problems at the same time. Focusing on the fact that traditional haze modeling contains depth information in its formula, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network aims to estimate both a dehazed image and a fully scaled depth map from a single hazy RGB input with end-to-end training. The network contains a single dense encoder and four separate decoders; each of them shares the encoded image representation while performing individual tasks. We suggest a novel depth-transmission consistency loss in the training scheme to fully utilize the correlation between the depth information and transmission map. To demonstrate the robustness and effectiveness of our algorithm, we performed various ablation studies and compared our results to those of state-of-the-art algorithms in dehazing and single image depth estimation, both qualitatively and quantitatively. Furthermore, we show the generality of our network by applying it to some real-world examples.
ER  - 

TY  - CONF
TI  - Internet of Things (IoT)-based Collaborative Control of a Redundant Manipulator for Teleoperated Minimally Invasive Surgeries
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9737
EP  - 9742
AU  - H. Su
AU  - S. E. Ovur
AU  - Z. Li
AU  - Y. Hu
AU  - J. Li
AU  - A. Knoll
AU  - G. Ferrigno
AU  - E. De Momi
PY  - 2020
KW  - control engineering computing
KW  - end effectors
KW  - human-robot interaction
KW  - Internet of Things
KW  - medical robotics
KW  - motion control
KW  - phantoms
KW  - redundant manipulators
KW  - surgery
KW  - telerobotics
KW  - Things-based collaborative control
KW  - teleoperated Minimally Invasive surgeries
KW  - Robot-assisted Minimally Invasive Surgery scenario
KW  - hierarchical operational space formulation
KW  - 7-DoFs redundant manipulator
KW  - multiple operational tasks
KW  - motion constraint
KW  - collision avoidance
KW  - undergoing surgical operation
KW  - Internet of Robotic Things
KW  - human-robot interaction
KW  - compliant swivel motion
KW  - HTC VIVE PRO controllers
KW  - robot elbow
KW  - smooth swivel motion
KW  - KUKA LWR4+ slave robot
KW  - SIGMA 7 master manipulator
KW  - Internet of Things-based human-robot collaborative control scheme
KW  - priority levels
KW  - Collision avoidance
KW  - Manipulators
KW  - Surgery
KW  - Task analysis
KW  - Force
KW  - Tools
DO  - 10.1109/ICRA40945.2020.9197321
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, an Internet of Things-based human-robot collaborative control scheme is developed in Robot-assisted Minimally Invasive Surgery scenario. A hierarchical operational space formulation is designed to exploit the redundancies of the 7-DoFs redundant manipulator to handle multiple operational tasks based on their priority levels, such as guaranteeing a remote center of motion constraint and avoiding collision with a swivel motion without influencing the undergoing surgical operation. Furthermore, the concept of the Internet of Robotic Things is exploited to facilitate the best action of the robot in human-robot interaction. Instead of utilizing compliant swivel motion, HTC VIVE PRO controllers, used as the Internet of Things technology, is adopted to detect the collision. A virtual force is applied to the robot elbow, enabling a smooth swivel motion for human-robot interaction. The effectiveness of the proposed strategy is validated using experiments performed on a patient phantom in a lab setup environment, with a KUKA LWR4+ slave robot and a SIGMA 7 master manipulator. By comparison with previous works, the results show improved performances in terms of the accuracy of the RCM constraint and surgical tip.
ER  - 

TY  - CONF
TI  - Passive Dynamic Balancing and Walking in Actuated Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9775
EP  - 9781
AU  - J. Reher
AU  - N. Csomay-Shanklin
AU  - D. L. Christensen
AU  - B. Bristow
AU  - A. D. Ames
AU  - L. Smoot
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - mechanical stability
KW  - pendulums
KW  - robot dynamics
KW  - passive dynamic balancing
KW  - passive dynamic systems
KW  - dynamic behaviors
KW  - actuated robots
KW  - robotic assistive devices
KW  - robotic systems
KW  - passive system
KW  - passive bipedal robot
KW  - dynamically stable periodic walking gaits
KW  - passive dynamic walking
KW  - Legged locomotion
KW  - Dynamics
KW  - Nonlinear dynamical systems
KW  - Robot sensing systems
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9197400
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The control of passive dynamic systems remains a challenging problem in the field of robotics, and insights from their study can inform everything from dynamic behaviors on actuated robots to robotic assistive devices. In this work, we explore the use of flat actuated environments for realizing passive dynamic balancing and locomotion. Specifically, we utilize a novel omnidirectional actuated floor to dynamically stabilize two robotic systems. We begin with an inverted pendulum to demonstrate the ability to control a passive system through an active environment. We then consider a passive bipedal robot wherein dynamically stable periodic walking gaits are generated through an optimization that leverages the actuated floor. The end result is the ability to demonstrate passive dynamic walking experimentally through the use of actuated environments.
ER  - 

TY  - CONF
TI  - Biped Stabilization by Linear Feedback of the Variable-Height Inverted Pendulum Model
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9782
EP  - 9788
AU  - S. Caron
PY  - 2020
KW  - feedback
KW  - humanoid robots
KW  - legged locomotion
KW  - nonlinear control systems
KW  - pendulums
KW  - robot dynamics
KW  - stability
KW  - stabilization
KW  - linear feedback
KW  - variable-height
KW  - pendulum model
KW  - balancing strategy
KW  - height variations
KW  - well-known ankle strategy
KW  - biped stabilizer
KW  - input feasibility
KW  - state viability constraints
KW  - resulting stabilizer
KW  - Mathematical model
KW  - Lips
KW  - Three-dimensional displays
KW  - Feedback control
KW  - Legged locomotion
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196715
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The variable-height inverted pendulum (VHIP) model enables a new balancing strategy by height variations of the center of mass, in addition to the well-known ankle strategy. We propose a biped stabilizer based on linear feedback of the VHIP that is simple to implement, coincides with the state-of-the-art for small perturbations and is able to recover from larger perturbations thanks to this new strategy. This solution is based on "best-effort" pole placement of a 4D divergent component of motion for the VHIP under input feasibility and state viability constraints. We complement it with a suitable whole-body admittance control law and test the resulting stabilizer on the HRP-4 humanoid robot.
ER  - 

TY  - CONF
TI  - Stability Criteria of Balanced and Steppable Unbalanced States for Full-Body Systems with Implications in Robotic and Human Gait
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9789
EP  - 9795
AU  - W. Z. Peng
AU  - C. Mummolo
AU  - J. H. Kim
PY  - 2020
KW  - gait analysis
KW  - humanoid robots
KW  - legged locomotion
KW  - position control
KW  - robot dynamics
KW  - stability
KW  - steppable unbalanced state boundary
KW  - full-body systems
KW  - double support contact configurations
KW  - steppable states
KW  - biped system
KW  - full-order nonlinear system dynamics
KW  - DARwIn-OP humanoid robot
KW  - balance stability boundaries
KW  - DS contact configuration
KW  - human gait
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9196820
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Biped walking involves a series of transitions between single support (SS) and double support (DS) contact configurations that can include both balanced and unbalanced states. The new concept of steppability is introduced to partition the set of unbalanced states into steppable states and falling (unsteppable) states based on the ability of a biped system to respond to forward velocity perturbations by stepping. In this work, a complete system-specific analysis of the stepping process including full-order nonlinear system dynamics is presented for the DARwIn-OP humanoid robot and a human subject in the sagittal plane with respect to both balance stability and steppability. The balance stability and steppability of each system are analyzed by numerical construction of its balance stability boundaries (BSB) for the initial SS and final DS contact configuration and the steppable unbalanced state boundary (SUB). These results are presented with center of mass (COM) trajectories obtained from walking experiments to benchmark robot controller performance and analyze the variation of balance stability and steppability with COM and swing foot position along the progression of a step cycle. For each system, DS BSBs were obtained with both constrained and unconstrained arms in order to demonstrate the ability of this approach to incorporate the effects of angular momentum and system-specific characteristics such as actuation torque, velocity, and angle limits.
ER  - 

TY  - CONF
TI  - Material Handling by Humanoid Robot While Pushing Carts Using a Walking Pattern Based on Capture Point
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9796
EP  - 9801
AU  - J. C. Vaz
AU  - P. Oh
PY  - 2020
KW  - friction
KW  - humanoid robots
KW  - legged locomotion
KW  - materials handling
KW  - motion control
KW  - robot dynamics
KW  - robot self balance
KW  - zero moment point pattern
KW  - walking pattern dynamic model
KW  - arm compliance
KW  - friction compensation
KW  - capture point method
KW  - cart pushing
KW  - humanoid robot
KW  - material handling
KW  - Humanoid robots
KW  - Legged locomotion
KW  - Friction
KW  - Robot sensing systems
KW  - Wrist
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196872
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a study that evaluates the effects on the walking pattern of a full-sized humanoid robot as it pushes different carts. Furthermore, it discuss a modified Zero Moment Point (ZMP) pattern based on a capture point method, and a friction compensation method for the arms. Humanoid researchers have demonstrated that robots can perform a wide range of tasks including handling tools, climbing ladders, and patrolling rough terrain. However, when it comes to handling objects while walking, humanoids are relatively limited; it becomes more apparent when humanoids have to push a cart. Many challenges become evident under such circumstances; for example, the walking pattern will be severely affected by the external force opposed by the cart. Therefore, an appropriate walking pattern dynamic model and arm compliance are needed to mitigate external forces. This becomes crucial in order to ensure the robot's self-balance and minimize external disturbances.
ER  - 

TY  - CONF
TI  - Interconnection and Damping Assignment Passivity-Based Control for Gait Generation in Underactuated Compass-Like Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9802
EP  - 9808
AU  - P. Arpenti
AU  - F. Ruggiero
AU  - V. Lippiello
PY  - 2020
KW  - control system synthesis
KW  - damping
KW  - gait analysis
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - interconnection and damping assignment passivity-based control
KW  - gait generation
KW  - compass-like biped robot
KW  - dynamic parameter
KW  - port-Hamiltonian framework
KW  - controller discretization
KW  - parametric uncertainties
KW  - Legged locomotion
KW  - Potential energy
KW  - Damping
KW  - Transmission line matrix methods
KW  - Mathematical model
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196598
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A compass-like biped robot can go down a gentle slope without the need of actuation through a proper choice of its dynamic parameter and starting from a suitable initial condition. Addition of control actions is requested to generate additional gaits and robustify the existing one. This paper designs an interconnection and damping assignment passivity-based control, rooted within the port-Hamiltonian framework, to generate further gaits with respect to state-of-the-art methodologies, enlarge the basin of attraction of existing gaits, and further robustify the system against controller discretization and parametric uncertainties. The performance of the proposed algorithm is validated through numerical simulations and comparison with existing passivity-based techniques.
ER  - 

TY  - CONF
TI  - Multi-Robot Path Deconfliction through Prioritization by Path Prospects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9809
EP  - 9815
AU  - W. Wu
AU  - S. Bhattacharya
AU  - A. Prorok
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - path prospects
KW  - prioritization rule
KW  - heterogeneous robot teams
KW  - multirobot path deconfliction
KW  - conflict-free path planning
KW  - mobile robots
KW  - conflict-free path plans
KW  - prioritization heuristics
KW  - Robot kinematics
KW  - Collision avoidance
KW  - Planning
KW  - Trajectory
KW  - Heuristic algorithms
KW  - Couplings
DO  - 10.1109/ICRA40945.2020.9196813
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work deals with the problem of planning conflict-free paths for mobile robots in cluttered environments. Since centralized, coupled planning algorithms are computationally intractable for large numbers of robots, we consider decoupled planning, in which robots plan their paths sequentially in order of priority. Choosing how to prioritize the robots is a key consideration. State-of-the-art prioritization heuristics, however, do not model the coupling between a robot's mobility and its environment. This is particularly relevant when prioritizing between robots with different degrees of mobility. In this paper, we propose a prioritization rule that can be computed online by each robot independently, and that provides consistent, conflict-free path plans. Our innovation is to formalize a robot's path prospects to reach its goal from its current location. To this end, we consider the number of homology classes of trajectories, which capture distinct prospects of paths for each robot. This measure is used as a prioritization rule, whenever any robots enter negotiation to deconflict path plans. We perform simulations with heterogeneous robot teams and compare our method to five benchmarks. Our method achieves the highest success rate, and strikes a good balance between makespan and flowtime objectives.
ER  - 

TY  - CONF
TI  - A Connectivity-Prediction Algorithm and its Application in Active Cooperative Localization for Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9824
EP  - 9830
AU  - L. Zhang
AU  - Z. Zhang
AU  - R. Siegwart
AU  - J. J. Chung
PY  - 2020
KW  - Markov processes
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - path planning
KW  - probability
KW  - infinite power series expansion theorem
KW  - finite-term approximation
KW  - computational feasibility
KW  - adverse impacts
KW  - higher order series terms
KW  - active CL
KW  - leader-follower architecture
KW  - Markov decision process
KW  - one-step planning horizon
KW  - optimal motion strategy
KW  - MDP model
KW  - connectivity-prediction algorithm
KW  - multirobot systems
KW  - future connectivity
KW  - mobile robots
KW  - range-limited communication
KW  - active motion planning
KW  - quadratic forms
KW  - random normal variables
KW  - Prediction algorithms
KW  - Robot sensing systems
KW  - Planning
KW  - Uncertainty
KW  - Computational modeling
KW  - Gaussian distribution
DO  - 10.1109/ICRA40945.2020.9197083
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method for predicting the probability of future connectivity between mobile robots with range-limited communication. In particular, we focus on its application to active motion planning for cooperative localization (CL). The probability of connection is modeled by the distribution of quadratic forms in random normal variables and is computed by the infinite power series expansion theorem. A finite-term approximation is made to realize the computational feasibility and three more modifications are designed to handle the adverse impacts introduced by the omission of the higher order series terms. On the basis of this algorithm, an active and CL problem with leader-follower architecture is then reformulated into a Markov Decision Process (MDP) with a one-step planning horizon, and the optimal motion strategy is generated by minimizing the expected cost of the MDP. Extensive simulations and comparisons are presented to show the effectiveness and efficiency of both the proposed prediction algorithm and the MDP model.
ER  - 

TY  - CONF
TI  - Behavior Mixing with Minimum Global and Subgroup Connectivity Maintenance for Large-Scale Multi-Robot Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9845
EP  - 9851
AU  - W. Luo
AU  - S. Yi
AU  - K. Sycara
PY  - 2020
KW  - collision avoidance
KW  - distributed algorithms
KW  - mobile robots
KW  - multi-robot systems
KW  - trees (mathematics)
KW  - large-scale multirobot systems
KW  - robot team
KW  - connected communication graph
KW  - minimum inter-robot connectivity constraints
KW  - activated connectivity constraints
KW  - behavior mixing controllers
KW  - distributed minimum connectivity constraint spanning tree algorithm
KW  - provably minimum connectivity maintenance
KW  - subgroup connectivity maintenance
KW  - minimum global connectivity maintenance
KW  - collision avoidance
KW  - distributed MCCST algorithm
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Task analysis
KW  - Safety
KW  - Multi-robot systems
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197429
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many cases the multi-robot systems are desired to execute simultaneously multiple behaviors with different controllers, and sequences of behaviors in real time, which we call behavior mixing. Behavior mixing is accomplished when different subgroups of the overall robot team change their controllers to collectively achieve given tasks while maintaining connectivity within and across subgroups in one connected communication graph. In this paper, we present a provably minimum connectivity maintenance framework to ensure the subgroups and overall robot team stay connected at all times while providing the highest freedom for behavior mixing. In particular, we propose a real-time distributed Minimum Connectivity Constraint Spanning Tree (MCCST) algorithm to select the minimum inter-robot connectivity constraints preserving subgroup and global connectivity that are least likely to be violated by the original controllers. With the employed safety and connectivity barrier certificates for the activated connectivity constraints and collision avoidance, the behavior mixing controllers are thus minimally modified from the original controllers. We demonstrate the effectiveness and scalability of our approach via simulations of up to 100 robots with multiple behaviors.
ER  - 

TY  - CONF
TI  - Energy-Optimal Cooperative Manipulation via Provable Internal-Force Regulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9859
EP  - 9865
AU  - C. K. Verginis
AU  - D. V. Dimarogonas
PY  - 2020
KW  - cooperative systems
KW  - decentralised control
KW  - force control
KW  - manipulator dynamics
KW  - mobile robots
KW  - multi-robot systems
KW  - position control
KW  - internal-force regulation
KW  - optimal cooperative robotic manipulation problem
KW  - energy resources
KW  - rigid cooperative manipulation systems
KW  - rigid grasping contacts
KW  - energy-optimal conditions
KW  - arising internal forces
KW  - inter-agent forces
KW  - closed form expression
KW  - standard inverse dynamics
KW  - force distribution
KW  - robotic agents
KW  - nonzero inter-agent internal force vector
KW  - internal force minimization
KW  - Grasping
KW  - Force
KW  - Robots
KW  - Dynamics
KW  - Acceleration
KW  - Jacobian matrices
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196696
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers the optimal cooperative robotic manipulation problem in terms of energy resources. In particular, we consider rigid cooperative manipulation systems, i.e., with rigid grasping contacts, and study energy-optimal conditions in the sense of minimization of the arising internal forces, which are inter-agent forces that do not contribute to object motion. Firstly, we use recent results to derive a closed form expression for the internal forces. Secondly, by using a standard inverse dynamics control protocol, we provide novel conditions on the force distribution to the robotic agents for provable internal force minimization. Moreover, we derive novel results on the provable achievement of a desired non-zero inter-agent internal force vector. Extensive simulation results in a realistic environment verify the theoretical analysis.
ER  - 

TY  - CONF
TI  - Robot Telekinesis: Application of a Unimanual and Bimanual Object Manipulation Technique to Robot Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9866
EP  - 9872
AU  - J. H. Lee
AU  - Y. Kim
AU  - S. -G. An
AU  - S. -H. Bae
PY  - 2020
KW  - end effectors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - industrial manipulators
KW  - industrial robots
KW  - sensors
KW  - robot control
KW  - dangerous industrial robots
KW  - collaborative robots
KW  - teaching pendant
KW  - direct teaching
KW  - novel robot interaction technique
KW  - robot arm
KW  - unimanual hand gestures
KW  - bimanual hand gestures
KW  - robot telekinesis
KW  - bimanual object manipulation technique
KW  - unimanual object manipulation technique
KW  - production lines
KW  - Robot sensing systems
KW  - End effectors
KW  - Task analysis
KW  - Education
KW  - Service robots
KW  - Collaboration
DO  - 10.1109/ICRA40945.2020.9197517
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unlike large and dangerous industrial robots at production lines in factories that are strictly fenced off, collaborative robots are smaller and safer and can be installed adjacent to human workers and collaborate with them. However, controlling and teaching new moves to collaborative robots can be difficult and time-consuming when using existing methods, such as pressing buttons on a teaching pendant and physically grabbing and moving the robot via direct teaching. We present Robot Telekinesis, a novel robot interaction technique that lets the user remotely control the movement of the end effector of a robot arm with unimanual and bimanual hand gestures that closely resemble handling a physical object. Through formal evaluation, we show that using a teaching pendant is slow and confusing and that direct teaching is fast and intuitive but physically demanding. Robot Telekinesis is as fast and intuitive as direct teaching without the need for physical contact or physical effort.
ER  - 

TY  - CONF
TI  - A Set-Theoretic Approach to Multi-Task Execution and Prioritization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9873
EP  - 9879
AU  - G. Notomista
AU  - S. Mayya
AU  - M. Selvaggio
AU  - M. Santos
AU  - C. Secchi
PY  - 2020
KW  - optimisation
KW  - redundant manipulators
KW  - safety-critical software
KW  - set theory
KW  - task analysis
KW  - time-varying systems
KW  - constrained optimization problem
KW  - redundant robotic manipulator
KW  - set theoretic approach
KW  - multitask execution
KW  - safety critical tasks
KW  - robotic system
KW  - optimization based task execution
KW  - set based tasks
KW  - time varying priorities
KW  - multitask prioritization
KW  - control barrier functions
KW  - Task analysis
KW  - Robot kinematics
KW  - Jacobian matrices
KW  - Aerospace electronics
KW  - Manipulators
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196741
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Executing multiple tasks concurrently is important in many robotic applications. Moreover, the prioritization of tasks is essential in applications where safety-critical tasks need to precede application-related objectives, in order to protect both the robot from its surroundings and vice versa. Furthermore, the possibility of switching the priority of tasks during their execution gives the robotic system the flexibility of changing its objectives over time. In this paper, we present an optimization-based task execution and prioritization framework that lends itself to the case of time-varying priorities as well as variable number of tasks. We introduce the concept of extended set-based tasks, encode them using control barrier functions, and execute them by means of a constrained-optimization problem, which can be efficiently solved in an online fashion. Finally, we show the application of the proposed approach to the case of a redundant robotic manipulator.
ER  - 

TY  - CONF
TI  - Variable Impedance Control in Cartesian Latent Space while Avoiding Obstacles in Null Space
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9888
EP  - 9894
AU  - D. Parent
AU  - A. Colom√©
AU  - C. Torras
PY  - 2020
KW  - collision avoidance
KW  - feedback
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulator dynamics
KW  - mobile robots
KW  - motion control
KW  - variable impedance control
KW  - cartesian latent space
KW  - null space
KW  - human-robot interaction
KW  - assistive robots
KW  - Cartesian impedance control
KW  - joint control
KW  - desired interaction
KW  - environmental feedback
KW  - robot arm
KW  - operational space
KW  - variable stiffness
KW  - precision requirements
KW  - dimensionality reduction
KW  - freedom relevant
KW  - redundant ones
KW  - task-redundant DoF
KW  - human head
KW  - Aerospace electronics
KW  - Task analysis
KW  - Trajectory
KW  - Jacobian matrices
KW  - Manipulators
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9197192
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human-robot interaction is one of the keys of assistive robots. Robots are expected to be compliant with people but at the same time correctly perform the tasks. In such applications, Cartesian impedance control is preferred over joint control, as the desired interaction and environmental feedback can be described more naturally, and the force to be exerted by the robot can be readily adjusted.This paper addresses the problem of controlling a robot arm in the operational space with variable stiffness so as to continuously adapt the force exerted in each phase of motion according to the precision requirements. Moreover, performing dimensionality reduction we can separate the degrees of freedom (DoF) relevant for the task from the redundant ones. The stiffness of the former can be adjusted constantly to achieve the required accuracy, while task-redundant DoF can be used to achieve other goals such as avoiding obstacles by moving in the directions where accuracy is not critical. The designed method is tested teaching the robot to give water to drink to a model of human head. Our empirical results demonstrate that the robot can learn precision requirements from demonstration. Furthermore, dimensionality reduction is proved to be useful to avoid obstacles.
ER  - 

TY  - CONF
TI  - MagicHand: Context-Aware Dexterous Grasping Using an Anthropomorphic Robotic Hand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9895
EP  - 9901
AU  - H. Li
AU  - J. Tan
AU  - H. He
PY  - 2020
KW  - dexterous manipulators
KW  - infrared spectra
KW  - object detection
KW  - robot vision
KW  - target object
KW  - grasping poses
KW  - grasp strategies
KW  - MagicHand system
KW  - context-aware dexterous grasping
KW  - robotic grasping
KW  - context-aware anthropomorphic robotic hand grasping system
KW  - NIR spectra
KW  - molecular level
KW  - RGB-D images
KW  - Grasping
KW  - Three-dimensional displays
KW  - Neurons
KW  - Cameras
KW  - Robot vision systems
KW  - Dexterous Grasping
KW  - Characteristics of Objects Recognition
KW  - NIR Spectrum
KW  - RGB-D Images
DO  - 10.1109/ICRA40945.2020.9196538
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Understanding of characteristics of objects such as fragility, rigidity, texture and dimensions facilitates and innovates robotic grasping. In this paper, we propose a context- aware anthropomorphic robotic hand (MagicHand) grasping system which is able to gather various information about its target object and generate grasping strategies based on the perceived information. In this work, NIR spectra of target objects are perceived to recognize materials on a molecular level and RGB-D images are collected to estimate dimensions of the objects. We selected six most used grasping poses and our system is able to decide the most suitable grasp strategies based on the characteristics of an object. Through multiple experiments, the performance of the MagicHand system is demonstrated.
ER  - 

TY  - CONF
TI  - Learning Pregrasp Manipulation of Objects from Ungraspable Poses
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9917
EP  - 9923
AU  - Z. Sun
AU  - K. Yuan
AU  - W. Hu
AU  - C. Yang
AU  - Z. Li
PY  - 2020
KW  - feedback
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - ungraspable poses
KW  - robotic grasping
KW  - model-free deep reinforcement learning
KW  - feedback control policies
KW  - visual information
KW  - robot arm
KW  - object-table clearance
KW  - pregrasp manipulation learning
KW  - human bimanual manipulation
KW  - Grasping
KW  - Robustness
KW  - Training
KW  - Grippers
KW  - Sensors
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196982
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In robotic grasping, objects are often occluded in ungraspable configurations such that no feasible grasp pose can be found, e.g. large flat boxes on the table that can only be grasped once lifted. Inspired by human bimanual manipulation, e.g. one hand to lift up things and the other to grasp, we address this type of problems by introducing pregrasp manipulation - push and lift actions. We propose a model-free Deep Reinforcement Learning framework to train feedback control policies that utilize visual information and proprioceptive states of the robot to autonomously discover robust pregrasp manipulation. The robot arm learns to push the object first towards a support surface and then lift up one side of the object, creating an object-table clearance for possible grasping solutions. Furthermore, we show the robustness of the proposed learning framework in training pregrasp policies that can be directly transferred to a real robot. Lastly, we evaluate the effectiveness and generalization ability of the learned policy in real-world experiments, and demonstrate pregrasp manipulation of objects with various sizes, shapes, weights, and surface friction.
ER  - 

TY  - CONF
TI  - Picking Thin Objects by Tilt-and-Pivot Manipulation and Its Application to Bin Picking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9932
EP  - 9938
AU  - Z. Tong
AU  - T. He
AU  - C. H. Kim
AU  - Y. Hin Ng
AU  - Q. Xu
AU  - J. Seo
PY  - 2020
KW  - dexterous manipulators
KW  - end effectors
KW  - grippers
KW  - industrial manipulators
KW  - manipulator kinematics
KW  - robot vision
KW  - convex polygonal objects
KW  - bin picking scenarios
KW  - manipulation process
KW  - robotic dexterous in-hand manipulation
KW  - tilt-and-pivot manipulation
KW  - thin object picking
KW  - tilt-and-pivot kinematics
KW  - tilt-and-pivot planning
KW  - Grippers
KW  - Shape
KW  - Solids
KW  - Surface treatment
KW  - Hardware
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197493
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces the technique of tilt-and-pivot manipulation, a new method for picking thin, rigid objects lying on a flat surface through robotic dexterous in-hand manipulation. During the manipulation process, the gripper is controlled to reorient about the contact with the object such that its finger can get in the space between the object and the supporting surface, which is formed by tilting up the object, with no relative sliding motion at the contact. As a result, a pinch grasp can be obtained on the faces of the thin object with ease. We discuss issues regarding the kinematics and planning of tilt-and-pivot, effector shape design, and the overall practicality of the manipulation technique, which is general enough to be applicable to any rigid convex polygonal objects. We also present a set of experiments in a range of bin picking scenarios.
ER  - 

TY  - CONF
TI  - Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9939
EP  - 9945
AU  - Z. -L. Ni
AU  - G. -B. Bian
AU  - Z. -G. Hou
AU  - X. -H. Zhou
AU  - X. -L. Xie
AU  - Z. Li
PY  - 2020
KW  - convolutional neural nets
KW  - edge detection
KW  - image capture
KW  - image coding
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - robot vision
KW  - surgery
KW  - robotic surgical instruments
KW  - robot-assisted surgery
KW  - deep learning models
KW  - attention-guided lightweight network
KW  - LWANet
KW  - lightweight network MobileNetV2
KW  - depthwise separable convolution
KW  - transposed convolution
KW  - surgical instrument
KW  - encoder-decoder architecture
KW  - attention fusion block
KW  - Instruments
KW  - Convolution
KW  - Semantics
KW  - Computational efficiency
KW  - Decoding
KW  - Surgery
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9197425
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.
ER  - 

TY  - CONF
TI  - Automated robotic breast ultrasound acquisition using ultrasound feedback
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9946
EP  - 9952
AU  - M. K. Welleweerd
AU  - A. G. de Groot
AU  - S. O. H. de Looijer
AU  - F. J. Siepel
AU  - S. Stramigioli
PY  - 2020
KW  - biological tissues
KW  - biomedical ultrasonics
KW  - feedback
KW  - image registration
KW  - medical image processing
KW  - medical robotics
KW  - phantoms
KW  - surgery
KW  - visual servoing
KW  - robotic 3D breast US acquisitions
KW  - US feedback
KW  - visual servoing algorithm
KW  - patient specific scans
KW  - tissue deformations
KW  - US probe
KW  - ultrasound feedback
KW  - automated robotic breast ultrasound acquisition
KW  - Breast
KW  - Probes
KW  - Trajectory
KW  - Safety
KW  - Robot kinematics
KW  - Skin
DO  - 10.1109/ICRA40945.2020.9196736
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Current challenges in automated robotic breast ultrasound (US) acquisitions include keeping acoustic coupling between the breast and the US probe, minimizing tissue deformations and safety. In this paper, we present how an autonomous 3D breast US acquisition can be performed utilizing a 7DOF robot equipped with a linear US transducer. Robotic 3D breast US acquisitions would increase the diagnostic value of the modality since they allow patient specific scans and have a high reproducibility, accuracy and efficiency. Additionally, 3D US acquisitions allow more flexibility in examining the breast and simplify registration with preoperative images like MRI. To overcome the current challenges, the robot follows a reference- based trajectory adjusted by a visual servoing algorithm. The reference trajectory is a patient specific trajectory coming from e.g. an MRI. The visual servoing algorithm commands in-plane rotations and corrects the probe contact based on confidence maps. A safety aware, intrinsically passive framework is utilised to actuate the robot. The approach is illustrated with experiments on a phantom, which show that the robot only needs minor pre-procedural information to consistently image the phantom while relying mainly on US feedback.
ER  - 

TY  - CONF
TI  - Robust and Accurate 3D Curve to Surface Registration with Tangent and Normal Vectors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9953
EP  - 9959
AU  - Z. Min
AU  - D. Zhu
AU  - M. Q. . -H. Meng
PY  - 2020
KW  - biomechanics
KW  - image reconstruction
KW  - image registration
KW  - medical image processing
KW  - surgery
KW  - image-guided surgery
KW  - pre-to-intraoperative registration
KW  - intra-operative 3D data
KW  - tangent vectors
KW  - sparse intraoperative data points
KW  - pre-operative model points
KW  - probabilistic distribution
KW  - multidimensional point
KW  - maximum likelihood problem
KW  - rigid registration
KW  - intraoperative point
KW  - mean target registration error value
KW  - size 0.6795 mm
KW  - Three-dimensional displays
KW  - Surgery
KW  - Robustness
KW  - Probes
KW  - Probabilistic logic
KW  - Robots
KW  - Biomedical imaging
DO  - 10.1109/ICRA40945.2020.9196923
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a robust and accurate approach for the rigid registration of pre-operative and intraoperative point sets in image-guided surgery (IGS). Three challenges are identified in the pre-to-intraoperative registration: the intra-operative 3D data (usually forms a 3D curve in space) (1) is often contaminated with noise and outliers; (2) usually only covers a partial region of the whole pre-operative model; (3) is usually sparse. To tackle those challenges, we utilize the tangent vectors extracted from the sparse intraoperative data points and the normal vectors extracted from the pre-operative model points. Our first contribution is to formulate a novel probabilistic distribution of the error between a pair of corresponding tangent and normal vectors. The second contribution is, based on the novel distribution, we formulate the registration of two multi-dimensional (6D) point sets as a maximum likelihood (ML) problem and solve it under the expectation maximization (EM) framework. Our last contribution is, in order to facilitate the computation process, the derivatives of the objective function with respect to desired parameters are presented. We conduct extensive experiments to demonstrate that our approach outperforms the state-of-the-art methods. Importantly, in the context of anteriro cruciate ligament (ACL) reconstruction, our method can achieve as low as 0.6795 mm mean target registration error (TRE) value with considerable noises and very limited overlapping ratios.
ER  - 

TY  - CONF
TI  - Single-Shot Pose Estimation of Surgical Robot Instruments‚Äô Shafts from Monocular Endoscopic Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9960
EP  - 9966
AU  - M. Yoshimura
AU  - M. M. Marinho
AU  - K. Harada
AU  - M. Mitsuishi
PY  - 2020
KW  - collision avoidance
KW  - endoscopes
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - neural net architecture
KW  - pose estimation
KW  - robot vision
KW  - surgery
KW  - single-shot pose estimation
KW  - monocular endoscopic images
KW  - minimally invasive surgery
KW  - collision-avoidance algorithm
KW  - online estimation
KW  - monocular endoscope
KW  - art vision-based marker-less
KW  - position estimation
KW  - surgical robot instrument shafts
KW  - annotated training dataset
KW  - improved pose-estimation deep-learning architecture
KW  - Instruments
KW  - Robots
KW  - Pose estimation
KW  - Surgery
KW  - Shafts
KW  - Endoscopes
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196779
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical robots are used to perform minimally invasive surgery and alleviate much of the burden imposed on surgeons. Our group has developed a surgical robot to aid in the removal of tumors at the base of the skull via access through the nostrils. To avoid injuring the patients, a collision-avoidance algorithm that depends on having an accurate model for the poses of the instruments' shafts is used. Given that the model's parameters can change over time owing to interactions between instruments and other disturbances, the online estimation of the poses of the instrument's shaft is essential. In this work, we propose a new method to estimate the pose of the surgical instruments' shafts using a monocular endoscope. Our method is based on the use of an automatically annotated training dataset and an improved pose-estimation deep-learning architecture. In preliminary experiments, we show that our method can surpass state of the art vision-based marker-less pose estimation techniques (providing an error decrease of 55% in position estimation, 64% in pitch, and 69% in yaw) by using artificial images.
ER  - 

TY  - CONF
TI  - End-to-End Real-time Catheter Segmentation with Optical Flow-Guided Warping during Endovascular Intervention
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9967
EP  - 9973
AU  - A. Nguyen
AU  - D. Kundrat
AU  - G. Dagnino
AU  - W. Chi
AU  - M. E. M. K. Abdelaziz
AU  - Y. Guo
AU  - Y. Ma
AU  - T. M. Y. Kwok
AU  - C. Riga
AU  - G. -Z. Yang
PY  - 2020
KW  - catheters
KW  - feature extraction
KW  - image segmentation
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - medical image processing
KW  - medical robotics
KW  - neural nets
KW  - surgery
KW  - deep learning framework
KW  - segmentation network
KW  - encoder-decoder architecture
KW  - flow network
KW  - optical flow information
KW  - frame-to-frame temporal continuity
KW  - catheter segmentation
KW  - robot-assisted endovascular intervention
KW  - flow-guided warping function
KW  - optical flow-guided warping
KW  - Catheters
KW  - Image segmentation
KW  - X-ray imaging
KW  - Real-time systems
KW  - Machine learning
KW  - Motion segmentation
DO  - 10.1109/ICRA40945.2020.9197307
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate real-time catheter segmentation is an important pre-requisite for robot-assisted endovascular intervention. Most of the existing learning-based methods for catheter segmentation and tracking are only trained on smallscale datasets or synthetic data due to the difficulties of ground-truth annotation. Furthermore, the temporal continuity in intraoperative imaging sequences is not fully utilised. In this paper, we present FW-Net, an end-to-end and real-time deep learning framework for endovascular intervention. The proposed FW-Net has three modules: a segmentation network with encoder-decoder architecture, a flow network to extract optical flow information, and a novel flow-guided warping function to learn the frame-to-frame temporal continuity. We show that by effectively learning temporal continuity, the network can successfully segment and track the catheters in real-time sequences using only raw ground-truth for training. Detailed validation results confirm that our FW-Net outperforms stateof-the-art techniques while achieving real-time performance.
ER  - 

TY  - CONF
TI  - Pathological Airway Segmentation with Cascaded Neural Networks for Bronchoscopic Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9974
EP  - 9980
AU  - H. Zhang
AU  - M. Shen
AU  - P. L. Shah
AU  - G. -Z. Yang
PY  - 2020
KW  - computerised tomography
KW  - image segmentation
KW  - lung
KW  - medical image processing
KW  - neural nets
KW  - cascaded 2D-3D model
KW  - pathological CT scans
KW  - 3D adversarial training model
KW  - novel 2D neural network
KW  - airway tree
KW  - pathological abnormalities
KW  - preoperative chest CT scans
KW  - patient-specific airway maps
KW  - peripheral airways
KW  - enhanced visualisation
KW  - 3D airway maps
KW  - robotic bronchoscopic intervention
KW  - bronchoscopic navigation
KW  - cascaded neural networks
KW  - pathological airway segmentation
KW  - Three-dimensional displays
KW  - Atmospheric modeling
KW  - Training
KW  - Two dimensional displays
KW  - Computed tomography
KW  - Image segmentation
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196756
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robotic bronchoscopic intervention requires detailed 3D airway maps for both localisation and enhanced visualisation, especially at peripheral airways. Patient-specific airway maps can be generated from preoperative chest CT scans. Due to pathological abnormalities and anatomical variations, automatically delineating the airway tree with distal branches is a challenging task. In the paper, we propose a cascaded 2D+3D model that has been tailored for airway segmentation from pathological CT scans. A novel 2D neural network is developed to generate the initial predictions where the peripheral airways are refined by a 3D adversarial training model. A sampling strategy based on a sequence of morphological operations is employed for the concatenation of the 2D and 3D models. The method has been validated on 20 pathological CT scans with results demonstrating improved segmentation accuracy and consistency, especially in peripheral airways.
ER  - 

TY  - CONF
TI  - Design of 3D-printed assembly mechanisms based on special wooden joinery techniques and its application to a robotic hand
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9981
EP  - 9987
AU  - A. Katsumaru
AU  - R. Ozawa
PY  - 2020
KW  - assembling
KW  - design engineering
KW  - plastics
KW  - printers
KW  - robots
KW  - three-dimensional printing
KW  - part-joining quality
KW  - design method
KW  - robotic hand
KW  - 3D-printed assembly mechanisms
KW  - robotic systems
KW  - plastic materials
KW  - Japanese wooden joinery techniques
KW  - assembling 3D-printed parts
KW  - Gears
KW  - Robots
KW  - Pins
KW  - Shape
KW  - Three-dimensional displays
KW  - Thumb
KW  - Printers
DO  - 10.1109/ICRA40945.2020.9197475
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, it has become possible to easily design and fabricate robotic systems in the laboratory and at home due to the recent development of 3D printer technology. On the other hand, the strength of the plastic materials used in reasonably priced 3D printers and the accuracy of the printed parts are generally low. These problems affect the part-joining quality. Therefore, this paper describes a design method inspired by ancient Japanese wooden joinery techniques for assembling 3D-printed parts and presents the design of a robotic hand as its application. The joinery techniques use special shapes to assemble components and allow us to assemble the robotic hand without glue, screws or nails and to easily disassemble it.
ER  - 

TY  - CONF
TI  - Parallel gripper with displacement-magnification mechanism and extendable finger mechanism
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 9988
EP  - 9993
AU  - J. Tanaka
AU  - A. Sugahara
PY  - 2020
KW  - force control
KW  - grippers
KW  - impact (mechanical)
KW  - gripping force
KW  - product height
KW  - impact force
KW  - stacked rack-and-pinion system
KW  - mechanism verification
KW  - parallel gripper
KW  - gripper displacement-magnification mechanism
KW  - extendable finger mechanism
KW  - size 95.0 mm
KW  - size 110.0 mm
KW  - size 214.0 mm
KW  - mass 1.36 kg
KW  - size 60.0 mm
KW  - velocity 100.0 mm/s
KW  - Grippers
KW  - Gears
KW  - Nails
KW  - Thumb
KW  - Force
KW  - Piezoelectric transducers
DO  - 10.1109/ICRA40945.2020.9196746
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a gripper displacement-magnification mechanism and an extendable finger mechanism, both of which can be attached to a commercially available parallel gripper. We then verify the operation of the mechanism in order to expand applications of the parallel gripper. The displacement-magnification mechanism has a stacked rack-and-pinion system that doubles displacement. The extendable finger mechanism has two nails that extend and contract, reducing impact force and detecting changes in product height from expansion and contraction amounts. The parallel gripper has a width of 95 mm, a depth of 110 mm, and a height of 214 mm and weighs 1.36 kg. It has an open/close stroke of 60 mm, a gripping force of 7.4 N, and an opening/closing speed of 100 mm/s or more. Further, it was confirmed that the ends and inclinations of products can be reliably detected using the extending/contracting nail. The mechanism verification confirmed that our parallel gripper achieved the desired performance and is therefore useful.
ER  - 

TY  - CONF
TI  - A Shape Memory Polymer Adhesive Gripper For Pick-and-Place Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10010
EP  - 10016
AU  - C. Son
AU  - S. Kim
PY  - 2020
KW  - adhesion
KW  - adhesives
KW  - control system synthesis
KW  - grippers
KW  - manipulators
KW  - polymers
KW  - shape memory effects
KW  - shape memory polymer adhesive gripper
KW  - smart adhesive applications
KW  - pick-and-place applications
KW  - reversible dry adhesion
KW  - gecko grippers
KW  - high adhesion strength
KW  - SMP adhesive mechanics
KW  - reversible dry adhesive properties
KW  - single surface contact grippers
KW  - SMP adhesive gripper
KW  - Switched mode power supplies
KW  - Grippers
KW  - Adhesives
KW  - Heating systems
KW  - Shape
KW  - Rough surfaces
KW  - Surface roughness
DO  - 10.1109/ICRA40945.2020.9197511
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Over the past few years, shape memory polymer (SMP) has been extensively studied in terms of its remarkable reversible dry adhesive properties and related smart adhesive applications. However, its exceptional properties have not been exploited for further opportunities such as pick-and-place applications, which would otherwise advance the robotic manipulation. This work explores the use of an SMP to design an adhesive gripper that picks and places a target solid object employing the reversible dry adhesion of an SMP. Compared with other single surface contact grippers including vacuum, electromagnetic, electroadhesion, and gecko grippers, the SMP adhesive gripper interacts with not only flat and smooth dry surfaces but also moderately rough and even wet surfaces for pick-and-place with high adhesion strength (> 2 atmospheres). In this work, associated physical mechanisms, SMP adhesive mechanics, and thermal conditions are studied. In particular, the numerical and experimental study elucidates that the optimal compositional and topological SMP design may substantially enhance its adhesion strength and reversibility, which leads to a strong grip force simultaneously with a minimized releasing force. Finally, the versatility and utility of the SMP adhesive gripper are highlighted through diverse pick-and-place demonstrations.
ER  - 

TY  - CONF
TI  - Multi-person Pose Tracking using Sequential Monte Carlo with Probabilistic Neural Pose Predictor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10024
EP  - 10030
AU  - M. Okada
AU  - S. Takenaka
AU  - T. Taniguchi
PY  - 2020
KW  - image matching
KW  - Monte Carlo methods
KW  - neural nets
KW  - pose estimation
KW  - probability
KW  - multiperson pose tracking
KW  - sequential Monte Carlo
KW  - probabilistic neural pose predictor
KW  - uncertainty-aware modeling
KW  - critical tracking errors
KW  - tracking scheme
KW  - multiple predictions
KW  - prediction errors
KW  - proposal distribution
KW  - epistemic uncertainty
KW  - heteroscedastic aleatoric uncertainty
KW  - neural modeling
KW  - MOTA score
KW  - PoseTrack2018 validation dataset
KW  - pose matching
KW  - time-sequence information
KW  - Uncertainty
KW  - Probabilistic logic
KW  - Adaptive optics
KW  - Optical imaging
KW  - Pose estimation
KW  - Optical sensors
KW  - Predictive models
DO  - 10.1109/ICRA40945.2020.9196509
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is an effective strategy for the multi-person pose tracking task in videos to employ prediction and pose matching in a frame-by-frame manner. For this type of approach, uncertainty-aware modeling is essential because precise prediction is impossible. However, previous studies have relied on only a single prediction without incorporating uncertainty, which can cause critical tracking errors if the prediction is unreliable. This paper proposes an extension to this approach with Sequential Monte Carlo (SMC). This naturally reformulates the tracking scheme to handle multiple predictions (or hypotheses) of poses, thereby mitigating the negative effect of prediction errors. An important component of SMC, i.e., a proposal distribution, is designed as a probabilistic neural pose predictor, which can propose diverse and plausible hypotheses by incorporating epistemic uncertainty and heteroscedastic aleatoric uncertainty. In addition, a recurrent architecture is introduced to our neural modeling to utilize time-sequence information of poses to manage difficult situations, such as the frequent disappearance and reappearances of poses. Compared to existing baselines, the proposed method achieves a state-of-the-art MOTA score on the PoseTrack2018 validation dataset by reducing approximately 50% of tracking errors from a state-of-the art baseline method.
ER  - 

TY  - CONF
TI  - 4D Generic Video Object Proposals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10031
EP  - 10037
AU  - A. O≈°ep
AU  - P. Voigtlaender
AU  - M. Weber
AU  - J. Luiten
AU  - B. Leibe
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image segmentation
KW  - object detection
KW  - stereo image processing
KW  - video signal processing
KW  - stereo video
KW  - 4D-GVT
KW  - data-driven object instance segmentation
KW  - neural networks
KW  - spatio-temporal object proposals
KW  - 4D generic video tubes
KW  - object categories
KW  - 4D generic video object proposals
KW  - Proposals
KW  - Electron tubes
KW  - Three-dimensional displays
KW  - Image segmentation
KW  - Video sequences
KW  - Motion segmentation
KW  - Probabilistic logic
DO  - 10.1109/ICRA40945.2020.9196949
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many high-level video understanding methods require input in the form of object proposals. Currently, such proposals are predominantly generated with the help of neural networks that were trained for detecting and segmenting a set of known object classes, which limits their applicability to cases where all objects of interest are represented in the training set. We propose an approach that can reliably extract spatio-temporal object proposals for both known and unknown object categories from stereo video. Our 4D Generic Video Tubes (4D-GVT) method combines motion cues, stereo data, and data-driven object instance segmentation in a probabilistic framework to compute a compact set of video-object proposals that precisely localizes object candidates and their contours in 3D space and time.
ER  - 

TY  - CONF
TI  - Simultaneous Tracking and Elasticity Parameter Estimation of Deformable Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10038
EP  - 10044
AU  - A. Sengupta
AU  - R. Lagneau
AU  - A. Krupa
AU  - E. Marchand
AU  - M. Marchal
PY  - 2020
KW  - deformation
KW  - finite element analysis
KW  - image colour analysis
KW  - manipulators
KW  - parameter estimation
KW  - robot vision
KW  - visual information
KW  - simulated object
KW  - elasticity parameter estimation
KW  - tracked object
KW  - soft objects
KW  - deformable object
KW  - simultaneous tracking
KW  - interactive finite element method simulations
KW  - RGB-D sensor
KW  - robotic manipulation
KW  - Strain
KW  - Elasticity
KW  - Estimation
KW  - Deformable models
KW  - Force measurement
KW  - Force
KW  - Force sensors
DO  - 10.1109/ICRA40945.2020.9196770
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a novel method to simultaneously track the deformation of soft objects and estimate their elasticity parameters. The tracking of the deformable object is performed by combining the visual information captured by a RGB-D sensor with interactive Finite Element Method simulations of the object. The visual information is more particularly used to distort the simulated object. In parallel, the elasticity parameter estimation minimizes the error between the tracked object and a simulated object deformed by the forces that are measured using a force sensor. Once the elasticity parameters are estimated, our tracking algorithm can be used to estimate the deformation forces applied to an object without the use of a force sensor. We validated our method on several soft objects with different shape complexities. Our evaluations show the ability of our method to estimate the elasticity parameters as well as its use to estimate the forces applied to a deformable object without any force sensor. These results open novel perspectives to better track and control deformable objects during robotic manipulations.
ER  - 

TY  - CONF
TI  - AVOT: Audio-Visual Object Tracking of Multiple Objects for Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10045
EP  - 10051
AU  - J. Wilson
AU  - M. C. Lin
PY  - 2020
KW  - audio signal processing
KW  - audio-visual systems
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - tracking
KW  - multiple objects
KW  - visually based trackers
KW  - object collisions
KW  - audio-visual object tracking neural network
KW  - tracking error
KW  - AVOT end
KW  - audio-visual inputs
KW  - visually based object detection
KW  - tracking methods
KW  - OpenCV object tracking implementations
KW  - deep learning method
KW  - audio-visual dataset
KW  - single-modality deep learning methods
KW  - audio onset
KW  - multimodal object tracking
KW  - state-of-the-art object tracking
KW  - Object tracking
KW  - Object detection
KW  - Visualization
KW  - Neural networks
KW  - Machine learning
KW  - Feature extraction
KW  - Streaming media
DO  - 10.1109/ICRA40945.2020.9197528
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing state-of-the-art object tracking can run into challenges when objects collide, occlude, or come close to one another. These visually based trackers may also fail to differentiate between objects with the same appearance but different materials. Existing methods may stop tracking or incorrectly start tracking another object. These failures are uneasy for trackers to recover from since they often use results from previous frames. By using audio of the impact sounds from object collisions, rolling, etc., our audio-visual object tracking (AVOT) neural network can reduce tracking error and drift. We train AVOT end to end and use audio-visual inputs over all frames. Our audio-based technique may be used in conjunction with other neural networks to augment visually based object detection and tracking methods. We evaluate its runtime frames-per-second (FPS) performance and intersection over union (IoU) performance against OpenCV object tracking implementations and a deep learning method. Our experiments, using the synthetic Sound-20K audio-visual dataset, demonstrate that AVOT outperforms single-modality deep learning methods, when there is audio from object collisions. A proposed scheduler network to switch between AVOT and other methods based on audio onset maximizes accuracy and performance over all frames in multimodal object tracking.
ER  - 

TY  - CONF
TI  - Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10052
EP  - 10058
AU  - G. Chen
AU  - S. Shen
AU  - L. Wen
AU  - S. Luo
AU  - L. Bo
PY  - 2020
KW  - agricultural robots
KW  - cameras
KW  - convolutional neural nets
KW  - distributed processing
KW  - farming
KW  - image filtering
KW  - image matching
KW  - image sensors
KW  - object detection
KW  - object tracking
KW  - robot vision
KW  - efficient pig counting
KW  - large-scale pig farming
KW  - automated pig counting method
KW  - pig movements
KW  - pig grouping houses
KW  - real-time automated pig counting system
KW  - pig detection algorithm
KW  - deformable pig shapes
KW  - pig body part
KW  - keypoints tracking
KW  - spatial-aware temporal response filtering
KW  - pig occlusion
KW  - pig overlapping
KW  - monocular fisheye camera
KW  - inspection robot
KW  - deep convolution neural network
KW  - keypoints association
KW  - efficient on-line tracking method
KW  - tracking failures
KW  - edge computing device
KW  - Cameras
KW  - Tracking
KW  - Robot vision systems
KW  - Inspection
KW  - Skeleton
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9197211
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Pig counting is a crucial task for large-scale pig farming. Pigs are usually visually counted by human. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. The existing works only focused on pig counting using single image, and its level of accuracy faced challenges due to pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the needs of pig counting for large pig grouping houses. Towards addressing these challenges, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it achieved performance superior to human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformable pig shapes. This detection included a deep convolution neural network (CNN) for pig body part keypoints detection and the keypoints association method to identify individual pigs. It then employed an efficient on-line tracking method to associate pigs across image frames. Finally, pig counts were estimated by a novel spatial-aware temporal response filtering (STRF) method to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.
ER  - 

TY  - CONF
TI  - 6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10059
EP  - 10066
AU  - C. Wang
AU  - R. Mart√≠n-Mart√≠n
AU  - D. Xu
AU  - J. Lv
AU  - C. Lu
AU  - L. Fei-Fei
AU  - S. Savarese
AU  - Y. Zhu
PY  - 2020
KW  - closed loop systems
KW  - image colour analysis
KW  - learning (artificial intelligence)
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - https://sites.google.com/view/6packtracking
KW  - physical robot
KW  - interframe motion
KW  - 3D keypoints
KW  - real time novel object instances
KW  - RGB-D data
KW  - NOCS category-level 6D pose estimation benchmark
KW  - keypoint matching
KW  - object instance
KW  - known object categories
KW  - deep learning approach
KW  - anchor-based keypoints
KW  - category-level 6D pose tracker
KW  - 6-PACK
KW  - simple vision-based closed-loop manipulation tasks
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Robustness
KW  - Robots
KW  - Real-time systems
KW  - Tracking
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9196679
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
ER  - 

TY  - CONF
TI  - Designing Ferromagnetic Soft Robots (FerroSoRo) with Level-Set-Based Multiphysics Topology Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10067
EP  - 10074
AU  - J. Tian
AU  - X. Zhao
AU  - X. D. Gu
AU  - S. Chen
PY  - 2020
KW  - actuators
KW  - deformation
KW  - design engineering
KW  - elastomers
KW  - grippers
KW  - optimisation
KW  - sensitivity analysis
KW  - topology
KW  - shape sensitivity analysis
KW  - gripper
KW  - flytrap structure
KW  - material layout
KW  - innovative structures
KW  - bionic medical devices
KW  - compliant actuators
KW  - level-set-based multiphysics topology optimization method
KW  - adjoint variable method
KW  - material time derivative
KW  - sub-objective function
KW  - architect ferromagnetic soft active structures
KW  - design domain
KW  - structural topology optimization
KW  - ferromagnetic soft elastomers
KW  - external magnetic field
KW  - shift morphology
KW  - soft elastomer matrix
KW  - ferromagnetic particles
KW  - flexible electronics
KW  - soft machines
KW  - external environmental stimulus
KW  - change configurations
KW  - flexible locomotion
KW  - soft active materials
KW  - FerroSoRo
KW  - designing ferromagnetic soft robots
KW  - Soft magnetic materials
KW  - Optimization
KW  - Topology
KW  - Magnetic domains
KW  - Magnetoacoustic effects
KW  - Level set
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197457
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft active materials can generate flexible locomotion and change configurations through large deformations when subjected to an external environmental stimulus. They can be engineered to design 'soft machines' such as soft robots, compliant actuators, flexible electronics, or bionic medical devices. By embedding ferromagnetic particles into soft elastomer matrix, the ferromagnetic soft matter can generate flexible movement and shift morphology in response to the external magnetic field. By taking advantage of this physical property, soft active structures undergoing desired motions can be generated by tailoring the layouts of the ferromagnetic soft elastomers. Structural topology optimization has emerged as an attractive tool to achieve innovative structures by optimizing the material layout within a design domain, and it can be utilized to architect ferromagnetic soft active structures. In this paper, the level-set-based topology optimization method is employed to design ferromagnetic soft robots (FerroSoRo). The objective function comprises a sub-objective function for the kinematics requirement and a sub-objective function for minimum compliance. Shape sensitivity analysis is derived using the material time derivative and adjoint variable method. Three examples, including a gripper, an actuator, and a flytrap structure, are studied to demonstrate the effectiveness of the proposed framework.
ER  - 

TY  - CONF
TI  - Exoskeleton-covered soft finger with vision-based proprioception and tactile sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10075
EP  - 10081
AU  - Y. She
AU  - S. Q. Liu
AU  - P. Yu
AU  - E. Adelson
PY  - 2020
KW  - actuators
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - grippers
KW  - intelligent sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - tactile sensors
KW  - high-resolution proprioceptive sensing
KW  - rich tactile sensing
KW  - highly underactuated exoskeleton
KW  - robotic gripper
KW  - tactile information
KW  - proprioception CNN
KW  - human finger proprioception
KW  - proprioceptive state
KW  - peripheral environment
KW  - vision-based proprioception
KW  - rigid-body robots
KW  - soft robots
KW  - accurate proprioception
KW  - elasticity
KW  - tactile sensor
KW  - previous GelSight sensing techniques
KW  - exoskeleton-covered soft finger
KW  - size 0.77 mm
KW  - Soft robotics
KW  - Cameras
KW  - Robot vision systems
KW  - Ink
DO  - 10.1109/ICRA40945.2020.9197369
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots offer significant advantages in adaptability, safety, and dexterity compared to conventional rigid-body robots. However, it is challenging to equip soft robots with accurate proprioception and tactile sensing due to their high flexibility and elasticity. In this work, we describe the development of a vision-based proprioceptive and tactile sensor for soft robots called GelFlex, which is inspired by previous GelSight sensing techniques. More specifically, we develop a novel exoskeleton-covered soft finger with embedded cameras and deep learning methods that enable high-resolution proprioceptive sensing and rich tactile sensing. To do so, we design features along the axial direction of the finger, which enable high-resolution proprioceptive sensing, and incorporate a reflective ink coating on the surface of the finger to enable rich tactile sensing. We design a highly underactuated exoskeleton with a tendon-driven mechanism to actuate the finger. Finally, we assemble 2 of the fingers together to form a robotic gripper and successfully perform a bar stock classification task, which requires both shape and tactile information. We train neural networks for proprioception and shape (box versus cylinder) classification using data from the embedded sensors. The proprioception CNN had over 99% accuracy on our testing set (all six joint angles were within 1¬∞ of error) and had an average accumulative distance error of 0.77 mm during live testing, which is better than human finger proprioception. These proposed techniques offer soft robots the high-level ability to simultaneously perceive their proprioceptive state and peripheral environment, providing potential solutions for soft robots to solve everyday manipulation tasks. We believe the methods developed in this work can be widely applied to different designs and applications.
ER  - 

TY  - CONF
TI  - Tuning the Energy Landscape of Soft Robots for Fast and Strong Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10082
EP  - 10088
AU  - J. Sun
AU  - B. Tighe
AU  - J. Zhao
PY  - 2020
KW  - actuators
KW  - deformation
KW  - design engineering
KW  - flexible manipulators
KW  - grippers
KW  - mobile robots
KW  - pneumatic actuators
KW  - energy landscape
KW  - soft body structures
KW  - fast motion
KW  - strong motion
KW  - soft module
KW  - soft bistable module
KW  - fast robots
KW  - strong soft robots
KW  - soft gripper
KW  - soft jumping robot
KW  - soft actuator
KW  - twisted-and-coiled actuator
KW  - Soft robotics
KW  - Springs
KW  - Potential energy
KW  - Shape
KW  - Actuators
KW  - Plastics
DO  - 10.1109/ICRA40945.2020.9196737
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robots demonstrate great potential compared with traditional rigid robots owing to their inherently soft body structures. Although researchers have made tremendous progress in recent years, existing soft robots are in general plagued by a main issue: slow speeds and small forces. In this work, we aim to address this issue by actively designing the energy landscape of the soft body: the total strain energy with respect to the robot's deformation. With such a strategy, a soft robot's dynamics can be tuned to have fast and strong motion. We introduce the general design principle using a soft module with two stable states that can rapidly switch from one state to the other under external forces. We characterize the required triggering (switching) force with respect to design parameters (e.g., the initial shape of the module). We then apply the soft bistable module to develop fast and strong soft robots, whose triggering forces are generated by a soft actuator - twisted-and-coiled actuator (TCA). We demonstrate a soft gripper that can hold weights more than 8 times its own weight, and a soft jumping robot that can jump more than 5 times its body height. We envision our strategies will overcome the weakness of soft robots to unleash their potential for diverse applications.
ER  - 

TY  - CONF
TI  - REBOund: Untethered Origami Jumping Robot with Controllable Jump Height
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10089
EP  - 10095
AU  - J. Carlson
AU  - J. Friedman
AU  - C. Kim
AU  - C. Sung
PY  - 2020
KW  - deformation
KW  - finite element analysis
KW  - legged locomotion
KW  - pneumatic actuators
KW  - shear modulus
KW  - springs (mechanical)
KW  - custom release mechanisms
KW  - quick compression
KW  - fold pattern
KW  - controllable jump height
KW  - jumping maneuvers
KW  - control strategies
KW  - robot body
KW  - model fold patterns
KW  - potential energy storage
KW  - parametric origami tessellation
KW  - face deformations
KW  - nonlinear spring
KW  - pseudorigid-body model
KW  - mechanical testing system
KW  - stored potential energy
KW  - reconfigurable expanding bistable origami pattern
KW  - untethered origami jumping robot
KW  - REBOund robot
DO  - 10.1109/ICRA40945.2020.9196534
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Origami robots are well-suited for jumping maneuvers because of their light weight and ability to incorporate actuation and control strategies directly into the robot body. However, existing origami robots often model fold patterns as rigidly foldable and fail to take advantage of deformation in an origami sheet for potential energy storage. In this paper, we consider a parametric origami tessellation, the Reconfigurable Expanding Bistable Origami (REBO) pattern, which leverages face deformations to act as a nonlinear spring. We present a pseudo-rigid-body model for the REBO for computing its energy stored when compressed to a given displacement and compare that model to experimental measurements taken on a mechanical testing system. This stored potential energy, when released quickly, can cause the pattern to jump. Using our model and experimental data, we design and fabricate a jumping robot, REBOund, that uses the spring-like REBO pattern as its body. Four lightweight servo motors with custom release mechanisms allow for quick compression and release of the origami pattern, allowing the fold pattern to jump over its own height even when carrying 5 times its own weight in electronics and power. We further demonstrate that small geometric changes to the pattern allow us to change the jump height without changing the actuation or control mechanism.
ER  - 

TY  - CONF
TI  - Motion Intensity Extraction Scheme for Simultaneous Recognition of Wrist/Hand Motions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10112
EP  - 10117
AU  - M. Kim
AU  - W. K. Chung
AU  - K. Kim
PY  - 2020
KW  - electromyography
KW  - gesture recognition
KW  - medical signal processing
KW  - muscle
KW  - pattern classification
KW  - sEMG signals
KW  - motion intensity feature
KW  - grasping motions
KW  - motion intensity extraction scheme
KW  - surface electromyography
KW  - muscular information representing gestures
KW  - sEMG-based motion recognition methods
KW  - Muscles
KW  - Crosstalk
KW  - Feature extraction
KW  - Wrist
KW  - Pattern recognition
KW  - Microsoft Windows
KW  - Electrodes
DO  - 10.1109/ICRA40945.2020.9197467
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surface electromyography contains muscular information representing gestures and corresponding forces. However, conventional sEMG-based motion recognition methods, such as pattern classification and regression, have intrinsic limitations due to the complex characteristics of sEMG signals. In this paper, motion intensity, a highly selective sEMG feature proportional to the level of muscle contraction, is proposed. The motion intensity feature allows proportional and simultaneous recognition of multiple degrees of freedom. The proposed method was demonstrated in terms of simultaneous recognition of wrist/hand motions. The result shows that the proposed method can successfully decompose sEMG signals into highly selective signals to target motions. In future works, the proposed method will be adapted for more subjects and to sEMG applications for practical evaluation considering various grasping motions.
ER  - 

TY  - CONF
TI  - Simultaneous Online Motion Discrimination and Evaluation of Whole-body Exercise by Synergy Probes for Home Rehabilitation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10118
EP  - 10124
AU  - F. M. Ramos
AU  - M. Hayashibe
PY  - 2020
KW  - biomechanics
KW  - image motion analysis
KW  - learning (artificial intelligence)
KW  - patient rehabilitation
KW  - synergy probe
KW  - whole-body exercise
KW  - simultaneous online motion discrimination
KW  - home rehabilitation sessions
KW  - reconstructed movement
KW  - online data
KW  - training data
KW  - Task analysis
KW  - Probes
KW  - Training data
KW  - Hidden Markov models
KW  - Torso
KW  - Real-time systems
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197232
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The development of algorithms for motion discrimination in home rehabilitation sessions poses numerous challenges. Recent studies have used the concept of synergies to discriminate a set of movements. However, the discrimination depends on the correlation of the reconstructed movement with the online data, and the training data requires well-defined movements. In this paper, we introduced the concept of a synergy probe, which makes a direct comparison between synergies and online data. The system represents synergies and movements in the same space and monitors their behavior. The results indicated that conventional methods are influenced by the segmentation of training data, and even though the reconstructed movement is similar to the ground-truth, it does not provide sufficient information to evaluate the data in real time. The synergy probes were used to discriminate and evaluate the performance of natural whole-body exercises without segmentation or previous determination of movements. An analysis of the results also demonstrated the possibility to identify the strategies used by the subjects for movement. Such information aids in gaining a better insight and can prove beneficial in home rehabilitation.
ER  - 

TY  - CONF
TI  - Validation of a Forward Kinematics Based Controller for a mobile Tethered Pelvic Assist Device to Augment Pelvic Forces during Walking
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10133
EP  - 10139
AU  - D. M. Stramel
AU  - S. K. Agrawal
PY  - 2020
KW  - force control
KW  - gait analysis
KW  - medical robotics
KW  - mobile robots
KW  - motion control
KW  - patient rehabilitation
KW  - robot kinematics
KW  - mobile TPAD frame
KW  - treadmill walking
KW  - motor control patterns
KW  - open loop controller
KW  - treadmill based robotic trainer
KW  - pelvic force augmentation
KW  - mobile tethered pelvic assist device
KW  - forward kinematics controller
KW  - mobile device controller
KW  - rehabilitation robotic devices
KW  - overground gait training robotic devices
KW  - Belts
KW  - Legged locomotion
KW  - Pelvis
KW  - Kinematics
KW  - Training
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196585
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - For those with irregular gait, re-calibration of motor control strategies and retraining of coordination are key goals. Thoughtful external forces or resistances during repetitive tasks can reprogram motor control patterns and strategies. Prior work in our lab has utilized this theory to improve gait in various patient groups using the Tethered Pelvic Assist Device (TPAD), a treadmill-based robotic trainer. In this paper, we propose a new, portable extension of the TPAD, which relies on an open-loop, forward kinematics based controller to remove the restriction of walking in the laboratory on a treadmill, and therefore accommodates overground ambulation. To evaluate the effects of this new control scheme and the effects of the users holding the mobile TPAD frame, a dataset of walking in four conditions was collected from eight healthy individuals. When applying a constant pelvic loading force of 10% body weight, the mean ground reaction force increased by 8.2¬±7.7% when the individual holds the walker frame and 11.1¬±7.8% when no hand contact is made. The mobile TPAD was shown to still induce a targeted loading on individuals during treadmill walking. The validation of this mobile device's controller and characterization of holding the frame allow overground studies to be conducted, and now opens the door to new training paradigms for overground gait training.
ER  - 

TY  - CONF
TI  - Model Learning for Control of a Paralyzed Human Arm with Functional Electrical Stimulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10148
EP  - 10154
AU  - D. N. Wolf
AU  - Z. A. Hall
AU  - E. M. Schearer
PY  - 2020
KW  - biomechanics
KW  - biomedical electrodes
KW  - feedback
KW  - feedforward
KW  - Gaussian processes
KW  - handicapped aids
KW  - learning (artificial intelligence)
KW  - medical control systems
KW  - medical robotics
KW  - neuromuscular stimulation
KW  - neurophysiology
KW  - patient rehabilitation
KW  - regression analysis
KW  - paralyzed human arm
KW  - functional electrical stimulation
KW  - restoring reaching ability
KW  - tetraplegia
KW  - shoulder-arm complex
KW  - full-arm 3D reaching tasks
KW  - Gaussian process regression model
KW  - feedforward-feedback control structure
KW  - paralyzed upper limb
KW  - FES-driven 3D reaching controller
KW  - Muscles
KW  - Wrist
KW  - Iron
KW  - Force
KW  - Robots
KW  - Ground penetrating radar
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9196992
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Functional electrical stimulation (FES) is a promising technique for restoring reaching ability to individuals with tetraplegia. To this point, the complexities of goal-directed reaching motions and the shoulder-arm complex have prevented the realization of this potential in full-arm 3D reaching tasks. We trained a Gaussian process regression model to form the basis of a feedforward-feedback control structure capable of achieving reaching motions with a paralyzed upper limb. Over a series of 95 reaches of at least 10 cm in length, the controller achieved an average accuracy (measured by the Euclidean distance of the wrist to the final target position) of 3.8 cm and an average error along the path of 3.5 cm. This controller is the first demonstration of an accurate, complete-arm, FES-driven 3D reaching controller to be implemented with an individual with tetraplegia.
ER  - 

TY  - CONF
TI  - Transient Behavior and Predictability in Manipulating Complex Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10155
EP  - 10161
AU  - R. Nayeem
AU  - S. Bazzi
AU  - N. Hogan
AU  - D. Sternad
PY  - 2020
KW  - feedback
KW  - feedforward
KW  - haptic interfaces
KW  - human-robot interaction
KW  - manipulator dynamics
KW  - motion control
KW  - virtual reality
KW  - robot control
KW  - internal dynamics
KW  - transient behavior
KW  - predictable dynamics
KW  - virtual object
KW  - robotic manipulandum
KW  - predictable steady state
KW  - feedforward controller
KW  - inverse dynamics
KW  - haptic feedback
KW  - Transient analysis
KW  - Robots
KW  - Steady-state
KW  - Task analysis
KW  - Dynamics
KW  - Force
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9196977
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Relatively little work in human and robot control has examined the control of underactuated objects with internal dynamics, such as transporting a cup of coffee, a task that presents little problems for humans. This study examined how humans move a `cup of coffee' with a view to identify principles that may be useful for robot control. The specific focus was on how humans choose initial conditions to safely reach a steady state. We hypothesized that subjects choose initial conditions that minimized the transient duration to reach the steady state faster, as it presented more predictable dynamics. In the experiment, the cup of coffee was reduced to a 2-D cup with a sliding ball inside which was simulated in a virtual environment. Human subjects interacted with this virtual object via a robotic manipulandum that provided haptic feedback. Participants moved the cup between two targets without losing the ball; they were instructed to explore different initial conditions before initiating the continuous interaction. Results showed that subjects converged to a small set of initial conditions that decreased their transient durations and achieved a predictable steady state faster. Simulations with a simple feedforward controller and inverse dynamics calculations confirmed that these initial conditions indeed led to shorter transients and less complex interaction forces. These results may inform robot control of objects with internal dynamics where the effects of initial conditions need further investigation.
ER  - 

TY  - CONF
TI  - A Variable-Fractional Order Admittance Controller for pHRI
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10162
EP  - 10168
AU  - D. Sirintuna
AU  - Y. Aydin
AU  - O. Caldiran
AU  - O. Tokatli
AU  - V. Patoglu
AU  - C. Basdogan
PY  - 2020
KW  - augmented reality
KW  - control engineering computing
KW  - drilling
KW  - groupware
KW  - human-robot interaction
KW  - industrial robots
KW  - production engineering computing
KW  - user interfaces
KW  - pHRI
KW  - labor intensive tasks
KW  - fractional order control
KW  - fractional order variable admittance controller
KW  - integer order variable admittance controller
KW  - realistic drilling task
KW  - transparent interaction
KW  - augmented reality headset
KW  - human sensory capabilities
KW  - variable-fractional order admittance controller
KW  - automation driven manufacturing environments
KW  - collaborative robots
KW  - augmented reality interfaces
KW  - production workflow
KW  - cognitive skills
KW  - physical human robot interaction
KW  - cobots
KW  - stability
KW  - drilling depth
KW  - Admittance
KW  - Task analysis
KW  - Robot sensing systems
KW  - Collaboration
KW  - Stability criteria
DO  - 10.1109/ICRA40945.2020.9197288
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In today's automation driven manufacturing environments, emerging technologies like cobots (collaborative robots) and augmented reality interfaces can help integrating humans into the production workflow to benefit from their adaptability and cognitive skills. In such settings, humans are expected to work with robots side by side and physically interact with them. However, the trade-off between stability and transparency is a core challenge in the presence of physical human robot interaction (pHRI). While stability is of utmost importance for safety, transparency is required for fully exploiting the precision and ability of robots in handling labor intensive tasks. In this work, we propose a new variable admittance controller based on fractional order control to handle this trade-off more effectively. We compared the performance of fractional order variable admittance controller with a classical admittance controller with fixed parameters as a baseline and an integer order variable admittance controller during a realistic drilling task. Our comparisons indicate that the proposed controller led to a more transparent interaction compared to the other controllers without sacrificing the stability. We also demonstrate a use case for an augmented reality (AR) headset which can augment human sensory capabilities for reaching a certain drilling depth otherwise not possible without changing the role of the robot as the decision maker.
ER  - 

TY  - CONF
TI  - Assistive Gym: A Physics Simulation Framework for Assistive Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10169
EP  - 10176
AU  - Z. Erickson
AU  - V. Gangaram
AU  - A. Kapusta
AU  - C. K. Liu
AU  - C. C. Kemp
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - robot programming
KW  - service robots
KW  - physics simulation framework
KW  - autonomous robots
KW  - physical interaction
KW  - physics simulations
KW  - physical assistance
KW  - open source physics
KW  - assistive robots
KW  - simulated environments
KW  - robotic manipulator
KW  - assistive gym models
KW  - commercial robots
KW  - assistive robotics research
KW  - ADL
KW  - activities of daily living
KW  - Task analysis
KW  - Manipulators
KW  - Physics
KW  - Tools
KW  - Human-robot interaction
KW  - Mobile robots
DO  - 10.1109/ICRA40945.2020.9197411
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research.
ER  - 

TY  - CONF
TI  - Learning Whole-Body Human-Robot Haptic Interaction in Social Contexts
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10177
EP  - 10183
AU  - J. Campbell
AU  - K. Yamane
PY  - 2020
KW  - control engineering computing
KW  - force sensors
KW  - haptic interfaces
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - telerobotics
KW  - whole-body human-robot haptic interaction
KW  - learning-from-demonstration framework
KW  - human-robot social interactions
KW  - human-robot contact
KW  - LfD framework
KW  - teleoperated bimanual robot
KW  - force sensors
KW  - Robot sensing systems
KW  - Haptic interfaces
KW  - Force
KW  - Spatiotemporal phenomena
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196933
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a learning-from-demonstration (LfD) framework for teaching human-robot social interactions that involve whole-body haptic interaction, i.e. direct human-robot contact over the full robot body. The performance of existing LfD frameworks suffers in such interactions due to the high dimensionality and spatiotemporal sparsity of the demonstration data. We show that by leveraging this sparsity, we can reduce the data dimensionality without incurring a significant accuracy penalty, and introduce three strategies for doing so. By combining these techniques with an LfD framework for learning multimodal human-robot interactions, we can model the spatiotemporal relationship between the tactile and kinesthetic information during whole-body haptic interactions. Using a teleoperated bimanual robot equipped with 61 force sensors, we experimentally demonstrate that a model trained with 121 sample hugs from 4 participants generalizes well to unseen inputs and human partners.
ER  - 

TY  - CONF
TI  - Human Preferences in Using Damping to Manage Singularities During Physical Human-Robot Collaboration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10184
EP  - 10190
AU  - M. G. Carmichael
AU  - R. Khonasty
AU  - S. Aldini
AU  - D. Liu
PY  - 2020
KW  - damping
KW  - human-robot interaction
KW  - manipulator kinematics
KW  - mobile robots
KW  - kinematic singularities
KW  - damping-based strategy
KW  - human operator
KW  - human preferences
KW  - physical human-robot collaboration
KW  - robot manipulator
KW  - kinematic singular configuration
KW  - double-blind A/B pairwise comparison testing protocol
KW  - singularities handling
KW  - Damping
KW  - Manipulators
KW  - Collaboration
KW  - Jacobian matrices
KW  - Kinematics
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197093
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When a robot manipulator approaches a kinematic singular configuration, control strategies need to be employed to ensure safe and robust operation. If this manipulator is being controlled by a human through physical human-robot collaboration, the choice of strategy for handling singularities can have a significant effect on the feelings and impressions of the user. To date the preferences of humans during physical human-robot collaboration regarding strategies for managing kinematic singularities have yet to be thoroughly explored.This work presents an empirical study of a damping-based strategy for handling singularities with regard to the preferences of the human operator. Two different parameters, damping rate and damping asymmetry, are tested using a double-blind A/B pairwise comparison testing protocol. Participants included two cohorts made up of the general public (n=51) and people working within a robotic research centre (n=18). In total 105 individual trials were performed. Results indicate a preference for a faster, asymmetric damping behavior that slows motions towards singularities whilst allowing for faster motions away.
ER  - 

TY  - CONF
TI  - MOCA-MAN: A MObile and reconfigurable Collaborative Robot Assistant for conjoined huMAN-robot actions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10191
EP  - 10197
AU  - W. Kim
AU  - P. Balatti
AU  - E. Lamon
AU  - A. Ajoudani
PY  - 2020
KW  - gesture recognition
KW  - human-robot interaction
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - multi-robot systems
KW  - mobile manipulators
KW  - supernumerary limbs
KW  - reconfiguration potential
KW  - MObile Collaborative robot Assistant
KW  - supernumerary body
KW  - MOCA-MAN
KW  - hand gesture recognition system
KW  - mobile base
KW  - long distance co-carrying tasks
KW  - manipulating tools
KW  - conjoined actions
KW  - performing heavy manipulation tasks
KW  - prolonged manipulation tasks
KW  - close-proximity manipulation
KW  - mobile robot assistant
KW  - reconfigurable collaborative robot assistant
KW  - conjoined huMAN-robot actions
KW  - collaborative robotic system
KW  - Admittance
KW  - Task analysis
KW  - Collaboration
KW  - Robot sensing systems
KW  - Clamps
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9197115
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The objective of this paper is to create a new collaborative robotic system that subsumes the advantages of mobile manipulators and supernumerary limbs. By exploiting the reconfiguration potential of a MObile Collaborative robot Assistant (MOCA), we create a collaborative robot that can function autonomously, in close proximity to humans, or be physically coupled to the human counterpart as a supernumerary body (MOCA-MAN). Through an admittance interface and a hand gesture recognition system, the controller can give higher priority to the mobile base (e.g., for long distance co-carrying tasks) or the arm movements (e.g., for manipulating tools), when performing conjoined actions. The resulting system has a high potential not only to reduce waste associated with the equipment waiting and setup times, but also to mitigate the human effort when performing heavy or prolonged manipulation tasks. The performance of the proposed system, i.e., MOCA-MAN, is evaluated by multiple subjects in two different use-case scenarios, which require large mobility or close-proximity manipulation.
ER  - 

TY  - CONF
TI  - Closing the Force Loop to Enhance Transparency in Time-delayed Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10198
EP  - 10204
AU  - R. Balachandran
AU  - J. -H. Ryu
AU  - M. Jorda
AU  - C. Ott
AU  - A. Albu-Schaeffer
PY  - 2020
KW  - delays
KW  - force control
KW  - human-robot interaction
KW  - stability
KW  - telerobotics
KW  - master-slave teleoperation system
KW  - bilateral controllers
KW  - force transparency
KW  - force loop
KW  - force control
KW  - time delayed teleoperation
KW  - KUKA lightweight robots
KW  - time domain passivity
KW  - Force
KW  - Iron
KW  - Force measurement
KW  - Robots
KW  - Stability analysis
KW  - Delays
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197420
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the present paper, we first adopt explicit force control from general robotics and embed it into teleoperation systems to enhance the transparency by reducing the effect of the perceived inertia to the human operator and simultaneously improve contact perception. To ensure stability of the proposed teleoperation system considering time-delays, we propose a sequential design procedure based on time domain passivity approach. Experimental results of master-slave teleoperation system, based on KUKA light-weight-robots, for different values of delays are presented. Comparative analysis is conducted considering two existing approaches, namely 2-channel and 4-channel architecture based bilateral controllers, and its results clearly indicate significant improvement in force transparency owing to the proposed method. The proposed system is finally validated considering a real industrial assembly scenario.
ER  - 

TY  - CONF
TI  - Evaluation of an Exoskeleton-based Bimanual Teleoperation Architecture with Independently Passivated Slave Devices
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10205
EP  - 10211
AU  - F. Porcini
AU  - D. Chiaradia
AU  - S. Marcheschi
AU  - M. Solazzi
AU  - A. Frisoli
PY  - 2020
KW  - delays
KW  - motion control
KW  - stability
KW  - telerobotics
KW  - simulated time delay
KW  - control loop frequency
KW  - multiDoFs devices
KW  - TDPA
KW  - time domain passivity approach
KW  - exoskeletal master
KW  - bimanual teleoperation system
KW  - communication delay
KW  - bilateral teleoperation
KW  - haptic feedback
KW  - robotic platforms
KW  - rescue robotics
KW  - independently passivated slave devices
KW  - exoskeleton-based bimanual teleoperation architecture
KW  - Exoskeletons
KW  - Task analysis
KW  - Computer architecture
KW  - Stability analysis
KW  - Robots
KW  - Delays
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9197079
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Search and rescue robotics is becoming a relevant topic in the last years and the growing number of robotic platforms and dedicated projects is the evidence of the interest in this area. In this context, the possibility to drive a remote robot with an exoskeleton is a promising strategy to enhance dexterity, reduce operator effort and save time. However, the use of haptic feedback (bilateral teleoperation) may lead to instability in the presence of communication delay and more complex is the case of bimanual teleoperation where the two arms can exchange energy. In this work, we present a bimanual teleoperation system based on an exoskeletal master, where multi-degrees of freedom (multi-DoFs) and kinematically different devices are involved. In the implemented architecture the two slaves are managed in parallel and independently passivated using the Time Domain Passivity Approach (TDPA) extended for multi-DoFs devices. To investigate the stability of the architecture we designed two tasks highly related to real disaster scenarios: the first one was useful to verify the system behavior in case of small movements and constrained configurations, whereas the second experiment was designed to involve larger contact forces and movements. Moreover, we compared the effect of both delay and low control loop frequency on the stability of the system when TDPA was applied. From the results, it was evident that the overall system exhibited a stable behavior with the use of the TDPA, even passivating the two slaves independently, under simulated time delay and in presence of a low control loop frequency.
ER  - 

TY  - CONF
TI  - Hand-worn Haptic Interface for Drone Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10212
EP  - 10218
AU  - M. Macchini
AU  - T. Havy
AU  - A. Weber
AU  - F. Schiano
AU  - D. Floreano
PY  - 2020
KW  - autonomous aerial vehicles
KW  - data gloves
KW  - human-robot interaction
KW  - mobile robots
KW  - motion control
KW  - robot vision
KW  - telerobotics
KW  - trajectory control
KW  - drone teleoperation
KW  - remote radio controllers
KW  - wearable interface
KW  - drone trajectory
KW  - hand motion
KW  - haptic system
KW  - robotic systems
KW  - teleoperation performance
KW  - remote controllers
KW  - human-robot interfaces
KW  - hand-worn haptic interface
KW  - data glove
KW  - line of sight
KW  - search-and-rescue missions
KW  - Haptic interfaces
KW  - Drones
KW  - Task analysis
KW  - Robot sensing systems
KW  - Hardware
DO  - 10.1109/ICRA40945.2020.9196664
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Drone teleoperation is usually accomplished using remote radio controllers, devices that can be hard to master for inexperienced users. Moreover, the limited amount of information fed back to the user about the robot's state, often limited to vision, can represent a bottleneck for operation in several conditions. In this work, we present a wearable interface for drone teleoperation and its evaluation through a user study. The two main features of the proposed system are a data glove to allow the user to control the drone trajectory by hand motion and a haptic system used to augment their awareness of the environment surrounding the robot. This interface can be employed for the operation of robotic systems in line of sight (LoS) by inexperienced operators and allows them to safely perform tasks common in inspection and search-and-rescue missions such as approaching walls and crossing narrow passages with limited visibility conditions. In addition to the design and implementation of the wearable interface, we performed a systematic study to assess the effectiveness of the system through three user studies (n = 36) to evaluate the users' learning path and their ability to perform tasks with limited visibility. We validated our ideas in both a simulated and a real-world environment. Our results demonstrate that the proposed system can improve teleoperation performance in different cases compared to standard remote controllers, making it a viable alternative to standard Human-Robot Interfaces.
ER  - 

TY  - CONF
TI  - Toward Human-like Teleoperated Robot Motion: Performance and Perception of a Choreography-inspired Method in Static and Dynamic Tasks for Rapid Pose Selection of Articulated Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10219
EP  - 10225
AU  - A. Bushman
AU  - M. Asselmeier
AU  - J. Won
AU  - A. LaViers
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - mobile robots
KW  - service robots
KW  - telerobotics
KW  - virtual reality
KW  - remotely-operated robot
KW  - remote telepresence
KW  - Baxter robot
KW  - Xbox One controller
KW  - JBJ
KW  - limb
KW  - multiple joints
KW  - successfully completed tasks
KW  - joint-by-joint method
KW  - choreography-inspired method
KW  - performance data
KW  - static tasks
KW  - RCC method
KW  - dynamic tasks
KW  - human-likeness
KW  - robotic motion
KW  - teleoperated robot motion
KW  - rapid pose selection
KW  - articulated robots
KW  - robot choreography center
KW  - Task analysis
KW  - Training
KW  - Dynamics
KW  - Joints
KW  - Robot motion
KW  - Manipulators
DO  - 10.1109/ICRA40945.2020.9196742
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In some applications, operators may want to create fluid, human-like motion on a remotely-operated robot, for example, a device used for remote telepresence. This paper examines two methods of controlling the pose of a Baxter robot via an Xbox One controller. The first method is a joint- by-joint (JBJ) method in which one joint of each limb is specified in sequence. The second method of control, named Robot Choreography Center (RCC), utilizes choreographic abstractions in order to simultaneously move multiple joints of the limb of the robot in a predictable manner. Thirty-eight users were asked to perform four tasks with each method. Success rate and duration of successfully completed tasks were used to analyze the performances of the participants. Analysis of the preferences of the users found that the joint-by-joint (JBJ) method was considered to be more precise, easier to use, safer, and more articulate, while the choreography-inspired (RCC) method of control was perceived as faster, more fluid, and more expressive. Moreover, performance data found that while both methods of control were over 80% successful for the two static tasks, the RCC method was an average of 11.85% more successful for the two more difficult, dynamic tasks. Future work will leverage this framework to investigate ideas of fluidity, expressivity, and human-likeness in robotic motion through online user studies with larger participant pools.
ER  - 

TY  - CONF
TI  - Helping Robots Learn: A Human-Robot Master-Apprentice Model Using Demonstrations via Virtual Reality Teleoperation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10226
EP  - 10233
AU  - J. DelPreto
AU  - J. I. Lipton
AU  - L. Sanneman
AU  - A. J. Fay
AU  - C. Fourie
AU  - C. Choi
AU  - D. Rus
PY  - 2020
KW  - control engineering computing
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - multi-robot systems
KW  - robot programming
KW  - telerobotics
KW  - virtual reality
KW  - grasping task
KW  - human perception
KW  - human-robot master-apprentice model
KW  - virtual reality teleoperation
KW  - artificial intelligence
KW  - self-supervised learning
KW  - Robots
KW  - Grasping
KW  - Task analysis
KW  - Three-dimensional displays
KW  - Solid modeling
KW  - Virtual reality
KW  - Pipelines
DO  - 10.1109/ICRA40945.2020.9196754
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As artificial intelligence becomes an increasingly prevalent method of enhancing robotic capabilities, it is important to consider effective ways to train these learning pipelines and to leverage human expertise. Working towards these goals, a master-apprentice model is presented and is evaluated during a grasping task for effectiveness and human perception. The apprenticeship model augments self-supervised learning with learning by demonstration, efficiently using the human's time and expertise while facilitating future scalability to supervision of multiple robots; the human provides demonstrations via virtual reality when the robot cannot complete the task autonomously. Experimental results indicate that the robot learns a grasping task with the apprenticeship model faster than with a solely self-supervised approach and with fewer human interventions than a solely demonstration-based approach; 100% grasping success is obtained after 150 grasps with 19 demonstrations. Preliminary user studies evaluating workload, usability, and effectiveness of the system yield promising results for system scalability and deployability. They also suggest a tendency for users to overestimate the robot's skill and to generalize its capabilities, especially as learning improves.
ER  - 

TY  - CONF
TI  - A Framework for Interactive Virtual Fixture Generation for Shared Teleoperation in Unstructured Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10234
EP  - 10241
AU  - V. Pruks
AU  - J. -H. Ryu
PY  - 2020
KW  - feature selection
KW  - haptic interfaces
KW  - telecontrol
KW  - virtual reality
KW  - unstructured environments
KW  - human operator performance
KW  - remote environment
KW  - task execution
KW  - user interface
KW  - camera images
KW  - interactive selection
KW  - feature selection
KW  - interactive virtual fixture generation
KW  - shared teleoperation
KW  - 6-DOF haptic feedback
KW  - Tools
KW  - Feature extraction
KW  - Task analysis
KW  - Robots
KW  - Detectors
KW  - Three-dimensional displays
KW  - Haptic interfaces
DO  - 10.1109/ICRA40945.2020.9196579
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Virtual fixtures (VFs) improve human operator performance in teleoperation scenarios. However, the generation of VFs is challenging, especially in unstructured environments. In this work, we introduce a framework for the interactive generation of VF. The method is based on the observation that a human can easily understand just by looking at the remote environment which VF could help in task execution. We propose a user interface that detects features on camera images and permits interactive selection of the features. We demonstrate how the feature selection can be used for designing VF, providing 6-DOF haptic feedback. In order to make the proposed framework more generally applicable to a wider variety of applications, we formalize the process of virtual fixture generation (VFG) into the specification of features, geometric primitives, and constraints. The framework can be extended further by the introduction of additional components. Through the human subject study, we demonstrate the proposed framework is intuitive, easy to use while effective, especially for performing hard contact tasks.
ER  - 

TY  - CONF
TI  - Local Obstacle-Skirting Path Planning for a Fast Bi-steerable Rover using B√©zier Curves
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10242
EP  - 10248
AU  - M. Fnadi
AU  - W. Du
AU  - R. Gomes da Silva
AU  - F. Plumet
AU  - F. Benamar
PY  - 2020
KW  - automatic guided vehicles
KW  - collision avoidance
KW  - curve fitting
KW  - mobile robots
KW  - navigation
KW  - off-road vehicles
KW  - predictive control
KW  - stability
KW  - steering systems
KW  - vehicle dynamics
KW  - local obstacle-skirting path planning
KW  - obstacle avoidance
KW  - off-road mobile robots
KW  - global reference path
KW  - smooth path
KW  - lateral stability
KW  - double-steering rover
KW  - online cubic Bezier curves
KW  - bi-steerable rover
KW  - constrained model predictive control
KW  - navigation
KW  - autonomous guided vehicles
KW  - Safety
KW  - Collision avoidance
KW  - Mobile robots
KW  - Lead
KW  - Path planning
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197563
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper focuses on local path planning for obstacle avoidance tasks dedicated to off-road mobile robots. This approach calculates a new local path for the vehicle using a set of cubic Bezier curves once the safety distance is not respected; otherwise, the vehicle follows the global reference path which is defined off-line. Two basic steps are used to determine this new path. Firstly, some significant points that should belong to the planned path are extracted on-line according to the obstacle's sizes and the current state of the vehicle, these points are approved as waypoints. Secondly, on-line cubic Bezier curves are computed to create a smooth path for these points such that the safety and lateral stability of the vehicle are ensured (i.e., preventing huge curvatures and wide-variation in steering angles). This path will be used as a reference to be performed by the vehicle using a constrained model predictive control. The validation of our navigation strategy is performed via numerical simulations and experiments using a fast double-steering rover.
ER  - 

TY  - CONF
TI  - Collision Avoidance with Proximity Servoing for Redundant Serial Robot Manipulators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10249
EP  - 10255
AU  - Y. Ding
AU  - U. Thomas
PY  - 2020
KW  - collision avoidance
KW  - human-robot interaction
KW  - manipulators
KW  - motion control
KW  - quadratic programming
KW  - repulsive motions
KW  - instantaneous optimal joint velocities
KW  - quadratic optimization problem
KW  - proximity sensing skins
KW  - collision avoidance
KW  - low-latency perception
KW  - proximity sensors
KW  - fastreacting motions
KW  - safe human-robot interaction
KW  - redundant serial robot manipulators
KW  - proximity servoing
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Task analysis
KW  - Manipulators
KW  - Skin
DO  - 10.1109/ICRA40945.2020.9196759
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Collision avoidance is a key technology towards safe human-robot interaction, especially on-line and fastreacting motions are required. Skins with proximity sensors mounted on the robot's outer shell provide an interesting approach to occlusion-free and low-latency perception. However, collision avoidance algorithms which make extensive use of these properties for fast-reacting motions have not yet been fully investigated. We present an improved collision avoidance algorithm for proximity sensing skins by formulating a quadratic optimization problem with inequality constraints to compute instantaneous optimal joint velocities. Compared to common repulsive force methods, our algorithm confines the approach velocity to obstacles and keeps motions pointing away from obstacles unrestricted. Since with repulsive motions the robot only moves in one direction, opposite to obstacles, our approach has better exploitation of the redundancy space to maintain the task motion and gets stuck less likely in local minima. Furthermore, our method incorporates an active behaviour for avoiding obstacles and evaluates all potentially colliding obstacles for the whole arm, rather than just the single nearest obstacle. We demonstrate the effectiveness of our method with simulations and on real robot manipulators in comparison with commonly used repulsive force methods and our prior proposed approach.
ER  - 

TY  - CONF
TI  - Predicting Obstacle Footprints from 2D Occupancy Maps by Learning from Physical Interactions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10256
EP  - 10262
AU  - M. Kollmitz
AU  - D. B√ºscher
AU  - W. Burgard
PY  - 2020
KW  - collision avoidance
KW  - convolutional neural nets
KW  - laser ranging
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - indoor robot localization
KW  - obstacle avoidance
KW  - laser scanners
KW  - collision events
KW  - 2D occupancy maps
KW  - obstacle footprint prediction
KW  - physical interaction learning
KW  - horizontal scanning 2D laser range finders
KW  - convolutional neural network
KW  - Two dimensional displays
KW  - Collision avoidance
KW  - Image segmentation
KW  - Training
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197474
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Horizontally scanning 2D laser rangefinders are a popular approach for indoor robot localization because of the high accuracy of the sensors and the compactness of the required 2D maps. As the scanners in this configuration only provide information about one slice of the environment, the measurements typically do not capture the full extent of a large variety of obstacles, including chairs or tables. Accordingly, obstacle avoidance based on laser scanners mounted in such a fashion is likely to fail. In this paper, we propose a learning-based approach to predict collisions in 2D occupancy maps. Our approach is based on a convolutional neural network which is trained on a 2D occupancy map and collision events recorded with a bumper while the robot is navigating in its environment. As the network operates on local structures only, it can generalize to new environments. In addition, the robot can collect and integrate new collision examples after an initial training phase. Extensive experiments carried out in simulation and a realistic real-world environment confirm that our approach allows robots to learn from collision events to avoid collisions in the future.
ER  - 

TY  - CONF
TI  - Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10263
EP  - 10269
AU  - S. Eiffert
AU  - H. Kong
AU  - N. Pirmarzdashti
AU  - S. Sukkarieh
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - Monte Carlo methods
KW  - recurrent neural nets
KW  - tree searching
KW  - Monte Carlo tree search
KW  - generative recurrent neural networks
KW  - integrated path
KW  - motion models
KW  - traffic
KW  - robotic path planning
KW  - dynamic environments
KW  - effective path planning
KW  - motion prediction accuracy
KW  - planned robotic actions
KW  - generative RNNs
KW  - action space
KW  - crowd dynamics
KW  - social response
KW  - learnt model
KW  - Robots
KW  - Predictive models
KW  - Path planning
KW  - Decoding
KW  - Collision avoidance
KW  - Training
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196631
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.
ER  - 

TY  - CONF
TI  - Safety-Critical Rapid Aerial Exploration of Unknown Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10270
EP  - 10276
AU  - A. Singletary
AU  - T. Gurriet
AU  - P. Nilsson
AU  - A. D. Ames
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - helicopters
KW  - mobile robots
KW  - sensors
KW  - uncertain systems
KW  - high-speed flight
KW  - uncertain environments
KW  - controller level
KW  - state uncertainty
KW  - nonlinear system dynamics
KW  - high-fidelity simulation
KW  - cave environment
KW  - safety-critical rapid aerial exploration
KW  - collision avoidance
KW  - aerial vehicles
KW  - unknown environments
KW  - quadrotor
KW  - onboard sensors
KW  - Safety
KW  - Collision avoidance
KW  - Three-dimensional displays
KW  - Drones
KW  - Trajectory
KW  - Vehicle dynamics
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197416
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper details a novel approach to collision avoidance for aerial vehicles that enables high-speed flight in uncertain environments. This framework is applied at the controller level and provides safety regardless of the planner that is used. The method is shown to be robust to state uncertainty and disturbances, and is computed entirely online utilizing the full nonlinear system dynamics. The effectiveness of this method is shown in a high-fidelity simulation of a quadrotor with onboard sensors rapidly and safely exploring a cave environment utilizing a simple planner.
ER  - 

TY  - CONF
TI  - Reconfigurable Magnetic Microswarm for Thrombolysis under Ultrasound Imaging
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10285
EP  - 10291
AU  - Q. Wang
AU  - B. Wang
AU  - J. Yu
AU  - K. Schweizer
AU  - B. J. Nelson
AU  - L. Zhang
PY  - 2020
KW  - biochemistry
KW  - biological tissues
KW  - biomedical materials
KW  - biomedical ultrasonics
KW  - blood
KW  - magnetic particles
KW  - microrobots
KW  - nanomedicine
KW  - nanoparticles
KW  - patient treatment
KW  - reconfigurable magnetic microswarm
KW  - thrombolysis
KW  - ultrasound imaging
KW  - magnetic nanoparticle microswarm
KW  - tissue plasminogen activator
KW  - oscillating magnetic field
KW  - aspect ratio
KW  - out-of-plane fluid convection
KW  - lysis rate
KW  - microswarm-induced fluid convection
KW  - Ultrasonic imaging
KW  - Magnetic resonance imaging
KW  - Convection
KW  - Coagulation
KW  - Coils
KW  - Micro/nanorobot
KW  - magnetic control
KW  - collective behavior
KW  - thrombolysis
KW  - ultrasound imaging
DO  - 10.1109/ICRA40945.2020.9197432
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose thrombolysis using a magnetic nanoparticle microswarm with tissue plasminogen activator (tPA) under ultrasound imaging. The microswarm is generated in blood using an oscillating magnetic field and can be navigated with locomotion along both the long and short axis. By modulating the input field, the aspect ratio of the microswarm can be reversibly tuned, showing the ability to adapt to different confined environments. Simulation results indicate that both in-plane and out-of-plane fluid convection are induced around the microswarm, which can be further enhanced by tuning the aspect ratio of the microswarm. Under ultrasound imaging, the microswarm is navigated in a microchannel towards a blood clot and deformed to obtain optimal lysis. Experimental results show that the lysis rate reaches -0.1725 ¬± 0.0612 mm3/min in the 37¬∞C blood environment under the influence of the microswarm-induced fluid convection and tPA. The lysis rate is enhanced 2.5-fold compared to that without the microswarm (-0.0681 ¬± 0.0263 mm3/min). Our method provides a new strategy to increase the efficiency of thrombolysis by applying microswarm-induced fluid convection, indicating that swarming micro/nanorobots have the potential to act as effective tools towards targeted therapy.
ER  - 

TY  - CONF
TI  - Improving Optical Micromanipulation with Force-Feedback Bilateral Coupling
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10292
EP  - 10298
AU  - E. Gerena
AU  - F. Legendre
AU  - Y. Vitry
AU  - S. R√©gnier
AU  - S. Haliyo
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - micromanipulators
KW  - radiation pressure
KW  - robot vision
KW  - telerobotics
KW  - haptic device
KW  - transparent force feedback
KW  - user dexterity
KW  - microsized shapes
KW  - contact forces
KW  - optical micromanipulation
KW  - force-feedback bilateral coupling
KW  - interactive approaches
KW  - visual feedback
KW  - haptic feedback teleoperation systems
KW  - optical tweezers platform
KW  - 2D image
KW  - Optical feedback
KW  - Optical sensors
KW  - Force
KW  - Haptic interfaces
KW  - Optical imaging
KW  - Three-dimensional displays
KW  - High-speed optical techniques
DO  - 10.1109/ICRA40945.2020.9197424
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Micromanipulation is challenging due to the specific physical effects governing the microworld. Interactive approaches using only visual feedback are limited to the 2D image of the microscope, and have forcibly lower bandwidth. Recently, haptic feedback teleoperation systems have been developed to try to overcome those difficulties. This paper explores the case of an optical tweezers platform coupled to an haptic device providing transparent force feedback. The impact of haptic feedback regarding user dexterity on tactile exploration tasks is studied using 3 Œºm microbeads and a test bench with micro sized shapes. The results reveal a consistent improvement in both users' trajectory tracking and their control of the contact forces. This also validates the experimental setup which performed reliably on 140 different trials of the evaluation.
ER  - 

TY  - CONF
TI  - Maneuver at Micro Scale: Steering by Actuation Frequency Control in Micro Bristle Robots*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10299
EP  - 10304
AU  - Z. Hao
AU  - D. Kim
AU  - A. R. Mohazab
AU  - A. Ansari
PY  - 2020
KW  - microactuators
KW  - microrobots
KW  - piezoelectric actuators
KW  - steering systems
KW  - vibrations
KW  - resonance-based steering mechanism
KW  - differential steering
KW  - on-board piezoelectric actuator
KW  - frequency-controlled locomotion
KW  - steering mechanism
KW  - microbristle robots
KW  - actuation frequency control
KW  - principal actuation frequency components
KW  - size 6.0 mm
KW  - size 400.0 mum
KW  - size 12.0 mm
KW  - size 8.0 mm
KW  - Robots
KW  - Resonant frequency
KW  - Actuators
KW  - Vibrations
KW  - Steady-state
KW  - Wires
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196694
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel steering mechanism, which leads to frequency-controlled locomotion demonstrated for the first time in micro bristle robots. The miniaturized robots are 3D-printed, 12 mm √ó 8 mm √ó 6 mm in size, with bristle feature sizes down to 400 Œºm. The robots can be steered by utilizing the distinct resonance behaviors of the asymmetrical bristle sets. The left and right sets of the bristles have different diameters, and thus different stiffnesses and resonant frequencies. The unique response of each bristle side to the vertical vibrations of a single on-board piezoelectric actuator causes differential steering of the robot. The robot can be modeled as two coupled uniform bristle robots, representing the left and the right sides. At distinct frequencies, the robots can move in all four principal directions: forward, backward, left and right. Furthermore, the full 360¬∞ 2D plane can be covered by superimposing the principal actuation frequency components with desired amplitudes. In addition to miniaturized robots, the presented resonance-based steering mechanism can be applied over multiple scales and to other mechanical systems.
ER  - 

TY  - CONF
TI  - Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10305
EP  - 10311
AU  - K. Jayaram
AU  - J. Shum
AU  - S. Castellanos
AU  - E. F. Helbling
AU  - R. J. Wood
PY  - 2020
KW  - gait analysis
KW  - legged locomotion
KW  - microrobots
KW  - motion control
KW  - insect-size microrobot
KW  - mechanically dexterous legged robot
KW  - HAMR-Jr's open-loop locomotion
KW  - HAMR-VI microrobot
KW  - design process
KW  - fabrication process
KW  - independently actuated degrees of freedom
KW  - mass 320.0 mg
KW  - frequency 1.0 Hz to 200.0 Hz
KW  - size 22.5 mm
KW  - Legged locomotion
KW  - Actuators
KW  - Resonant frequency
KW  - Heat-assisted magnetic recording
KW  - Fabrication
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197436
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Here we present HAMR-Jr, a 22.5mm, 320mg quadrupedal microrobot. With eight independently actuated degrees of freedom, HAMR-Jr is, to our knowledge, the most mechanically dexterous legged robot at its scale and is capable of high-speed locomotion (13.91bodylengthss-1) at a variety of stride frequencies (1-200Hz) using multiple gaits. We achieved this using a design and fabrication process that is flexible, allowing scaling with minimum changes to our workflow. We further characterized HAMR-Jr's open-loop locomotion and compared it with the larger scale HAMR-VI microrobot to demonstrate the effectiveness of scaling laws in predicting running performance.
ER  - 

TY  - CONF
TI  - Reality as a simulation of reality: robot illusions, fundamental limits, and a physical demonstration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10327
EP  - 10334
AU  - D. A. Shell
AU  - J. M. O‚ÄôKane
PY  - 2020
KW  - collision avoidance
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - fundamental limits
KW  - physical demonstration
KW  - robot behavior
KW  - potential mismatches
KW  - convincing illusion
KW  - system simulation
KW  - simulated systems
KW  - simple multirobot experiment
KW  - robot navigating
KW  - robot illusions
KW  - Robot sensing systems
KW  - Software
KW  - Sensor systems
KW  - Mobile robots
KW  - Emulation
DO  - 10.1109/ICRA40945.2020.9196761
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider problems in which robots conspire to present a view of the world that differs from reality. The inquiry is motivated by the problem of validating robot behavior physically despite there being a discrepancy between the robots we have at hand and those we wish to study, or the environment for testing that is available versus that which is desired, or other potential mismatches in this vein. After formulating the concept of a convincing illusion, essentially a notion of system simulation that takes place in the real world, we examine the implications of this type of simulability in terms of infrastructure requirements. Time is one important resource: some robots may be able to simulate some others but, perhaps, only at a rate that is slower than real-time. This difference gives a way of relating the simulating and the simulated systems in a form that is relative. We establish some theorems, including one with the flavor of an impossibility result, and providing several examples throughout. Finally, we present data from a simple multi-robot experiment based on this theory, with a robot navigating amid an unbounded field of obstacles."Truth is beautiful, without doubt; but so are lies."-Ralph Waldo Emerson.
ER  - 

TY  - CONF
TI  - Finding Missing Skills for High-Level Behaviors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10335
EP  - 10341
AU  - A. Pacheck
AU  - S. Moarref
AU  - H. Kress-Gazit
PY  - 2020
KW  - robots
KW  - temporal logic
KW  - KUKA IIWA arm
KW  - Baxter robot
KW  - LTL specifications
KW  - correct-by-construction robot control
KW  - LTL synthesis
KW  - high-level robot tasks
KW  - linear temporal logic
KW  - Task analysis
KW  - Maintenance engineering
KW  - Games
KW  - Robot control
KW  - Manipulators
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9197223
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Recently, Linear Temporal Logic (LTL) has been used as a formalism for defining high-level robot tasks, and LTL synthesis has been used to automatically create correct-by-construction robot control. The underlying premise of this approach is that the robot has a set of actions, or skills, that can be composed to achieve the high- level task. In this paper we consider LTL specifications that cannot be synthesized into robot control due to lack of appropriate skills; we present algorithms for automatically suggesting new or modified skills for the robot that will guarantee the task will be achieved. We demonstrate our approach with a physical Baxter robot and a simulated KUKA IIWA arm.
ER  - 

TY  - CONF
TI  - Near-Optimal Reactive Synthesis Incorporating Runtime Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10342
EP  - 10348
AU  - S. Bharadwaj
AU  - A. P. Vinod
AU  - R. Dimitrova
AU  - U. Topcu
PY  - 2020
KW  - control system synthesis
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - suboptimal control
KW  - mission specification
KW  - dynamic environment
KW  - performance metric
KW  - task-critical information
KW  - strategy synthesis
KW  - time-varying information
KW  - online re-synthesis
KW  - pre-specified representative information scenarios
KW  - performance suboptimality
KW  - runtime information
KW  - near-optimal reactive synthesis
KW  - switching mechanism
KW  - robotic motion planning
KW  - Runtime
KW  - Switches
KW  - Games
KW  - Robots
KW  - Measurement
KW  - Safety
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196581
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of optimal reactive synthesis - compute a strategy that satisfies a mission specification in a dynamic environment, and optimizes a given performance metric. We incorporate task-critical information, that is only available at runtime, into the strategy synthesis in order to improve performance. Existing approaches to utilising such time-varying information require online re-synthesis, which is not computationally feasible in real-time applications. In this paper, we presynthesize a set of strategies corresponding to candidate instantiations (pre-specified representative information scenarios). We then propose a novel switching mechanism to dynamically switch between the strategies at runtime while guaranteeing all safety and liveness goals are met. We also characterize bounds on the performance suboptimality. We demonstrate our approach on two examples - robotic motion planning where the likelihood of the position of the robot's goal is updated in real-time, and an air traffic management problem for urban air mobility.
ER  - 

TY  - CONF
TI  - Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10349
EP  - 10355
AU  - A. K. Bozkurt
AU  - Y. Wang
AU  - M. M. Zavlanos
AU  - M. Pajic
PY  - 2020
KW  - control system synthesis
KW  - decision theory
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - mobile robots
KW  - path planning
KW  - probability
KW  - temporal logic
KW  - motion planning
KW  - MDP
KW  - RL-based synthesis approach
KW  - discount factors
KW  - model-free RL algorithm
KW  - total discounted reward
KW  - optimal policy
KW  - transition probabilities
KW  - LTL formula
KW  - Markov decision process
KW  - unknown stochastic environment
KW  - control policy synthesis
KW  - reinforcement learning frame-work
KW  - model-free reinforcement learning
KW  - linear temporal logic specifications
KW  - control synthesis
KW  - Learning (artificial intelligence)
KW  - Automata
KW  - Planning
KW  - Markov processes
KW  - Computational modeling
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196796
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a reinforcement learning (RL) frame-work to synthesize a control policy from a given linear temporal logic (LTL) specification in an unknown stochastic environment that can be modeled as a Markov Decision Process (MDP). Specifically, we learn a policy that maximizes the probability of satisfying the LTL formula without learning the transition probabilities. We introduce a novel rewarding and discounting mechanism based on the LTL formula such that (i) an optimal policy maximizing the total discounted reward effectively maximizes the probabilities of satisfying LTL objectives, and (ii) a model-free RL algorithm using these rewards and discount factors is guaranteed to converge to such a policy. Finally, we illustrate the applicability of our RL-based synthesis approach on two motion planning case studies.
ER  - 

TY  - CONF
TI  - R-Min: a Fast Collaborative Underactuated Parallel Robot for Pick-and-Place Operations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10365
EP  - 10371
AU  - G. Jeanneau
AU  - V. B√©goc
AU  - S. Briot
AU  - A. Goldsztejn
PY  - 2020
KW  - collision avoidance
KW  - end effectors
KW  - human-robot interaction
KW  - springs (mechanical)
KW  - trajectory control
KW  - parallel manipulator
KW  - pick-and-place operations
KW  - human operator
KW  - acceleration
KW  - planar five-bar mechanism
KW  - passive joints
KW  - planar seven-bar mechanism
KW  - supplementary passive leg
KW  - collaborative parallel robot
KW  - pick-and-place trajectory
KW  - R-Min
KW  - collaborative underactuated parallel robot
KW  - tension spring
KW  - end-effector
KW  - degrees of freedom
KW  - impact force
KW  - Collision avoidance
KW  - Collaboration
KW  - Prototypes
KW  - Parallel robots
KW  - Robot sensing systems
KW  - Springs
DO  - 10.1109/ICRA40945.2020.9196990
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces an intrinsically safe parallel manipulator dedicated to fast pick-and-place operations, called R-Min. It has been designed to reduce the risk of injury during a collision with a human operator, while maintaining high speed and acceleration. The proposed architecture is based on a modification of the well-known planar five-bar mechanism, where additional passive joints are introduced to the distal links in order to create a planar seven-bar mechanism with two degrees of underactuation, so that it can passively reconfigure in case of collision. A supplementary passive leg, in which a tension spring is mounted, is added between the base and the end-effector in order to constrain the additional degrees of freedom. A prototype of this new collaborative parallel robot is designed and its equilibrium configurations under several types of loadings are analyzed. Its dynamics is also studied. We analyze the impact force occurring during a collision between our prototype and the head of an operator and compare these results with those that would have been obtained with a rigid five-bar mechanism. Simulation results of impact during a standard pick-and-place trajectory of duration 0.3 s show that a regular five-bar mechanism would injure a human, while our robot would avoid the trauma.
ER  - 

TY  - CONF
TI  - High-Flexibility Locomotion and Whole-Torso Control for a Wheel-Legged Robot on Challenging Terrain*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10372
EP  - 10377
AU  - K. Xu
AU  - S. Wang
AU  - X. Wang
AU  - J. Wang
AU  - Z. Chen
AU  - D. Liu
PY  - 2020
KW  - mobile robots
KW  - motion control
KW  - robot kinematics
KW  - wheels
KW  - high-flexibility locomotion
KW  - whole-torso control
KW  - challenging terrain
KW  - six-wheel-legged robot
KW  - irregular terrain
KW  - heavy-duty work
KW  - Stewart platforms
KW  - wheels
KW  - diverse degrees
KW  - traversability
KW  - rough terrain
KW  - sand-gravel terrain
KW  - parallel suspension system
KW  - Legged locomotion
KW  - Wheels
KW  - Torso
KW  - Force
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197526
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a parallel six-wheel-legged robot that can traverse irregular terrain while carrying objectives to do heavy-duty work. This robot is equipped with six Stewart platforms as legs and tightly integrates the additional degrees of freedom introduced by the wheels. The presented control strategy with physical system used to adapt the diverse degrees of each leg to irregular terrain such that robot increases the traversability, and simultaneously to maintain the horizontal whole-torso pose. This strategy makes use of Contact Scheduler (CS) and Whole-Torso Control (WTC) to control the multiple degrees of freedom (DOF) leg for performing high-flexibility locomotion and adapting the rough terrain like actively parallel suspension system. We conducted experiments on flat, slope, soft and sand-gravel surface, which validate the proposed control method and physical system. Especially, we attempt to traverse over sand-gravel terrain with 3 people about 240kg payload.
ER  - 

TY  - CONF
TI  - The Prince‚Äôs tears, a large cable-driven parallel robot for an artistic exhibition
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10378
EP  - 10383
AU  - J. -P. Merlet
AU  - Y. Papegay
AU  - A. -V. Gasc
PY  - 2020
KW  - exhibitions
KW  - mobile robots
KW  - trajectory control
KW  - cable length estimation
KW  - robot position
KW  - glass powder
KW  - cable-driven parallel robot
KW  - artistic exhibition
KW  - positioning control
KW  - CDPR geometry
KW  - exhibition place
KW  - Prince tears
KW  - time 3.0 d
KW  - time 174.0 hour
KW  - time 32.0 d
KW  - mass 1.5 ton
KW  - Meters
KW  - Length measurement
KW  - Winches
KW  - Powders
KW  - Kinematics
KW  - Robot kinematics
KW  - cable-driven parallel robot
KW  - kinematics
KW  - art
DO  - 10.1109/ICRA40945.2020.9197011
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the development and results of a large 3 d.o.f cable-driven parallel robot (CDPR) that has been extensively used between June and August 2019 for an artistic exhibition. The purpose of the exhibition was to 3D print a wall of glass powder, which will slowly collapse after the deposit of each layer. Positioning control on the assigned trajectory was an issue because of the CDPR geometry imposed by the specific configuration of the exhibition place. We describe how this problem was solved using a combination of cable length estimation based on the winch rotation measured by encoder, together with 3 on-board lidars that were used to provide a measure of the robot position. To the best of our knowledge this is the first time that such method was used for controlling a large CDPR. This CDPR has run for 174 hours since 6/18/2019, averaging a run time of 4h15mn per day. The 3D printing of the wall started on 7/18/2019 and stops on 8/31/2019. During this period the robot was used for 32 days with an average of 2h18mn run-time per day. The robot has traveled on a total distance of 4757 meters, of which 3893 meters on the assigned trajectory. During the period 76 layers have been deposited, representing a mass of 1.5 tons of glass powder.
ER  - 

TY  - CONF
TI  - Singularity analysis and reconfiguration mode of the 3-CRS parallel manipulator
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10384
EP  - 10390
AU  - C. Bouzgarrou
AU  - A. Koessler
AU  - N. Bouton
PY  - 2020
KW  - manipulator kinematics
KW  - reconfiguration mode
KW  - 3-CRS parallel manipulator
KW  - original parallel mechanism
KW  - motorized cylindrical joint
KW  - parallel robotics community
KW  - dimensional synthesis
KW  - geometric approach
KW  - relative geometric configurations
KW  - singularity analysis problem
KW  - Manipulators
KW  - Kinematics
KW  - Transmission line matrix methods
KW  - Force
KW  - Geometry
KW  - Prototypes
DO  - 10.1109/ICRA40945.2020.9197337
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The 3-CRS manipulator is an original parallel mechanism having 6 degrees of freedom (DOFs) with only 3 limbs. This mechanism uses a motorized cylindrical joint per limb. This new paradigm of actuation opens research fields on new families of robots that should particularly interest the parallel robotics community. According to its dimensional synthesis, this mechanism can have remarkable kinematic properties such as a large orientation workspace or reconfiguration capabilities. In this paper, we introduce this mechanism and we study its singularities by using a geometric approach. This approach simplifies considerably singularity analysis problem by considering the relative geometric configurations of three planes defined by the distal links of the limbs. Thanks to that, a reconfiguration mode of the 3-CRS, that doubles its reachable workspace, is highlighted. This property is illustrated on a physical prototype of the robot.
ER  - 

TY  - CONF
TI  - Trajectory optimization for a class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10391
EP  - 10397
AU  - N. Kumar
AU  - S. Coros
PY  - 2020
KW  - actuators
KW  - collision avoidance
KW  - end effectors
KW  - manipulator kinematics
KW  - mobile robots
KW  - multi-robot systems
KW  - optimisation
KW  - position control
KW  - constrained collaborative mobile agents family
KW  - ground mobile bases
KW  - mobile robots
KW  - closed-loop kinematic chains
KW  - revolute joints
KW  - closed- loop kinematic chains
KW  - standalone trajectory optimization method
KW  - CCMA system
KW  - fixed design parameters
KW  - control policy optimization
KW  - manipulation capabilities
KW  - tracked mobile bases
KW  - Kinematics
KW  - Mobile robots
KW  - Trajectory optimization
KW  - Mobile agents
KW  - Task analysis
KW  - Parallel Robots
KW  - Optimization and Optimal Control
KW  - Multi-Robot Systems
DO  - 10.1109/ICRA40945.2020.9197048
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family which consists of ground mobile bases with non-holonomic constraints. Moreover, these mobile robots are constrained by closed-loop kinematic chains consisting of revolute joints which can be either passive or actuated. We also describe a novel trajectory optimization method which is general with respect to number of mobile robots, topology of the closed- loop kinematic chains and placement of the actuators at the revolute joints. We also extend the standalone trajectory optimization method to optimize concurrently the design parameters and the control policy. We describe various CCMA system examples, in simulation, differing in design, topology, number of mobile robots and actuation space. The simulation results for standalone trajectory optimization with fixed design parameters is presented for CCMA system examples. We also show how this method can be used for tasks other than end-effector positioning such as internal collision avoidance and external obstacle avoidance. The concurrent design and control policy optimization is demonstrated, in simulations, to increase the CCMA system workspace and manipulation capabilities. Finally, the trajectory optimization method is validated in experiments through two 4-DOF prototypes consisting of 3 tracked mobile bases.
ER  - 

TY  - CONF
TI  - Development of Body Rotational Wheeled Robot and its Verification of Effectiveness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10405
EP  - 10411
AU  - B. -S. Sim
AU  - K. -J. Kim
AU  - K. -H. Yu
PY  - 2020
KW  - collision avoidance
KW  - friction
KW  - mobile robots
KW  - traction
KW  - wheels
KW  - step-obstacle climbing
KW  - body rotational wheeled robot
KW  - scattered obstacles
KW  - driving environment
KW  - step-type obstacle
KW  - main body rotation mechanism
KW  - robot wheels
KW  - wheel-drive robot
KW  - body mass
KW  - slope traveling
KW  - downhill wheel
KW  - mechanical effect
KW  - robot platform
KW  - Mobile robots
KW  - Wheels
KW  - Force
KW  - Gears
KW  - Friction
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197047
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A wheeled robot operating on various terrains such as scattered obstacles and slopes is required to cope with and overcome the driving environment. In this paper, in order to overcome a step-type obstacle and to steadily ascend on the slope, the main body rotation mechanism, which controls the load distribution on the robot wheels was proposed for a wheel-drive robot. By rotating the center of the body mass, the friction/traction force required for climbing step obstacles can be reduced. In the case of slope traveling, the slip can be suppressed, and the traveling ability improved by controlling the load distribution excessively increased on the downhill wheel due to the attitude change of the robot's body. The mechanical effect of the proposed body rotation mechanism was analyzed. In addition, based on the design and manufacture of the robot platform, the effectiveness of the proposed mechanism was convincingly demonstrated by indoor test for step-obstacle climbing and slope-traveling.
ER  - 

TY  - CONF
TI  - Radar Sensors in Collaborative Robotics: Fast Simulation and Experimental Validation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10452
EP  - 10458
AU  - C. Stetco
AU  - B. Ubezio
AU  - S. M√ºhlbacher-Karrer
AU  - H. Zangl
PY  - 2020
KW  - collision avoidance
KW  - CW radar
KW  - FM radar
KW  - frequency modulation
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - radar computing
KW  - road vehicle radar
KW  - sensors
KW  - collaborative robotics
KW  - radar systems
KW  - robot systems
KW  - optimization
KW  - machine learning approaches
KW  - realistic simulation models
KW  - radar sensor simulations
KW  - relative velocities
KW  - Lambertian reflectance model
KW  - reflection estimates
KW  - frequency modulated continuous wave radar
KW  - simulation environments
KW  - Radar
KW  - Robot sensing systems
KW  - Radar antennas
KW  - Chirp
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9197180
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - With the availability of small system in package realizations, radar systems become more and more attractive for a variety of applications in robotics, in particular also for collaborative robotics. As the simulation of robot systems in realistic scenarios has become an important tool, not only for design and optimization, but also e.g. for machine learning approaches, realistic simulation models are needed. In the case of radar sensor simulations, this means providing more realistic results than simple proximity sensors, e.g. in the presence of multiple objects and/or humans, objects with different relative velocities and differentiation between background and foreground movement. Due to the short wavelength in the millimeter range, we propose to utilize methods known from computer graphics (e.g. z-buffer, Lambertian reflectance model) to quickly acquire depth images and reflection estimates. This information is used to calculate an estimate of the received signal for a Frequency Modulated Continuous Wave (FMCW) radar by superposition of the corresponding signal contributions. Due to the moderate computational complexity, the approach can be used with various simulation environments such as V-Rep or Gazebo. Validity and benefits of the approach are demonstrated by means of a comparison with experimental data obtained with a radar sensor on a UR10 arm in different scenarios.
ER  - 

TY  - CONF
TI  - Transferable Task Execution from Pixels through Deep Planning Domain Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10459
EP  - 10465
AU  - K. Kase
AU  - C. Paxton
AU  - H. Mazhar
AU  - T. Ogata
AU  - D. Fox
PY  - 2020
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - robot vision
KW  - symbolic operators
KW  - manipulation tasks
KW  - transferable task execution
KW  - visual input
KW  - symbolic planning methods
KW  - partially-observable world
KW  - hierarchical model
KW  - high-level model
KW  - deep planning domain learning
KW  - symbolic world state
KW  - DPDL
KW  - STRIPS
KW  - logical predicates
KW  - low-level policy learning
KW  - photorealistic kitchen scenario
KW  - Task analysis
KW  - Planning
KW  - Robot sensing systems
KW  - Grounding
KW  - Robustness
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196597
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While robots can learn models to solve many manipulation tasks from raw visual input, they cannot usually use these models to solve new problems. On the other hand, symbolic planning methods such as STRIPS have long been able to solve new problems given only a domain definition and a symbolic goal, but these approaches often struggle on the real world robotic tasks due to the challenges of grounding these symbols from sensor data in a partially-observable world. We propose Deep Planning Domain Learning (DPDL), an approach that combines the strengths of both methods to learn a hierarchical model. DPDL learns a high-level model which predicts values for a large set of logical predicates consisting of the current symbolic world state, and separately learns a low-level policy which translates symbolic operators into executable actions on the robot. This allows us to perform complex, multistep tasks even when the robot has not been explicitly trained on them. We show our method on manipulation tasks in a photorealistic kitchen scenario.
ER  - 


TY  - CONF
TI  - Depth by Poking: Learning to Estimate Depth from Self-Supervised Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10466
EP  - 10472
AU  - B. Goodrich
AU  - A. Kuefler
AU  - W. D. Richards
PY  - 2020
KW  - end effectors
KW  - image colour analysis
KW  - manipulator kinematics
KW  - mean square error methods
KW  - mobile robots
KW  - neural nets
KW  - optical radar
KW  - robot vision
KW  - unsupervised learning
KW  - robotic manipulation
KW  - neural network
KW  - RGB-D images
KW  - physical interactions
KW  - autonomous grasping policy
KW  - end effector position labels
KW  - forward kinematics
KW  - manipulation systems
KW  - structured light sensors
KW  - unsupervised deep learning
KW  - self-supervised grasping
KW  - depth estimation
KW  - LiDAR sensors
KW  - root mean squared error
KW  - Estimation
KW  - Uncertainty
KW  - Robot sensing systems
KW  - Training
KW  - Grasping
DO  - 10.1109/ICRA40945.2020.9196797
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate depth estimation remains an open problem for robotic manipulation; even state of the art techniques including structured light and LiDAR sensors fail on reflective or transparent surfaces. We address this problem by training a neural network model to estimate depth from RGB-D images, using labels from physical interactions between a robot and its environment. Our network predicts, for each pixel in an input image, the z position that a robot's end effector would reach if it attempted to grasp or poke at the corresponding position. Given an autonomous grasping policy, our approach is self-supervised as end effector position labels can be recovered through forward kinematics, without human annotation. Although gathering such physical interaction data is expensive, it is necessary for training and routine operation of state of the art manipulation systems. Therefore, this depth estimator comes for free while collecting data for other tasks (e.g., grasping, pushing, placing). We show our approach achieves significantly lower root mean squared error than traditional structured light sensors and unsupervised deep learning methods on dif cult, industry-scale jumbled bin datasets.
ER  - 

TY  - CONF
TI  - Online Learning of Object Representations by Appearance Space Feature Alignment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10473
EP  - 10479
AU  - S. Pirk
AU  - M. Khansari
AU  - Y. Bai
AU  - C. Lynch
AU  - P. Sermanet
PY  - 2020
KW  - image representation
KW  - learning (artificial intelligence)
KW  - online learning
KW  - object representations
KW  - appearance space feature alignment
KW  - monocular videos
KW  - self-supervised model
KW  - OCN
KW  - leverage self-supervision
KW  - online adaptation
KW  - online model
KW  - object identification error
KW  - offline baseline
KW  - robotic pointing task
KW  - object adaptation
KW  - object-contrastive network
KW  - Videos
KW  - Robots
KW  - Training
KW  - Measurement
KW  - Object recognition
KW  - Video sequences
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9196567
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a self-supervised approach for learning representations of objects from monocular videos and demonstrate it is particularly useful for robotics. The main contributions of this paper are: 1) a self-supervised model called Object-Contrastive Network (OCN) that can discover and disentangle object attributes from video without using any labels; 2) we leverage self-supervision for online adaptation: the longer our online model looks at objects in a video, the lower the object identification error, while the offline baseline remains with a large fixed error; 3) we show the usefulness of our approach for a robotic pointing task; a robot can point to objects similar to the one presented in front of it. Videos illustrating online object adaptation and robotic pointing are provided as supplementary material.
ER  - 

TY  - CONF
TI  - Visual Prediction of Priors for Articulated Object Interaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10480
EP  - 10486
AU  - C. Moses
AU  - M. Noseworthy
AU  - L. P. Kaelbling
AU  - T. Lozano-P√©rez
AU  - N. Roy
PY  - 2020
KW  - feature extraction
KW  - intelligent robots
KW  - mobile robots
KW  - object detection
KW  - visual servoing
KW  - exploratory behavior
KW  - visual features learning
KW  - contextual multiarmed bandit
KW  - parameterized action space
KW  - articulated object interaction
KW  - contextual prior prediction
KW  - Robots
KW  - Visualization
KW  - Training
KW  - Kinematics
KW  - Gaussian processes
KW  - Optimization
KW  - Kernel
DO  - 10.1109/ICRA40945.2020.9196541
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Exploration in novel settings can be challenging without prior experience in similar domains. However, humans are able to build on prior experience quickly and efficiently. Children exhibit this behavior when playing with toys. For example, given a toy with a yellow and blue door, a child will explore with no clear objective, but once they have discovered how to open the yellow door, they will most likely be able to open the blue door much faster. Adults also exhibit this behaviour when entering new spaces such as kitchens. We develop a method, Contextual Prior Prediction, which provides a means of transferring knowledge between interactions in similar domains through vision. We develop agents that exhibit exploratory behavior with increasing efficiency, by learning visual features that are shared across environments, and how they correlate to actions. Our problem is formulated as a Contextual Multi-Armed Bandit where the contexts are images, and the robot has access to a parameterized action space. Given a novel object, the objective is to maximize reward with few interactions. A domain which strongly exhibits correlations between visual features and motion is kinemetically constrained mechanisms. We evaluate our method on simulated prismatic and revolute joints1.
ER  - 

TY  - CONF
TI  - MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection, Segmentation, and Grasping Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10487
EP  - 10493
AU  - R. Araki
AU  - T. Onishi
AU  - T. Hirakawa
AU  - T. Yamashita
AU  - H. Fujiyoshi
PY  - 2020
KW  - convolutional neural nets
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - MT-DSSD
KW  - object detection
KW  - semantic object segmentation
KW  - grasping point detection
KW  - multitask learning
KW  - grasping operation
KW  - multitask deconvolutional single shot detector
KW  - robot manipulation
KW  - Amazon Robotics Challenge dataset
KW  - Grasping
KW  - Robots
KW  - Object detection
KW  - Semantics
KW  - Task analysis
KW  - Deconvolution
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197251
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents the multi-task Deconvolutional Single Shot Detector (MT-DSSD), which runs three tasks-object detection, semantic object segmentation, and grasping detection for a suction cup-in a single network based on the DSSD. Simultaneous execution of object detection and segmentation by multi-task learning improves the accuracy of these two tasks. Additionally, the model detects grasping points and performs the three recognition tasks necessary for robot manipulation. The proposed model can perform fast inference, which reduces the time required for grasping operation. Evaluations using the Amazon Robotics Challenge (ARC) dataset showed that our model has better object detection and segmentation performance than comparable methods, and robotic experiments for grasping show that our model can detect the appropriate grasping point.
ER  - 

TY  - CONF
TI  - Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10494
EP  - 10501
AU  - Y. Lin
AU  - C. Tang
AU  - F. -J. Chu
AU  - P. A. Vela
PY  - 2020
KW  - convolutional neural nets
KW  - dexterous manipulators
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object recognition
KW  - path planning
KW  - robot vision
KW  - shape recognition
KW  - synthetic data
KW  - deep networks
KW  - primitive shape
KW  - object grasping
KW  - segmentation-based architecture
KW  - monocular depth input
KW  - backbone deep network
KW  - parametrized grasp families
KW  - shape primitive region
KW  - task-free grasping
KW  - shape primitives
KW  - task-relevant grasp prediction
KW  - ranking algorithm
KW  - task-free grasp prediction
KW  - Shape
KW  - Grasping
KW  - Image segmentation
KW  - Three-dimensional displays
KW  - Task analysis
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197256
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - A segmentation-based architecture is proposed to decompose objects into multiple primitive shapes from monocular depth input for robotic manipulation. The backbone deep network is trained on synthetic data with 6 classes of primitive shapes generated by a simulation engine. Each primitive shape is designed with parametrized grasp families, permitting the pipeline to identify multiple grasp candidates per shape primitive region. The grasps are priority ordered via proposed ranking algorithm, with the first feasible one chosen for execution. On task-free grasping of individual objects, the method achieves a 94% success rate. On task-oriented grasping, it achieves a 76% success rate. Overall, the method supports the hypothesis that shape primitives can support task-free and task-relevant grasp prediction.
ER  - 

TY  - CONF
TI  - Stillleben: Realistic Scene Synthesis for Deep Learning in Robotics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10502
EP  - 10508
AU  - M. Schwarz
AU  - S. Behnke
PY  - 2020
KW  - cameras
KW  - image segmentation
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - rendering (computer graphics)
KW  - robot vision
KW  - realistic scene synthesis
KW  - robotics
KW  - training data
KW  - deep learning
KW  - synthesis pipeline
KW  - cluttered scene perception tasks
KW  - semantic segmentation
KW  - object detection
KW  - physically realistic scenes
KW  - high-quality rasterization
KW  - material parameters
KW  - camera sensors
KW  - deep neural network
KW  - training frames
KW  - iterative render-and-compare approaches
KW  - YCB-Video dataset
KW  - Stillleben
KW  - Training
KW  - Rendering (computer graphics)
KW  - Robots
KW  - Engines
KW  - Pipelines
KW  - Task analysis
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9197309
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Training data is the key ingredient for deep learning approaches, but difficult to obtain for the specialized domains often encountered in robotics. We describe a synthesis pipeline capable of producing training data for cluttered scene perception tasks such as semantic segmentation, object detection, and correspondence or pose estimation. Our approach arranges object meshes in physically realistic, dense scenes using physics simulation. The arranged scenes are rendered using high-quality rasterization with randomized appearance and material parameters. Noise and other transformations introduced by the camera sensors are simulated. Our pipeline can be run online during training of a deep neural network, yielding applications in life-long learning and in iterative render-and-compare approaches. We demonstrate the usability by learning semantic segmentation on the challenging YCB-Video dataset without actually using any training frames, where our method achieves performance comparable to a conventionally trained model. Additionally, we show successful application in a real-world regrasping system.
ER  - 

TY  - CONF
TI  - A Generative Approach Towards Improved Robotic Detection of Marine Litter
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10525
EP  - 10531
AU  - J. Hong
AU  - M. Fulton
AU  - J. Sattar
PY  - 2020
KW  - image classification
KW  - learning (artificial intelligence)
KW  - object detection
KW  - support vector machines
KW  - data scarcity problems
KW  - underwater image datasets
KW  - visual detection
KW  - marine debris
KW  - two-stage variational autoencoder
KW  - generated imagery
KW  - two-stage VAE
KW  - binary classifier
KW  - multiclass classifier
KW  - augmentation process
KW  - trash images
KW  - underwater trash classification problem
KW  - data-dependent task
KW  - quality images
KW  - Training
KW  - Image color analysis
KW  - Plastics
KW  - Gallium nitride
KW  - Task analysis
KW  - Shape
KW  - Decoding
DO  - 10.1109/ICRA40945.2020.9197575
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects "good quality" images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.
ER  - 

TY  - CONF
TI  - Spatiotemporal Representation Learning with GAN Trained LSTM-LSTM Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10548
EP  - 10555
AU  - Y. Fu
AU  - S. Sen
AU  - J. Reimann
AU  - C. Theurer
PY  - 2020
KW  - convolutional neural nets
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - robots
KW  - unstructured environments
KW  - unsupervised representation learning architecture
KW  - underlying representation
KW  - high-dimensional raw video inputs
KW  - spatiotemporal representation learning
KW  - lower-dimensional latent space
KW  - two-stage learning approach
KW  - convolutional neural network
KW  - Long Short-Term Network
KW  - LSTM-LSTM cells
KW  - hierarchical representation learning
KW  - low-dimensional representation
KW  - video prediction task
KW  - GAN trained LSTM-LSTM networks
KW  - robot behavior learning
KW  - layered spatiotemporal memory long short-term memory
KW  - generative adversarial network
KW  - ConvNet
KW  - Spatiotemporal phenomena
KW  - Gallium nitride
KW  - Task analysis
KW  - Training
KW  - Generative adversarial networks
KW  - Robots
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9196858
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning robot behaviors in unstructured environments often requires handcrafting the features for a given task. In this paper, we present and evaluate an unsupervised representation learning architecture, Layered Spatiotemporal Memory Long Short-Term Memory (LSTM-LSTM), that learns the underlying representation without knowledge of the task. The goal of this architecture is to learn the dynamics of the environment from high-dimensional raw video inputs. Using a Generative Adversarial Network (GAN) framework with the proposed network, this architecture is able to learn a spatiotemporal representation in its lower-dimensional latent space directly from raw input sequences. We show that our approach learns the spatial and temporal information simultaneously as opposed to a two-stage learning approach of alternating between training a Convolutional Neural Network (ConvNet) and a Long Short-Term Network (LSTM). Furthermore, by using LSTM-LSTM cells that shrink in size with the increase in the number of layers, the network learns a hierarchical representation with a low-dimensional representation at the top layer. We show that this architecture achieves state-of-the-art results with a substantially lower-dimensional representation than existing methods. We evaluate our approach on a video prediction task with standard benchmark datasets like Moving MNIST and KTH Action, as well as a simulated robot dataset.
ER  - 

TY  - CONF
TI  - Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10556
EP  - 10562
AU  - A. E. Tekden
AU  - A. Erdem
AU  - E. Erdem
AU  - M. Imre
AU  - M. Y. Seker
AU  - E. Ugur
PY  - 2020
KW  - backpropagation
KW  - graph theory
KW  - neural nets
KW  - robot programming
KW  - groups of articulated objects
KW  - learning action effects
KW  - complex robotic systems
KW  - graph neural networks
KW  - Belief Regulated Dual Propagation nets
KW  - object interaction
KW  - object trajectory level
KW  - belief regulator
KW  - physics predictor
KW  - PropNets
KW  - general-purpose learnable physics engine
KW  - BRDPN
KW  - robotics domain
KW  - Robots
KW  - Physics
KW  - Engines
KW  - Predictive models
KW  - History
KW  - Neural networks
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196878
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot's knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online.
ER  - 

TY  - CONF
TI  - Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory Predictions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10563
EP  - 10569
AU  - H. Cui
AU  - T. Nguyen
AU  - F. -C. Chou
AU  - T. -H. Lin
AU  - J. Schneider
AU  - D. Bradley
AU  - N. Djuric
PY  - 2020
KW  - intelligent transportation systems
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - road safety
KW  - road traffic
KW  - road vehicles
KW  - traffic engineering computing
KW  - trajectory control
KW  - vehicle dynamics
KW  - deep learning
KW  - deep convnets
KW  - deep kinematic models
KW  - kinematically feasible vehicle trajectory predictions
KW  - self driving vehicles
KW  - traffic safety
KW  - autonomous technology
KW  - kinematically feasible motion prediction
KW  - vehicle kinematics
KW  - physically grounded vehicle motion models
KW  - Predictive models
KW  - Kinematics
KW  - Trajectory
KW  - Hidden Markov models
KW  - Radar tracking
KW  - Data models
KW  - Interpolation
DO  - 10.1109/ICRA40945.2020.9197560
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Self-driving vehicles (SDVs) hold great potential for improving traffic safety and are poised to positively affect the quality of life of millions of people. To unlock this potential one of the critical aspects of the autonomous technology is understanding and predicting future movement of vehicles surrounding the SDV. This work presents a deep-learning- based method for kinematically feasible motion prediction of such traffic actors. Previous work did not explicitly encode vehicle kinematics and instead relied on the models to learn the constraints directly from the data, potentially resulting in kinematically infeasible, suboptimal trajectory predictions. To address this issue we propose a method that seamlessly combines ideas from the AI with physically grounded vehicle motion models. In this way we employ best of the both worlds, coupling powerful learning models with strong feasibility guarantees for their outputs. The proposed approach is general, being applicable to any type of learning method. Extensive experiments using deep convnets on real-world data strongly indicate its benefits, outperforming the existing state-of-the-art.
ER  - 

TY  - CONF
TI  - Human Driver Behavior Prediction based on UrbanFlow*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10570
EP  - 10576
AU  - Z. Qiao
AU  - J. Zhao
AU  - J. Zhu
AU  - Z. Tyree
AU  - P. Mudalige
AU  - J. Schneider
AU  - J. M. Dolan
PY  - 2020
KW  - decision making
KW  - driver information systems
KW  - mobile robots
KW  - road safety
KW  - road traffic
KW  - road vehicles
KW  - traffic engineering computing
KW  - transportation
KW  - human driver behavior prediction
KW  - public transportation systems
KW  - fully automatic transportation environments
KW  - autonomous vehicle decision making
KW  - planning
KW  - LSTM-based trajectory prediction method
KW  - urban scenario
KW  - Trajectory
KW  - Autonomous vehicles
KW  - Data collection
KW  - Automobiles
KW  - Roads
KW  - Drones
DO  - 10.1109/ICRA40945.2020.9196918
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers' behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers' driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird's-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.
ER  - 

TY  - CONF
TI  - Environment Prediction from Sparse Samples for Robotic Information Gathering
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10577
EP  - 10583
AU  - J. A. Caley
AU  - G. A. Hollinger
PY  - 2020
KW  - handwritten character recognition
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural net architecture
KW  - robot vision
KW  - environment prediction
KW  - sparse samples
KW  - robotics applications
KW  - neural network architecture
KW  - spatially correlated data fields
KW  - spatially continuous samples
KW  - biased loss functions
KW  - reconstruction error
KW  - robotic information gathering trials
KW  - MNIST hand written digits dataset
KW  - ocean monitoring
KW  - regional ocean modeling system ocean dataset
KW  - ROMS
KW  - Robots
KW  - Data models
KW  - Oceans
KW  - Neural networks
KW  - Convolution
KW  - Network architecture
KW  - Logic gates
DO  - 10.1109/ICRA40945.2020.9197263
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robots often require a model of their environment to make informed decisions. In unknown environments, the ability to infer the value of a data field from a limited number of samples is essential to many robotics applications. In this work, we propose a neural network architecture to model these spatially correlated data fields based on a limited number of spatially continuous samples. Additionally, we provide a method based on biased loss functions to suggest future areas of exploration to minimize reconstruction error. We run simulated robotic information gathering trials on both the MNIST hand written digits dataset and a Regional Ocean Modeling System (ROMS) ocean dataset for ocean monitoring. Our method outperforms Gaussian process regression in both environments for modeling the data field and action selection.
ER  - 

TY  - CONF
TI  - Predicting Pushing Action Effects on Spatial Object Relations by Learning Internal Prediction Models
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10584
EP  - 10590
AU  - F. Paus
AU  - T. Huang
AU  - T. Asfour
PY  - 2020
KW  - graph theory
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - spatial object relations
KW  - learning internal prediction models
KW  - robot tasks
KW  - possible action consequences
KW  - action parameters
KW  - desired goal states
KW  - parametrizing pushing actions
KW  - high-level planner
KW  - object-centric graphs
KW  - synthetic data set
KW  - goal state
KW  - possible pushing action candidates
KW  - high prediction accuracy
KW  - humanoid robot ARMAR-6
KW  - learned internal model
KW  - pushing action effects
KW  - Predictive models
KW  - Two dimensional displays
KW  - Robots
KW  - Data models
KW  - Physics
KW  - Three-dimensional displays
KW  - Analytical models
DO  - 10.1109/ICRA40945.2020.9197295
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Understanding the effects of actions is essential for planning and executing robot tasks. By imagining possible action consequences, a robot can choose specific action parameters to achieve desired goal states. We present an approach for parametrizing pushing actions based on learning internal prediction models. These pushing actions must fulfill constraints given by a high-level planner, e. g., after the push the brown box must be to the right of the orange box. In this work, we represent the perceived scenes as object-centric graphs and learn an internal model, which predicts object pose changes due to pushing actions. We train this internal model on a large synthetic data set, which was generated in simulation, and record a smaller data set on the real robot for evaluation. For a given scene and goal state, the robot generates a set of possible pushing action candidates by sampling the parameter space and then evaluating the candidates by internal simulation, i. e., by comparing the predicted effect resulting from the internal model with the desired effect provided by the high-level planner. In the evaluation, we show that our model achieves high prediction accuracy in scenes with a varying number of objects and, in contrast to state-of-the-art approaches, is able to generalize to scenes with more objects than seen during training. In experiments on the humanoid robot ARMAR-6, we validate the transfer from simulation and show that the learned internal model can be used to manipulate scenes into desired states effectively.
ER  - 

TY  - CONF
TI  - Learning of Key Pose Evaluation for Efficient Multi-contact Motion Planner
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10591
EP  - 10597
AU  - S. Noda
AU  - M. Murooka
AU  - Y. Asano
AU  - R. Ishizaki
AU  - T. Kawakami
AU  - T. Watabe
AU  - K. Okada
AU  - T. Yoshiike
AU  - M. Inaba
PY  - 2020
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - neural nets
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - robust control
KW  - transfer functions
KW  - locomotion
KW  - uneven terrain
KW  - multicontact motion planning
KW  - pose evaluation
KW  - neural network
KW  - activation function
KW  - robust robotics system
KW  - humanoid robots
KW  - deep learning
KW  - depth image
KW  - Planning
KW  - Trajectory
KW  - Torque
KW  - Legged locomotion
KW  - Jacobian matrices
KW  - Knee
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197189
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - It is necessary to use not only foot but also hand, knee and other body parts to support body weight for locomotion in uneven terrain. Such multi-contact motion planning is an important research topic including lots of previous works; however, a problem of computational speed of planning is still remaining. In this paper, we propose a learning-based algorithm to speed up the planning. The algorithm reduces replanning of contact states by learning an evaluation function of key pose to reach goal. We investigated the learning performance by comparing three neural network configurations and two activation function. This research aims at achieving robust robotics system in unknown environments.
ER  - 

TY  - CONF
TI  - Differentiable Gaussian Process Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10598
EP  - 10604
AU  - M. Bhardwaj
AU  - B. Boots
AU  - M. Mukadam
PY  - 2020
KW  - collision avoidance
KW  - Gaussian processes
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - motion control
KW  - optimisation
KW  - trajectory optimization
KW  - robotics tasks
KW  - trajectory optimization algorithms
KW  - differentiable extension
KW  - GPMP2 algorithm
KW  - learning-based approach
KW  - Gaussian process motion planning algorithm
KW  - motion planning
KW  - Planning
KW  - Conferences
KW  - Automation
KW  - Gaussian processes
KW  - Artificial intelligence
KW  - Trajectory optimization
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197260
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.
ER  - 

TY  - CONF
TI  - Learn and Link: Learning Critical Regions for Efficient Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10605
EP  - 10611
AU  - D. Molina
AU  - K. Kumar
AU  - S. Srivastava
PY  - 2020
KW  - convolutional neural nets
KW  - learning (artificial intelligence)
KW  - neurocontrollers
KW  - path planning
KW  - sampling methods
KW  - convolutional neural networks
KW  - sampling-based planners
KW  - planning time
KW  - motion planning problems
KW  - open motion planning library
KW  - sampling-based algorithms
KW  - uniform sampling
KW  - sampling-based motion planners
KW  - Planning
KW  - Probabilistic logic
KW  - Robots
KW  - Density measurement
KW  - Task analysis
KW  - Buildings
KW  - Convolutional neural networks
DO  - 10.1109/ICRA40945.2020.9196833
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a new approach to learning for motion planning (MP) where critical regions of an environment are learned from a given set of motion plans and used to improve performance on new environments and problem instances. We introduce a new suite of sampling-based motion planners, Learn and Link. Our planners leverage critical regions to overcome the limitations of uniform sampling, while still maintaining guarantees of correctness inherent to sampling-based algorithms. We also show that convolutional neural networks (CNNs) can be used to identify critical regions for motion planning problems. We evaluate Learn and Link against planners from the Open Motion Planning Library (OMPL) using an extensive suite of experiments on challenging motion planning problems. We show that our approach requires far less planning time than existing sampling-based planners.
ER  - 

TY  - CONF
TI  - Pose-Estimate-Based Target Tracking for Human-Guided Remote Sensor Mounting with a UAV
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10636
EP  - 10642
AU  - D. R. McArthur
AU  - Z. An
AU  - D. J. Cappelleri
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image sequences
KW  - pose estimation
KW  - SLAM (robots)
KW  - target tracking
KW  - pose-estimate-based target tracking
KW  - human-guided remote sensor mounting
KW  - autonomous aerial manipulation
KW  - unstructured environments
KW  - UAV localization
KW  - PBTT method
KW  - target point
KW  - fully on-board computation
KW  - RGB-D camera
KW  - downward-facing optical flow camera
KW  - horizontal localization
KW  - autonomous flight tests
KW  - interacting-boomcopter UAV platform
KW  - UAV position estimator
KW  - Target tracking
KW  - Cameras
KW  - Unmanned aerial vehicles
KW  - Visualization
KW  - Task analysis
KW  - Surface cleaning
KW  - Three-dimensional displays
DO  - 10.1109/ICRA40945.2020.9196514
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present a method for pose-estimate-based target tracking (PBTT) that enables the performance of autonomous aerial manipulation operations in unstructured environments using fully on-board computation for both UAV localization and target tracking. The PBTT method does not depend on extracting traditional visual features (e.g. using SIFT, SURF, ORB, etc.) on or near the target. Instead, the algorithm combines input from an RGB-D camera and the UAV's position estimator (which utilizes a downward-facing optical flow camera for horizontal localization) to track a target point selected by a human operator. The effectiveness of the PBTT method is evaluated through several autonomous flight tests performed with the Interacting-Boomcopter (I-BC) UAV platform in unstructured environments and in the presence of light wind disturbances.
ER  - 

TY  - CONF
TI  - EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10651
EP  - 10657
AU  - N. J. Sanket
AU  - C. M. Parameshwara
AU  - C. D. Singh
AU  - A. V. Kuruttukulam
AU  - C. Ferm√ºller
AU  - D. Scaramuzza
AU  - Y. Aloimonos
PY  - 2020
KW  - cameras
KW  - collision avoidance
KW  - control engineering computing
KW  - helicopters
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - deep dynamic obstacle dodging
KW  - dynamic obstacle avoidance
KW  - quadrotor
KW  - deep learning
KW  - single event camera
KW  - shallow neural networks
KW  - ego-motion
KW  - low light testing scenario
KW  - EVDodgeNet
KW  - Cameras
KW  - Collision avoidance
KW  - Motion segmentation
KW  - Machine learning
KW  - Optical imaging
KW  - Robot vision systems
KW  - Image segmentation
DO  - 10.1109/ICRA40945.2020.9196877
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dynamic obstacle avoidance on quadrotors requires low latency. A class of sensors that are particularly suitable for such scenarios are event cameras. In this paper, we present a deep learning based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and on-board computation. Our approach uses a series of shallow neural networks for estimating both the ego-motion and the motion of independently moving objects. The networks are trained in simulation and directly transfer to the real world without any fine-tuning or retraining. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with obstacles of different shapes and sizes, achieving an overall success rate of 70% including objects of unknown shape and a low light testing scenario. To our knowledge, this is the first deep learning - based solution to the problem of dynamic obstacle avoidance using event cameras on a quadrotor. Finally, we also extend our work to the pursuit task by merely reversing the control policy, proving that our navigation stack can cater to different scenarios.
ER  - 

TY  - CONF
TI  - On training datasets for machine learning-based visual relative localization of micro-scale UAVs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10674
EP  - 10680
AU  - V. Walter
AU  - M. Vrba
AU  - M. Saska
PY  - 2020
KW  - autonomous aerial vehicles
KW  - image classification
KW  - learning (artificial intelligence)
KW  - microrobots
KW  - mobile robots
KW  - object detection
KW  - training datasets
KW  - machine learning-based visual relative localization
KW  - microscale UAVs
KW  - relative Microscale Unmanned Aerial Vehicle localization sensor UVDAR
KW  - automatically annotated dataset MIDGARD
KW  - MAVs
KW  - visual object detection
KW  - carefully crafted training dataset
KW  - annotated camera footage
KW  - Cameras
KW  - Training
KW  - Observers
KW  - Visualization
KW  - Image color analysis
KW  - Global navigation satellite system
KW  - Position measurement
DO  - 10.1109/ICRA40945.2020.9196947
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - By leveraging our relative Micro-scale Unmanned Aerial Vehicle localization sensor UVDAR, we generated an automatically annotated dataset MIDGARD, which the community is invited to use for training and testing their machine learning systems for the detection and localization of Microscale Unmanned Aerial Vehicles (MAVs) by other MAVs. Furthermore, we provide our system as a mechanism for rapidly generating custom annotated datasets specifically tailored for the needs of a given application. The recent literature is rich in applications of machine learning methods in automation and robotics. One particular subset of these methods is visual object detection and localization, using means such as Convolutional Neural Networks, which nowadays enable objects to be detected and classified with previously inconceivable precision and reliability. Most of these applications, however, rely on a carefully crafted training dataset of annotated camera footage. These must contain the objects of interest in environments similar to those where the detector is expected to operate. Notably, the positions of the objects must be provided in annotations. For non-laboratory settings, the construction of such datasets requires many man-hours of manual annotation, which is especially the case for use onboard Micro-scale Unmanned Aerial Vehicles. In this paper, we are providing for the community a practical alternative to that kind of approach.
ER  - 

TY  - CONF
TI  - Dynamic Actor-Advisor Programming for Scalable Safe Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10681
EP  - 10687
AU  - L. Zhu
AU  - Y. Cui
AU  - T. Matsubara
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - scalable safe reinforcement learning
KW  - real-world robots
KW  - complex strict constraints
KW  - safe reinforcement learning algorithms
KW  - high-dimensional systems
KW  - DAAP
KW  - sample efficiency
KW  - dynamic actor-advisor programming
KW  - dynamic policy programming framework
KW  - constraint violation risk
KW  - Dynamic programming
KW  - Programming
KW  - Robots
KW  - Learning (artificial intelligence)
KW  - Heuristic algorithms
KW  - Task analysis
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197200
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Real-world robots have complex strict constraints. Therefore, safe reinforcement learning algorithms that can simultaneously minimize the total cost and the risk of constraint violation are crucial. However, almost no algorithms exist that can scale to high-dimensional systems to the best of our knowledge. In this paper, we propose Dynamic Actor-Advisor Programming (DAAP), as an algorithm for sample-efficient and scalable safe reinforcement learning. DAAP employs two control policies, actor and advisor. They are updated to minimize total cost and risk of constraint violation intertwiningly and smoothly towards each other's direction by using the other as the baseline policy in the Kullback-Leibler divergence of Dynamic Policy Programming framework. We demonstrate the scalability and sample efficiency of DAAP through its application on simulated robot arm control tasks with performance comparisons to baselines.
ER  - 

TY  - CONF
TI  - Discrete Deep Reinforcement Learning for Mapless Navigation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10688
EP  - 10694
AU  - E. Marchesini
AU  - A. Farinelli
PY  - 2020
KW  - discrete systems
KW  - gradient methods
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - optimisation
KW  - state-space methods
KW  - mapless navigation
KW  - discrete state space algorithms
KW  - continuous alternatives
KW  - double deep Q-network
KW  - parallel asynchronous training
KW  - training time
KW  - proximal policy optimization algorithms
KW  - original discrete algorithm
KW  - continuous algorithms
KW  - continuous deep deterministic policy gradient
KW  - multibatch priority experience replay
KW  - discrete deep reinforcement
KW  - Training
KW  - Navigation
KW  - Robot kinematics
KW  - Robot sensing systems
KW  - Optimization
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9196739
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Our goal is to investigate whether discrete state space algorithms are a viable solution to continuous alternatives for mapless navigation. To this end we present an approach based on Double Deep Q-Network and employ parallel asynchronous training and a multi-batch Priority Experience Replay to reduce the training time. Experiments show that our method trains faster and outperforms both the continuous Deep Deterministic Policy Gradient and Proximal Policy Optimization algorithms. Moreover, we train the models in a custom environment built on the recent Unity learning toolkit and show that they can be exported on the TurtleBot3 simulator and to the real robot without further training. Overall our optimized method is 40% faster compared to the original discrete algorithm. This setting significantly reduces the training times with respect to the continuous algorithms, maintaining a similar level of success rate hence being a viable alternative for mapless navigation.
ER  - 

TY  - CONF
TI  - Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10695
EP  - 10701
AU  - Y. Xiao
AU  - J. Hoffman
AU  - T. Xia
AU  - C. Amato
PY  - 2020
KW  - decentralised control
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - recurrent neural nets
KW  - multirobot decentralized macro-action-based policies
KW  - centralized Q-net
KW  - decentralized control
KW  - decentralized multiagent reinforcement learning
KW  - decentralized Q-net
KW  - decentralized exploration
KW  - macro-action based decentralized multiagent double deep recurrent Q-net
KW  - Parallel-MacDec-MADDRQN
KW  - Robot kinematics
KW  - Training
KW  - Tools
KW  - Task analysis
KW  - Machine learning
KW  - History
DO  - 10.1109/ICRA40945.2020.9196684
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many real-world multi-robot tasks, high-quality solutions often require a team of robots to perform asynchronous actions under decentralized control. Decentralized multi-agent reinforcement learning methods have difficulty learning decentralized policies because of the environment appearing to be non-stationary due to other agents also learning at the same time. In this paper, we address this challenge by proposing a macro-action-based decentralized multi-agent double deep recurrent Q-net (MacDec-MADDRQN) which trains each decentralized Q-net using a centralized Q-net for action selection. A generalized version of MacDec-MADDRQN with two separate training environments, called Parallel-MacDec-MADDRQN, is also presented to leverage either centralized or decentralized exploration. The advantages and the practical nature of our methods are demonstrated by achieving near-centralized results in simulation and having real robots accomplish a warehouse tool delivery task in an efficient way.
ER  - 

TY  - CONF
TI  - Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10702
EP  - 10708
AU  - M. Turchetta
AU  - A. Krause
AU  - S. Trimpe
PY  - 2020
KW  - Bayes methods
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - optimisation
KW  - pendulums
KW  - robust model-free reinforcement learning
KW  - multiobjective Bayesian optimization
KW  - autonomous agent
KW  - exogenous reward signal
KW  - test conditions
KW  - pure reward maximization
KW  - model-free case
KW  - robust model-free RL problem
KW  - multiobjective optimization problem
KW  - robustness indicators
KW  - robust formulation
KW  - Robustness
KW  - Optimization
KW  - Training
KW  - Control theory
KW  - Computational modeling
KW  - Learning (artificial intelligence)
KW  - Bayes methods
DO  - 10.1109/ICRA40945.2020.9197000
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In reinforcement learning (RL), an autonomous agent learns to perform complex tasks by maximizing an exogenous reward signal while interacting with its environment. In real world applications, test conditions may differ substantially from the training scenario and, therefore, focusing on pure reward maximization during training may lead to poor results at test time. In these cases, it is important to trade-off between performance and robustness while learning a policy. While several results exist for robust, model-based RL, the model-free case has not been widely investigated. In this paper, we cast the robust, model-free RL problem as a multi-objective optimization problem. To quantify the robustness of a policy, we use delay margin and gain margin, two robustness indicators that are common in control theory. We show how these metrics can be estimated from data in the model-free setting. We use multi-objective Bayesian optimization (MOBO) to solve efficiently this expensive-to-evaluate, multi-objective optimization problem. We show the benefits of our robust formulation both in sim-to-real and pure hardware experiments to balance a Furuta pendulum.
ER  - 

TY  - CONF
TI  - A Unified Framework for Piecewise Semantic Reconstruction in Dynamic Scenes via Exploiting Superpixel Relations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10737
EP  - 10743
AU  - Y. Di
AU  - H. Morimitsu
AU  - Z. Lou
AU  - X. Ji
PY  - 2020
KW  - image motion analysis
KW  - image reconstruction
KW  - image segmentation
KW  - image sequences
KW  - motion estimation
KW  - object detection
KW  - dense piecewise semantic reconstruction
KW  - superpixel relations
KW  - structure-from-motion
KW  - superpixel relation analysis
KW  - motion relations
KW  - semantic instance segmentation
KW  - dynamic scenes
KW  - moving objects
KW  - spatial relations
KW  - Semantics
KW  - Motion segmentation
KW  - Image reconstruction
KW  - Motion estimation
KW  - Pipelines
KW  - Silicon
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9197240
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel framework for dense piecewise semantic reconstruction in dynamic scenes containing complex background and moving objects via exploiting superpixel relations. We utilize two kinds of superpixel relations: motion relations and spatial relations, each having three subcategories: coplanar, hinge, and crack. Spatial relations provide constraints on the spatial locations of neighboring superpixels and thus can be used to reconstruct dynamic scenes. However, spatial relations can not be estimated directly with epipolar geometry due to moving objects in dynamic scenes. We synthesize the results of semantic instance segmentation and motion relations to estimate spatial relations. Given consecutive frames, we mainly develop our method in five main stages: preprocessing, motion estimation, superpixel relation analysis, reconstruction and refinement. Extensive experiments on various datasets demonstrate that our method outperforms competitors in reconstruction quality. Furthermore, our method presents a feasible way to incorporate semantic information in Structure-from-Motion (SFM) based reconstruction pipelines.
ER  - 

TY  - CONF
TI  - Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10744
EP  - 10750
AU  - K. Zieli≈Ñski
AU  - D. Belter
PY  - 2020
KW  - cameras
KW  - graph theory
KW  - image colour analysis
KW  - image sensors
KW  - mobile robots
KW  - normal distribution
KW  - robot vision
KW  - SLAM (robots)
KW  - camera origin
KW  - pose graph
KW  - NDT-OM
KW  - keyframe-based dense mapping
KW  - keyframe-based mapping system
KW  - RGB-D sensor
KW  - 2D view-dependent structures
KW  - uncertainty model
KW  - RGB-D cameras
KW  - view-dependent local maps
KW  - normal distribution transform maps
KW  - global map
KW  - loop closure detection
KW  - autonomous robots
KW  - SLAM
KW  - Ellipsoids
KW  - Three-dimensional displays
KW  - Robot sensing systems
KW  - Cameras
KW  - Two dimensional displays
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196865
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.
ER  - 

TY  - CONF
TI  - Informative Path Planning for Active Field Mapping under Localization Uncertainty
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10751
EP  - 10757
AU  - M. Popoviƒá
AU  - T. Vidal-Calleja
AU  - J. J. Chung
AU  - J. Nieto
AU  - R. Siegwart
PY  - 2020
KW  - Gaussian processes
KW  - mobile robots
KW  - path planning
KW  - informative path planning
KW  - active field mapping
KW  - localization uncertainty
KW  - information gathering algorithms
KW  - efficient data collection
KW  - fundamental problem
KW  - implicit requirement
KW  - high-quality maps
KW  - informative planning framework
KW  - active mapping
KW  - Gaussian process model
KW  - target environmental field
KW  - utility function
KW  - field mapping objectives
KW  - GP-based mapping scenarios
KW  - mean pose uncertainty
KW  - map error
KW  - indoor temperature mapping scenario
KW  - Uncertainty
KW  - Planning
KW  - Robot sensing systems
KW  - Trajectory
KW  - Manuals
KW  - Robot localization
DO  - 10.1109/ICRA40945.2020.9197034
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Information gathering algorithms play a key role in unlocking the potential of robots for efficient data collection in a wide range of applications. However, most existing strategies neglect the fundamental problem of the robot pose uncertainty, which is an implicit requirement for creating robust, high-quality maps. To address this issue, we introduce an informative planning framework for active mapping that explicitly accounts for the pose uncertainty in both the mapping and planning tasks. Our strategy exploits a Gaussian Process (GP) model to capture a target environmental field given the uncertainty on its inputs. For planning, we formulate a new utility function that couples the localization and field mapping objectives in GP-based mapping scenarios in a principled way, without relying on manually-tuned parameters. Extensive simulations show that our approach outperforms existing strategies, reducing mean pose uncertainty and map error. We present a proof of concept in an indoor temperature mapping scenario.
ER  - 

TY  - CONF
TI  - Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping with Streaming Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10758
EP  - 10764
AU  - J. A. Stork
AU  - T. Stoyanov
PY  - 2020
KW  - Gaussian processes
KW  - mobile robots
KW  - path planning
KW  - regression analysis
KW  - robot vision
KW  - SLAM (robots)
KW  - sparse Gaussian process experts
KW  - implicit surface mapping
KW  - streaming data
KW  - creating maps
KW  - robotics
KW  - navigation
KW  - compact surface map
KW  - continuous implicit surface map
KW  - range data
KW  - approximate Gaussian process experts
KW  - GP models
KW  - model complexity
KW  - prediction error
KW  - real-world data sets
KW  - compact surface models
KW  - accurate implicit surface models
KW  - exact GP regression
KW  - subsampled data
KW  - Surface treatment
KW  - Data models
KW  - Predictive models
KW  - Covariance matrices
KW  - Gaussian processes
KW  - Computational modeling
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196620
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Creating maps is an essential task in robotics and provides the basis for effective planning and navigation. In this paper, we learn a compact and continuous implicit surface map of an environment from a stream of range data with known poses. For this, we create and incrementally adjust an ensemble of approximate Gaussian process (GP) experts which are each responsible for a different part of the map. Instead of inserting all arriving data into the GP models, we greedily trade-off between model complexity and prediction error. Our algorithm therefore uses less resources on areas with few geometric features and more where the environment is rich in variety. We evaluate our approach on synthetic and real-world data sets and analyze sensitivity to parameters and measurement noise. The results show that we can learn compact and accurate implicit surface models under different conditions, with a performance comparable to or better than that of exact GP regression with subsampled data.
ER  - 

TY  - CONF
TI  - Robust Method for Removing Dynamic Objects from Point Clouds
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10765
EP  - 10771
AU  - S. Pagad
AU  - D. Agarwal
AU  - S. Narayanan
AU  - K. Rangan
AU  - H. Kim
AU  - G. Yalla
PY  - 2020
KW  - image capture
KW  - image filtering
KW  - image registration
KW  - image representation
KW  - object detection
KW  - optical radar
KW  - robot vision
KW  - 3D point cloud maps
KW  - dynamic object removal
KW  - laser scans
KW  - lidar scans
KW  - object detection
KW  - voxel traversal method
KW  - Three-dimensional displays
KW  - Vehicle dynamics
KW  - Octrees
KW  - Object detection
KW  - Laser modes
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197168
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - 3D point cloud maps are an accumulation of laser scans obtained at different positions and times. Since laser scans represent a snapshot of the surrounding at the time of capture, they often contain moving objects which may not be observed at all times. Dynamic objects in point cloud maps decrease the quality of maps and affect localization accuracy, hence it is important to remove the dynamic objects from 3D point cloud maps. In this paper, we present a robust method to remove dynamic objects from 3D point cloud maps. Given a registered set of 3D point clouds, we build an occupancy map in which the voxels represent the occupancy state of the volume of space over an extended time period. After building the occupancy map, we use it as a filter to remove dynamic points in lidar scans before adding the points to the map. Furthermore, we accelerate the process of building occupancy maps using object detection and a novel voxel traversal method. Once the occupancy map is built, dynamic object removal can run in real-time. Our approach works well on wide urban roads with stopped or moving traffic and the occupancy maps get better with the inclusion of more lidar scans from the same scene.
ER  - 

TY  - CONF
TI  - Real-Time Semantic Stereo Matching
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10780
EP  - 10787
AU  - P. L. Dovesi
AU  - M. Poggi
AU  - L. Andraghetti
AU  - M. Mart√≠
AU  - H. Kjellstr√∂m
AU  - A. Pieropan
AU  - S. Mattoccia
PY  - 2020
KW  - image matching
KW  - image segmentation
KW  - inference mechanisms
KW  - neural nets
KW  - semantic networks
KW  - stereo image processing
KW  - augmented reality
KW  - deep neural networks
KW  - semantic segmentation
KW  - inference
KW  - semantic stereo image matching
KW  - coarse-to-fine estimations
KW  - embedded devices
KW  - GPU
KW  - embedded Jetson TX2
KW  - Semantics
KW  - Feature extraction
KW  - Task analysis
KW  - Estimation
KW  - Three-dimensional displays
KW  - Image segmentation
KW  - Computer architecture
DO  - 10.1109/ICRA40945.2020.9196784
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Scene understanding is paramount in robotics, self-navigation, augmented reality, and many other fields. To fully accomplish this task, an autonomous agent has to infer the 3D structure of the sensed scene (to know where it looks at) and its content (to know what it sees). To tackle the two tasks, deep neural networks trained to infer semantic segmentation and depth from stereo images are often the preferred choices. Specifically, Semantic Stereo Matching can be tackled by either standalone models trained for the two tasks independently or joint end-to-end architectures. Nonetheless, as proposed so far, both solutions are inefficient because requiring two forward passes in the former case or due to the complexity of a single network in the latter, although jointly tackling both tasks is usually beneficial in terms of accuracy. In this paper, we propose a single compact and lightweight architecture for real-time semantic stereo matching. Our framework relies on coarse-to-fine estimations in a multi-stage fashion, allowing: i) very fast inference even on embedded devices, with marginal drops in accuracy, compared to state-of-the-art networks, ii) trade accuracy for speed, according to the specific application requirements. Experimental results on high-end GPUs as well as on an embedded Jetson TX2 confirm the superiority of semantic stereo matching compared to standalone tasks and highlight the versatility of our framework on any hardware and for any application.
ER  - 

TY  - CONF
TI  - Multi-Task Learning for Single Image Depth Estimation and Segmentation Based on Unsupervised Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10788
EP  - 10794
AU  - Y. Lu
AU  - M. Sarkis
AU  - G. Lu
PY  - 2020
KW  - computer vision
KW  - convolutional neural nets
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - regression analysis
KW  - single image depth estimation
KW  - unsupervised network
KW  - deep neural networks
KW  - computer vision tasks
KW  - image segmentation
KW  - encoder-decoder-based interactive convolutional neural network
KW  - multitask learning framework
KW  - CNN
KW  - pixel depth regression
KW  - Image segmentation
KW  - Estimation
KW  - Task analysis
KW  - Training
KW  - Feature extraction
KW  - Neural networks
KW  - Image reconstruction
DO  - 10.1109/ICRA40945.2020.9196723
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep neural networks have significantly enhanced the performance of various computer vision tasks, including single image depth estimation and image segmentation. However, most existing approaches handle them in supervised manners and require a large number of ground truth labels that consume extensive human efforts and are not always available in real scenarios. In this paper, we propose a novel framework to estimate disparity maps and segment images simultaneously by jointly training an encoder-decoder-based interactive convolutional neural network (CNN) for single image depth estimation and a multiple class CNN for image segmentation. Learning the neural network for one task can be beneficial from simultaneously learning from another one under a multi-task learning framework. We show that our proposed model can learn per-pixel depth regression and segmentation from just a single image input. Extensive experiments on available public datasets, including KITTI, Cityscapes urban, and PASCAL-VOC demonstrate the effectiveness of our model compared with other state-of-the-art methods for both tasks.
ER  - 

TY  - CONF
TI  - Leveraging the Template and Anchor Framework for Safe, Online Robotic Gait Design
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10869
EP  - 10875
AU  - J. Liu
AU  - P. Zhao
AU  - Z. Gan
AU  - M. Johnson-Roberson
AU  - R. Vasudevan
PY  - 2020
KW  - control system synthesis
KW  - legged locomotion
KW  - predictive control
KW  - reduced order systems
KW  - robot dynamics
KW  - safety-preserving controllers
KW  - model predictive control
KW  - 5-link RABBIT model
KW  - anchor framework
KW  - online robotic gait design
KW  - online control design
KW  - bipedal robot
KW  - reduced-order model
KW  - control synthesis
KW  - template framework
KW  - safe robotic gait design
KW  - Legged locomotion
KW  - Rabbits
KW  - Safety
KW  - Foot
KW  - Predictive models
KW  - Control systems
KW  - Bipeds
KW  - underactuated system
KW  - safety guarantee
DO  - 10.1109/ICRA40945.2020.9197017
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Online control design using a high-fidelity, full-order model for a bipedal robot can be challenging due to the size of the state space of the model. A commonly adopted solution to overcome this challenge is to approximate the fullorder model (anchor) with a simplified, reduced-order model (template), while performing control synthesis. Unfortunately it is challenging to make formal guarantees about the safety of an anchor model using a controller designed in an online fashion using a template model. To address this problem, this paper proposes a method to generate safety-preserving controllers for anchor models by performing reachability analysis on template models by relying on functions that bound the difference between the two models. This paper describes how this reachable set can be incorporated into a Model Predictive Control framework to select controllers that result in safe walking on the anchor model in an online fashion. The method is illustrated on a 5-link RABBIT model, and is shown to allow the robot to walk safely while utilizing controllers designed in an online fashion.
ER  - 

TY  - CONF
TI  - Unified Push Recovery Fundamentals: Inspiration from Human Study
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10876
EP  - 10882
AU  - C. McGreavy
AU  - K. Yuan
AU  - D. Gordon
AU  - K. Tan
AU  - W. J. Wolfslag
AU  - S. Vijayakumar
AU  - Z. Li
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - mechanical stability
KW  - motion control
KW  - position control
KW  - predictive control
KW  - time optimal control
KW  - unified push recovery fundamentals
KW  - humanoid robots
KW  - stepping strategies
KW  - balance strategies
KW  - minimum jerk controller
KW  - human behaviour
KW  - model-predictive control
KW  - recovery motions
KW  - robotic systems
KW  - human balance recovery
KW  - legged machines
KW  - time-optimal performance
KW  - Modulation
KW  - Robots
KW  - Hip
KW  - Stability criteria
KW  - Foot
KW  - Force
DO  - 10.1109/ICRA40945.2020.9196911
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Currently for balance recovery, humans outperform humanoid robots which use hand-designed controllers in terms of the diverse actions. This study aims to close this gap by finding core control principles that are shared across ankle, hip, toe and stepping strategies by formulating experiments to test human balance recoveries and define criteria to quantify the strategy in use. To reveal fundamental principles of balance strategies, our study shows that a minimum jerk controller can accurately replicate comparable human behaviour at the Centre of Mass level. Therefore, we formulate a general Model-Predictive Control (MPC) framework to produce recovery motions in any system, including legged machines, where the framework parameters are tuned for time-optimal performance in robotic systems.
ER  - 

TY  - CONF
TI  - DISCO: Double Likelihood-free Inference Stochastic Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10969
EP  - 10975
AU  - L. Barcelos
AU  - R. Oliveira
AU  - R. Possas
AU  - L. Ott
AU  - F. Ramos
PY  - 2020
KW  - Bayes methods
KW  - control system synthesis
KW  - differential equations
KW  - Monte Carlo methods
KW  - predictive control
KW  - probability
KW  - robust control
KW  - sampling methods
KW  - stochastic systems
KW  - transforms
KW  - uncertain systems
KW  - double likelihood-free inference stochastic control
KW  - complex physical systems
KW  - control strategies
KW  - analytical tractability
KW  - probabilistic inference
KW  - simulation parameters
KW  - likelihood function
KW  - modern simulators
KW  - nonanalytical model
KW  - classical control
KW  - model parameters
KW  - DISCO
KW  - differential equations
KW  - numerical solvers
KW  - uncertainty assessment
KW  - Bayesian statistics
KW  - likelihood-free inference
KW  - control framework design
KW  - unscented transform
KW  - information theoretical model predictive control
KW  - Monte Carlo sampling
KW  - robotics tasks
KW  - posterior distribution
KW  - Uncertainty
KW  - Trajectory
KW  - Mathematical model
KW  - Computational modeling
KW  - Numerical models
KW  - Stochastic processes
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9196931
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate simulation of complex physical systems enables the development, testing, and certification of control strategies before they are deployed into the real systems. As simulators become more advanced, the analytical tractability of the differential equations and associated numerical solvers incorporated in the simulations diminishes, making them difficult to analyse. A potential solution is the use of probabilistic inference to assess the uncertainty of the simulation parameters given real observations of the system. Unfortunately the likelihood function required for inference is generally expensive to compute or totally intractable. In this paper we propose to leverage the power of modern simulators and recent techniques in Bayesian statistics for likelihood-free inference to design a control framework that is efficient and robust with respect to the uncertainty over simulation parameters. The posterior distribution over simulation parameters is propagated through a potentially non-analytical model of the system with the unscented transform, and a variant of the information theoretical model predictive control. This approach provides a more efficient way to evaluate trajectory roll outs than Monte Carlo sampling, reducing the online computation burden. Experiments show that the controller proposed attained superior performance and robustness on classical control and robotics tasks when compared to models not accounting for the uncertainty over model parameters.
ER  - 

TY  - CONF
TI  - Sufficiently Accurate Model Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 10991
EP  - 10997
AU  - C. Zhang
AU  - A. Khan
AU  - S. Paternain
AU  - A. Ribeiro
PY  - 2020
KW  - learning (artificial intelligence)
KW  - control algorithms
KW  - primal-dual method
KW  - sufficiently accurate models
KW  - traditional control
KW  - error characteristics
KW  - inaccurate physical measurements
KW  - planning algorithms
KW  - robot
KW  - accurate model learning
KW  - Task analysis
KW  - Optimization
KW  - Data models
KW  - Adaptation models
KW  - Heuristic algorithms
KW  - Neural networks
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9197502
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modeling how a robot interacts with the environment around it is an important prerequisite for designing control and planning algorithms. In fact, the performance of controllers and planners is highly dependent on the quality of the model. One popular approach is to learn data driven models in order to compensate for inaccurate physical measurements and to adapt to systems that evolve over time. In this paper, we investigate a method to regularize model learning techniques to provide better error characteristics for traditional control and planning algorithms. This work proposes learning "Sufficiently Accurate" models of dynamics using a primal-dual method that can explicitly enforce constraints on the error in pre-defined parts of the state-space. The result of this method is that the error characteristics of the learned model is more predictable and can be better utilized by planning and control algorithms. The characteristics of Sufficiently Accurate models are analyzed through experiments on a simulated ball paddle system.
ER  - 

TY  - CONF
TI  - Towards Plan Transformations for Real-World Mobile Fetch and Place
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11011
EP  - 11017
AU  - G. Kazhoyan
AU  - A. Niedzwiecki
AU  - M. Beetz
PY  - 2020
KW  - manipulators
KW  - mobile robots
KW  - path planning
KW  - service robots
KW  - cleaning tasks
KW  - mobile manipulation plans
KW  - plan transformations
KW  - mobile fetch and place
KW  - robot behavior
KW  - table setting tasks
KW  - Task analysis
KW  - Planning
KW  - Runtime
KW  - Manipulators
KW  - Transforms
KW  - Complexity theory
DO  - 10.1109/ICRA40945.2020.9197446
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we present an approach and an implemented framework for applying plan transformations to real-world mobile manipulation plans, in order to specialize them to the specific situation at hand. The framework can improve execution cost and achieve better performance by autonomously transforming robot's behavior at runtime. To demonstrate the feasibility of our approach, we apply three example transformations to the plan of a PR2 robot performing simple table setting and cleaning tasks in the real world. Based on a large amount of experiments in a fast plan projection simulator, we make conclusions on improved execution performance.
ER  - 

TY  - CONF
TI  - Planning an Efficient and Robust Base Sequence for a Mobile Manipulator Performing Multiple Pick-and-place Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11018
EP  - 11024
AU  - J. Xu
AU  - K. Harada
AU  - W. Wan
AU  - T. Ueshiba
AU  - Y. Domae
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - motion control
KW  - redundant manipulators
KW  - mobile manipulator
KW  - planned base positions
KW  - robust base sequence
KW  - precomputed reachability database
KW  - base positioning uncertainty
KW  - collision free inverse kinematics solutions
KW  - multiple pick and place tasks
KW  - kinematic redundancy
KW  - Manipulators
KW  - Databases
KW  - Task analysis
KW  - Robustness
KW  - Grasping
KW  - Uncertainty
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196999
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we address efficiently and robustly collecting objects stored in different trays using a mobile manipulator. A resolution complete method, based on precomputed reachability database, is proposed to explore collision-free inverse kinematics (IK) solutions and then a resolution complete set of feasible base positions can be determined. This method approximates a set of representative IK solutions that are especially helpful when solving IK and checking collision are treated separately. For real world applications, we take into account the base positioning uncertainty and plan a sequence of base positions that reduce the number of necessary base movements for collecting the target objects, the base sequence is robust in that the mobile manipulator is able to complete the part-supply task even there is certain deviation from the planned base positions. Our experiments demonstrate both the efficiency compared to regular base sequence and the feasibility in real world applications.
ER  - 

TY  - CONF
TI  - Towards Mobile Multi-Task Manipulation in a Confined and Integrated Environment with Irregular Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11025
EP  - 11031
AU  - Z. Han
AU  - J. Allspaw
AU  - G. LeMasurier
AU  - J. Parrillo
AU  - D. Giger
AU  - S. R. Ahmadzadeh
AU  - H. A. Yanco
PY  - 2020
KW  - assembling
KW  - control engineering computing
KW  - industrial manipulators
KW  - machining
KW  - mobile robots
KW  - production engineering computing
KW  - software architecture
KW  - irregular objects
KW  - mechanical parts
KW  - complex task sets
KW  - integrated task sets
KW  - IEEE International Conference on Robots and Automation
KW  - FetchIt! Mobile Manipulation Challenge
KW  - mobile multitask manipulation
KW  - confined environment
KW  - integrated environment
KW  - confined space
KW  - machining
KW  - assembly
KW  - software architecture
KW  - Gears
KW  - Navigation
KW  - Task analysis
KW  - Manipulators
KW  - Three-dimensional displays
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9197395
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The FetchIt! Mobile Manipulation Challenge, held at the IEEE International Conference on Robots and Automation (ICRA) in May 2019, offered an environment with complex and integrated task sets, irregular objects, confined space, and machining, introducing new challenges in the mobile manipulation domain. Here we describe our efforts to address these challenges by demonstrating the assembly of a kit of mechanical parts in a caddy. In addition to implementation details, we examine the issues in this task set extensively, and we discuss our software architecture in the hope of providing a base for other researchers. To evaluate performance and consistency, we conducted 20 full runs, then examined failure cases with possible solutions. We conclude by identifying future research directions to address the open challenges.
ER  - 

TY  - CONF
TI  - Linear Time-Varying MPC for Nonprehensile Object Manipulation with a Nonholonomic Mobile Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11032
EP  - 11038
AU  - F. Bertoncelli
AU  - F. Ruggiero
AU  - L. Sabattini
PY  - 2020
KW  - collision avoidance
KW  - friction
KW  - linear systems
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - predictive control
KW  - time-varying systems
KW  - trajectory control
KW  - wheels
KW  - linear time-varying MPC
KW  - nonprehensile object manipulation
KW  - nonholonomic mobile robot
KW  - nonprehensile manipulation motion primitive
KW  - unilateral constraint
KW  - manipulated object
KW  - linear time-varying model predictive control
KW  - pushing manipulation
KW  - Mobile robots
KW  - Friction
KW  - Task analysis
KW  - Dynamics
KW  - Mathematical model
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197173
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a technique to manipulate an object with a nonholonomic mobile robot by pushing, which is a nonprehensile manipulation motion primitive. Such a primitive involves unilateral constraints associated with the friction between the robot and the manipulated object. Violating this constraint produces the slippage of the object during the manipulation, preventing the correct achievement of the task. A linear time-varying model predictive control is designed to include the unilateral constraint within the control action properly. The approach is verified in a dynamic simulation environment through a Pioneer 3-DX wheeled robot executing the pushing manipulation of a package.
ER  - 

TY  - CONF
TI  - A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11039
EP  - 11045
AU  - M. Bajracharya
AU  - J. Borders
AU  - D. Helmick
AU  - T. Kollar
AU  - M. Laskey
AU  - J. Leichty
AU  - J. Ma
AU  - U. Nagarajan
AU  - A. Ochiai
AU  - J. Petersen
AU  - K. Shankar
AU  - K. Stone
AU  - Y. Takaoka
PY  - 2020
KW  - control engineering computing
KW  - force control
KW  - humanoid robots
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - mobile robots
KW  - motion control
KW  - position control
KW  - virtual reality
KW  - mobile manipulation system
KW  - one-shot teaching
KW  - mobile manipulation hardware
KW  - software system
KW  - human-level tasks
KW  - single demonstration
KW  - virtual reality
KW  - highly capable mobile manipulation robot
KW  - parameterized primitives
KW  - robust learned dense visual embeddings representation
KW  - task graph
KW  - taught behaviors
KW  - Task analysis
KW  - Robot kinematics
KW  - Robustness
KW  - Visualization
KW  - Aerospace electronics
KW  - Education
DO  - 10.1109/ICRA40945.2020.9196677
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.
ER  - 

TY  - CONF
TI  - 2D to 3D Line-Based Registration with Unknown Associations via Mixed-Integer Programming
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11046
EP  - 11052
AU  - S. A. Parkison
AU  - J. M. Walls
AU  - R. W. Wolcott
AU  - M. Saad
AU  - R. M. Eustice
PY  - 2020
KW  - calibration
KW  - image registration
KW  - integer programming
KW  - iterative methods
KW  - mobile robots
KW  - robot vision
KW  - iterative nearest-neighbor
KW  - mixed-integer program
KW  - data association
KW  - integer variables
KW  - 3D line-based registration
KW  - mixed-integer programming
KW  - rigid-body transformation
KW  - 3D point cloud data
KW  - mobile robotics
KW  - sensor calibration
KW  - linear line-based 2D-3D registration
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Cameras
KW  - Cost function
KW  - Robot sensing systems
KW  - Transforms
KW  - Symmetric matrices
DO  - 10.1109/ICRA40945.2020.9196718
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Determining the rigid-body transformation be-tween 2D image data and 3D point cloud data has applications for mobile robotics including sensor calibration and localizing into a prior map. Common approaches to 2D-3D registration use least-squares solvers assuming known associations often provided by heuristic front-ends, or iterative nearest-neighbor. We present a linear line-based 2D-3D registration algorithm formulated as a mixed-integer program to simultaneously solve for the correct transformation and data association. Our formulation is explicitly formulated to handle outliers, by modeling associations as integer variables. Additionally, we can constrain the registration to SE(2) to improve runtime and accuracy. We evaluate this search over multiple real-world data sets demonstrating adaptability to scene variation.
ER  - 

TY  - CONF
TI  - An efficient solution to the relative pose estimation with a common direction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11053
EP  - 11059
AU  - Y. Ding
AU  - J. Yang
AU  - H. Kong
PY  - 2020
KW  - accelerometers
KW  - cameras
KW  - eigenvalues and eigenfunctions
KW  - inertial systems
KW  - motion estimation
KW  - polynomial matrices
KW  - pose estimation
KW  - gravity direction
KW  - pose estimation
KW  - camera motion estimation
KW  - camera-IMU systems
KW  - camera-inertial measurement unit systems
KW  - 1DOF
KW  - degree of freedom
KW  - Gr√∂bner basis
KW  - polynomial eigenvalue problem
KW  - 3-point algorithm
KW  - Cameras
KW  - Eigenvalues and eigenfunctions
KW  - Gravity
KW  - Pose estimation
KW  - Transmission line matrix methods
KW  - Symmetric matrices
KW  - Motion estimation
DO  - 10.1109/ICRA40945.2020.9196636
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose an efficient solution to the calibrated camera motion estimation with a common direction. This case is relevant to smart phones, tablets, and other camera-IMU (Inertial measurement unit) systems, which have accelerometers to measure the gravity direction. We can align one of the axes of the camera with this common direction so that the relative rotation between the views reduces to only 1DOF (degree of freedom). This allows us to use only three point correspondences for relative pose estimation. Unlike previous work, we derive new constraints on the simplified essential matrix using an elimination strategy based on Gr√∂bner basis. In this case, computing the coefficients of these constraints require less computation and we only need to solve a polynomial eigenvalue problem. We show detailed analyses and comparisons against the existing 3-point algorithms, with satisfactory results obtained.
ER  - 

TY  - CONF
TI  - Task-Aware Novelty Detection for Visual-based Deep Learning in Autonomous Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11060
EP  - 11066
AU  - V. Chen
AU  - M. -K. Yoon
AU  - Z. Shao
PY  - 2020
KW  - computer vision
KW  - decision making
KW  - learning (artificial intelligence)
KW  - road safety
KW  - road traffic
KW  - safety-critical software
KW  - task-aware novelty detection
KW  - self-driving cars
KW  - trustworthy prediction
KW  - adversarial attacks
KW  - life-threatening decisions
KW  - learning framework
KW  - prediction model
KW  - network saliency
KW  - learning architecture
KW  - decision making
KW  - saliency map
KW  - in-house indoor driving environment
KW  - adversarial attacked images
KW  - target prediction
KW  - deep-learning driven safety-critical autonomous systems
KW  - Training
KW  - Task analysis
KW  - Predictive models
KW  - Roads
KW  - Data models
KW  - Autonomous systems
KW  - Decision making
DO  - 10.1109/ICRA40945.2020.9196720
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. This ability to determine the novelty of a new input with respect to a trained model is critical for such systems because novel inputs due to changes in the environment, adversarial attacks, or even unintentional noise can potentially lead to erroneous, perhaps life-threatening decisions. This paper proposes a learning framework that leverages information learned by the prediction model in a task-aware manner to detect novel scenarios. We use network saliency to provide the learning architecture with knowledge of the input areas that are most relevant to the decision-making and learn an association between the saliency map and the predicted output to determine the novelty of the input. We demonstrate the efficacy of this method through experiments on real-world driving datasets as well as through driving scenarios in our in-house indoor driving environment where the novel image can be sampled from another similar driving dataset with similar features or from adversarial attacked images from the training dataset. We find that our method is able to systematically detect novel inputs and quantify the deviation from the target prediction through this task-aware approach.
ER  - 

TY  - CONF
TI  - DirectShape: Direct Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11067
EP  - 11073
AU  - R. Wang
AU  - N. Yang
AU  - J. St√ºckler
AU  - D. Cremers
PY  - 2020
KW  - image reconstruction
KW  - image segmentation
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - shape recognition
KW  - stereo image processing
KW  - state-of-the-art deep learning based 3D object detectors
KW  - previous geometric approach
KW  - adaptive sparse point selection scheme
KW  - silhouette alignment term
KW  - dense stereo reconstruction
KW  - stereo image pair
KW  - 3D rigid-body poses
KW  - 3D bounding boxes
KW  - instance segmentations
KW  - simple bounding boxes
KW  - object level
KW  - autonomous driving
KW  - scene understanding
KW  - shape estimation
KW  - visual vehicle pose
KW  - shape priors
KW  - direct photometric alignment
KW  - Shape
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Automobiles
KW  - Current measurement
KW  - Solid modeling
KW  - Image reconstruction
DO  - 10.1109/ICRA40945.2020.9197095
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.
ER  - 

TY  - CONF
TI  - RoadText-1K: Text Detection & Recognition Dataset for Driving Videos
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11074
EP  - 11080
AU  - S. Reddy
AU  - M. Mathew
AU  - L. Gomez
AU  - M. Rusinol
AU  - D. Karatzas
AU  - C. V. Jawahar
PY  - 2020
KW  - image recognition
KW  - intelligent transportation systems
KW  - road traffic
KW  - text detection
KW  - traffic engineering computing
KW  - video signal processing
KW  - driver assistance
KW  - RoadText-1K dataset
KW  - text bounding boxes
KW  - driving videos
KW  - text detection
KW  - text recognition
KW  - intelligent systems
KW  - Videos
KW  - Text recognition
KW  - Vehicles
KW  - Image recognition
KW  - Task analysis
KW  - Roads
KW  - Semantics
DO  - 10.1109/ICRA40945.2020.9196577
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Perceiving text is crucial to understand semantics of outdoor scenes and hence is a critical requirement to build intelligent systems for driver assistance and self-driving. Most of the existing datasets for text detection and recognition comprise still images and are mostly compiled keeping text in mind. This paper introduces a new "RoadText-1K" dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. Our dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. State of the art methods for text detection, recognition and tracking are evaluated on the new dataset and the results signify the challenges in unconstrained driving videos compared to existing datasets. This suggests that RoadText-1K is suited for research and development of reading systems, robust enough to be incorporated into more complex downstream tasks like driver assistance and self-driving. The dataset can be found at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k.
ER  - 

TY  - CONF
TI  - End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11081
EP  - 11087
AU  - Z. Song
AU  - J. Lu
AU  - T. Zhang
AU  - H. Li
PY  - 2020
KW  - cameras
KW  - driver information systems
KW  - feature extraction
KW  - image sequences
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - road safety
KW  - robot vision
KW  - video signal processing
KW  - end-to-end learning
KW  - inter-vehicle distance
KW  - ADAS
KW  - monocular camera
KW  - advanced driver-assistance systems
KW  - relative velocity estimation method
KW  - multiple visual clues
KW  - time-consecutive monocular frames
KW  - deep feature clue
KW  - scene geometry clue
KW  - temporal optical flow clue
KW  - vehicle-centric sampling mechanism
KW  - light-weight deep neural network
KW  - Estimation
KW  - Three-dimensional displays
KW  - Cameras
KW  - Optical imaging
KW  - Two dimensional displays
KW  - Feature extraction
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9197557
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.
ER  - 

TY  - CONF
TI  - Learning an Action-Conditional Model for Haptic Texture Generation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11088
EP  - 11095
AU  - N. Heravi
AU  - W. Yuan
AU  - A. M. Okamura
AU  - J. Bohg
PY  - 2020
KW  - feedback
KW  - haptic interfaces
KW  - human-robot interaction
KW  - image texture
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - robot vision
KW  - tactile sensors
KW  - telerobotics
KW  - virtual reality
KW  - Haptic Texture generation
KW  - haptic sensory feedback
KW  - user interactions
KW  - immersive virtual reality
KW  - material properties
KW  - haptic vibration feedback
KW  - Penn Haptic Texture Toolkit
KW  - action-conditional model learning
KW  - GelSight measurements
KW  - teleoperation system
KW  - autonomous robot
KW  - GelSight image texture
KW  - Haptic interfaces
KW  - Autoregressive processes
KW  - Force
KW  - Acceleration
KW  - Predictive models
KW  - Solid modeling
KW  - Discrete Fourier transforms
DO  - 10.1109/ICRA40945.2020.9197447
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Rich haptic sensory feedback in response to user interactions is desirable for an effective, immersive virtual reality or teleoperation system. However, this feedback depends on material properties and user interactions in a complex, non-linear manner. Therefore, it is challenging to model the mapping from material and user interactions to haptic feedback in a way that generalizes over many variations of the user's input. Current methodologies are typically conditioned on user interactions, but require a separate model for each material. In this paper, we present a learned action-conditional model that uses data from a vision-based tactile sensor (GelSight) and user's action as input. This model predicts an induced acceleration that could be used to provide haptic vibration feedback to a user. We trained our proposed model on a publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with GelSight measurements of the different materials. We show that a unified model over all materials outperforms previous methods and generalizes to new actions and new instances of the material categories in the dataset.
ER  - 

TY  - CONF
TI  - Just Noticeable Differences for Joint Torque Feedback During Static Poses
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11096
EP  - 11102
AU  - H. Kim
AU  - H. H. Guo
AU  - A. T. Asbeck
PY  - 2020
KW  - biomechanics
KW  - feedback
KW  - haptic interfaces
KW  - torque control
KW  - visual perception
KW  - joint torque feedback
KW  - kinesthetic feedback
KW  - external torques
KW  - elbow
KW  - preload torques
KW  - test stimulus torques
KW  - stall torque
KW  - average torques
KW  - static poses
KW  - interweaving staircase method
KW  - extension direction
KW  - flexion direction
KW  - size 1.27 nm
KW  - size 0.27 nm
KW  - size 3.0 nm
KW  - time 120.0 s
KW  - size 0.54 nm
KW  - Torque
KW  - Muscles
KW  - Exoskeletons
KW  - Elbow
KW  - Skin
KW  - Force
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197537
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Joint torque feedback is a new and promising means of kinesthetic feedback for providing information to a person or guiding them during a motion task. However, little work has been done in determining the psychophysical parameters of how well humans can detect external torques. In this study, we determine the human perceptual ability to detect kinesthetic feedback at the elbow during all possible combinations of preload torques and test stimulus torques, with the elbow in a static posture. To accomplish this, we constructed an exoskeleton for the elbow providing joint torque feedback. The device is designed to convey 0.54 Nm of stall torque for up to 120 seconds via a semi-rigid sleeve structure. Using this device, we assessed perception capability using the Interweaving Staircase Method. We found that users could detect average torques of 0.14-0.18 Nm in the extension or flexion directions with no preload. When a preload of 1.27 Nm was applied, this increased to 0.25-0.27 Nm for when flexion stimuli were applied, and 0.180.3 Nm when extension stimuli were applied, depending on the preload direction.
ER  - 

TY  - CONF
TI  - Design of a Parallel Haptic Device with Gravity Compensation by using its System Weight
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11103
EP  - 11108
AU  - S. -m. Hur
AU  - J. Park
AU  - J. Park
AU  - Y. Oh
PY  - 2020
KW  - bars
KW  - compensation
KW  - couplings
KW  - design engineering
KW  - force feedback
KW  - gravity
KW  - haptic interfaces
KW  - manipulator kinematics
KW  - motion control
KW  - open loop systems
KW  - virtual reality
KW  - system weight
KW  - four-bar-linkage mechanism
KW  - linear motion
KW  - ring-type gimbal mechanism
KW  - gravity force
KW  - conceptual mechanical design
KW  - four-bar mechanism
KW  - gravity compensation
KW  - open-loop force display performance
KW  - GHap
KW  - parallel haptic device
KW  - 6 degree of freedom manipulator
KW  - forward kinematics
KW  - virtual reality
KW  - Gravity
KW  - Haptic interfaces
KW  - Jacobian matrices
KW  - Torque
KW  - Manipulators
KW  - Kinematics
DO  - 10.1109/ICRA40945.2020.9197065
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a 6 degree of freedom (DoF) manipulator for haptic application. The proposed haptic device, named GHap, is designed based on the four-bar-linkage mechanism for linear motion with the ring-type gimbal mechanism. To improve the force display ability, the device is designed to compensate the gravity force of the manipulator by its own weight. The conceptual mechanical design is compared by placing the third joint, which controls the four-bar mechanism, in two different configurations. The forward kinematics and the jacobian of GHap are presented. Finally, the gravity compensation method and open-loop force display performance of the proposed haptic device are validated by an experiment with the GHap prototype.
ER  - 

TY  - CONF
TI  - Multimodal tracking framework for visual odometry in challenging illumination conditions
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11133
EP  - 11139
AU  - A. Beauvisage
AU  - K. Ahiska
AU  - N. Aouf
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - feature extraction
KW  - image matching
KW  - motion estimation
KW  - robot vision
KW  - stereo image processing
KW  - visible spectrum
KW  - electromagnetic spectrum
KW  - extreme illumination conditions
KW  - camera setups
KW  - multimodal monocular visual odometry solution
KW  - multimodal tracking framework
KW  - stereo matching techniques
KW  - long wave infrared spectral bands
KW  - LWIR
KW  - MMS-VO
KW  - windowed bundle adjustment framework
KW  - motion estimation process
KW  - visible-thermal datasets
KW  - feature tracking
KW  - visual odometry trajectory
KW  - Cameras
KW  - Feature extraction
KW  - Bundle adjustment
KW  - Visual odometry
KW  - Lighting
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9196891
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Research on visual odometry and localisation is largely dominated by solutions developed in the visible spectrum, where illumination is a critical factor. Other parts of the electromagnetic spectrum are currently being investigated to generate solutions dealing with extreme illumination conditions. Multispectral setups are particularly interesting as they provide information from different parts of the spectrum at once. However, the main challenge of such camera setups is the lack of similarity between the images produced, which makes conventional stereo matching techniques obsolete.This work investigates a new way of concurrently processing images from different spectra for application to visual odometry. It particularly focuses on the visible and Long Wave InfraRed (LWIR) spectral bands where dissimilarity between pixel intensities is maximal. A new Multimodal Monocular Visual Odometry solution (MMS-VO) is presented. With this novel approach, features are tracked simultaneously, but only the camera providing the best tracking quality is used to estimate motion. Visual odometry is performed within a windowed bundle adjustment framework, by alternating between the cameras as the nature of the scene changes. Furthermore, the motion estimation process is robustified by selecting adequate keyframes based on parallax.The algorithm was tested on a series of visible-thermal datasets, acquired from a car with real driving conditions. It is shown that feature tracking could be performed in both modalities with the same set of parameters. Additionally, the MMS-VO provides a superior visual odometry trajectory as one camera can compensate when the other is not working.
ER  - 

TY  - CONF
TI  - Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11140
EP  - 11146
AU  - K. de Langis
AU  - J. Sattar
PY  - 2020
KW  - autonomous underwater vehicles
KW  - control engineering computing
KW  - convolutional neural nets
KW  - feature extraction
KW  - human-robot interaction
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - custom CNN
KW  - deep SORT algorithm
KW  - realtime tracking-by-detection
KW  - realtime diver detection
KW  - initial diver detection
KW  - appearance metric
KW  - simple online realtime tracking
KW  - human divers
KW  - autonomous underwater robots
KW  - underwater human-robot collaboration
KW  - realtime multidiver tracking re-identification
KW  - on-board tracking
KW  - on-board autonomous robot operations
KW  - multiperson tracking
KW  - Robots
KW  - Tracking
KW  - Feature extraction
KW  - Collaboration
KW  - Unmanned underwater vehicles
KW  - Task analysis
DO  - 10.1109/ICRA40945.2020.9197308
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g., to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.
ER  - 

TY  - CONF
TI  - Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11147
EP  - 11154
AU  - J. Zhan
AU  - J. Cartucho
AU  - S. Giannarou
PY  - 2020
KW  - biological tissues
KW  - biomedical optical imaging
KW  - medical image processing
KW  - medical robotics
KW  - surgery
KW  - visual servoing
KW  - autonomous tissue scanning
KW  - free-form motion
KW  - intraoperative tissue characterisation
KW  - imaging probes
KW  - tissue surface
KW  - robot-assisted local tissue scanning
KW  - motion stabilisation
KW  - periodic motion
KW  - free-form tissue motion
KW  - scanning trajectory
KW  - ultrasound tissue scanning
KW  - Three-dimensional displays
KW  - Probes
KW  - Tracking
KW  - Robots
KW  - Cameras
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197294
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is required for subsurface visualisation to characterise the state of the tissue. However, scanning of large tissue surfaces in the presence of motion is a challenging task for the surgeon. Recently, robot-assisted local tissue scanning has been investigated for motion stabilisation of imaging probes to facilitate the capturing of good quality images and reduce the surgeon's cognitive load. Nonetheless, these approaches require the tissue surface to be static or translating with periodic motion. To eliminate these assumptions, we propose a visual servoing framework for autonomous tissue scanning, able to deal with free-form tissue motion. The 3D structure of the surgical scene is recovered, and a feature-based method is proposed to estimate the motion of the tissue in real-time. The desired scanning trajectory is manually defined on a reference frame and continuously updated using projective geometry to follow the tissue motion and control the movement of the robotic arm. The advantage of the proposed method is that it does not require the learning of the tissue motion prior to scanning and can deal with free-form motion. We deployed this framework on the da Vinci¬Æsurgical robot using the da Vinci Research Kit (dVRK) for Ultrasound tissue scanning. Our framework can be easily extended to other probe-based imaging modalities.
ER  - 

TY  - CONF
TI  - 3D-Printed Electroactive Hydraulic Valves for Use in Soft Robotic Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11200
EP  - 11206
AU  - N. Bira
AU  - Y. Meng√º√ß
AU  - J. R. Davidson
PY  - 2020
KW  - design engineering
KW  - electrorheology
KW  - human-robot interaction
KW  - hydraulic actuators
KW  - hydraulic systems
KW  - industrial robots
KW  - valves
KW  - 3D-printed electroactive hydraulic valves
KW  - soft robotic applications
KW  - human-robot interaction
KW  - alternative locomotion techniques
KW  - open-source method
KW  - high-pressure electrorheological valves
KW  - electrorheological fluid-based control
KW  - deformable actuators
KW  - safety areas
KW  - design engineering
KW  - pressure 230.0 kPa
KW  - time 1.0 s to 3.0 s
KW  - Valves
KW  - Erbium
KW  - Three-dimensional displays
KW  - Soft robotics
KW  - Electrodes
KW  - Actuators
DO  - 10.1109/ICRA40945.2020.9196993
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Soft robotics promises developments in the research areas of safety, bio-mimicry, manipulation, human-robot interaction, and alternative locomotion techniques. The research presented here is directed towards developing an improved, low-cost, and open-source method for soft robotic control using electrorheological fluids in compact, 3D-printed electroactive hydraulic valves. We construct high-pressure electrorheological valves and deformable actuators using only commercially available materials and accessible fabrication methods. The printed valves were characterized with industrial-grade electrorheological fluid (RheOil 3.0), but the design is generalizable to other electrorheological fluids. Valve performance was shown to be an improvement over comparable work with demonstrated higher yield pressures at lower voltages (up to 230 kPa), larger flow rates (up to 15 ml/min) and lower response times (1 to 3 seconds, depending on design). The resulting valve and actuator systems enable future novel applications of electrorheological fluid-based control and hydraulics in soft robotics and other disciplines.
ER  - 

TY  - CONF
TI  - Variable Damping Control of a Robotic Arm to Improve Trade-off between Agility and Stability and Reduce User Effort
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11259
EP  - 11265
AU  - T. Bitz
AU  - F. Zahedi
AU  - H. Lee
PY  - 2020
KW  - damping
KW  - end effectors
KW  - human-robot interaction
KW  - stability
KW  - variable structure systems
KW  - stability
KW  - fixed damping controllers
KW  - variable robotic damping controller
KW  - robotic arm
KW  - physical human robot interaction
KW  - dual sided logistic function
KW  - end effector
KW  - 7 degree-of-freedom robot
KW  - root mean squared interaction forces
KW  - Damping
KW  - Stability analysis
KW  - Service robots
KW  - Acceleration
KW  - Safety
KW  - Impedance
DO  - 10.1109/ICRA40945.2020.9196572
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a variable damping controller to improve the trade-off between agility and stability in physical human-robot interaction (pHRI), while reducing user effort. Variable robotic damping, defined as a dual-sided logistic function, was determined in real time throughout a range of negative to positive values based on the user's intent of movement. To evaluate the effectiveness of the proposed controller, we performed a set of human experiments with subjects interacting with the end-effector of a 7 degree-of-freedom robot. Twelve subjects completed target reaching tasks under three robotic damping conditions: fixed positive, fixed negative, and variable damping. On average, the variable damping controller significantly shortened the rise time by 22.4% compared to the fixed positive damping. It is also important to note that the rise time in the variable damping condition was as fast as that in the fixed negative damping condition and there was no statistical difference between the two conditions. The variable damping controller significantly decreased the percentage overshoot by 49.6% and shortened the settling time by 29.0% compared to the fixed negative damping. Both the maximum and mean root-mean-squared (RMS) interaction forces were significantly lower in the variable damping condition than the other two fixed damping conditions, i.e., the variable damping controller reduced user effort. The maximum and mean RMS interaction forces were at least 17.3% and 20.3% lower than any of the fixed damping conditions, respectively. The results of this study demonstrate that humans can extract the benefits of the variable damping controller in the context of pHRI, as it significantly improves the trade-off between agility and stability and reduces user effort in comparison to fixed damping controllers.
ER  - 

TY  - CONF
TI  - Cognitive and motor compliance in intentional human-robot interaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11291
EP  - 11297
AU  - H. F. Chame
AU  - J. Tani
PY  - 2020
KW  - cognitive systems
KW  - humanoid robots
KW  - human-robot interaction
KW  - mobile robots
KW  - torque feedback
KW  - humanoid Torobo
KW  - bio-inspired study
KW  - cognitive compliance
KW  - human environments
KW  - adaptive robotics
KW  - natural cognition
KW  - subjective experience
KW  - intentional human-robot interaction
KW  - motor compliance
KW  - Torque
KW  - Joints
KW  - Neural networks
KW  - Predictive models
KW  - Robot sensing systems
KW  - Stochastic processes
DO  - 10.1109/ICRA40945.2020.9196896
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Embodiment and subjective experience in humanrobot interaction are important aspects to consider when studying both natural cognition and adaptive robotics to human environments. Although several researches have focused on nonverbal communication and collaboration, the study of autonomous physical interaction has obtained less attention. From the perspective of neurorobotics, we investigate the relation between intentionality, motor compliance, cognitive compliance, and behavior emergence. We propose a variational model inspired by the principles of predictive coding and active inference to study intentionality and cognitive compliance, and an intermittent control concept for motor deliberation and compliance based on torque feed-back. Our experiments with the humanoid Torobo portrait interesting perspectives for the bio-inspired study of developmental and social processes.
ER  - 

TY  - CONF
TI  - Adaptive Authority Allocation in Shared Control of Robots Using Bayesian Filters
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11298
EP  - 11304
AU  - R. Balachandran
AU  - H. Mishra
AU  - M. Cappelli
AU  - B. Weber
AU  - C. Secchi
AU  - C. Ott
AU  - A. Albu-Schaeffer
PY  - 2020
KW  - Bayes methods
KW  - delays
KW  - mobile robots
KW  - stability
KW  - telerobotics
KW  - adaptive authority allocation
KW  - Bayesian filter
KW  - control framework
KW  - autonomous system
KW  - human operator
KW  - time-varying measurement noise characteristics
KW  - system-driven adaptive shared control framework
KW  - stability proof
KW  - teleoperation
KW  - Bayes methods
KW  - Robots
KW  - Uncertainty
KW  - Task analysis
KW  - Measurement uncertainty
KW  - Resource management
KW  - Noise measurement
KW  - Adaptive authority allocation
KW  - shared control
KW  - teleoperation
KW  - Kalman filter
KW  - Bayesian filters
DO  - 10.1109/ICRA40945.2020.9196941
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In the present paper, we propose a novel system-driven adaptive shared control framework in which the autonomous system allocates the authority among the human operator and itself. Authority allocation is based on a metric derived from a Bayesian filter, which is being adapted online according to real measurements. In this way, time-varying measurement noise characteristics are incorporated. We present the stability proof for the proposed shared control architecture with adaptive authority allocation, which includes time delay in the communication channel between the operator and the robot. Furthermore, the proposed method is validated through experiments and a user-study evaluation. The obtained results indicate significant improvements in task execution compared with pure teleoperation.
ER  - 

TY  - CONF
TI  - Tactile Telerobots for Dull, Dirty, Dangerous, and Inaccessible Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11305
EP  - 11310
AU  - J. A. Fishel
AU  - T. Oliver
AU  - M. Eichermueller
AU  - G. Barbieri
AU  - E. Fowler
AU  - T. Hartikainen
AU  - L. Moss
AU  - R. Walker
PY  - 2020
KW  - computational complexity
KW  - dexterous manipulators
KW  - haptic interfaces
KW  - telerobotics
KW  - first-generation telerobot
KW  - task complexity
KW  - tactile telerobots
KW  - inaccessible tasks
KW  - highly-dexterous bimanual tactile telerobot
KW  - bare human hands
KW  - anthropomorphic robot hands
KW  - biomimetic tactile sensors
KW  - in-hand manipulation
KW  - autonomous robotic hands
KW  - robotic dexterity
KW  - Robot sensing systems
KW  - Haptic interfaces
KW  - Task analysis
KW  - Force
KW  - Manipulators
KW  - Electrodes
DO  - 10.1109/ICRA40945.2020.9196888
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The sense of touch, which is essential for human dexterity, is virtually absent from today's robotic hands. In this work we present progress in creating a highly-dexterous bimanual tactile telerobot, and evaluate its performance compared to bare human hands. The system, consisting of anthropomorphic robot hands, biomimetic tactile sensors, and advanced haptic gloves, enables a human operator to intuitively control and feel what the robotic hands are touching. Through carefully tuned tactile and kinematic mapping it was possible to intuitively perform dexterous operations, including pick and place tasks and even in-hand manipulation, a challenge for most autonomous robotic hands. Performance of the system was evaluated in standard measures of human and robotic dexterity such as the Box and Block test and other YCB benchmarks. This first-generation telerobot was found to have promising performance with the pilot able to do the same tasks in the telerobot between 1/4th to 1/12th the speed of their bare hands depending on the task complexity.
ER  - 

TY  - CONF
TI  - A Novel Orientability Index and the Kinematic Design of the RemoT-ARM: A Haptic Master with Large and Dexterous Workspace
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11319
EP  - 11325
AU  - G. Li
AU  - E. Del Bianco
AU  - F. Caponetto
AU  - V. Katsageorgiou
AU  - N. G. Tsagarakis
AU  - I. Sarakoglou
PY  - 2020
KW  - control system synthesis
KW  - dexterous manipulators
KW  - haptic interfaces
KW  - manipulator kinematics
KW  - optimal systems
KW  - performance index
KW  - telerobotics
KW  - RemoT-ARM
KW  - kinematic design
KW  - dexterous workspace
KW  - performance index
KW  - relative orientability index
KW  - target workspace
KW  - performance indices
KW  - haptic master device dexterity
KW  - 6-DOF haptic master device
KW  - workspace matching degree
KW  - Manipulators
KW  - Indexes
KW  - Haptic interfaces
KW  - Kinematics
KW  - Performance evaluation
KW  - Force
KW  - Phantoms
DO  - 10.1109/ICRA40945.2020.9196710
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Orientability is an important performance index to evaluate the dexterity of haptic master devices. Currently, most of the existing haptic master devices have limited workspace and limited dexterity. In this paper, we present the RemoT-ARM, a 6 Degree-of-Freedom (DOF) haptic master device that can provide larger and more dexterous workspace for operators. To evaluate its reachability of orientations, we propose a novel orientability index. Furthermore, a relative orientability index is proposed to characterize the matching degree of the workspace of a given manipulator to its target workspace. The volume, the manipulability and the condition number are also introduced as performance indices to evaluate the size and the isotropy of the workspace. According to these performance indices, all possible configurations for the RemoT-ARM have been taken into consideration, analyzed, and compared to finalize its optimal configuration.
ER  - 

TY  - CONF
TI  - RAVEN-S: Design and Simulation of a Robot for Teleoperated Microgravity Rodent Dissection Under Time Delay
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11332
EP  - 11337
AU  - A. Lewis
AU  - D. Drajeske
AU  - J. Raiti
AU  - A. Berens
AU  - J. Rosen
AU  - B. Hannaford
PY  - 2020
KW  - aerospace instrumentation
KW  - biocontrol
KW  - manipulators
KW  - medical robotics
KW  - mobile robots
KW  - space research
KW  - space vehicles
KW  - surgery
KW  - telerobotics
KW  - zero gravity experiments
KW  - teleoperated Microgravity Rodent dissection
KW  - International Space Station
KW  - ISS
KW  - biological effects
KW  - spaceflight
KW  - Rodent Habitat
KW  - Microgravity Science Glovebox
KW  - teleoperation
KW  - RAVEN II
KW  - rudimentary interaction force estimation
KW  - onboard dissection robot
KW  - RAVEN-S prototype design
KW  - communications time delay
KW  - robot design
KW  - robot simulation
KW  - Tools
KW  - Task analysis
KW  - Rodents
KW  - Delay effects
KW  - Delays
KW  - Force
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196691
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The International Space Station (ISS) serves as a research lab for a wide variety of experiments including some that study the biological effects of microgravity and spaceflight using the Rodent Habitat and Microgravity Science Glovebox (MSG). Astronauts train for onboard dissections of rodents following basic training. An alternative approach for conducting these experiments is teleoperation of a robot located on the ISS from earth by a scientist who is proficient in rodent dissection. This pilot study addresses (1) the effects of extreme time delay on skill degradation during Fundamentals of Laparoscopic Surgery (FLS) tasks and rodent dissections using RAVEN II; (2) derivation and testing of rudimentary interaction force estimation; (3) elicitation of design requirements for an onboard dissection robot, RAVEN-S; and (4) simulation of the RAVEN-S prototype design with dissection data. The results indicate that the tasks' completion times increased by a factor of up to 9 for a 3 s time delay while performing manipulation and cutting tasks (FLS model) and by a factor of up to 3 for a 0.75 s time delay during mouse dissection tasks (animal model). Average robot forces/torques of 14N/0.1Nm (peak 90N/0.75Nm) were measured along with average linear/angular velocities of 0.02m/s/4rad/s (peak 0.1m/s/40rad/s) during dissection. A triangular configuration of three arms with respect to the operation site showed the best configuration given the MSG geometry and the dissection tasks. In conclusion, the results confirm the feasibility of utilizing a surgically-inspired RAVEN-S robot for teleoperated rodent dissection for successful completion of the predefined tasks in the presence of communications time delay between the ISS and ground control.
ER  - 

TY  - CONF
TI  - Collision-free Navigation of Human-centered Robots via Markov Games
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11338
EP  - 11344
AU  - G. Ye
AU  - Q. Lin
AU  - T. -H. Juang
AU  - H. Liu
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - mobile robots
KW  - multi-agent systems
KW  - multi-robot systems
KW  - collision-free navigation
KW  - human-centered robots
KW  - Markov games
KW  - robot navigation
KW  - single-agent Markov decision process
KW  - static environment
KW  - multiagent formulation
KW  - primary agent
KW  - remaining auxiliary agents
KW  - path-following type adversarial training strategy
KW  - robust decentralized collision avoidance policy
KW  - real-world mobile robots
KW  - Collision avoidance
KW  - Robots
KW  - Markov processes
KW  - Navigation
KW  - Games
KW  - Robustness
KW  - Training
KW  - Collision-free navigation
KW  - human-centered robotics
KW  - deep reinforcement learning
KW  - multi-agent system
KW  - adversarial training
DO  - 10.1109/ICRA40945.2020.9196810
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We exploit Markov games as a framework for collision-free navigation of human-centered robots. Unlike the classical methods which formulate robot navigation as a single-agent Markov decision process with a static environment, our framework of Markov games adopts a multi-agent formulation with one primary agent representing the robot and the remaining auxiliary agents form a dynamic or even competing environment. Such a framework allows us to develop a path-following type adversarial training strategy to learn a robust decentralized collision avoidance policy. Through thorough experiments on both simulated and real-world mobile robots, we show that the learnt policy outperforms the state-of-the-art algorithms in both sample complexity and runtime robustness.
ER  - 

TY  - CONF
TI  - DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11345
EP  - 11352
AU  - A. J. Sathyamoorthy
AU  - J. Liang
AU  - U. Patel
AU  - T. Guan
AU  - R. Chandra
AU  - D. Manocha
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - pedestrians
KW  - trajectory control
KW  - DenseCAvoid
KW  - real-time navigation
KW  - dense crowds
KW  - anticipatory behaviors
KW  - pedestrian behaviors
KW  - visual sensors
KW  - pedestrian trajectory prediction algorithm
KW  - input frames
KW  - compute bounding boxes
KW  - pedestrian positions
KW  - future time
KW  - hybrid approach
KW  - deep reinforcement learning-based collision avoidance method
KW  - robust trajectories
KW  - static scenarios
KW  - dynamic scenarios
KW  - multiple pedestrians
KW  - robot freezing
KW  - trajectory lengths
KW  - mean arrival times
KW  - Collision avoidance
KW  - Navigation
KW  - Trajectory
KW  - Robot sensing systems
KW  - Robustness
KW  - Tracking
DO  - 10.1109/ICRA40945.2020.9197379
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present DenseCAvoid, a novel algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and compute bounding boxes that extrapolate to the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (~1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.
ER  - 

TY  - CONF
TI  - DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for Automated Driving Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11353
EP  - 11360
AU  - S. K. Bashetty
AU  - H. Ben Amor
AU  - G. Fainekos
PY  - 2020
KW  - cameras
KW  - Internet
KW  - public domain software
KW  - road safety
KW  - road vehicles
KW  - traffic engineering computing
KW  - video signal processing
KW  - real-world collision scenarios
KW  - autonomous vehicles
KW  - uncalibrated monocular camera source
KW  - DeepCrashTest
KW  - virtual crash tests
KW  - automated driving systems
KW  - dashcam crash videos
KW  - 3D vehicle trajectories
KW  - open-source implementation
KW  - Internet
KW  - Three-dimensional displays
KW  - Trajectory
KW  - Cameras
KW  - Videos
KW  - Tracking
KW  - Vehicle crash testing
KW  - Data mining
DO  - 10.1109/ICRA40945.2020.9197053
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The goal of this paper is to generate simulations with real-world collision scenarios for training and testing autonomous vehicles. We use numerous dashcam crash videos uploaded on the internet to extract valuable collision data and recreate the crash scenarios in a simulator. We tackle the problem of extracting 3D vehicle trajectories from videos recorded by an unknown and uncalibrated monocular camera source using a modular approach. A working architecture and demonstration videos along with the open-source implementation are provided with the paper.
ER  - 

TY  - CONF
TI  - Robotic Control of a Magnetic Swarm for On-Demand Intracellular Measurement
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11385
EP  - 11391
AU  - X. Wang
AU  - T. Wang
AU  - G. Shan
AU  - J. Law
AU  - C. Dai
AU  - Z. Zhang
AU  - Y. Sun
PY  - 2020
KW  - biochemistry
KW  - biomedical materials
KW  - biomedical optical imaging
KW  - cellular biophysics
KW  - dyes
KW  - fluorescence
KW  - magnetic particles
KW  - medical robotics
KW  - micromanipulators
KW  - nanomedicine
KW  - nanoparticles
KW  - pH
KW  - fluorescent dyes
KW  - biochemical measurements
KW  - ion concentrations
KW  - signal-to-noise ratios
KW  - dye-coated magnetic nanoparticles
KW  - magnetic micromanipulation systems
KW  - generated swarm
KW  - magnetic micromanipulation system
KW  - position control accuracy
KW  - intracellular pH mapping
KW  - global dye treatment
KW  - fluorescent dye concentration
KW  - intracellular measurement results
KW  - robotic control
KW  - on-demand intracellular measurement
KW  - pH sensitive fluorescent dye-coated magnetic nanoparticles
KW  - Coils
KW  - Magnetic devices
KW  - Magnetic separation
KW  - Magnetic resonance imaging
KW  - Magnetic particles
KW  - Signal to noise ratio
KW  - Magnetic nanoparticles
DO  - 10.1109/ICRA40945.2020.9197532
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In biology, fluorescent dyes are routinely used for biochemical measurements such as pH and ion concentrations. They, especially when used for detecting a low concentration of ions, suffer from low signal-to-noise ratios (SNR); and increasing the concentration of fluorescent dyes causes more sever cytotoxicity. We invented a new approach that uses a low amount of fluorescent dye-coated magnetic nanoparticles for on-demand, accurately aggregating the nanoparticles and thus fluorescent dyes in a local region inside a cell for intracellular measurement. Experiments proved this approach is capable of achieving a significantly higher SNR and lower cytotoxicity. Different from existing magnetic micromanipulation systems that generate large swarms (several microns and above) or cannot move the generated swarm to an arbitrary position, we developed a five-pole magnetic micromanipulation system and technique for generating a small swarm (e.g., 1 Œºm; capable of generating a magnetic swarm from 0.52 Œºm to 52.7 Œºm with an error <; 7.5 %) and accurately positioning the small swarm (position control accuracy: 0.76 Œºm). As an example, the system performed intracellular pH mapping using a 1 Œºm swarm of pH sensitive fluorescent dye-coated magnetic nanoparticles. The swarm had an SNR inside a cell 10 times that by the traditional method, i.e., global dye treatment, with both cases using the same fluorescent dye concentration. Our intracellular measurement results, for the first time, quantitatively revealed the existence of pH gradient and polarized pH distribution in live migrating cells.
ER  - 

TY  - CONF
TI  - Acoustofluidic Tweezers for the 3D Manipulation of Microparticles
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11392
EP  - 11397
AU  - X. Guo
AU  - Z. Ma
AU  - R. Goyal
AU  - M. Jeong
AU  - W. Pang
AU  - P. Fischer
AU  - X. Duan
AU  - T. Qiu
PY  - 2020
KW  - acoustic streaming
KW  - hydrodynamics
KW  - microfabrication
KW  - microfluidics
KW  - micromanipulators
KW  - position control
KW  - transducers
KW  - high-speed acoustic streaming
KW  - manipulation velocity
KW  - trapped particle
KW  - particle manipulation
KW  - centimeter distance
KW  - transducer surface
KW  - streaming flow field
KW  - hydrodynamic force
KW  - microfabricated gigahertz transducer
KW  - three-dimensional space
KW  - dynamic position control
KW  - spatial distance
KW  - microscale objects
KW  - microrobotics
KW  - noncontact manipulation
KW  - microparticle
KW  - 3D manipulation
KW  - acoustofluidic tweezers
KW  - Transducers
KW  - Acoustics
KW  - Force
KW  - Fluids
KW  - Three-dimensional displays
KW  - Streaming media
KW  - Hydrodynamics
DO  - 10.1109/ICRA40945.2020.9197265
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Non-contact manipulation is of great importance in the actuation of micro-robotics. It is challenging to contactless manipulate micro-scale objects over large spatial distance in fluid. Here, we describe a novel approach for the dynamic position control of microparticles in three-dimensional (3D) space, based on high-speed acoustic streaming generated by a micro-fabricated gigahertz transducer. The hydrodynamic force generated by the streaming flow field has a vertical component against gravity and a lateral component towards the center, thus the microparticle is able to be stably trapped at a position far from the transducer surface, and to be manipulated over centimeter distance in 3D. Only the hydrodynamic force is utilized in the system for particle manipulation, making it a versatile tool regardless the material properties of the trapped particle. The system shows high reliability and manipulation velocity, revealing its potentials for the applications in robotics and automation at small scales.
ER  - 

TY  - CONF
TI  - An online scheduling algorithm for human-robot collaborative kitting
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11430
EP  - 11435
AU  - R. Maderna
AU  - M. Poggiali
AU  - A. M. Zanchettin
AU  - P. Rocco
PY  - 2020
KW  - ergonomics
KW  - human-robot interaction
KW  - logistics
KW  - occupational health
KW  - occupational safety
KW  - productivity
KW  - robotic assembly
KW  - scheduling
KW  - warehousing
KW  - online scheduling algorithm
KW  - human-robot collaborative kitting
KW  - assembly line
KW  - key logistic task
KW  - human operators
KW  - work-related musculoskeletal disorders
KW  - picking operations
KW  - offline scheduler
KW  - warehouse
KW  - productivity analysis
KW  - Task analysis
KW  - Ergonomics
KW  - Robot kinematics
KW  - Strain
KW  - Collaboration
KW  - Scheduling algorithms
DO  - 10.1109/ICRA40945.2020.9197431
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In manufacturing, kitting is the process of grouping separate items together to be supplied as one unit to the assembly line. This is a key logistic task, which is usually performed manually by human operators. However, picking objects from the warehouse implies a great repetitiveness in arm motion. Moreover, the weight and position of items may increase the physical strain and induce the development of work-related musculoskeletal disorders. The inclusion of a collaborative robot in the process may help to reduce the operator's effort and increase productivity. This paper introduces an online scheduling algorithm to guide the picking operations of the human and the robot. The proposed approach has been experimentally evaluated and compared with an offline scheduler, as well as with the baseline case of manual kitting.
ER  - 

TY  - CONF
TI  - A Model-Free Approach to Meta-Level Control of Anytime Algorithms
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11436
EP  - 11442
AU  - J. Svegliato
AU  - P. Sharma
AU  - S. Zilberstein
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - optimisation
KW  - autonomous system
KW  - real-time planning problems
KW  - model-free approach
KW  - anytime algorithms
KW  - computation time
KW  - meta-level control technique
KW  - meta-level control problem
KW  - reinforcement learning methods
KW  - mobile robot domain
KW  - Learning (artificial intelligence)
KW  - Autonomous systems
KW  - Planning
KW  - Heuristic algorithms
KW  - Computational modeling
KW  - Real-time systems
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9196898
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Anytime algorithms offer a trade-off between solution quality and computation time that has proven to be useful in autonomous systems for a wide range of real-time planning problems. In order to optimize this trade-off, an autonomous system has to solve a challenging meta-level control problem: it must decide when to interrupt the anytime algorithm and act on the current solution. Prevailing meta-level control techniques, however, make a number of unrealistic assumptions that reduce their effectiveness and usefulness in the real world. Eliminating these assumptions, we first introduce a model-free approach to meta-level control based on reinforcement learning and prove its optimality. We then offer a general meta-level control technique that can use different reinforcement learning methods. Finally, we show that our approach is effective across several common benchmark domains and a mobile robot domain.
ER  - 

TY  - CONF
TI  - Simultaneous task allocation and motion scheduling for complex tasks executed by multiple robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11443
EP  - 11449
AU  - J. K. Behrens
AU  - K. Stepanova
AU  - R. Babuska
PY  - 2020
KW  - cutting
KW  - industrial manipulators
KW  - motion control
KW  - multi-robot systems
KW  - optimisation
KW  - rapid prototyping (industrial)
KW  - scheduling
KW  - spot welding
KW  - time-varying portion
KW  - generic optimization method
KW  - varying complexity
KW  - dual-arm robot
KW  - robot arm
KW  - motion scheduling
KW  - multiple robot coordination
KW  - simultaneous task allocation
KW  - additive manufacturing
KW  - cutting
KW  - spot welding
KW  - bolt tightening
KW  - bolt inserting
KW  - robot kinematics
KW  - Task analysis
KW  - Robot kinematics
KW  - Planning
KW  - Collision avoidance
KW  - Job shop scheduling
KW  - Resource management
KW  - task scheduling
KW  - dual-arm manipulation
KW  - motion planning
KW  - multi-robot systems
DO  - 10.1109/ICRA40945.2020.9197103
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The coordination of multiple robots operating simultaneously in the same workspace requires the integration of task allocation and motion scheduling. We focus on tasks in which the robot's actions are not confined to small volumes, but can also occupy a large time-varying portion of the workspace, such as in welding along a line. The optimization of such tasks presents a considerable challenge mainly due to the fact that different variants of task execution exist, for instance, there can be multiple starting points of lines or closed curves, differentfilling patterns of areas, etc. We propose a generic and computationally efficient optimization method which is based on constraint programming. It takes into account the kinematics of the robots and guarantees that the motions of the robots are collision-free while minimizing the overall makespan. We evaluate our approach on several use-cases of varying complexity: cutting, additive manufacturing, spot welding, inserting and tightening bolts, performed by a dual-arm robot. In terms of the makespan, the result is superior to task execution by one robot arm as well as by two arms not working simultaneously.
ER  - 

TY  - CONF
TI  - Efficient Planning for High-Speed MAV Flight in Unknown Environments Using Online Sparse Topological Graphs
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11450
EP  - 11456
AU  - M. Collins
AU  - N. Michael
PY  - 2020
KW  - aerospace navigation
KW  - air safety
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - graph theory
KW  - infinite horizon
KW  - microrobots
KW  - mobile robots
KW  - probability
KW  - robot vision
KW  - search problems
KW  - high-speed MAV flight
KW  - online sparse topological graphs
KW  - safe high-speed autonomous navigation
KW  - local planning grid
KW  - computationally-efficient planning architecture
KW  - safe high-speed operation
KW  - longer-term memory
KW  - motion primitive-based local receding horizon planner
KW  - memory-efficient sparse topological graph
KW  - planning system
KW  - complex simulation environments
KW  - robot decision making
KW  - probabilistic collision avoidance
KW  - safe rerouting
KW  - Planning
KW  - Collision avoidance
KW  - Robot sensing systems
KW  - Libraries
KW  - Safety
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9197167
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Safe high-speed autonomous navigation for MAVs in unknown environments requires fast planning to enable the robot to adapt and react quickly to incoming information about obstacles within the world. Furthermore, when operating in environments not known a priori, the robot may make decisions that lead to dead ends, necessitating global replanning through a map of the environment outside of a local planning grid. This work proposes a computationally-efficient planning architecture for safe high-speed operation in unknown environments that incorporates a notion of longer-term memory into the planner enabling the robot to accurately plan to locations no longer contained within a local map. A motion primitive-based local receding horizon planner that uses a probabilistic collision avoidance methodology enables the robot to generate safe plans at fast replan rates. To provide global guidance, a memory-efficient sparse topological graph is created online from a time history of the robot's path and a geometric notion of visibility within the environment to search for alternate pathways towards the desired goal if a dead end is encountered. The safety and performance of the proposed planning system is evaluated at speeds up to 10m/s, and the approach is tested in a set of large-scale, complex simulation environments containing dead ends. These scenarios lead to failure cases for competing methods; however, the proposed approach enables the robot to safely reroute and reach the desired goal.
ER  - 

TY  - CONF
TI  - Evaluating Adaptation Performance of Hierarchical Deep Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11457
EP  - 11463
AU  - N. Van Stolen
AU  - S. Hyun Kim
AU  - H. T. Tran
AU  - G. Chowdhary
PY  - 2020
KW  - learning (artificial intelligence)
KW  - multi-agent systems
KW  - neural nets
KW  - differentiated sub-policies
KW  - hierarchical controller
KW  - adaptation performance
KW  - hierarchical deep reinforcement learning
KW  - policy performance
KW  - confidence- based training process
KW  - Training
KW  - Adaptation models
KW  - Trajectory
KW  - Games
KW  - Learning (artificial intelligence)
KW  - Robots
KW  - Switches
DO  - 10.1109/ICRA40945.2020.9197052
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep Reinforcement Learning has been used to exploit specific environments, but has difficulty transferring learned policies to new situations. This issue poses a problem for practical applications of Reinforcement Learning, as real-world scenarios may introduce unexpected differences that drastically reduce policy performance. We propose the use of differentiated sub-policies governed by a hierarchical controller to support adaptation in such scenarios. We also introduce a confidence- based training process for the hierarchical controller which improves training stability and convergence times. We evaluate these methods in a new Capture the Flag environment designed to explore adaptation in autonomous multi-agent settings.
ER  - 

TY  - CONF
TI  - Iterator-Based Temporal Logic Task Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11472
EP  - 11478
AU  - S. A. Zudaire
AU  - M. Garrett
AU  - S. Uchite
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control system synthesis
KW  - discrete event systems
KW  - mobile robots
KW  - path planning
KW  - temporal logic
KW  - task specifications
KW  - universally quantified locations
KW  - constant time
KW  - hybrid control
KW  - discrete event controller
KW  - synthesised plan
KW  - iterator-based temporal logic task planning
KW  - robotic systems
KW  - state explosion
KW  - discrete locations
KW  - fixed-wing unmanned aerial vehicle
KW  - Task analysis
KW  - Robot sensing systems
KW  - Planning
KW  - Fires
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA40945.2020.9197274
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Temporal logic task planning for robotic systems suffers from state explosion when specifications involve large numbers of discrete locations. We provide a novel approach, particularly suited for task specifications with universally quantified locations, that has constant time with respect to the number of locations, enabling synthesis of plans for an arbitrary number of them. We propose a hybrid control framework that uses an iterator to manage the discretised workspace hiding it from a plan enacted by a discrete event controller. A downside of our approach is that it incurs in increased overhead when executing a synthesised plan. We demonstrate that the overhead is reasonable for missions of a fixed-wing Unmanned Aerial Vehicle in simulated and real scenarios for up to 700000 locations.
ER  - 

TY  - CONF
TI  - Reactive Temporal Logic Planning for Multiple Robots in Unknown Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11479
EP  - 11485
AU  - Y. Kantaros
AU  - M. Malencia
AU  - V. Kumar
AU  - G. J. Pappas
PY  - 2020
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - robot dynamics
KW  - temporal logic
KW  - multiple robots
KW  - reactive mission
KW  - unknown environment
KW  - temporal logic planning approaches
KW  - robot dynamics
KW  - known environments
KW  - abstraction-free LTL planning algorithm
KW  - complex mission planning
KW  - complex planning tasks
KW  - co-safe linear temporal logic formulas
KW  - reactive temporal logic planning
KW  - Robot sensing systems
KW  - Planning
KW  - Task analysis
KW  - Heuristic algorithms
KW  - Automata
DO  - 10.1109/ICRA40945.2020.9197570
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper proposes a new reactive mission planning algorithm for multiple robots that operate in unknown environments. The robots are equipped with individual sensors that allow them to collectively learn and continuously update a map of the unknown environment. The goal of the robots is to accomplish complex tasks, captured by global co-safe Linear Temporal Logic (LTL) formulas. The majority of existing temporal logic planning approaches rely on discrete abstractions of the robot dynamics operating in known environments and, as a result, they cannot be applied to the more realistic scenarios where the environment is initially unknown. In this paper, we address this novel challenge by proposing the first reactive, and abstraction-free LTL planning algorithm that can be applied for complex mission planning of multiple robots operating in unknown environments. Our algorithm is reactive in the sense that temporal logic planning is adapting to the updated map of the environment and abstraction-free as it does not rely on designing abstractions of robot dynamics. Our proposed algorithm is complete under mild assumptions on the structure of the environment and the sensor models. Our paper provides extensive numerical simulations and hardware experiments that illustrate the theoretical analysis and show that the proposed algorithm can address complex planning tasks in unknown environments.
ER  - 

TY  - CONF
TI  - Higher Order Function Networks for View Planning and Multi-View Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11486
EP  - 11492
AU  - S. Engin
AU  - E. Mitchell
AU  - D. Lee
AU  - V. Isler
AU  - D. D. Lee
PY  - 2020
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - robot vision
KW  - shape recognition
KW  - solid modelling
KW  - stereo image processing
KW  - Higher Order function networks
KW  - multiview reconstruction
KW  - visual inspection
KW  - neural network
KW  - shape information
KW  - deep learning
KW  - complete 3D reconstruction
KW  - Higher Order Functions
KW  - reconstruction quality
KW  - multiview HOF network
KW  - image acquisition
KW  - view planning
KW  - visibility quality
KW  - shape representation
KW  - Three-dimensional displays
KW  - Image reconstruction
KW  - Planning
KW  - Cameras
KW  - Surface reconstruction
KW  - Inspection
KW  - Shape
DO  - 10.1109/ICRA40945.2020.9197435
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We consider the problem of planning views for a robot to acquire images of an object for visual inspection and reconstruction. In contrast to offline methods which require a 3D model of the object as input or online methods which rely on only local measurements, our method uses a neural network which encodes shape information for a large number of objects. We build on recent deep learning methods capable of generating a complete 3D reconstruction of an object from a single image. Specifically, in this work, we extend a recent method which uses Higher Order Functions (HOF) to represent the shape of the object. We present a new generalization of this method to incorporate multiple images as input and establish a connection between visibility and reconstruction quality. This relationship forms the foundation of our view planning method where we compute viewpoints to visually cover the output of the multiview HOF network with as few images as possible. Experiments indicate that our method provides a good compromise between online and offline methods: Similar to online methods, our method does not require the true object model as input. In terms of number of views, it is much more efficient. In most cases, its performance is comparable to the optimal offline case even on object classes the network has not been trained on.
ER  - 

TY  - CONF
TI  - Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11493
EP  - 11499
AU  - K. Rana
AU  - B. Talbot
AU  - V. Dasagi
AU  - M. Milford
AU  - N. S√ºnderhauf
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - path planning
KW  - trajectory control
KW  - learned navigation strategies
KW  - residual reinforcement learning framework
KW  - robotic manipulation literature
KW  - mobile robots
KW  - residual control effect
KW  - sub-optimal classical controller
KW  - data efficiency
KW  - cluttered indoor navigation tasks
KW  - residual reactive navigation
KW  - Navigation
KW  - Robots
KW  - Uncertainty
KW  - Training
KW  - Task analysis
KW  - Learning (artificial intelligence)
KW  - Machine learning
DO  - 10.1109/ICRA40945.2020.9197386
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https://sites.google.com/view/srrn/home.
ER  - 

TY  - CONF
TI  - Online Grasp Plan Refinement for Reducing Defects During Robotic Layup of Composite Prepreg Sheets
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11500
EP  - 11507
AU  - R. K. Malhan
AU  - R. Jomy Joseph
AU  - A. V. Shembekar
AU  - A. M. Kabir
AU  - P. M. Bhatt
AU  - S. K. Gupta
PY  - 2020
KW  - dexterous manipulators
KW  - Gaussian processes
KW  - path planning
KW  - quality control
KW  - regression analysis
KW  - sheet materials
KW  - online grasp plan refinement
KW  - robotic layup
KW  - composite prepreg
KW  - high-performance composites
KW  - sheet layup
KW  - composite components
KW  - deformable sheets
KW  - robotic cell
KW  - layup process
KW  - manual layup
KW  - online refinement
KW  - environmental factors
KW  - Gaussian process regression model offline
KW  - grasp plans
KW  - GPR
KW  - Trajectory
KW  - Grasping
KW  - Grippers
KW  - Robot sensing systems
KW  - Computational modeling
KW  - Service robots
DO  - 10.1109/ICRA40945.2020.9196876
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - High-performance composites are increasingly being used in the industry. Sheet layup is a process of manufacturing composite components using deformable sheets. We have developed a robotic cell to automate the layup process and overcome the limitations of the manual layup. Generating offline trajectories for robots and executing them without online refinement can introduce defects in the process due to uncertainties in the model of the sheet and environmental factors. Our system computes layup and grasping trajectories for the robots and refines them during the layup process based on the sensor data. We use an approach that augments physical experiments with simulations to train a Gaussian process regression model offline. The use of GPR enables us to quickly refine grasp plans and perform a defect-free layup without slowing down the layup process. We present experimental results on two components.
ER  - 

TY  - CONF
TI  - Learning Continuous 3D Reconstructions for Geometrically Aware Grasping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 11516
EP  - 11522
AU  - M. Van der Merwe
AU  - Q. Lu
AU  - B. Sundaralingam
AU  - M. Matak
AU  - T. Hermans
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - image reconstruction
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - shape recognition
KW  - solid modelling
KW  - continuous 3D reconstructions
KW  - geometrically aware grasping
KW  - deep learning
KW  - grasp synthesis
KW  - unseen objects
KW  - partial object views
KW  - indirect geometric reasoning
KW  - explicit geometric reasoning
KW  - grasping system
KW  - reconstruction network
KW  - grasp success classifier
KW  - continuous grasp optimization
KW  - grasp metrics
KW  - 96 robot grasping trials
KW  - Three-dimensional displays
KW  - Optimization
KW  - Collision avoidance
KW  - Grasping
KW  - Geometry
KW  - Robot sensing systems
DO  - 10.1109/ICRA40945.2020.9196981
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning has enabled remarkable improvements in grasp synthesis for previously unseen objects from partial object views. However, existing approaches lack the ability to explicitly reason about the full 3D geometry of the object when selecting a grasp, relying on indirect geometric reasoning derived when learning grasp success networks. This abandons explicit geometric reasoning, such as avoiding undesired robot object collisions. We propose to utilize a novel, learned 3D reconstruction to enable geometric awareness in a grasping system. We leverage the structure of the reconstruction network to learn a grasp success classifier which serves as the objective function for a continuous grasp optimization. We additionally explicitly constrain the optimization to avoid undesired contact, directly using the reconstruction. We examine the role of geometry in grasping both in the training of grasp metrics and through 96 robot grasping trials. Our results can be found on https://sites.google.com/view/reconstruction-grasp/.
ER  - 


