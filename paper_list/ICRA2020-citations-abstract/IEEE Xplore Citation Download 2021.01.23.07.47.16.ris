TY  - CONF
TI  - Underwater Image Super-Resolution using Deep Residual Multipliers
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 900
EP  - 906
AU  - M. J. Islam
AU  - S. Sakib Enan
AU  - P. Luo
AU  - J. Sattar
PY  - 2020
KW  - image resolution
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - robot vision
KW  - underwater vehicles
KW  - single image super-resolution
KW  - autonomous underwater robots
KW  - adversarial training pipeline
KW  - perceptual quality
KW  - global content
KW  - local style information
KW  - USR-248
KW  - SISR
KW  - state-of-the-art models
KW  - deep residual multipliers
KW  - deep residual network-based generative model
KW  - underwater image super-resolution
KW  - noisy visual conditions
KW  - Training
KW  - Image resolution
KW  - Robots
KW  - Data models
KW  - Cameras
KW  - Pipelines
KW  - Generators
DO  - 10.1109/ICRA40945.2020.9197213
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a deep residual network-based generative model for single image super-resolution (SISR) of underwater imagery for use by autonomous underwater robots. We also provide an adversarial training pipeline for learning SISR from paired data. In order to supervise the training, we formulate an objective function that evaluates the perceptual quality of an image based on its global content, color, and local style information. Additionally, we present USR-248, a large-scale dataset of three sets of underwater images of `high' (640×480) and `low' (80 × 60, 160 × 120, and 320×240) resolution. USR-248 contains paired instances for supervised training of 2×, 4×, or 8× SISR models. Furthermore, we validate the effectiveness of our proposed model through qualitative and quantitative experiments and compare the results with several state-of-the-art models' performances. We also analyze its practical feasibility for applications such as scene understanding and attention modeling in noisy visual conditions.
ER  - 

TY  - CONF
TI  - Nonlinear Synchronization Control for Short-Range Mobile Sensors Drifting in Geophysical Flows
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 907
EP  - 913
AU  - C. Wei
AU  - H. G. Tanner
AU  - M. Ani Hsieh
PY  - 2020
KW  - actuators
KW  - mobile radio
KW  - oceanographic techniques
KW  - synchronisation
KW  - telecommunication control
KW  - wireless sensor networks
KW  - short-range mobile sensors drifting
KW  - geophysical flows
KW  - ocean monitoring applications
KW  - minimal actuation capabilities
KW  - active drifters
KW  - gyre flows
KW  - data exchange
KW  - nonlinear synchronization control strategy
KW  - rendezvous regions
KW  - large-scale mobile sensor networks
KW  - numerical simulations
KW  - small-scale experiments
KW  - Synchronization
KW  - Sensors
KW  - Vehicle dynamics
KW  - Orbits
KW  - Robots
KW  - Oscillators
KW  - Dynamics
DO  - 10.1109/ICRA40945.2020.9196701
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a synchronization controller for mobile sensors that are minimally actuated and can only communicate with each other over a very short range. This work is motivated by ocean monitoring applications where large-scale sensor networks consisting of drifters with minimal actuation capabilities, i.e., active drifters, are employed. We assume drifters are tasked to monitor regions consisting of gyre flows where their trajectories are periodic. As drifters in neighboring regions move into each other's proximity, it presents an opportunity for data exchange and synchronization to ensure future rendezvous. We present a nonlinear synchronization control strategy to ensure that drifters will periodically rendezvous and maximize the time they are in their rendezvous regions. Numerical simulations and small-scale experiments validate the efficacy of the control strategy and hint at extensions to large-scale mobile sensor networks.
ER  - 

TY  - CONF
TI  - Energy-based Safety in Series Elastic Actuation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 914
EP  - 920
AU  - W. Roozing
AU  - S. S. Groothuis
AU  - S. Stramigioli
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - energy-based safety
KW  - series elastic actuation
KW  - generic actuation passivity
KW  - energy storage
KW  - power flow properties
KW  - power limits
KW  - Safety
KW  - Robots
KW  - Energy storage
KW  - Actuators
KW  - Impedance
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9197448
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work presents the concept of energy-based safety for series-elastic actuation. Generic actuation passivity and safety is treated, defining several energy storage and power flow properties related to passivity. Safe behaviour is not guaranteed by passivity, but can be guaranteed by energy and power limits that adapt the nominal behaviour of an impedance controller. A discussion on power flows in series-elastic actuation is presented and an appropriate controller is developed. Experimental results validate the effectiveness of the energy-based safety in elastic actuation.
ER  - 

TY  - CONF
TI  - Safe high impedance control of a series-elastic actuator with a disturbance observer
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 921
EP  - 927
AU  - K. Haninger
AU  - A. Asignacion
AU  - S. Oh
PY  - 2020
KW  - actuators
KW  - elasticity
KW  - feedforward
KW  - observers
KW  - torque control
KW  - high-stiffness environment
KW  - DOB approaches
KW  - feedforward controller
KW  - DOB controllers
KW  - maximum safe stiffness
KW  - DOB torque control
KW  - passivity conditions
KW  - load port passivity
KW  - safe impedance range
KW  - torque tracking performance
KW  - series-elastic actuator applications
KW  - disturbance observer
KW  - safe high impedance control
KW  - Impedance
KW  - Torque
KW  - Torque control
KW  - Springs
KW  - Stability analysis
KW  - Safety
KW  - Measurement
DO  - 10.1109/ICRA40945.2020.9197402
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In many series-elastic actuator applications, the ability to safely render a wide range of impedance is important. Advanced torque control techniques such as the disturbance observer (DOB) can improve torque tracking performance, but their impact on safe impedance range is not established. Here, safety is defined with load port passivity, and passivity conditions are developed for two variants of DOB torque control. These conditions are used to determine the maximum safe stiffness and Z-region of the DOB controllers, which are analyzed and compared with the no DOB case. A feedforward controller is proposed which increases the maximum safe stiffness of the DOB approaches. The results are experimentally validated by manual excitation and in a high-stiffness environment.
ER  - 

TY  - CONF
TI  - Variable Stiffness Springs for Energy Storage Applications
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 928
EP  - 933
AU  - S. Y. Kim
AU  - T. Zhang
AU  - D. J. Braun
PY  - 2020
KW  - actuators
KW  - energy storage
KW  - mathematical analysis
KW  - rigidity
KW  - robot dynamics
KW  - springs (mechanical)
KW  - variable stiffness actuation technology
KW  - variable stiffness springs
KW  - energy storage capacity
KW  - linear helical springs
KW  - variable stiffness actuators
KW  - human performance augmentation
KW  - spring exoskeleton
KW  - controllable volume air spring
KW  - mathematical conditions
KW  - Springs
KW  - Energy storage
KW  - Actuators
KW  - Potential energy
KW  - Strain
KW  - Force
KW  - Valves
DO  - 10.1109/ICRA40945.2020.9197245
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Theory suggests an inverse relation between the stiffness and the energy storage capacity for linear helical springs: reducing the active length of the spring by 50% increases its stiffness by 100%, but reduces its energy storage capacity by 50%. State-of-the-art variable stiffness actuators used to drive robots are characterized by a similar inverse relation, implying reduced energy storage capacity for increased spring stiffness. This relation limits the potential of the variable stiffness actuation technology when it comes to human performance augmentation in natural tasks, e.g., jumping, weight-bearing and running, which may necessitate a spring exoskeleton with large stiffness range and high energy storage capacity. In this paper, we theoretically show that the trade-off between stiffness range and energy storage capacity is not fundamental; it is possible to develop variable stiffness springs with simultaneously increasing stiffness and energy storage capacity. Consistent with the theory, we experimentally show that a controllable volume air spring, has a direct relation between its stiffness range and energy storage capacity. The mathematical conditions presented in this paper may be used to develop actuators that could bypass the limited energy storage capacity of current variable stiffness spring technology.
ER  - 

TY  - CONF
TI  - Parallel-motion Thick Origami Structure for Robotic Design
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 934
EP  - 939
AU  - S. Liu
AU  - H. Wu
AU  - Y. Yang
AU  - M. Y. Wang
PY  - 2020
KW  - art
KW  - control system synthesis
KW  - grippers
KW  - motion control
KW  - paper
KW  - parallel-motion thick origami structure
KW  - robotic design
KW  - three-dimensional shapes
KW  - zero-thickness flat paper sheets
KW  - origami facets
KW  - multiple layer origami structures
KW  - parallel-motion gripper
KW  - Fasteners
KW  - Grippers
KW  - Robots
KW  - Shape
KW  - Actuators
KW  - Force
KW  - Electronic mail
DO  - 10.1109/ICRA40945.2020.9197339
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Structures with origami design enable objects to transform into various three-dimensional shapes. Traditionally origami structures are designed with zero-thickness flat paper sheets. However, the thickness and intersection of origami facets are non-negligible in most cases, uniquely when integrating origami design with robotic design because of the more efficient force transfer between thick plates compared with zero-thickness paper-sheets. Meanwhile, the single-layer-paper oriented initial design limited the shape transformation potential as multiple layer origami structures could conduct more variety of deformation. In this article, we are proposing a general design method of parallel-motion thick origami structures that could apply in robotic design like a parallel-motion gripper.
ER  - 

TY  - CONF
TI  - Real-time Simulation of Non-Deformable Continuous Tracks with Explicit Consideration of Friction and Grouser Geometry
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 948
EP  - 954
AU  - Y. Okada
AU  - S. Kojima
AU  - K. Ohno
AU  - S. Tadokoro
PY  - 2020
KW  - friction
KW  - mobile robots
KW  - motion control
KW  - tracked vehicles
KW  - trajectory control
KW  - velocity control
KW  - nondeformable continuous tracks
KW  - grouser geometry
KW  - real-time simulation
KW  - circular segments
KW  - robot body
KW  - segment link
KW  - track rotation
KW  - friction
KW  - rough terrain
KW  - track trajectory
KW  - velocity constraints
KW  - tracked vehicles
KW  - Robots
KW  - Trajectory
KW  - Tracking
KW  - Friction
KW  - Collision avoidance
KW  - Real-time systems
KW  - Wheels
DO  - 10.1109/ICRA40945.2020.9196776
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this study, we developed a real-time simulation method for non-deformable continuous tracks having grousers for rough terrain by explicitly considering the collision and friction between the tracks and the ground. In the proposed simulation method, an arbitrary trajectory of a track is represented with multiple linear and circular segments, each of which is a link connected to a robot body. The proposed method sets velocity constraints between each segment link and the robot body, to simulate the track rotation around the body. To maintain the shape of a track, it also restores the positions of the segment links when required. Experimental comparisons with other existing real-time simulation methods demonstrated that while the proposed method considered the grousers and the friction with the ground, it was comparable to them in terms of the computational speed. Experimental comparison of the simulations based on the proposed method and a physical robot exhibited that the former was comparable to the precise motion of the robot on rough or uneven terrain.
ER  - 

TY  - CONF
TI  - Test Your SLAM! The SubT-Tunnel dataset and metric for mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 955
EP  - 961
AU  - J. G. Rogers
AU  - J. M. Gregory
AU  - J. Fink
AU  - E. Stump
PY  - 2020
KW  - mobile robots
KW  - public domain software
KW  - robot vision
KW  - SLAM (robots)
KW  - SLAM
KW  - open source tools
KW  - robotic mapping algorithms
KW  - DARPA Subterranean challenge
KW  - SubT-Tunnel dataset
KW  - subterranean mine rescue dataset
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Measurement
KW  - Laser radar
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197156
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.
ER  - 

TY  - CONF
TI  - Uncertainty Measured Markov Decision Process in Dynamic Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 962
EP  - 968
AU  - S. Dutta
AU  - B. Rekabdar
AU  - C. Ekenna
PY  - 2020
KW  - Markov processes
KW  - mobile robots
KW  - path planning
KW  - robot motion planning
KW  - path planning method
KW  - tracking robot
KW  - dynamic environments
KW  - robot path planning
KW  - visual occlusions
KW  - moving targets
KW  - visioning
KW  - perception algorithms
KW  - partially observable Markov decision
KW  - pursuit-evasion
KW  - robot tracking
KW  - predictive path planning
KW  - Uncertainty
KW  - Planning
KW  - Robots
KW  - Markov processes
KW  - Target tracking
KW  - Probabilistic logic
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9197064
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Successful robot path planning is challenging in the presence of visual occlusions and moving targets. Classical methods to solve this problem have used visioning and perception algorithms in addition to partially observable markov decision processes to aid in path planning for pursuit-evasion and robot tracking. We present a predictive path planning process that measures and utilizes the uncertainty present during robot motion planning. We develop a variant of subjective logic in combination with the Markov decision process (MDP) and provide a measure for belief, disbelief, and uncertainty in relation to feasible trajectories being generated. We then model the MDP to identify the best path planning method from a list of possible choices. Our results show a high percentage accuracy based on the closest acquired proximity between a target and a tracking robot and a simplified pursuer trajectory in comparison with related work.
ER  - 

TY  - CONF
TI  - Natural Scene Facial Expression Recognition with Dimension Reduction Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 987
EP  - 992
AU  - S. Hu
AU  - Y. Hu
AU  - J. Li
AU  - X. Long
AU  - M. Chen
AU  - Q. Gu
PY  - 2020
KW  - emotion recognition
KW  - face recognition
KW  - image classification
KW  - neural nets
KW  - natural scene facial expression recognition
KW  - dimension reduction network
KW  - human-computer interaction
KW  - expression recognition methods
KW  - natural scenes
KW  - expression classification
KW  - pattern recognition problem
KW  - intra-class distance
KW  - inter-class distance
KW  - neural networks
KW  - generalization error
KW  - data dimension reduction module
KW  - general classification network
KW  - classification tasks
KW  - Feature extraction
KW  - Faces
KW  - Face recognition
KW  - Neural networks
KW  - Training
KW  - Dimensionality reduction
DO  - 10.1109/ICRA40945.2020.9197547
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As an external manifestation of human emotions, expression recognition plays an important role in human-computer interaction. Although existing expression recognition methods performs perfectly on constrained frontal faces, there are still many challenges in expression recognition in natural scenes due to different unrestricted conditions. Expression classification belongs to a pattern recognition problem where intra-class distance is greater than the inter-class distance, which leads to severe over-fitting when using neural networks for expression recognition. This paper proposes a novel net-work structure called Dimension Reduction Network which can effectively reduce generalization error. By adding a data dimension reduction module before the general classification network, a lot of redundant information is filtered, and only useful information is left. This can reduce the interference by irrelevant information when performing classification tasks and reduce generalization error. The proposed method does not require any modification to the classification network, only a small dimension reduction module needs to be added in front of the classification network. However, it can effectively reduce generalization error. We designed big and tiny versions of Dimension Reduction Network, both exceeds our baseline on AffectNet data set. The big version of our proposed method surpassed the state-of-the-art methods by more than 1.2% on AffectNet data set. Our code will open source3 when the paper is accepted.
ER  - 

TY  - CONF
TI  - Hand Pose Estimation for Hand-Object Interaction Cases using Augmented Autoencoder
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 993
EP  - 999
AU  - S. Li
AU  - H. Wang
AU  - D. Lee
PY  - 2020
KW  - image capture
KW  - image coding
KW  - image representation
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - pose estimation
KW  - solid modelling
KW  - annotated hand-object samples
KW  - hand-object interaction cases
KW  - augmented autoencoder
KW  - deep learning method
KW  - 3D point cloud
KW  - latent representation
KW  - auxiliary point cloud decoder
KW  - augmented clean hand data
KW  - hand pose estimation
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Decoding
KW  - Image reconstruction
KW  - Task analysis
KW  - Shape
KW  - Feature extraction
DO  - 10.1109/ICRA40945.2020.9197299
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hand pose estimation with objects is challenging due to object occlusion and the lack of large annotated datasets. To tackle these issues, we propose an Augmented Autoencoder based deep learning method using augmented clean hand data. Our method takes 3D point cloud of a hand with an augmented object as input and encodes the input to latent representation of the hand. From the latent representation, our method decodes 3D hand pose and we propose to use an auxiliary point cloud decoder to assist the formation of the latent space. Through quantitative and qualitative evaluation on both synthetic dataset and real captured data containing objects, we demonstrate state-of-the-art performance for hand pose estimation with objects, even using only a small number of annotated hand-object samples.
ER  - 

TY  - CONF
TI  - Accurate detection and 3D localization of humans using a novel YOLO-based RGB-D fusion approach and synthetic training data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1000
EP  - 1006
AU  - T. Linder
AU  - K. Y. Pfeiffer
AU  - N. Vaskevicius
AU  - R. Schirmer
AU  - K. O. Arras
PY  - 2020
KW  - feature extraction
KW  - image colour analysis
KW  - image fusion
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - object detection
KW  - synthetic training data
KW  - real-time detection
KW  - human 3D centroids
KW  - RGB-D data
KW  - image-based detection approach
KW  - YOLO v3 architecture
KW  - 3D centroid loss
KW  - mid-level feature fusion
KW  - transfer learning scheme
KW  - large-scale 2D object detection datasets
KW  - end-to-end 3D localization
KW  - precise 3D groundtruth
KW  - 3D localization accuracy
KW  - learning 3D localization
KW  - YOLO-based RGB-D fusion approach
KW  - depth-aware crop augmentation
KW  - intralogistics dataset
KW  - Three-dimensional displays
KW  - Two dimensional displays
KW  - Training
KW  - Detectors
KW  - Feature extraction
KW  - Robustness
KW  - Solid modeling
DO  - 10.1109/ICRA40945.2020.9196899
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While 2D object detection has made significant progress, robustly localizing objects in 3D space under presence of occlusion is still an unresolved issue. Our focus in this work is on real-time detection of human 3D centroids in RGB-D data. We propose an image-based detection approach which extends the YOLO v3 architecture with a 3D centroid loss and mid-level feature fusion to exploit complementary information from both modalities. We employ a transfer learning scheme which can benefit from existing large-scale 2D object detection datasets, while at the same time learning end-to-end 3D localization from our highly randomized, diverse synthetic RGB-D dataset with precise 3D groundtruth. We further propose a geometrically more accurate depth-aware crop augmentation for training on RGB-D data, which helps to improve 3D localization accuracy. In experiments on our challenging intralogistics dataset, we achieve state-of-the-art performance even when learning 3D localization just from synthetic data.
ER  - 

TY  - CONF
TI  - Wide-range Load Sensor Using Vacuum Sealed Quartz Crystal Resonator for Simultaneous Biosignals Measurement on Bed
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1015
EP  - 1020
AU  - Y. Murozaki
AU  - F. Arai
PY  - 2020
KW  - biomechanics
KW  - biomedical equipment
KW  - biomedical measurement
KW  - cardiology
KW  - crystal resonators
KW  - electrocardiography
KW  - force measurement
KW  - force sensors
KW  - medical signal detection
KW  - medical signal processing
KW  - microfabrication
KW  - microsensors
KW  - patient monitoring
KW  - pneumodynamics
KW  - pressure sensors
KW  - quartz
KW  - sensors
KW  - wide-range load sensor
KW  - vacuum sealed quartz crystal resonator
KW  - biosignal measurement
KW  - QCR load sensor
KW  - measurement range
KW  - sensor structure
KW  - force sensor
KW  - QCR load sensing system
KW  - Bonding
KW  - Weight measurement
KW  - Robot sensing systems
KW  - Force
KW  - Heart beat
KW  - Resists
KW  - Stress
DO  - 10.1109/ICRA40945.2020.9196533
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Monitoring of biosignals on a daily basis plays important roles for the health management of elderly. The monitoring system for the daily life, the system should not require the subjects to take special effort like wearing a sensor. We propose biosignals measurement using wide-range load sensor on the bed. The sensing system can detect the body weight, heartbeat and respiration simultaneously by just lying on the bed. We have developed load sensor using quartz crystal resonator (QCR load sensor) as wide-range load sensor. However, the measurement range was not sufficient for the simultaneous measurement of biosgnals on bed. To realize such sensing system, we propose a QCR load sensor utilizing vacuum sealing technology for expanding the measurement range. We improved the oscillation characteristics of the QCR by the vacuum sealing to stabilize the sensor output. Accordingly, the resolution of the sensor was improved. Moreover, the load capacity of the sensor was increased by improving the bonding strength of sensor structure. The fabricated sensor had a measurement range of 0.27 mN - 1180 N (4.4 × 106). This wide enough compared with the conventional force sensor (103 - 104).Also, we developed mechanically robust jig of QCR load sensor for practical use of QCR load sensor. We succeed in simultaneous measurement of weight, heart rate, and respiration rate using fabricated QCR load sensing system. The accuracy of heart rate and respiration rate measurement are 0.4 bpm (0.6 %) and 1.1 brpm (6.1 %), respectively, in standard deviation of error compared with ECG signal.
ER  - 

TY  - CONF
TI  - Joint Pedestrian Detection and Risk-level Prediction with Motion-Representation-by-Detection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1021
EP  - 1027
AU  - H. Kataoka
AU  - T. Suzuki
AU  - K. Nakashima
AU  - Y. Satoh
AU  - Y. Aoki
PY  - 2020
KW  - feature extraction
KW  - image motion analysis
KW  - image representation
KW  - object detection
KW  - pedestrians
KW  - traffic engineering computing
KW  - risk-level prediction
KW  - pedestrian near-miss detection
KW  - risk-level assignment
KW  - motion-representation-by-detection
KW  - pedestrian near-miss dataset
KW  - single-shot multibox detector with motion representation
KW  - SSD-MR
KW  - motion-based features extraction
KW  - Videos
KW  - Databases
KW  - Detectors
KW  - Feature extraction
KW  - Object detection
KW  - Accidents
KW  - Autonomous automobiles
DO  - 10.1109/ICRA40945.2020.9197399
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The paper presents a pedestrian near-miss detector with temporal analysis that provides both pedestrian detection and risk-level predictions which are demonstrated on a self-collected database. Our work makes three primary contributions: (i) The framework of pedestrian near-miss detection is proposed by providing both a pedestrian detection and risk-level assignment. Specifically, we have created a Pedestrian Near-Miss (PNM) dataset that categorizes traffic near-miss incidents based on their risk levels (high-, low-, and no-risk). Unlike existing databases, our dataset also includes manually localized pedestrian labels as well as a large number of incident-related videos. (ii) Single-Shot MultiBox Detector with Motion Representation (SSD-MR) is implemented to effectively extract motion-based features in a detected pedestrian. (iii) Using the self-collected PNM dataset and SSD-MR, our proposed method achieved +19.38% (on risk-level prediction) and +13.00% (on joint pedestrian detection and risk-level prediction) higher scores than that of the baseline SSD and LSTM. Additionally, the running time of our system is over 50 fps on a graphics processing unit (GPU).
ER  - 

TY  - CONF
TI  - Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1070
EP  - 1076
AU  - P. Gao
AU  - H. Zhang
PY  - 2020
KW  - graph theory
KW  - image matching
KW  - mobile robots
KW  - robot vision
KW  - SLAM (robots)
KW  - robotics applications
KW  - simultaneously localization and mapping
KW  - spatial relationship similarities
KW  - spatial cues
KW  - visual cues
KW  - old landmarks
KW  - long-term environment changes
KW  - landmark information
KW  - integrate landmark appearances
KW  - worst-case graph matching
KW  - place recognition performance
KW  - long-term place recognition
KW  - worst appearance similarity
KW  - similar appearances
KW  - worst-case scenario
KW  - graph matching problem
KW  - visual appearances
KW  - angular spatial relationships
KW  - graph representation
KW  - Visualization
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Strain
KW  - Image recognition
KW  - Tensile stress
DO  - 10.1109/ICRA40945.2020.9196906
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.
ER  - 

TY  - CONF
TI  - Linear RGB-D SLAM for Atlanta World
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1077
EP  - 1083
AU  - K. Joo
AU  - T. -H. Oh
AU  - F. Rameau
AU  - J. -C. Bazin
AU  - I. S. Kweon
PY  - 2020
KW  - cameras
KW  - image colour analysis
KW  - Kalman filters
KW  - mobile robots
KW  - object detection
KW  - object tracking
KW  - pose estimation
KW  - SLAM (robots)
KW  - Manhattan world assumption
KW  - orthogonal directions
KW  - Atlanta world
KW  - vertical direction
KW  - horizontal directions
KW  - SLAM techniques
KW  - Atlanta representation
KW  - Atlanta frame-aware linear SLAM framework
KW  - Atlanta structure
KW  - linear Kalman filter
KW  - linear RGB-D SLAM
KW  - simultaneous localization and mapping
KW  - tracking-by-detection scheme
KW  - scene structure
KW  - camera motion
KW  - planar map
KW  - synthetic datasets
KW  - real datasets
KW  - Simultaneous localization and mapping
KW  - Cameras
KW  - Three-dimensional displays
KW  - Tracking
KW  - Kalman filters
KW  - Visualization
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9196561
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a new linear method for RGB-D based simultaneous localization and mapping (SLAM). Compared to existing techniques relying on the Manhattan world assumption defined by three orthogonal directions, our approach is designed for the more general scenario of the Atlanta world. It consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction and thus can represent a wider range of scenes. Our approach leverages the structural regularity of the Atlanta world to decouple the non-linearity of camera pose estimations. This allows us separately to estimate the camera rotation and then the translation, which bypasses the inherent non-linearity of traditional SLAM techniques. To this end, we introduce a novel tracking-by-detection scheme to estimate the underlying scene structure by Atlanta representation. Thereby, we propose an Atlanta frame-aware linear SLAM framework which jointly estimates the camera motion and a planar map supporting the Atlanta structure through a linear Kalman filter. Evaluations on both synthetic and real datasets demonstrate that our approach provides favorable performance compared to existing state-of-the-art methods while extending their working range to the Atlanta world.
ER  - 

TY  - CONF
TI  - Stereo Visual Inertial Odometry with Online Baseline Calibration
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1084
EP  - 1090
AU  - Y. Fan
AU  - R. Wang
AU  - Y. Mao
PY  - 2020
KW  - calibration
KW  - cameras
KW  - distance measurement
KW  - inertial navigation
KW  - Jacobian matrices
KW  - Kalman filters
KW  - stereo image processing
KW  - update Jacobian sub-block
KW  - feature reprojection error
KW  - real-world outdoor dataset
KW  - EuRoC dataset
KW  - camera poses
KW  - IMU
KW  - inertial measurement unit
KW  - estimation performance
KW  - stereo-vision devices
KW  - stereo extrinsic parameters
KW  - multistate constraint Kalman filter
KW  - stereo VIO extrinsic parameters correction
KW  - online calibration method
KW  - camera extrinsic parameters
KW  - stereo visual inertial odometry
KW  - extrinsic parameter calibration
KW  - online baseline calibration
KW  - Cameras
KW  - Calibration
KW  - Estimation
KW  - Visualization
KW  - Acceleration
KW  - Jacobian matrices
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9197581
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Stereo-vision devices have rigorous requirements for extrinsic parameter calibration. In Stereo Visual Inertial Odometry (VIO), inaccuracy in or changes to camera extrinsic parameters may lead to serious degradation in estimation performance. In this manuscript, we propose an online calibration method for stereo VIO extrinsic parameters correction. In particular, we focus on Multi-State Constraint Kalman Filter (MSCKF [1]) framework to implement our method. The key component is to formulate stereo extrinsic parameters as part of the state variables and model the Jacobian of feature reprojection error with respect to stereo extrinsic parameters as sub-block of update Jacobian. Therefore we can estimate stereo extrinsic parameters simultaneously with inertial measurement unit (IMU) states and camera poses. Experiments on EuRoC dataset and real-world outdoor dataset demonstrate that the proposed algorithm produce higher positioning accuracy than the original S-MSCKF [2], and the noise of camera extrinsic parameters are self-corrected within the system.
ER  - 

TY  - CONF
TI  - Lidar-Monocular Visual Odometry using Point and Line Features
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1091
EP  - 1097
AU  - S. -S. Huang
AU  - Z. -Y. Ma
AU  - T. -J. Mu
AU  - H. Fu
AU  - S. -M. Hu
PY  - 2020
KW  - distance measurement
KW  - feature extraction
KW  - image sequences
KW  - mobile robots
KW  - motion estimation
KW  - optical radar
KW  - pose estimation
KW  - robot vision
KW  - stereo image processing
KW  - line features
KW  - lidar-visual odometry
KW  - pose estimation
KW  - line depth extraction
KW  - point-line bundle adjustment
KW  - purely visual motion tracking method
KW  - public KITTI odometry benchmark
KW  - lidar-monocular visual odometry approach
KW  - point-only based lidar-visual odometry
KW  - environment structure information
KW  - efficient lidar-monocular visual odometry system
KW  - Feature extraction
KW  - Cameras
KW  - Bundle adjustment
KW  - Laser radar
KW  - Image segmentation
KW  - Optimization
DO  - 10.1109/ICRA40945.2020.9196613
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We introduce a novel lidar-monocular visual odometry approach using point and line features. Compared to previous point-only based lidar-visual odometry, our approach leverages more environment structure information by introducing both point and line features into pose estimation. We provide a robust method for point and line depth extraction, and formulate the extracted depth as prior factors for point-line bundle adjustment. This method greatly reduces the features' 3D ambiguity and thus improves the pose estimation accuracy. Besides, we also provide a purely visual motion tracking method and a novel scale correction scheme, leading to an efficient lidar-monocular visual odometry system with high accuracy. The evaluations on the public KITTI odometry benchmark show that our technique achieves more accurate pose estimation than the state-of-the-art approaches, and is sometimes even better than those leveraging semantic information.
ER  - 

TY  - CONF
TI  - Probabilistic Data Association via Mixture Models for Robust Semantic SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1098
EP  - 1104
AU  - K. J. Doherty
AU  - D. P. Baxter
AU  - E. Schneeweiss
AU  - J. J. Leonard
PY  - 2020
KW  - Gaussian processes
KW  - image sensors
KW  - mobile robots
KW  - object detection
KW  - probability
KW  - robot vision
KW  - SLAM (robots)
KW  - target tracking
KW  - probabilistic data association
KW  - mixture models
KW  - robust semantic SLAM
KW  - robotic systems
KW  - cameras
KW  - lidar
KW  - visual models
KW  - reliable navigation
KW  - semantic uncertainty inherent
KW  - geometric uncertainty inherent
KW  - object detection methods
KW  - data association ambiguity
KW  - nonlinear Gaussian formulation
KW  - data association variables
KW  - max-marginalization
KW  - standard Gaussian posterior assumptions
KW  - max-mixture-type model
KW  - multiple data association hypotheses
KW  - indoor navigation tasks
KW  - outdoor semantic navigation tasks
KW  - semantic SLAM approaches
KW  - simultaneous localization and mapping
KW  - noisy odometry
KW  - Semantics
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Optimization
KW  - Object detection
KW  - Uncertainty
DO  - 10.1109/ICRA40945.2020.9197382
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern robotic systems sense the environment geometrically, through sensors like cameras, lidar, and sonar, as well as semantically, often through visual models learned from data, such as object detectors. We aim to develop robots that can use all of these sources of information for reliable navigation, but each is corrupted by noise. Rather than assume that object detection will eventually achieve near perfect performance across the lifetime of a robot, in this work we represent and cope with the semantic and geometric uncertainty inherent in object detection methods. Specifically, we model data association ambiguity, which is typically non-Gaussian, in a way that is amenable to solution within the common nonlinear Gaussian formulation of simultaneous localization and mapping (SLAM). We do so by eliminating data association variables from the inference process through max-marginalization, preserving standard Gaussian posterior assumptions. The result is a max-mixture-type model that accounts for multiple data association hypotheses. We provide experimental results on indoor and outdoor semantic navigation tasks with noisy odometry and object detection and find that the ability of the proposed approach to represent multiple hypotheses, including the "null" hypothesis, gives substantial robustness advantages in comparison to alternative semantic SLAM approaches.
ER  - 

TY  - CONF
TI  - Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1105
EP  - 1112
AU  - Y. Zhao
AU  - J. S. Smith
AU  - S. H. Karumanchi
AU  - P. A. Vela
PY  - 2020
KW  - closed loop systems
KW  - Global Positioning System
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - SLAM (robots)
KW  - stereo image processing
KW  - tracking
KW  - representative state-of-the-art visual-inertial SLAM systems
KW  - visual estimation module
KW  - stereo visual-inertial SLAM systems
KW  - open-loop analysis
KW  - closed-loop navigation tasks
KW  - accurate trajectory tracking
KW  - visualinertial SLAM systems
KW  - closed-loop benchmarking simulation
KW  - visual-inertial estimation
KW  - trajectory tracking performance
KW  - Visualization
KW  - Navigation
KW  - Simultaneous localization and mapping
KW  - Benchmark testing
KW  - Estimation
DO  - 10.1109/ICRA40945.2020.9197003
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.
ER  - 

TY  - CONF
TI  - PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1113
EP  - 1120
AU  - L. Pan
AU  - C. -M. Chew
AU  - G. H. Lee
PY  - 2020
KW  - codecs
KW  - convolutional neural nets
KW  - decoding
KW  - edge detection
KW  - graph theory
KW  - image coding
KW  - image filtering
KW  - image representation
KW  - image sampling
KW  - learning (artificial intelligence)
KW  - stereo image processing
KW  - visual perception
KW  - deep hierarchical encoder-decoder
KW  - unorganized 3D points
KW  - multiscale contextual information
KW  - image analysis
KW  - PointAtrousGraph
KW  - PAG
KW  - multiscale edge features
KW  - point clouds
KW  - Point Atrous Convolution
KW  - PAC
KW  - Edge-preserved Unpooling
KW  - multiscale point features
KW  - nonoverlapping maxpooling operations
KW  - critical edge features
KW  - EU modules
KW  - deep permutation-invariant hierarchical encoder-decoder
KW  - edge preserved pooling
KW  - chained skip subsampling-upsampling modules
KW  - 3D semantic perception applications
KW  - Three-dimensional displays
KW  - Picture archiving and communication systems
KW  - Convolution
KW  - Semantics
KW  - Image edge detection
KW  - Task analysis
KW  - Decoding
DO  - 10.1109/ICRA40945.2020.9197499
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edgepreserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping maxpooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling/upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.
ER  - 

TY  - CONF
TI  - Learning error models for graph SLAM
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1121
EP  - 1127
AU  - C. Reymann
AU  - S. Lacroix
PY  - 2020
KW  - autonomous aerial vehicles
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - resistance distance
KW  - covisibility graph
KW  - simulated UAV coverage path
KW  - uncertainty models
KW  - monocular graph SLAM
KW  - topological features
KW  - error model learning
KW  - UAV coverage path planning trajectories
KW  - Simultaneous localization and mapping
KW  - Resistance
KW  - Uncertainty
KW  - Computational modeling
KW  - Computer architecture
KW  - Predictive models
KW  - Cameras
DO  - 10.1109/ICRA40945.2020.9196864
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.
ER  - 

TY  - CONF
TI  - SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1128
EP  - 1134
AU  - M. Cornia
AU  - L. Baraldi
AU  - R. Cucchiara
PY  - 2020
KW  - human-robot interaction
KW  - natural language processing
KW  - robot vision
KW  - video signal processing
KW  - video captioning
KW  - computational requirements
KW  - fully-attentive captioning algorithm
KW  - language generation
KW  - transformer layers
KW  - decoding stages
KW  - image regions
KW  - caption quality
KW  - autonomous agents
KW  - domestic robots
KW  - SMArT
KW  - robotic explainability
KW  - natural language explanations
KW  - visual perception
KW  - shallow memory-aware transformer training
KW  - memory-aware encoding
KW  - image captioning
KW  - image captioning
KW  - Decoding
KW  - Computational modeling
KW  - Visualization
KW  - Magnetic heads
KW  - Robots
KW  - Natural languages
KW  - Encoding
DO  - 10.1109/ICRA40945.2020.9196653
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.
ER  - 

TY  - CONF
TI  - A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1135
EP  - 1141
AU  - L. Kästner
AU  - V. C. Frasineanu
AU  - J. Lambrecht
PY  - 2020
KW  - augmented reality
KW  - calibration
KW  - cameras
KW  - control engineering computing
KW  - image sensors
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - public domain software
KW  - robot vision
KW  - solid modelling
KW  - robotic environments
KW  - mobile robots
KW  - depth camera
KW  - deep learning-based calibration
KW  - open source 3D point cloud labeling tool
KW  - head mounted augmented reality device
KW  - 3D depth sensor data
KW  - Microsoft Hololens
KW  - neural network
KW  - VoteNet architecture
KW  - 3D-deep-learning-based augmented reality calibration
KW  - Robot sensing systems
KW  - Three-dimensional displays
KW  - Calibration
KW  - Neural networks
KW  - Augmented reality
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197155
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Augmented Reality and mobile robots are gaining increased attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate Augmented Reality devices using 3D depth sensor data. We use the depth camera of a Head Mounted Augmented Reality Device, the Microsoft Hololens, for deep learning-based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on raw point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.
ER  - 

TY  - CONF
TI  - Adversarial Feature Training for Generalizable Robotic Visuomotor Control
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1142
EP  - 1148
AU  - X. Chen
AU  - A. Ghadirzadeh
AU  - M. Björkman
AU  - P. Jensfelt
PY  - 2020
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - robotic policy training
KW  - large-scale data collection
KW  - task-setup
KW  - task-irrelevant objects
KW  - interactive samples
KW  - adversarial training
KW  - deep RL capabilities
KW  - transfer learning
KW  - robotic tasks
KW  - adversarial feature training
KW  - generalizable robotic visuomotor control
KW  - deep reinforcement learning
KW  - action-selection policies
KW  - image pixels mapping
KW  - visuomotor robotic policy training
KW  - Task analysis
KW  - Training
KW  - Visualization
KW  - Feature extraction
KW  - Robots
KW  - Trajectory
KW  - Clutter
DO  - 10.1109/ICRA40945.2020.9197505
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep reinforcement learning (RL) has enabled training action-selection policies, end-to-end, by learning a function which maps image pixels to action outputs. However, it's application to visuomotor robotic policy training has been limited because of the challenge of large-scale data collection when working with physical hardware. A suitable visuomotor policy should perform well not just for the task-setup it has been trained for, but also for all varieties of the task, including novel objects at different viewpoints surrounded by task-irrelevant objects. However, it is impractical for a robotic setup to sufficiently collect interactive samples in a RL framework to generalize well to novel aspects of a task. In this work, we demonstrate that by using adversarial training for domain transfer, it is possible to train visuomotor policies based on RL frameworks, and then transfer the acquired policy to other novel task domains. We propose to leverage the deep RL capabilities to learn complex visuomotor skills for uncomplicated task setups, and then exploit transfer learning to generalize to new task domains provided only still images of the task in the target domain. We evaluate our method on two real robotic tasks, picking and pouring, and compare it to a number of prior works, demonstrating its superiority.
ER  - 

TY  - CONF
TI  - Efficient Bimanual Manipulation Using Learned Task Schemas
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1149
EP  - 1155
AU  - R. Chitnis
AU  - S. Tulsiani
AU  - S. Gupta
AU  - A. Gupta
PY  - 2020
KW  - control engineering computing
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - parameterizations
KW  - sparse-reward tasks
KW  - robotic bimanual manipulation tasks
KW  - parameterized skills
KW  - state-independent task schema
KW  - model-free reinforcement learning
KW  - robotic systems
KW  - Task analysis
KW  - Learning (artificial intelligence)
KW  - Neural networks
KW  - Force
KW  - Geometry
KW  - End effectors
DO  - 10.1109/ICRA40945.2020.9196958
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We address the problem of effectively composing skills to solve sparse-reward tasks in the real world. Given a set of parameterized skills (such as exerting a force or doing a top grasp at a location), our goal is to learn policies that invoke these skills to efficiently solve such tasks. Our insight is that for many tasks, the learning process can be decomposed into learning a state-independent task schema (a sequence of skills to execute) and a policy to choose the parameterizations of the skills in a state-dependent manner. For such tasks, we show that explicitly modeling the schema's state-independence can yield significant improvements in sample efficiency for model-free reinforcement learning algorithms. Furthermore, these schemas can be transferred to solve related tasks, by simply re-learning the parameterizations with which the skills are invoked. We find that doing so enables learning to solve sparse-reward tasks on real-world robotic systems very efficiently. We validate our approach experimentally over a suite of robotic bimanual manipulation tasks, both in simulation and on real hardware. See videos at http://tinyurl.com/chitnis-schema.
ER  - 

TY  - CONF
TI  - Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1156
EP  - 1162
AU  - Q. Kuang
AU  - J. Wu
AU  - J. Pan
AU  - B. Zhou
PY  - 2020
KW  - autonomous aerial vehicles
KW  - computational geometry
KW  - image reconstruction
KW  - path planning
KW  - robot vision
KW  - SLAM (robots)
KW  - unmanned aerial vehicles
KW  - large-scale scene mapping
KW  - autonomous urban scene reconstruction
KW  - point cloud reconstruction
KW  - reconstruction quality
KW  - large-scale scene reconstruction
KW  - real-time UAV path planning
KW  - SLAM
KW  - Buildings
KW  - Image reconstruction
KW  - Three-dimensional displays
KW  - Path planning
KW  - Drones
KW  - Cameras
KW  - Layout
DO  - 10.1109/ICRA40945.2020.9196558
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.
ER  - 

TY  - CONF
TI  - A Fast Marching Gradient Sampling Strategy for Motion Planning using an Informed Certificate Set
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1163
EP  - 1168
AU  - S. Shi
AU  - J. Chen
AU  - Y. Xiong
PY  - 2020
KW  - collision avoidance
KW  - gradient methods
KW  - graph theory
KW  - mobile robots
KW  - path planning
KW  - sampling methods
KW  - convergence speed
KW  - safety certificate algorithms
KW  - fast marching gradient sampling strategy
KW  - sampling-based motion planning algorithms
KW  - marching seed
KW  - goal set
KW  - informed certificate set
KW  - planning space
KW  - RRT* algorithms
KW  - Planning
KW  - Safety
KW  - Convergence
KW  - Algorithms
KW  - Robots
KW  - Data structures
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9196685
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel fast marching gradient sampling strategy to accelerate the convergence speed of sampling-based motion planning algorithms. This strategy is based on an informed certificate set which consists of the robot states with exact collision status as well as the minimum distance and the gradient to the nearest obstacle. The informed certificate set covers almost the whole planning space such that it contains rich information for the planner. The best quality point in this set is selected as the marching seed to guide the search graph move steadily to the goal set. The distance and gradient information of the marching seed helps to generate a new sample with almost sure collision status. When a feasible solution has been found, this set can construct the restricted subset that can improve current path quality. This marching gradient sampling strategy is applied to the RRT and RRT* algorithms. Simulation experiments demonstrate that the convergence speed to a feasible solution or to the optimal solution is almost twice faster than that of the safety certificate algorithms.
ER  - 

TY  - CONF
TI  - Privacy-Aware UAV Flights through Self-Configuring Motion Planning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1169
EP  - 1175
AU  - Y. Luo
AU  - Y. Yu
AU  - Z. Jin
AU  - Y. Li
AU  - Z. Ding
AU  - Y. Zhou
AU  - Y. Liu
PY  - 2020
KW  - aircraft control
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - data privacy
KW  - decision making
KW  - mobile robots
KW  - motion control
KW  - privacy-aware UAV flights
KW  - unmanned aerial vehicle
KW  - uncertain obstacles
KW  - motion planning algorithms
KW  - privacy-preserving requirements
KW  - privacy risk aware motion planning method
KW  - privacy-sensitive sensor
KW  - safety
KW  - energy hard constraints
KW  - dynamically detected restricted areas
KW  - decision making method
KW  - test flights
KW  - DJI Matrice 100 UAV
KW  - self-configuring motion planning
KW  - Privacy
KW  - Planning
KW  - Cameras
KW  - Sensors
KW  - Trajectory
KW  - Safety
KW  - Unmanned aerial vehicles
DO  - 10.1109/ICRA40945.2020.9197564
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - During flights, an unmanned aerial vehicle (UAV) may not be allowed to move across certain areas due to soft constraints such as privacy restrictions. Current methods on self-adaption focus mostly on motion planning such that the trajectory does not trespass predetermined restricted areas. When the environment is cluttered with uncertain obstacles, however, these motion planning algorithms are not flexible enough to find a trajectory that satisfies additional privacy-preserving requirements within a tight time budget during the flights. In this paper, we propose a privacy risk aware motion planning method through the reconfiguration of privacy-sensitive sensors. It minimises environmental impact by re-configuring the sensor during flight, while still guaranteeing the safety and energy hard constraints such as collision avoidance and timeliness. First, we formulate a model for assessing privacy risks of dynamically detected restricted areas. In case the UAV cannot find a feasible solution to satisfy both hard and soft constraints from the current configuration, our decision making method can then produce an optimal reconfiguration of the privacy-sensitive sensor with a more efficient trajectory. We evaluate the proposal through various simulations with different settings in a virtual environment and also validate the approach through real test flights on DJI Matrice 100 UAV.
ER  - 

TY  - CONF
TI  - Improved C-Space Exploration and Path Planning for Robotic Manipulators Using Distance Information
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1176
EP  - 1182
AU  - B. Lacevic
AU  - D. Osmankovic
PY  - 2020
KW  - collision avoidance
KW  - manipulators
KW  - path planning
KW  - robot kinematics
KW  - trees (mathematics)
KW  - generalized bur captures large portions
KW  - free C-space
KW  - accelerated exploration
KW  - exact collision-free paths
KW  - improved C-space exploration
KW  - path planning
KW  - robotic manipulators
KW  - distance information
KW  - geometrical structure
KW  - star-like tree
KW  - arbitrary number
KW  - guaranteed collision-free edges
KW  - simple forward kinematics
KW  - RRT-like planning algorithm
KW  - generalized burs
KW  - Collision avoidance
KW  - Robot kinematics
KW  - Kinematics
KW  - Manipulators
KW  - Path planning
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196920
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a simple method to quickly explore C-spaces of robotic manipulators and thus facilitate path planning. The method is based on a novel geometrical structure called generalized bur. It is a star-like tree, rooted at a given point in free C-space, with an arbitrary number of guaranteed collision-free edges computed using distance information from the workspace and simple forward kinematics. Generalized bur captures large portions of free C-space, enabling accelerated exploration. The workspace is assumed to be decomposable into a finite set of (possibly overlapping) convex obstacles. When plugged in a suitable RRT-like planning algorithm, generalized burs enable significant performance improvements, while at the same time enabling exact collision-free paths.
ER  - 

TY  - CONF
TI  - Tuning-Free Contact-Implicit Trajectory Optimization
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1183
EP  - 1189
AU  - A. Ö. Önol
AU  - R. Corcodel
AU  - P. Long
AU  - T. Padır
PY  - 2020
KW  - humanoid robots
KW  - manipulators
KW  - mobile robots
KW  - tuning-free contact-implicit trajectory optimization
KW  - contact-implicit trajectory optimization framework
KW  - contact-interaction trajectories
KW  - robot architectures
KW  - trivial initial guess
KW  - parameter tuning
KW  - relaxed contact model
KW  - automatic penalty adjustment loop
KW  - contact information
KW  - mobile robot
KW  - nonprehensile manipulation
KW  - 7-DOF arm
KW  - planar locomotion
KW  - Robots
KW  - Task analysis
KW  - Trajectory optimization
KW  - Tuning
KW  - Computational modeling
DO  - 10.1109/ICRA40945.2020.9196805
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a contact-implicit trajectory optimization framework that can plan contact-interaction trajectories for different robot architectures and tasks using a trivial initial guess and without requiring any parameter tuning. This is achieved by using a relaxed contact model along with an automatic penalty adjustment loop for suppressing the relaxation. Moreover, the structure of the problem enables us to exploit the contact information implied by the use of relaxation in the previous iteration, such that the solution is explicitly improved with little computational overhead. We test the proposed approach in simulation experiments for non-prehensile manipulation using a 7-DOF arm and a mobile robot and for planar locomotion using a humanoid-like robot in zero gravity. The results demonstrate that our method provides an out-of-the-box solution with good performance for a wide range of applications.
ER  - 

TY  - CONF
TI  - Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1208
EP  - 1214
AU  - B. Zhou
AU  - F. Gao
AU  - J. Pan
AU  - S. Shen
PY  - 2020
KW  - autonomous aerial vehicles
KW  - gradient methods
KW  - helicopters
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - search problems
KW  - trajectory control
KW  - quadrotor trajectory replanning
KW  - replanning method
KW  - GTO
KW  - path-guided optimization approach
KW  - topological path searching algorithm
KW  - independent trajectory optimization
KW  - output superior replanned trajectories
KW  - gradient-based trajectory optimization
KW  - UAV replanning
KW  - Splines (mathematics)
KW  - Linear programming
KW  - Trajectory optimization
KW  - Robustness
KW  - Safety
DO  - 10.1109/ICRA40945.2020.9196996
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Gradient-based trajectory optimization (GTO) has gained wide popularity for quadrotor trajectory replanning. However, it suffers from local minima, which is not only fatal to safety but also unfavorable for smooth navigation. In this paper, we propose a replanning method based on GTO addressing this issue systematically. A path-guided optimization (PGO) approach is devised to tackle infeasible local minima, which improves the replanning success rate significantly. A topological path searching algorithm is developed to capture a collection of distinct useful paths in 3-D environments, each of which then guides an independent trajectory optimization. It activates a more comprehensive exploration of the solution space and output superior replanned trajectories. Benchmark evaluation shows that our method outplays state-of-the-art methods regarding replanning success rate and optimality. Challenging experiments of aggressive autonomous flight are presented to demonstrate the robustness of our method. We will release our implementation as an open-source package1.
ER  - 

TY  - CONF
TI  - Learning-based Path Planning for Autonomous Exploration of Subterranean Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1215
EP  - 1221
AU  - R. Reinhart
AU  - T. Dang
AU  - E. Hand
AU  - C. Papachristos
AU  - K. Alexis
PY  - 2020
KW  - autonomous aerial vehicles
KW  - graph theory
KW  - learning by example
KW  - mobile robots
KW  - optical radar
KW  - path planning
KW  - robot programming
KW  - sampled data systems
KW  - tunnels
KW  - autonomous exploration
KW  - subterranean environments
KW  - aerial robots
KW  - training expert
KW  - imitation learning
KW  - underground mine drifts
KW  - tunnels
KW  - graph based path planner
KW  - learning based path planning
KW  - LiDAR
KW  - range data sampling
KW  - Robot sensing systems
KW  - Path planning
KW  - Training
KW  - Training data
KW  - Planning
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9196662
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this work we present a new methodology on learning-based path planning for autonomous exploration of subterranean environments using aerial robots. Utilizing a recently proposed graph-based path planner as a "training expert" and following an approach relying on the concepts of imitation learning, we derive a trained policy capable of guiding the robot to autonomously explore underground mine drifts and tunnels. The algorithm utilizes only a short window of range data sampled from the onboard LiDAR and achieves an exploratory behavior similar to that of the training expert with a more than an order of magnitude reduction in computational cost, while simultaneously relaxing the need to maintain a consistent and online reconstructed map of the environment. The trained path planning policy is extensively evaluated both in simulation and experimentally within field tests relating to the autonomous exploration of underground mines.
ER  - 

TY  - CONF
TI  - Visual-Inertial Telepresence for Aerial Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1222
EP  - 1229
AU  - J. Lee
AU  - R. Balachandran
AU  - Y. S. Sarkisov
AU  - M. De Stefano
AU  - A. Coelho
AU  - K. Shinde
AU  - M. J. Kim
AU  - R. Triebel
AU  - K. Kondak
PY  - 2020
KW  - aerospace robotics
KW  - distance measurement
KW  - feedback
KW  - grippers
KW  - haptic interfaces
KW  - human-robot interaction
KW  - manipulators
KW  - object tracking
KW  - robot vision
KW  - sensor fusion
KW  - telerobotics
KW  - virtual reality
KW  - visual-inertial telepresence
KW  - haptic device
KW  - virtual reality
KW  - 3D visual feedback
KW  - inertial sensors
KW  - object tracking algorithm
KW  - marker tracking algorithm
KW  - visual-inertial odometry
KW  - aerial manipulation
KW  - remotely located teleoperator
KW  - onboard visual sensors
KW  - human in the loop
KW  - Three-dimensional displays
KW  - Manipulators
KW  - Visualization
KW  - Cameras
KW  - Task analysis
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9197394
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a novel telepresence system for enhancing aerial manipulation capabilities. It involves not only a haptic device, but also a virtual reality that provides a 3D visual feedback to a remotely-located teleoperator in real-time. We achieve this by utilizing onboard visual and inertial sensors, an object tracking algorithm and a pregenerated object database. As the virtual reality has to closely match the real remote scene, we propose an extension of a marker tracking algorithm with visual-inertial odometry. Both indoor and outdoor experiments show benefits of our proposed system in achieving advanced aerial manipulation tasks, namely grasping, placing, force exertion and peg-in-hole insertion.
ER  - 

TY  - CONF
TI  - Distributed Rotor-Based Vibration Suppression for Flexible Object Transport and Manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1230
EP  - 1236
AU  - H. Yang
AU  - M. -S. Kim
AU  - D. Lee
PY  - 2020
KW  - autonomous aerial vehicles
KW  - control engineering computing
KW  - controllability
KW  - helicopters
KW  - mechanical engineering computing
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - rotors
KW  - vibration control
KW  - RVM design
KW  - optimal placement
KW  - flexible object transport
KW  - manipulated object
KW  - object size
KW  - quadrotor usage
KW  - distributed RVMs
KW  - constrained optimization problem
KW  - aerial-ground manipulator system
KW  - robot-based vibration suppression module
KW  - distributed rotor-based vibration suppression
KW  - controllability gramian
KW  - multiple aerial-ground manipulator system
KW  - Rotors
KW  - Vibrations
KW  - Mathematical model
KW  - Manipulators
KW  - Controllability
KW  - Torque
DO  - 10.1109/ICRA40945.2020.9196908
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The RVM (Robot-based Vibration Suppression Modules) is proposed for the manipulation and transport of a large flexible object. Since the RVM is easily attachable/detachable to the object, this RVM allows distributing over the manipulated object so that it is scalable to the object size. The composition of the system is partly motivated by the MAGMaS (Multiple Aerial-Ground Manipulator System) [1]- [3], however, since the quadrotor usage is mechanically too complicated and its design is not optimized for manipulation, thus we overcome these limitations using distributed RVMs and newly developed theory. For this, we first provide a constrained optimization problem of RVM design with the minimum number of rotors, so that the feasible thrust force is maximized while it minimizes undesirable wrench and its own weight. Then, we derive the full dynamics and elucidate a controllability condition with multiple distributed RVMs and show that even if multiple, their structures turn out similar to [2] composed with a single quadrotor. We also elucidate the optimal placement of the RVM via the usage of controllability gramian which is not even alluded in [2] and established for the first time here. Experiments are performed to demonstrate the effectiveness of the proposed theory.
ER  - 

TY  - CONF
TI  - Aerial Manipulation using Model Predictive Control for Opening a Hinged Door
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1237
EP  - 1242
AU  - D. Lee
AU  - H. Seo
AU  - D. Kim
AU  - H. J. Kim
PY  - 2020
KW  - aerospace robotics
KW  - collision avoidance
KW  - control system synthesis
KW  - dynamic programming
KW  - manipulators
KW  - observers
KW  - position control
KW  - predictive control
KW  - robust control
KW  - three-term control
KW  - model predictive control
KW  - hinged door
KW  - environment interaction
KW  - aerial robot
KW  - multirotor-based aerial manipulator
KW  - daily-life moving structure
KW  - collision avoidance
KW  - differential dynamic programming
KW  - disturbance observer
KW  - robust controller
KW  - Manipulators
KW  - Vehicle dynamics
KW  - Mathematical model
KW  - Dynamics
KW  - Trajectory
KW  - Servomotors
DO  - 10.1109/ICRA40945.2020.9197524
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Existing studies for environment interaction with an aerial robot have been focused on interaction with static surroundings. However, to fully explore the concept of an aerial manipulation, interaction with moving structures should also be considered. In this paper, a multirotor-based aerial manipulator opening a daily-life moving structure, a hinged door, is presented. In order to address the constrained motion of the structure and to avoid collisions during operation, model predictive control (MPC) is applied to the derived coupled system dynamics between the aerial manipulator and the door involving state constraints. By implementing a constrained version of differential dynamic programming (DDP), MPC can generate position setpoints to the disturbance observer (DOB)-based robust controller in real-time, which is validated by our experimental results.
ER  - 

TY  - CONF
TI  - Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1243
EP  - 1249
AU  - B. Jeon
AU  - Y. Lee
AU  - H. J. Kim
PY  - 2020
KW  - autonomous aerial vehicles
KW  - collision avoidance
KW  - graph theory
KW  - mobile robots
KW  - object detection
KW  - quadratic programming
KW  - video recording
KW  - dense environment
KW  - drone
KW  - autonomous videography task
KW  - 3-D obstacle environment
KW  - moving object
KW  - target motion prediction module
KW  - hierarchical chasing planner
KW  - covariant optimization
KW  - bi-level structure
KW  - smooth planner
KW  - graph-search method
KW  - chasing corridor
KW  - subsequent phase
KW  - smooth trajectory
KW  - dynamically feasible trajectory
KW  - integrated motion planner
KW  - real-time aerial videography
KW  - autonomous videography task
KW  - source code
KW  - quadratic programming
KW  - Drones
KW  - Trajectory
KW  - Safety
KW  - Optimization
KW  - Measurement
KW  - Shape
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196703
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https://github.com/icsl-Jeon/traj_gen_vis and https://www.youtube.com/watch?v=_JSwXBwYRl8.
ER  - 

TY  - CONF
TI  - FG-GMM-based Interactive Behavior Estimation for Autonomous Driving Vehicles in Ramp Merging Control *
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1250
EP  - 1255
AU  - Y. Lyu
AU  - C. Dong
AU  - J. M. Dolan
PY  - 2020
KW  - automobiles
KW  - Gaussian processes
KW  - graph theory
KW  - mobile robots
KW  - probability
KW  - road traffic control
KW  - road vehicles
KW  - traffic engineering computing
KW  - autonomous driving vehicles
KW  - autonomous driving cars
KW  - factor graph
KW  - human-designed models
KW  - FG-GMM-based interactive behavior estimation
KW  - ramp merging control
KW  - significant social interaction
KW  - probabilistic graphical model merging control model
KW  - Merging
KW  - Automobiles
KW  - Estimation
KW  - Autonomous vehicles
KW  - Probabilistic logic
KW  - Mathematical model
KW  - Roads
DO  - 10.1109/ICRA40945.2020.9197218
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Interactive behavior is important for autonomous driving vehicles, especially for scenarios like ramp merging which require significant social interaction between autonomous driving vehicles and human-driven cars. This paper enhances our previous Probabilistic Graphical Model (PGM) merging control model for the interactive behavior of autonomous driving vehicles. To better estimate the interactive behavior for autonomous driving cars, a Factor Graph (FG) is used to describe the dependency among observations and estimate other cars' intentions. Real trajectories are used to approximate the model instead of human-designed models or cost functions. Forgetting factors and a Gaussian Mixture Model (GMM) are also applied in the intention estimation process for stabilization, interpolation and smoothness. The advantage of the factor graph is that the relationship between its nodes can be described by self-defined functions, instead of probabilistic relationships as in PGM, giving more flexibility. Continuity of GMM also provides higher accuracy than the previous discrete speed transition model. The proposed method enhances the overall performance of intention estimation, in terms of collision rate and average distance between cars after merging, which means it is safer and more efficient.
ER  - 

TY  - CONF
TI  - Cooperative Perception and Localization for Cooperative Driving
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1256
EP  - 1262
AU  - A. Miller
AU  - K. Rim
AU  - P. Chopra
AU  - P. Kelkar
AU  - M. Likhachev
PY  - 2020
KW  - cooperative systems
KW  - Kalman filters
KW  - location based services
KW  - mobile robots
KW  - multi-robot systems
KW  - nonlinear filters
KW  - road vehicles
KW  - sensor fusion
KW  - vehicle sensors
KW  - extended Kalman filters
KW  - fully autonomous road vehicles
KW  - cooperative driving
KW  - cooperative perception
KW  - high fidelity sensors
KW  - low fidelity sensors
KW  - localization information
KW  - Sensor systems
KW  - Time measurement
KW  - Roads
KW  - Fuses
KW  - Current measurement
KW  - Bandwidth
DO  - 10.1109/ICRA40945.2020.9197463
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Fully autonomous vehicles are expected to share the road with less advanced vehicles for a significant period of time. Furthermore, an increasing number of vehicles on the road are equipped with a variety of low-fidelity sensors which provide some perception and localization data, but not at a high enough quality for full autonomy. In this paper, we develop a perception and localization system that allows a vehicle with low-fidelity sensors to incorporate high-fidelity observations from a vehicle in front of it, allowing both vehicles to operate with full autonomy. The resulting system generates perception and localization information that is both low-noise in regions covered by high-fidelity sensors and avoids false negatives in areas only observed by low-fidelity sensors, while dealing with latency and dropout of the communication link between the two vehicles. At its core, the system uses a set of Extended Kalman filters which incorporate observations from both vehicles' sensors and extrapolate them using information about the road geometry. The perception and localization algorithms are evaluated both in simulation and on real vehicles as part of a full cooperative driving system.
ER  - 

TY  - CONF
TI  - Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1263
EP  - 1269
AU  - T. Manderson
AU  - S. Wapnick
AU  - D. Meger
AU  - G. Dudek
PY  - 2020
KW  - collision avoidance
KW  - mobile robots
KW  - off-road vehicles
KW  - robot programming
KW  - robot vision
KW  - supervised learning
KW  - autonomous driving
KW  - vision based controllers
KW  - navigation learning
KW  - model robustmess
KW  - planning foresight
KW  - self supervised method
KW  - collision avoidance
KW  - sparse aerial images
KW  - off road driving
KW  - smooth terrain traversal
KW  - visual obstructions
KW  - on-board sensors
KW  - terrain roughness
KW  - model free reinforcement learning
KW  - unstructured outdoor environments
KW  - on-board camera
KW  - rough terrain
KW  - Predictive models
KW  - Navigation
KW  - Cameras
KW  - Planning
KW  - Computational modeling
KW  - Visualization
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9196879
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both first-person and overhead aerial image inputs to our model. We nd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only first-person imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad_driving/.
ER  - 

TY  - CONF
TI  - RoadTrack: Realtime Tracking of Road Agents in Dense and Heterogeneous Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1270
EP  - 1277
AU  - R. Chandra
AU  - U. Bhattacharya
AU  - T. Randhavane
AU  - A. Bera
AU  - D. Manocha
PY  - 2020
KW  - collision avoidance
KW  - image motion analysis
KW  - object detection
KW  - object tracking
KW  - road traffic
KW  - traffic engineering computing
KW  - video signal processing
KW  - traffic videos
KW  - road agents
KW  - dense environments
KW  - heterogeneous environments
KW  - Road-Track
KW  - tracking-by-detection approach
KW  - bounding box region
KW  - Simultaneous Collision Avoidance and Interaction model
KW  - Tracking
KW  - Predictive models
KW  - Roads
KW  - Videos
KW  - Collision avoidance
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9196612
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a realtime tracking algorithm, Road-Track, to track heterogeneous road-agents in dense traffic videos. Our approach is designed for dense traffic scenarios that consist of different road-agents such as pedestrians, two-wheelers, cars, buses, etc. sharing the road. We use the tracking-by-detection approach where we track a road-agent by matching the appearance or bounding box region in the current frame with the predicted bounding box region propagated from the previous frame. Roadtrack uses a novel motion model called the Simultaneous Collision Avoidance and Interaction (SimCAI) model to predict the motion of road-agents by modeling collision avoidance and interactions between the road-agents for the next frame. We demonstrate the advantage of RoadTrack on a dataset of dense traffic videos and observe an accuracy of 75.8% on this dataset, outperforming prior state-of-the-art tracking algorithms by at least 5.2%. RoadTrack operates in realtime at approximately 30 fps and is at least 4× faster than prior tracking algorithms on standard tracking datasets.
ER  - 

TY  - CONF
TI  - Association-Free Multilateration Based on Times of Arrival
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1294
EP  - 1300
AU  - D. Frisch
AU  - U. D. Hanebeck
PY  - 2020
KW  - position measurement
KW  - radar tracking
KW  - target tracking
KW  - time-of-arrival estimation
KW  - spatially distributed receivers
KW  - static measurements
KW  - multitarget trackers
KW  - association-free multilateration
KW  - times of arrival estimation
KW  - uncorrelated measurement
KW  - initialization routine
KW  - Receivers
KW  - Noise measurement
KW  - Position measurement
KW  - Optimization
KW  - Acoustic measurements
KW  - Target tracking
KW  - Acoustics
DO  - 10.1109/ICRA40945.2020.9197455
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Multilateration systems reconstruct the location of a target that transmits electromagnetic or acoustic signals. The employed measurements for localization are the times of arrival (TOAs) of the transmitted signal, measured by a number of spatially distributed receivers at known positions. We present a novel multilateration algorithm to localize multiple targets that transmit indistinguishable signals at unknown times. That is, each receiver measures merely a set of TOAs with no association to the targets. Our method does not need any prior information. Therefore, it can provide uncorrelated, static measurements to be introduced into a separate tracker subsequently, or an initialization routine for multi target trackers.
ER  - 

TY  - CONF
TI  - Adversarial Feature Disentanglement for Place Recognition Across Changing Appearance
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1301
EP  - 1307
AU  - L. Tang
AU  - Y. Wang
AU  - Q. Luo
AU  - X. Ding
AU  - R. Xiong
PY  - 2020
KW  - feature extraction
KW  - image matching
KW  - image sequences
KW  - mobile robots
KW  - neural nets
KW  - robot vision
KW  - supervised learning
KW  - adversarial feature disentanglement
KW  - seasonal variation
KW  - visual place recognition
KW  - image descriptors
KW  - adversarial network
KW  - image sequences
KW  - domain related features
KW  - self supervised manner
KW  - image matching
KW  - Feature extraction
KW  - Training
KW  - Image reconstruction
KW  - Image recognition
KW  - Robustness
KW  - Machine learning
KW  - Neural networks
DO  - 10.1109/ICRA40945.2020.9196518
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - When robots move autonomously for long-term, varied appearance such as the transition from day to night and seasonal variation brings challenges to visual place recognition. Defining an appearance condition (e.g. a season, a kind of weather) as a domain, we consider that the desired representation for place recognition (i) should be domain-unrelated so that images from different time can be matched regardless of varied appearance, (ii) should be learned in a self-supervised manner without the need of massive manually labeled data, and (iii) should be able to train among multiple domains in one model to keep limited model complexity. This paper sets to find domain-unrelated features across extremely changing appearance, which can be used as image descriptors to match between images collected at different conditions. We propose to use the adversarial network to disentangle domain-unrelated and domain-related features, which are named place and appearance features respectively. During training, only domain information is needed without requiring manually aligned image sequences. Experiments demonstrated that our method can disentangle place and appearance features in both toy case and images from the real world, and the place feature is qualified in place recognition tasks under different appearance conditions. The proposed network is also adaptable to multiple domains without increasing model capacity and shows favorable generalization.
ER  - 

TY  - CONF
TI  - A Fast and Accurate Solution for Pose Estimation from 3D Correspondences
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1308
EP  - 1314
AU  - L. Zhou
AU  - S. Wang
AU  - M. Kaess
PY  - 2020
KW  - approximation theory
KW  - computational geometry
KW  - computer vision
KW  - convex programming
KW  - least squares approximations
KW  - minimisation
KW  - pose estimation
KW  - pose estimation
KW  - point-to-plane correspondences
KW  - computer vision
KW  - least-squares problem
KW  - global minimizer
KW  - real-time applications
KW  - local minimizer
KW  - Cayley-Gibbs-Rodriguez parameterization
KW  - first-order optimality conditions
KW  - 3D correspondences
KW  - CGR parameterization
KW  - Three-dimensional displays
KW  - Pose estimation
KW  - Cost function
KW  - Approximation algorithms
KW  - Real-time systems
KW  - Iterative closest point algorithm
KW  - Simultaneous localization and mapping
DO  - 10.1109/ICRA40945.2020.9197023
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Estimating pose from given 3D correspondences, including point-to-point, point-to-line and point-to-plane correspondences, is a fundamental task in computer vision with many applications. We present a fast and accurate solution for the least-squares problem of this task. Previous works mainly focus on studying the way to find the global minimizer of the least-squares problem. However, existing works that show the ability to achieve the global minimizer are still unsuitable for real-time applications. Furthermore, as one of contributions of this paper, we prove that there exist ambiguous configurations for any number of lines and planes. These configurations have several solutions in theory, which makes the correct solution may come from a local minimizer when the data are with noise. Previous works based on convex optimization which is unable to find local minimizers do not work in the ambiguous configuration. Our algorithm is efficient and able to reveal local minimizers. We employ the Cayley-Gibbs-Rodriguez (CGR) parameterization of the rotation to derive a general rational cost for the three cases of 3D correspondences. The main contribution of this paper is to solve the first-order optimality conditions of the least-squares problem, which are of a complicated rational form. The central idea of our algorithm is to introduce some intermediate unknowns to simplify the problem. Extensive experimental results show that our algorithm is more stable than previous algorithms when the number N of correspondences is small. Besides, when N is large, our algorithm achieves the same accuracy as the state-of-the-art algorithm [1], but our algorithm is about 7 times faster than [1] in real applications.
ER  - 

TY  - CONF
TI  - Ground Texture Based Localization Using Compact Binary Descriptors
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1315
EP  - 1321
AU  - J. Fabian Schmid
AU  - S. F. Simon
AU  - R. Mester
PY  - 2020
KW  - image matching
KW  - image texture
KW  - pose estimation
KW  - robot vision
KW  - ground texture based localization
KW  - compact binary descriptors
KW  - global localization
KW  - subsequent local localization updates
KW  - compact binary feature descriptors
KW  - localization success rates
KW  - self-contained method
KW  - matching strategy
KW  - identity matching
KW  - Feature extraction
KW  - Cameras
KW  - Robots
KW  - Latches
KW  - Task analysis
KW  - Asphalt
KW  - Pose estimation
DO  - 10.1109/ICRA40945.2020.9197221
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Ground texture based localization is a promising approach to achieve high-accuracy positioning of vehicles. We present a self-contained method that can be used for global localization as well as for subsequent local localization updates, i.e. it allows a robot to localize without any knowledge of its current whereabouts, but it can also take advantage of a prior pose estimate to reduce computation time significantly. Our method is based on a novel matching strategy, which we call identity matching, that is based on compact binary feature descriptors. Identity matching treats pairs of features as matches only if their descriptors are identical. While other methods for global localization are faster to compute, our method reaches higher localization success rates, and can switch to local localization after the initial localization.
ER  - 

TY  - CONF
TI  - Reliable Data Association for Feature-Based Vehicle Localization using Geometric Hashing Methods
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1322
EP  - 1328
AU  - I. Hofstetter
AU  - M. Sprunk
AU  - F. Ries
AU  - M. Haueis
PY  - 2020
KW  - feature extraction
KW  - optical radar
KW  - reliability
KW  - road vehicles
KW  - sensor fusion
KW  - feature-based vehicle localization
KW  - data association
KW  - local environment
KW  - plausible feature associations
KW  - safe localization
KW  - localization features
KW  - geometric hashing methods
KW  - error propagation
KW  - cylindrical objects
KW  - LiDAR data
KW  - Feature extraction
KW  - Reliability
KW  - Data mining
KW  - Noise measurement
KW  - Standards
KW  - Visualization
KW  - Object recognition
DO  - 10.1109/ICRA40945.2020.9196601
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Reliable data association represents a main challenge of feature-based vehicle localization and is the key to integrity of localization. Independent of the type of features used, incorrect associations between detected and mapped features will provide erroneous position estimates. Only if the uniqueness of a local environment is represented by the features that are stored in the map, the reliability of localization is enhanced. In this work, a new approach based on Geometric Hashing is introduced to the field of data association for feature-based vehicle localization. Without any information on a prior position, the proposed method allows to efficiently search large map regions for plausible feature associations. Therefore, odometry and GNSS-based inputs can be neglected, which reduces the risk of error propagation and enables safe localization. The approach is demonstrated on approximately 10min of data recorded in an urban scenario. Cylindrical objects without distinctive descriptors, which were extracted from LiDAR data, serve as localization features. Experimental results both demonstrate the feasibility as well as limitations of the approach.
ER  - 

TY  - CONF
TI  - Context-Aware Task Execution Using Apprenticeship Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1329
EP  - 1335
AU  - A. F. Abdelrahman
AU  - A. Mitrevski
AU  - P. G. Plöger
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - service robots
KW  - ubiquitous computing
KW  - demonstrated motion
KW  - learned policy
KW  - perceived behaviour
KW  - context-aware task execution
KW  - apprenticeship learning
KW  - assistive service robots
KW  - human-oriented tasks
KW  - task parameters
KW  - optimal behaviour
KW  - robot-to-human object hand-over
KW  - reinforcement learning
KW  - demonstrator
KW  - contextualized variants
KW  - demonstrated action
KW  - dynamic movement primitives
KW  - compact motion representations
KW  - model-based C-REPS algorithm
KW  - hand-over position
KW  - context variables
KW  - simulated task executions
KW  - evaluating emergent behaviours
KW  - context-aware action generalization
KW  - Robots
KW  - Task analysis
KW  - Trajectory
KW  - Context modeling
KW  - Encoding
KW  - Adaptation models
KW  - Learning (artificial intelligence)
DO  - 10.1109/ICRA40945.2020.9197476
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - An essential measure of autonomy in assistive service robots is adaptivity to the various contexts of human-oriented tasks, which are subject to subtle variations in task parameters that determine optimal behaviour. In this work, we propose an apprenticeship learning approach to achieving context-aware action generalization on the task of robot-to-human object hand-over. The procedure combines learning from demonstration and reinforcement learning: a robot first imitates a demonstrator's execution of the task and then learns contextualized variants of the demonstrated action through experience. We use dynamic movement primitives as compact motion representations, and a model-based C-REPS algorithm for learning policies that can specify hand-over position, conditioned on context variables. Policies are learned using simulated task executions, before transferring them to the robot and evaluating emergent behaviours. We additionally conduct a user study involving participants assuming different postures and receiving an object from a robot, which executes hand-overs by either imitating a demonstrated motion, or adapting its motion to hand-over positions suggested by the learned policy. The results confirm the hypothesized improvements in the robot's perceived behaviour when it is context-aware and adaptive, and provide useful insights that can inform future developments.
ER  - 

TY  - CONF
TI  - Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1336
EP  - 1342
AU  - R. Rayyes
AU  - H. Donat
AU  - J. Steil
PY  - 2020
KW  - adaptive control
KW  - hierarchical systems
KW  - learning (artificial intelligence)
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - radial basis function networks
KW  - stability
KW  - hierarchical interest-driven goal babbling
KW  - bootstrapping
KW  - sensorimotor skills
KW  - time-dependent changes
KW  - intrinsic motivation signal
KW  - online associative radial basis function network
KW  - associative dynamic network
KW  - parameter-sharing technique
KW  - exhaustive parameter tuning
KW  - learning process
KW  - physical robot manipulator
KW  - data-driven robot model learning
KW  - stability
KW  - Task analysis
KW  - Current measurement
KW  - Robot sensing systems
KW  - Service robots
KW  - Stability analysis
KW  - Measurement uncertainty
DO  - 10.1109/ICRA40945.2020.9196763
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a novel hierarchical online learning scheme for fast and efficient bootstrapping of sensorimotor skills. Our scheme permits rapid data-driven robot model learning in a "learning while behaving" fashion. It is updated continuously to adapt to time-dependent changes and driven by an intrinsic motivation signal. It utilizes an online associative radial basis function network, which is the first associative dynamic network to be constructed from scratch with high stability. Moreover, we propose a parameter-sharing technique to increase efficiency, stabilize the online scheme, avoid exhaustive parameter tuning, and speed up the learning process. We apply our proposed algorithms on a 7-DoF physical robot manipulator and demonstrate their performance and efficiency.
ER  - 

TY  - CONF
TI  - Robot-Supervised Learning for Object Segmentation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1343
EP  - 1349
AU  - V. Florence
AU  - J. J. Corso
AU  - B. Griffin
PY  - 2020
KW  - computer vision
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - manipulators
KW  - object detection
KW  - robot manipulator
KW  - human supervision
KW  - foreground segmentation technique
KW  - grasped object
KW  - state-of-the-art adaptable in-hand object segmentation
KW  - segmentation performance
KW  - robot-supervised
KW  - unstructured changing environments
KW  - deep learning
KW  - object detection
KW  - human annotators
KW  - learning-based segmentation methods
KW  - robotics applications
KW  - annotated training data
KW  - pixelwise segmentation
KW  - Robot kinematics
KW  - Manipulators
KW  - Image segmentation
KW  - Robot sensing systems
KW  - Object segmentation
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196543
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To be effective in unstructured and changing environments, robots must learn to recognize new objects. Deep learning has enabled rapid progress for object detection and segmentation in computer vision; however, this progress comes at the price of human annotators labeling many training examples. This paper addresses the problem of extending learning-based segmentation methods to robotics applications where annotated training data is not available. Our method enables pixelwise segmentation of grasped objects. We factor the problem of segmenting the object from the background into two sub-problems: (1) segmenting the robot manipulator and object from the background and (2) segmenting the object from the manipulator. We propose a kinematics-based foreground segmentation technique to solve (1). To solve (2), we train a self-recognition network that segments the robot manipulator. We train this network without human supervision, leveraging our foreground segmentation technique from (1) to label a training set of images containing the robot manipulator without a grasped object. We demonstrate experimentally that our method outperforms state-of-the-art adaptable in-hand object segmentation. We also show that a training set composed of automatically labelled images of grasped objects improves segmentation performance on a test set of images of the same objects in the environment.
ER  - 

TY  - CONF
TI  - Gradient and Log-based Active Learning for Semantic Segmentation of Crop and Weed for Agricultural Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1350
EP  - 1356
AU  - R. Sheikh
AU  - A. Milioto
AU  - P. Lottes
AU  - C. Stachniss
AU  - M. Bennewitz
AU  - T. Schultz
PY  - 2020
KW  - agriculture
KW  - convolutional neural nets
KW  - entropy
KW  - image sampling
KW  - image segmentation
KW  - industrial robots
KW  - learning (artificial intelligence)
KW  - crop
KW  - weed
KW  - agricultural robots
KW  - annotated datasets
KW  - supervised learning
KW  - tedious time-intensive task
KW  - active learning
KW  - image data
KW  - existing semantic segmentation CNN
KW  - growth stage
KW  - rough foreground segmentation
KW  - substantially different field
KW  - challenging datasets
KW  - agricultural robotics domain
KW  - entropy based sampling
KW  - human labeling effort
KW  - Image segmentation
KW  - Semantics
KW  - Task analysis
KW  - Agriculture
KW  - Robots
KW  - Training
KW  - Sugar industry
DO  - 10.1109/ICRA40945.2020.9196722
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Annotated datasets are essential for supervised learning. However, annotating large datasets is a tedious and time-intensive task. This paper addresses active learning in the context of semantic segmentation with the goal of reducing the human labeling effort. Our application is agricultural robotics and we focus on the task of distinguishing between crop and weed plants from image data. A key challenge in this application is the transfer of an existing semantic segmentation CNN to a new field, in which growth stage, weeds, soil, and weather conditions differ. We propose a novel approach that, given a trained model on one field together with rough foreground segmentation, refines the network on a substantially different field providing an effective method of selecting samples to annotate for supporting the transfer. We evaluated our approach on two challenging datasets from the agricultural robotics domain and show that we achieve a higher accuracy with a smaller number of samples compared to random sampling as well as entropy based sampling, which consequently reduces the required human labeling effort.
ER  - 

TY  - CONF
TI  - Learning How to Walk: Warm-starting Optimal Control Solver with Memory of Motion
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1357
EP  - 1363
AU  - T. S. Lembono
AU  - C. Mastalli
AU  - P. Fernbach
AU  - N. Mansard
AU  - S. Calinon
PY  - 2020
KW  - humanoid robots
KW  - iterative methods
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - optimal control
KW  - path planning
KW  - regression analysis
KW  - trajectory control
KW  - optimal control solver
KW  - locomotion task
KW  - humanoid robot
KW  - HPP Loco3D
KW  - versatile locomotion planner
KW  - whole-body trajectory
KW  - regression problem
KW  - single-step motion
KW  - multistep motion
KW  - predicted motion
KW  - Crocoddyl control solver
KW  - Trajectory
KW  - Databases
KW  - Optimal control
KW  - Task analysis
KW  - Legged locomotion
KW  - Ground penetrating radar
DO  - 10.1109/ICRA40945.2020.9196727
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we propose a framework to build a memory of motion for warm-starting an optimal control solver for the locomotion task of a humanoid robot. We use HPP Loco3D, a versatile locomotion planner, to generate offline a set of dynamically consistent whole-body trajectory to be stored as the memory of motion. The learning problem is formulated as a regression problem to predict a single-step motion given the desired contact locations, which is used as a building block for producing multi-step motions. The predicted motion is then used as a warm-start for the fast optimal control solver Crocoddyl. We have shown that the approach manages to reduce the required number of iterations to reach the convergence from ~9.5 to only ~3.0 iterations for the single-step motion and from ~6.2 to ~4.5 iterations for the multi-step motion, while maintaining the solution's quality.
ER  - 

TY  - CONF
TI  - Feedback Linearization for Uncertain Systems via Reinforcement Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1364
EP  - 1371
AU  - T. Westenbroek
AU  - D. Fridovich-Keil
AU  - E. Mazumdar
AU  - S. Arora
AU  - V. Prabhu
AU  - S. S. Sastry
AU  - C. J. Tomlin
PY  - 2020
KW  - approximation theory
KW  - continuous time systems
KW  - control system synthesis
KW  - feedback
KW  - function approximation
KW  - learning (artificial intelligence)
KW  - linearisation techniques
KW  - nonlinear control systems
KW  - optimisation
KW  - uncertain systems
KW  - model-free policy optimization techniques
KW  - feedback linearization
KW  - nonlinear control
KW  - nonlinear plant
KW  - feedback controller
KW  - linear control techniques
KW  - exact linearizing controllers
KW  - learned linearizing controller
KW  - model-free policy optimization algorithms
KW  - Feedback linearization
KW  - Conferences
KW  - Automation
KW  - Uncertain systems
KW  - Learning (artificial intelligence)
KW  - Control design
KW  - Nonlinear systems
DO  - 10.1109/ICRA40945.2020.9197158
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel approach to control design for nonlinear systems which leverages model-free policy optimization techniques to learn a linearizing controller for a physical plant with unknown dynamics. Feedback linearization is a technique from nonlinear control which renders the input-output dynamics of a nonlinear plant linear under application of an appropriate feedback controller. Once a linearizing controller has been constructed, desired output trajectories for the nonlinear plant can be tracked using a variety of linear control techniques. However, the calculation of a linearizing controller requires a precise dynamics model for the system. As a result, model-based approaches for learning exact linearizing controllers generally require a simple, highly structured model of the system with easily identifiable parameters. In contrast, the model-free approach presented in this paper is able to approximate the linearizing controller for the plant using general function approximation architectures. Specifically, we formulate a continuous-time optimization problem over the parameters of a learned linearizing controller whose optima are the set of parameters which best linearize the plant. We derive conditions under which the learning problem is (strongly) convex and provide guarantees which ensure the true linearizing controller for the plant is recovered. We then discuss how model-free policy optimization algorithms can be used to solve a discrete-time approximation to the problem using data collected from the real-world plant. The utility of the framework is demonstrated in simulation and on a real-world robotic platform.
ER  - 

TY  - CONF
TI  - Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1380
EP  - 1386
AU  - B. van Amsterdam
AU  - M. J. Clarkson
AU  - D. Stoyanov
PY  - 2020
KW  - feature extraction
KW  - gesture recognition
KW  - image segmentation
KW  - medical robotics
KW  - recurrent neural nets
KW  - robot dynamics
KW  - robot kinematics
KW  - surgery
KW  - multitask recurrent neural network
KW  - surgical gesture recognition
KW  - surgical data science
KW  - computer-aided intervention
KW  - robotic kinematic information
KW  - robot kinematic data
KW  - Task analysis
KW  - Training
KW  - Kinematics
KW  - Estimation
KW  - Needles
KW  - Gesture recognition
KW  - Surgery
DO  - 10.1109/ICRA40945.2020.9197301
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Surgical gesture recognition is important for surgical data science and computer-aided intervention. Even with robotic kinematic information, automatically segmenting surgical steps presents numerous challenges because surgical demonstrations are characterized by high variability in style, duration and order of actions. In order to extract discriminative features from the kinematic signals and boost recognition accuracy, we propose a multi-task recurrent neural network for simultaneous recognition of surgical gestures and estimation of a novel formulation of surgical task progress. To show the effectiveness of the presented approach, we evaluate its application on the JIGSAWS dataset, that is currently the only publicly available dataset for surgical gesture recognition featuring robot kinematic data. We demonstrate that recognition performance improves in multi-task frameworks with progress estimation without any additional manual labelling and training.
ER  - 

TY  - CONF
TI  - Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1387
EP  - 1393
AU  - N. Yilmaz
AU  - J. Y. Wu
AU  - P. Kazanzides
AU  - U. Tumerdem
PY  - 2020
KW  - force control
KW  - force sensors
KW  - mean square error methods
KW  - medical robotics
KW  - neurocontrollers
KW  - robot dynamics
KW  - surgical robotic systems
KW  - internal joint torques
KW  - robot inverse dynamics
KW  - da Vinci surgical robot
KW  - environment forces
KW  - model-based approaches
KW  - external force sensor
KW  - external force estimation
KW  - da Vinci research kit
KW  - neural network based inverse dynamics identification
KW  - normalized rootmean-square error
KW  - NRMSE
KW  - tool/tissue interaction forces
KW  - Robots
KW  - Dynamics
KW  - Force
KW  - Estimation
KW  - Training
KW  - Biological neural networks
DO  - 10.1109/ICRA40945.2020.9197445
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Most current surgical robotic systems lack the ability to sense tool/tissue interaction forces, which motivates research in methods to estimate these forces from other available measurements, primarily joint torques. These methods require the internal joint torques, due to the robot inverse dynamics, to be subtracted from the measured joint torques. This paper presents the use of neural networks to estimate the inverse dynamics of the da Vinci surgical robot, which enables estimation of the external environment forces. Experiments with motions in free space demonstrate that the neural networks can estimate the internal joint torques within 10% normalized rootmean-square error (NRMSE), which outperforms model-based approaches in the literature. Comparison with an external force sensor shows that the method is able to estimate environment forces within about 10% NRMSE.
ER  - 

TY  - CONF
TI  - Reliable Trajectories for Dynamic Quadrupeds using Analytical Costs and Learned Initializations
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1410
EP  - 1416
AU  - O. Melon
AU  - M. Geisert
AU  - D. Surovik
AU  - I. Havoutis
AU  - M. Fallon
PY  - 2020
KW  - learning (artificial intelligence)
KW  - legged locomotion
KW  - motion control
KW  - navigation
KW  - nonlinear programming
KW  - path planning
KW  - predictive control
KW  - reliability
KW  - robot dynamics
KW  - robust control
KW  - trajectory control
KW  - dynamic quadrupeds
KW  - dynamic traversal
KW  - legged robotics
KW  - robust dynamic motion
KW  - uneven terrain navigation
KW  - TOWR
KW  - learning based scheme
KW  - whole body tracking controller
KW  - trajectory optimization for walking robots
KW  - model predictive control
KW  - dynamic trajectory reliability
KW  - nonlinear program
KW  - dynamic motions
KW  - Legged locomotion
KW  - Dynamics
KW  - Foot
KW  - Trajectory optimization
DO  - 10.1109/ICRA40945.2020.9196562
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Dynamic traversal of uneven terrain is a major objective in the field of legged robotics. The most recent model predictive control approaches for these systems can generate robust dynamic motion of short duration; however, planning over a longer time horizon may be necessary when navigating complex terrain. A recently-developed framework, Trajectory Optimization for Walking Robots (TOWR), computes such plans but does not guarantee their reliability on real platforms, under uncertainty and perturbations. We extend TOWR with analytical costs to generate trajectories that a state-of-the-art whole-body tracking controller can successfully execute. To reduce online computation time, we implement a learning-based scheme for initialization of the nonlinear program based on offline experience. The execution of trajectories as long as 16 footsteps and 5.5 s over different terrains by a real quadruped demonstrates the effectiveness of the approach on hardware. This work builds toward an online system which can efficiently and robustly replan dynamic trajectories.
ER  - 

TY  - CONF
TI  - On the Hardware Feasibility of Nonlinear Trajectory Optimization for Legged Locomotion based on a Simplified Dynamics
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1417
EP  - 1423
AU  - A. Bratta
AU  - R. Orsolino
AU  - M. Focchi
AU  - V. Barasuol
AU  - G. G. Muscolo
AU  - C. Semini
PY  - 2020
KW  - hydraulic actuators
KW  - legged locomotion
KW  - motion control
KW  - path planning
KW  - position control
KW  - robot dynamics
KW  - HyQ robot
KW  - Hydraulically actuated Quadruped robot
KW  - simplified nonlinear nonconvex trajectory optimization
KW  - single rigid body dynamics-based trajectory optimizer
KW  - leg collision
KW  - leg model
KW  - joint positions
KW  - admissible contact forces
KW  - joint-torque limits
KW  - challenging terrain
KW  - robust motions
KW  - feasibility constraints
KW  - motion planning
KW  - computational efficiency
KW  - simplified dynamics
KW  - legged locomotion
KW  - nonlinear trajectory optimization
KW  - hardware feasibility
KW  - Legged locomotion
KW  - Foot
KW  - Collision avoidance
KW  - Force
KW  - Trajectory
KW  - Aerodynamics
DO  - 10.1109/ICRA40945.2020.9196903
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Simplified models are useful to increase the computational efficiency of a motion planning algorithm, but their lack of accuracy have to be managed. We propose two feasibility constraints to be included in a Single Rigid Body Dynamics-based trajectory optimizer in order to obtain robust motions in challenging terrain. The first one finds an approximate relationship between joint-torque limits and admissible contact forces, without requiring the joint positions. The second one proposes a leg model to prevent leg collision with the environment. Such constraints have been included in a simplified nonlinear non-convex trajectory optimization problem. We demonstrate the feasibility of the resulting motion plans both in simulation and on the Hydraulically actuated Quadruped (HyQ) robot, considering experiments on an irregular terrain.
ER  - 

TY  - CONF
TI  - Agile Legged-Wheeled Reconfigurable Navigation Planner Applied on the CENTAURO Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1424
EP  - 1430
AU  - V. S. Raghavan
AU  - D. Kanoulas
AU  - D. G. Caldwell
AU  - N. G. Tsagarakis
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - navigation
KW  - path planning
KW  - search problems
KW  - CENTAURO robot
KW  - agile legged wheeled reconfigurable navigation planner
KW  - hybrid legged-wheeled robots
KW  - Theta* based planner
KW  - trapezium-like search
KW  - Robot kinematics
KW  - Mobile robots
KW  - Planning
KW  - Wheels
KW  - Navigation
KW  - Collision avoidance
DO  - 10.1109/ICRA40945.2020.9197407
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid legged-wheeled robots such as the CEN-TAURO, are capable of varying their footprint polygon to carry out various agile motions. This property can be advantageous for wheeled-only planning in cluttered spaces, which is our focus. In this paper, we present an improved algorithm that builds upon our previously introduced preliminary footprint varying A* planner, which was based on the rectangular symmetry of the foot support polygon. In particular, we introduce a Theta* based planner with trapezium-like search, which aims to further reduce the limitations imposed upon the wheeled-only navigation of the CENTAURO robot by the low-dimensional search space, maintaining the real-time computational efficiency. The method is tested on the simulated and real full-size CENTAURO robot in cluttered environments.
ER  - 

TY  - CONF
TI  - Bounded haptic teleoperation of a quadruped robot’s foot posture for sensing and manipulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1431
EP  - 1437
AU  - G. Xin
AU  - J. Smith
AU  - D. Rytz
AU  - W. Wolfslag
AU  - H. -C. Lin
AU  - M. Mistry
PY  - 2020
KW  - control engineering computing
KW  - force feedback
KW  - haptic interfaces
KW  - legged locomotion
KW  - mechanical engineering computing
KW  - motion control
KW  - position control
KW  - quadratic programming
KW  - robot dynamics
KW  - telerobotics
KW  - quadruped robot ANYmal
KW  - force feedback
KW  - bounded haptic teleoperation
KW  - control framework
KW  - operator-guided haptic exploration
KW  - torso
KW  - foot posture control
KW  - whole-body controller
KW  - analytical Cartesian impedance controllers
KW  - null space projector
KW  - contact forces
KW  - force-feedback
KW  - 7D haptic joystick
KW  - Impedance
KW  - Aerospace electronics
KW  - Robot kinematics
KW  - Haptic interfaces
KW  - Torso
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197501
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a control framework to teleoperate a quadruped robot's foot for operator-guided haptic exploration of the environment. Since one leg of a quadruped robot typically only has 3 actuated degrees of freedom (DoFs), the torso is employed to assist foot posture control via a hierarchical whole-body controller. The foot and torso postures are controlled by two analytical Cartesian impedance controllers cascaded by a null space projector. The contact forces acting on supporting feet are optimized by quadratic programming (QP). The foot's Cartesian impedance controller may also estimate contact forces from trajectory tracking errors, and relay the force-feedback to the operator. A 7D haptic joystick, Sigma.7, transmits motion commands to the quadruped robot ANYmal, and renders the force feedback. Furthermore, the joystick's motion is bounded by mapping the foot's feasible force polytope constrained by the friction cones and torque limits in order to prevent the operator from driving the robot to slipping or falling over. Experimental results demonstrate the efficiency of the proposed framework.
ER  - 

TY  - CONF
TI  - Pinbot: A Walking Robot with Locking Pin Arrays for Passive Adaptability to Rough Terrains
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1438
EP  - 1444
AU  - S. Noh
AU  - A. M. Dollar
PY  - 2020
KW  - design engineering
KW  - legged locomotion
KW  - motion control
KW  - robot dynamics
KW  - legged robots
KW  - passive adaptability
KW  - locking pin arrays
KW  - pin array mechanisms
KW  - walking robot
KW  - legged robot design
KW  - rough terrain locomotion
KW  - unstructured terrains
KW  - rough terrains
KW  - stable locomotion
KW  - Conferences
KW  - Automation
DO  - 10.1109/ICRA40945.2020.9197342
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - To date, many control strategies for legged robots have been proposed for stable locomotion over rough and unstructured terrains. However, these approaches require sensing information throughout locomotion, which may be noisy or unavailable at times. An alternative solution to rough terrain locomotion is a legged robot design that can passively adapt to the variations in the terrain without requiring knowledge of them. This paper presents one such solution in the design of a walking robot that employs pin array mechanisms to passively adapt to rough terrains. The pins are passively dropped over the terrain to conform to its variations and then locked to provide a statically stable stance. Locomotion is achieved with parallel four-bar linkages that swing forward the platforms in an alternating manner. Experimental evaluation of the robot demonstrates that the pin arrays enable legged locomotion over rough terrains under open-loop control.
ER  - 

TY  - CONF
TI  - Planning for the Unexpected: Explicitly Optimizing Motions for Ground Uncertainty in Running
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1445
EP  - 1451
AU  - K. Green
AU  - R. L. Hatton
AU  - J. Hurst
PY  - 2020
KW  - legged locomotion
KW  - motion control
KW  - optimisation
KW  - robot dynamics
KW  - ground uncertainty
KW  - actuation plans
KW  - dynamic model
KW  - bipedal running
KW  - fixed body trajectory
KW  - passive dynamics
KW  - reduced order model
KW  - emergent robustness
KW  - legged robots
KW  - legged locomotion
KW  - linked inputs
KW  - input linking
KW  - hybrid dynamics
KW  - running model
KW  - optimization procedure
KW  - standard trajectory optimization
KW  - robust gaits
KW  - Legged locomotion
KW  - Springs
KW  - Dynamics
KW  - Foot
KW  - Robustness
KW  - Acceleration
DO  - 10.1109/ICRA40945.2020.9197049
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We propose a method to generate actuation plans for a reduced order, dynamic model of bipedal running. This method explicitly enforces robustness to ground uncertainty. The plan generated is not a fixed body trajectory that is aggressively stabilized: instead, the plan interacts with the passive dynamics of the reduced order model to create emergent robustness. The goal is to create plans for legged robots that will be robust to imperfect perception of the environment, and to work with dynamics that are too complex to optimize in real-time. Working within this dynamic model of legged locomotion, we optimize a set of disturbance cases together with the nominal case, all with linked inputs. The input linking is nontrivial due to the hybrid dynamics of the running model but our solution is effective and has analytical gradients. The optimization procedure proposed is significantly slower than a standard trajectory optimization, but results in robust gaits that reject disturbances extremely effectively without any replanning required.
ER  - 

TY  - CONF
TI  - One-Shot Multi-Path Planning for Robotic Applications Using Fully Convolutional Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1460
EP  - 1466
AU  - T. Kulvicius
AU  - S. Herzog
AU  - T. Lüddecke
AU  - M. Tamosiunaite
AU  - F. Wörgötter
PY  - 2020
KW  - convolutional neural nets
KW  - iterative methods
KW  - mobile robots
KW  - neurocontrollers
KW  - path planning
KW  - robot action execution
KW  - motion trajectory
KW  - iterative methods
KW  - fully convolutional neural network
KW  - network prediction iteration
KW  - optimal paths
KW  - single path predictions
KW  - simultaneously generated paths
KW  - shot multipath planning
KW  - robotic applications
KW  - Two dimensional displays
KW  - Three-dimensional displays
KW  - Training
KW  - Robots
KW  - Path planning
KW  - Prediction algorithms
KW  - Planning
DO  - 10.1109/ICRA40945.2020.9196719
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Path planning is important for robot action execution, since a path or a motion trajectory for a particular action has to be defined first before the action can be executed. Most of the current approaches are iterative methods where the trajectory is generated by predicting the next state based on the current state. Here we propose a novel method by utilising a fully convolutional neural network, which allows generation of complete paths even for several agents with one network prediction iteration. We demonstrate that our method is able to successfully generate optimal or close to optimal paths (less than 10% longer) in more than 99% of the cases for single path predictions in 2D and 3D environments. Furthermore, we show that the network is - without specific training on such cases - able to create (close to) optimal paths in 96% of the cases for two and in 84% of the cases for three simultaneously generated paths.
ER  - 

TY  - CONF
TI  - Efficient Iterative Linear-Quadratic Approximations for Nonlinear Multi-Player General-Sum Differential Games
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1475
EP  - 1481
AU  - D. Fridovich-Keil
AU  - E. Ratner
AU  - L. Peters
AU  - A. D. Dragan
AU  - C. J. Tomlin
PY  - 2020
KW  - approximation theory
KW  - decision making
KW  - differential games
KW  - iterative methods
KW  - linear quadratic control
KW  - multi-agent systems
KW  - multi-robot systems
KW  - nonlinear control systems
KW  - iterative linear-quadratic regulator
KW  - linear dynamics
KW  - quadratic costs
KW  - linear-quadratic games
KW  - complex interactive behavior
KW  - efficient iterative linear-quadratic approximations
KW  - nonlinear multiplayer general-sum differential games
KW  - robotics
KW  - multiple decision making agents
KW  - expressive theoretical framework
KW  - multiagent problems
KW  - numerical solution techniques
KW  - state dimension
KW  - single agent optimal control problem
KW  - ILQR
KW  - repeated approximations
KW  - three-player 14-state simulated intersection problem
KW  - hardware collision-avoidance test
KW  - time 0.25 s
KW  - time 50.0 ms
KW  - Games
KW  - Heuristic algorithms
KW  - Approximation algorithms
KW  - Optimal control
KW  - Iterative methods
KW  - Trajectory
KW  - Automobiles
DO  - 10.1109/ICRA40945.2020.9197129
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many problems in robotics involve multiple decision making agents. To operate efficiently in such settings, a robot must reason about the impact of its decisions on the behavior of other agents. Differential games offer an expressive theoretical framework for formulating these types of multi-agent problems. Unfortunately, most numerical solution techniques scale poorly with state dimension and are rarely used in real-time applications. For this reason, it is common to predict the future decisions of other agents and solve the resulting decoupled, i.e., single-agent, optimal control problem. This decoupling neglects the underlying interactive nature of the problem; however, efficient solution techniques do exist for broad classes of optimal control problems. We take inspiration from one such technique, the iterative linear-quadratic regulator (ILQR), which solves repeated approximations with linear dynamics and quadratic costs. Similarly, our proposed algorithm solves repeated linear-quadratic games. We experimentally benchmark our algorithm in several examples with a variety of initial conditions and show that the resulting strategies exhibit complex interactive behavior. Our results indicate that our algorithm converges reliably and runs in real-time. In a three-player, 14-state simulated intersection problem, our algorithm initially converges in <; 0.25 s. Receding horizon invocations converge in <; 50 ms in a hardware collision-avoidance test.
ER  - 

TY  - CONF
TI  - Path-Following Model Predictive Control of Ballbots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1498
EP  - 1504
AU  - T. K. Jespersen
AU  - M. al Ahdab
AU  - J. d. Dios F. Mendez
AU  - M. R. Damgaard
AU  - K. D. Hansen
AU  - R. Pedersen
AU  - T. Bak
PY  - 2020
KW  - mobile robots
KW  - predictive control
KW  - robot dynamics
KW  - model predictive control
KW  - path-following tasks
KW  - dynamically unstable mobile robots
KW  - single ball
KW  - simplied version
KW  - physical ballbot system
KW  - high fidelity model
KW  - online implementation
KW  - quaternion-based model
KW  - Robots
KW  - Solid modeling
KW  - Friction
KW  - Quaternions
KW  - Planning
KW  - Acceleration
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196634
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper introduces a novel approach for model predictive control of ballbots for path-following tasks. Ballbots are dynamically unstable mobile robots which are designed to balance on a single ball. The model presented in this paper is a simplied version of a full quaternion-based model of ballbots' underactuated dynamics which is suited for online implementation. Furthermore, the approach is extended to handle nearby obstacles directly in the MPC formulation. The presented controller is validated through simulation on a high fidelity model as well as through real-world experiments on a physical ballbot system.
ER  - 

TY  - CONF
TI  - Underactuated Waypoint Trajectory Optimization for Light Painting Photography
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1505
EP  - 1510
AU  - C. Eilers
AU  - J. Eschmann
AU  - R. Menzenbach
AU  - B. Belousov
AU  - F. Muratore
AU  - J. Peters
PY  - 2020
KW  - control system synthesis
KW  - nonlinear control systems
KW  - optimisation
KW  - pendulums
KW  - photography
KW  - trajectory control
KW  - underactuated waypoint trajectory optimization
KW  - light painting photography
KW  - control engineering
KW  - auxiliary optimization variables
KW  - waypoint activations
KW  - letter drawing task
KW  - long exposure photography
KW  - r control engineering
KW  - Linear programming
KW  - Task analysis
KW  - Trajectory optimization
KW  - Painting
KW  - Photography
DO  - 10.1109/ICRA40945.2020.9196516
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Despite their abundance in robotics and nature, underactuated systems remain a challenge for control engineering. Trajectory optimization provides a generally applicable solution, however its efficiency strongly depends on the skill of the engineer to frame the problem in an optimizer-friendly way. This paper proposes a procedure that automates such problem reformulation for a class of tasks in which the desired trajectory is specified by a sequence of waypoints. The approach is based on introducing auxiliary optimization variables that represent waypoint activations. To validate the proposed method, a letter drawing task is set up where shapes traced by the tip of a rotary inverted pendulum are visualized using long exposure photography.
ER  - 

TY  - CONF
TI  - Whole-Body Walking Generation using Contact Parametrization: A Non-Linear Trajectory Optimization Approach
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1511
EP  - 1517
AU  - S. Dafarra
AU  - G. Romualdi
AU  - G. Metta
AU  - D. Pucci
PY  - 2020
KW  - humanoid robots
KW  - legged locomotion
KW  - optimal control
KW  - optimisation
KW  - robot dynamics
KW  - trajectory control
KW  - humanoid robot model
KW  - walking surface
KW  - contact parametrization
KW  - complementarity-free
KW  - predefined contact sequence
KW  - optimal control
KW  - walking trajectories
KW  - dynamic equations
KW  - optimization problem
KW  - direct multiple shooting approach
KW  - body walking generation
KW  - nonlinear trajectory optimization
KW  - centroidal dynamics
KW  - humanoid robot kinematics
KW  - humanoid robot dynamics
KW  - Legged locomotion
KW  - Foot
KW  - Force
KW  - Trajectory
KW  - Robot kinematics
KW  - Mathematical model
DO  - 10.1109/ICRA40945.2020.9196801
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we describe a planner capable of generating walking trajectories by using the centroidal dynamics and the full kinematics of a humanoid robot model. The interaction between the robot and the walking surface is modeled explicitly through a novel contact parametrization. The approach is complementarity-free and does not need a predefined contact sequence. By solving an optimal control problem we obtain walking trajectories. In particular, through a set of constraints and dynamic equations, we model the robot in contact with the ground. We describe the objective the robot needs to achieve with a set of tasks. The whole optimal control problem is transcribed into an optimization problem via a Direct Multiple Shooting approach and solved with an off-the-shelf solver. We show that it is possible to achieve walking motions automatically by specifying a minimal set of references, such as a constant desired Center of Mass velocity and a reference point on the ground.
ER  - 

TY  - CONF
TI  - Controlling Fast Height Variation of an Actively Articulated Wheeled Humanoid Robot Using Center of Mass Trajectory
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1518
EP  - 1524
AU  - M. V. Otubela
AU  - C. McGinn
PY  - 2020
KW  - adaptive control
KW  - humanoid robots
KW  - legged locomotion
KW  - motion control
KW  - nonlinear dynamical systems
KW  - optimal control
KW  - path planning
KW  - quadratic programming
KW  - robot dynamics
KW  - robot kinematics
KW  - robust control
KW  - splines (mathematics)
KW  - stability criteria
KW  - trajectory control
KW  - task-space inverse dynamics controller
KW  - optimal 7th order spline coefficients
KW  - dynamic stability
KW  - TSID controller
KW  - simplified passive dynamics model
KW  - Aerobot platform
KW  - fast height adaptation
KW  - actively articulated wheeled humanoid robot
KW  - center of mass trajectory
KW  - hybrid wheel-legged robots
KW  - complex terrain
KW  - purely wheeled morphologies
KW  - highly adaptive behaviours
KW  - nonlinear dynamics control problem
KW  - hybrid humanoid platform
KW  - offline trajectory optimisation
KW  - optimal center of mass kinematic trajectories
KW  - fast height variation control
KW  - nonlinear zero moment point
KW  - optimal control
KW  - sequential quadratic programming
KW  - robot kinematics
KW  - stability criterion
KW  - motion plan
KW  - task Jacobians
KW  - robust control
KW  - Aerodynamics
KW  - Mathematical model
KW  - Acceleration
KW  - Robot kinematics
KW  - Stability analysis
DO  - 10.1109/ICRA40945.2020.9196569
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Hybrid wheel-legged robots have begun to demonstrate the ability to adapt to complex terrain traditionally inaccessible to purely wheeled morphologies. Further research is needed into how their dynamics can be optimally controlled for developing highly adaptive behaviours on challenging terrain. Using optimal center of mass (COM) kinematic trajectories, this work examines the nonlinear dynamics control problem for fast height adaptation on the hybrid humanoid platform known as Aerobot. We explore the dynamics control problem through experimentation with an offline trajectory optimisation (TO) method and a task-space inverse dynamics (TSID) controller for varying the robot's height. Our TO approach uses sequential quadratic programming (SQP) to solve optimal 7th order spline coefficients for the robot's kinematics. The nonlinear Zero Moment Point (ZMP) is used to model a stability criterion that is constrained in the TO problem to ensure dynamic stability. Our TSID controller follows motion plans based on using task jacobians and a simplified passive dynamics model of the Aerobot platform. Results exhibit fast height adaptation on the Aerobot platform with significantly differing results between the control methods that prompts new research into how it may be controlled online.
ER  - 

TY  - CONF
TI  - Contact-Aware Controller Design for Complementarity Systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1525
EP  - 1531
AU  - A. Aydinoglu
AU  - V. M. Preciado
AU  - M. Posa
PY  - 2020
KW  - control system synthesis
KW  - mobile robots
KW  - motion control
KW  - multi-robot systems
KW  - optimisation
KW  - robot dynamics
KW  - robust control
KW  - tactile sensors
KW  - multicontact motion
KW  - combinatoric structure
KW  - real-time control
KW  - tactile sensors
KW  - robust control
KW  - complementarity structure
KW  - contact dynamics
KW  - control framework
KW  - multicontact robotics problems
KW  - contact-aware controller design
KW  - robotic tasks
KW  - locomotion
KW  - Lyapunov methods
KW  - Control systems
KW  - Force
KW  - Dynamics
KW  - Task analysis
KW  - Tactile sensors
DO  - 10.1109/ICRA40945.2020.9197568
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - While many robotic tasks, like manipulation and locomotion, are fundamentally based in making and breaking contact with the environment, state-of-the-art control policies struggle to deal with the hybrid nature of multi-contact motion. Such controllers often rely heavily upon heuristics or, due to the combinatoric structure in the dynamics, are unsuitable for real-time control. Principled deployment of tactile sensors offers a promising mechanism for stable and robust control, but modern approaches often use this data in an ad hoc manner, for instance to guide guarded moves. In this work, by exploiting the complementarity structure of contact dynamics, we propose a control framework which can close the loop on rich, tactile sensors. Critically, this framework is non-combinatoric, enabling optimization algorithms to automatically synthesize provably stable control policies. We demonstrate this approach on three different underactuated, multi-contact robotics problems.
ER  - 

TY  - CONF
TI  - Learning to Generate 6-DoF Grasp Poses with Reachability Awareness
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1532
EP  - 1538
AU  - X. Lou
AU  - Y. Yang
AU  - C. Choi
PY  - 2020
KW  - convolutional neural nets
KW  - learning systems
KW  - manipulators
KW  - neurocontrollers
KW  - stability
KW  - 3D CNN
KW  - 6-DoF grasp poses
KW  - reachability awareness
KW  - voxel-based deep 3D convolutional neural network
KW  - reachability predictor
KW  - robot
KW  - grasp pose stability
KW  - Grasping
KW  - Three-dimensional displays
KW  - Robot kinematics
KW  - Planning
KW  - Measurement
KW  - Data models
KW  - Grasping
KW  - Deep Learning in Robotics and Automation
KW  - Perception for Grasping and Manipulation
DO  - 10.1109/ICRA40945.2020.9197413
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Motivated by the stringent requirements of unstructured real-world where a plethora of unknown objects reside in arbitrary locations of the surface, we propose a voxel-based deep 3D Convolutional Neural Network (3D CNN) that generates feasible 6-DoF grasp poses in unrestricted workspace with reachability awareness. Unlike the majority of works that predict if a proposed grasp pose within the restricted workspace will be successful solely based on grasp pose stability, our approach further learns a reachability predictor that evaluates if the grasp pose is reachable or not from robot's own experience. To avoid the laborious real training data collection, we exploit the power of simulation to train our networks on a large-scale synthetic dataset. This work is an early attempt that simultaneously learns grasping reachability while proposing feasible grasp poses with 3D CNN. Experimental results in both simulation and real-world demonstrate that our approach outperforms several other methods and achieves 82.5% grasping success rate on unknown objects.
ER  - 

TY  - CONF
TI  - Enhancing Grasp Pose Computation in Gripper Workspace Spheres
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1539
EP  - 1545
AU  - M. Sorour
AU  - K. Elgeneidy
AU  - M. Hanheide
AU  - M. Abdalmjed
AU  - A. Srinivasan
AU  - G. Neumann
PY  - 2020
KW  - computational geometry
KW  - dexterous manipulators
KW  - grippers
KW  - path planning
KW  - pose estimation
KW  - position control
KW  - robot vision
KW  - grasp pose computation
KW  - gripper workspace spheres
KW  - registered point cloud
KW  - gripper position sampling
KW  - orientation sampling
KW  - object orientation estimation
KW  - jaw gripper
KW  - Franka Panda gripper
KW  - geometric based methods
KW  - multifingered hands
KW  - Intel RealSense-D435 depth camera
KW  - Grippers
KW  - Three-dimensional displays
KW  - Ellipsoids
KW  - Grasping
KW  - Measurement
KW  - Shape
KW  - Planning
KW  - grasping
KW  - manipulation
DO  - 10.1109/ICRA40945.2020.9196863
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, enhancement to the novel grasp planning algorithm based on gripper workspace spheres is presented. Our development requires a registered point cloud of the target from different views, assuming no prior knowledge of the object, nor any of its properties. This work features a new set of metrics for grasp pose candidates evaluation, as well as exploring the impact of high object sampling on grasp success rates. In addition to gripper position sampling, we now perform orientation sampling about the x, y, and z-axes, hence the grasping algorithm no longer require object orientation estimation. Successful experiments have been conducted on a simple jaw gripper (Franka Panda gripper) as well as a complex, high Degree of Freedom (DoF) hand (Allegro hand) as a proof of its versatility. Higher grasp success rates of 76% and 85.5% respectively has been reported by real world experiments.
ER  - 

TY  - CONF
TI  - Minimal Work: A Grasp Quality Metric for Deformable Hollow Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1546
EP  - 1552
AU  - J. Xu
AU  - M. Danielczuk
AU  - J. Ichnowski
AU  - J. Mahler
AU  - E. Steinbach
AU  - K. Goldberg
PY  - 2020
KW  - deformation
KW  - elasticity
KW  - grippers
KW  - linear programming
KW  - Robotiq gripper
KW  - UR5 robot
KW  - object empirical stiffness
KW  - linear program
KW  - manipulation task
KW  - wrench resistance
KW  - physical grasps
KW  - work quality metric
KW  - wrench-based quality metrics
KW  - real-world grasps
KW  - gripper jaw displacements
KW  - grasp force
KW  - external wrench
KW  - object deformation
KW  - robot grasping
KW  - deformable hollow objects
KW  - grasp quality metric
KW  - Measurement
KW  - Force
KW  - Task analysis
KW  - Strain
KW  - Computational modeling
KW  - Friction
KW  - Grippers
DO  - 10.1109/ICRA40945.2020.9197062
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot grasping of deformable hollow objects such as plastic bottles and cups is challenging, as the grasp should resist disturbances while minimally deforming the object so as not to damage it or dislodge liquids. We propose minimal work as a novel grasp quality metric that combines wrench resistance and object deformation. We introduce an efficient algorithm to compute the work required to resist an external wrench for a manipulation task by solving a linear program. The algorithm first computes the minimum required grasp force and an estimation of the gripper jaw displacements based on the object's empirical stiffness at different locations. The work done by the jaws is the product of the grasp force and the displacements. Grasps requiring minimal work are considered to be of high quality. We collect 460 physical grasps with a UR5 robot and a Robotiq gripper. We consider a grasp to be successful if it completes the task without damaging the object or dislodging the content. Physical experiments suggest that the minimal work quality metric reaches 74.2% balanced accuracy, a metric that is the raw accuracy normalized by the number of successful and failed real-world grasps, and is up to 24.2% higher than classical wrench-based quality metrics.
ER  - 

TY  - CONF
TI  - Hierarchical 6-DoF Grasping with Approaching Direction Selection
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1553
EP  - 1559
AU  - Y. Choi
AU  - H. Kee
AU  - K. Lee
AU  - J. Choy
AU  - J. Min
AU  - S. Lee
AU  - S. Oh
PY  - 2020
KW  - convolutional neural nets
KW  - entropy
KW  - geometry
KW  - grippers
KW  - hierarchical systems
KW  - iterative methods
KW  - learning systems
KW  - neurocontrollers
KW  - optimisation
KW  - position control
KW  - cluttered objects
KW  - cross entropy method
KW  - iterative direction optimization
KW  - derivative-free optimization
KW  - geometry-based prior
KW  - point clouds
KW  - input grasp representations
KW  - robot arm
KW  - detection problem
KW  - hierarchical approach
KW  - robot grasping
KW  - hierarchical 6-DoF grasping
KW  - surface normal directions
KW  - approaching direction selection method
KW  - grasp quality
KW  - fully convolutional grasp quality network
KW  - Grasping
KW  - Three-dimensional displays
KW  - Grippers
KW  - Service robots
KW  - Manipulators
KW  - Geometry
DO  - 10.1109/ICRA40945.2020.9196678
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - In this paper, we tackle the problem of 6-DoF grasp detection which is crucial for robot grasping in cluttered real-world scenes. Unlike existing approaches which synthesize 6-DoF grasp data sets and train grasp quality networks with input grasp representations based on point clouds, we rather take a novel hierarchical approach which does not use any 6-DoF grasp data. We cast the 6-DoF grasp detection problem as a robot arm approaching direction selection problem using the existing 4-DoF grasp detection algorithm, by exploiting a fully convolutional grasp quality network for evaluating the quality of an approaching direction. To select the best approaching direction with the highest grasp quality, we propose an approaching direction selection method which leverages a geometry-based prior and a derivative-free optimization method. Specifically, we optimize the direction iteratively using the cross entropy method with initial samples of surface normal directions. Our algorithm efficiently finds diverse 6-DoF grasps by the novel way of evaluating and optimizing approaching directions. We validate that the proposed method outperforms other selection methods in scenarios with cluttered objects in a physics-based simulator. Finally, we show that our method outperforms the state-of-the-art grasp detection method in real-world experiments with robots.
ER  - 

TY  - CONF
TI  - Geometric Characterization of Two-Finger Basket Grasps of 2-D Objects: Contact Space Formulation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1560
EP  - 1566
AU  - E. D. Rimon
AU  - F. T. Pokorny
AU  - W. Wan
PY  - 2020
KW  - dexterous manipulators
KW  - grippers
KW  - contact space formulation
KW  - two-finger basket grasps
KW  - high-dimensional configuration space
KW  - low-dimensional contact space
KW  - two-finger contacts
KW  - object boundary
KW  - critical finger opening
KW  - two-finger robot hand
KW  - geometric techniques
KW  - depth and drop-off finger opening
KW  - Robots
KW  - Gravity
KW  - Search problems
KW  - Security
KW  - Layout
KW  - Safety
KW  - Air pollution
DO  - 10.1109/ICRA40945.2020.9196946
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper considers basket grasps, where a two-finger robot hand forms a basket that can safely lift and carry rigid objects in a 2-D gravitational environment. The two-finger basket grasps form special points in a high-dimensional configuration space of the object and two-finger robot hand. This paper establishes that all two-finger basket grasps can be found in a low-dimensional contact space that parametrizes the two-finger contacts along the supported object boundary. Using contact space, each basket grasp is associated with its depth that provides a security measure while carrying the object, as well as its safety margin away from a critical finger opening where the object drops-off into its intended destination. Geometric techniques that compute the depth and drop-off finger opening are described and illustrated with detailed graphical and numerical examples.
ER  - 

TY  - CONF
TI  - Robust Sound Source Localization considering Similarity of Back-Propagation Signals
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1574
EP  - 1580
AU  - I. An
AU  - B. Jo
AU  - Y. Kwon
AU  - J. -w. Choi
AU  - S. -e. Yoon
PY  - 2020
KW  - acoustic generators
KW  - acoustic radiators
KW  - acoustic wave propagation
KW  - backpropagation
KW  - Monte Carlo methods
KW  - ray tracing
KW  - signal processing
KW  - transient response
KW  - ground-truth sound source position
KW  - robust sound source localization
KW  - backpropagation signals
KW  - direct reflection acoustic ray paths
KW  - backward sound propagation path estimation
KW  - ray tracing
KW  - impulse response
KW  - Monte Carlo localization method
KW  - size 3.0 m
KW  - noise figure 77.0 dB
KW  - noise figure 67.0 dB
KW  - Acoustics
KW  - Direction-of-arrival estimation
KW  - Array signal processing
KW  - Three-dimensional displays
KW  - Microphone arrays
KW  - Position measurement
DO  - 10.1109/ICRA40945.2020.9196743
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel, robust sound source localization algorithm considering back-propagation signals. Sound propagation paths are estimated by generating direct and reflection acoustic rays based on ray tracing in a backward manner. We then compute the back-propagation signals by designing and using the impulse response of the backward sound propagation based on the acoustic ray paths. For identifying the 3D source position, we use a well-established Monte Carlo localization method. Candidates for a source position are determined by identifying convergence regions of acoustic ray paths. Those candidates are validated by measuring similarities between back-propagation signals, under the assumption that the back-propagation signals of different acoustic ray paths should be similar near the ground-truth sound source position. Thanks to considering similarities of back-propagation signals, our approach can localize a source position with an averaged error of 0.55 m in a room of 7 m by 7 m area with 3 m height in tested environments. We also place additional 67 dB and 77 dB white noise at the background, to test the robustness of our approach. Overall, we observe a 7 % to 100 % improvement in accuracy over the state-of-the-art method.
ER  - 

TY  - CONF
TI  - BatVision: Learning to See 3D Spatial Layout with Two Ears
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1581
EP  - 1587
AU  - J. H. Christensen
AU  - S. Hornauer
AU  - S. X. Yu
PY  - 2020
KW  - acoustic signal processing
KW  - audio signal processing
KW  - bioacoustics
KW  - cameras
KW  - ear
KW  - image colour analysis
KW  - image sensors
KW  - mechanoception
KW  - mobile robots
KW  - object detection
KW  - path planning
KW  - robot vision
KW  - stereo image processing
KW  - visual perception
KW  - machine vision
KW  - 3D spatial layout
KW  - ears
KW  - nonvisual perception
KW  - artificial systems
KW  - ultrasound complement camera-based vision
KW  - information gain
KW  - harness sound
KW  - machine perception
KW  - low- cost BatVision system
KW  - short chirps
KW  - artificial human pinnae pair
KW  - stereo camera
KW  - color images
KW  - scene depths
KW  - trained BatVision
KW  - 2D visual scenes
KW  - vision system
KW  - robot navigation
KW  - Microphones
KW  - Visualization
KW  - Ear
KW  - Chirp
KW  - Cameras
KW  - Three-dimensional displays
KW  - Training
DO  - 10.1109/ICRA40945.2020.9196934
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception.Inspired by bats' echolocation mechanism, we design a low- cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.
ER  - 

TY  - CONF
TI  - Self-Supervised Learning for Alignment of Objects and Sound
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1588
EP  - 1594
AU  - X. Liu
AU  - X. Liu
AU  - D. Guo
AU  - H. Liu
AU  - F. Sun
AU  - H. Min
PY  - 2020
KW  - human-robot interaction
KW  - learning (artificial intelligence)
KW  - object detection
KW  - source separation
KW  - human-robot interaction
KW  - scene understanding
KW  - sound source separation task
KW  - self-supervised learning framework
KW  - object detection
KW  - sound separation modules
KW  - sound components
KW  - visual information
KW  - audio information
KW  - Visualization
KW  - Videos
KW  - Feature extraction
KW  - Object detection
KW  - Spectrogram
KW  - Training
KW  - Robots
DO  - 10.1109/ICRA40945.2020.9197566
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The sound source separation problem has many useful applications in the field of robotics, such as human-robot interaction, scene understanding, etc. However, it remains a very challenging problem. In this paper, we utilize both visual and audio information of videos to perform the sound source separation task. A self-supervised learning framework is proposed to implement the object detection and sound separation modules simultaneously. Such an approach is designed to better find the alignment between the detected objects and separated sound components. Our experiments, conducted on both the synthetic and real datasets, validate this approach and demonstrate the effectiveness of the proposed model in the task of object and sound alignment.
ER  - 

TY  - CONF
TI  - The OmniScape Dataset
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1603
EP  - 1608
AU  - A. R. Sekkat
AU  - Y. Dupuis
AU  - P. Vasseur
AU  - P. Honeine
PY  - 2020
KW  - cameras
KW  - image segmentation
KW  - motorcycles
KW  - object detection
KW  - stereo image processing
KW  - traffic engineering computing
KW  - omnidirectional images
KW  - semantic segmentation
KW  - depth map
KW  - ground truth images
KW  - CARLA Simulator
KW  - open-source simulator
KW  - catadioptric images
KW  - OmniScape dataset
KW  - autonomous driving research
KW  - Grand Theft Auto V
KW  - two-wheeled vehicles
KW  - motorcycle
KW  - Cameras
KW  - Semantics
KW  - Vehicle dynamics
KW  - Motorcycles
KW  - Image segmentation
KW  - Virtual environments
KW  - Roads
DO  - 10.1109/ICRA40945.2020.9197144
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.
ER  - 

TY  - CONF
TI  - An ERT-based Robotic Skin with Sparsely Distributed Electrodes: Structure, Fabrication, and DNN-based Signal Processing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1617
EP  - 1624
AU  - K. Park
AU  - H. Park
AU  - H. Lee
AU  - S. Park
AU  - J. Kim
PY  - 2020
KW  - biomedical electrodes
KW  - carbon nanotubes
KW  - manipulators
KW  - neural nets
KW  - tactile sensors
KW  - tomography
KW  - cylindrical surface
KW  - sensor output images
KW  - 3D-shaped sensors
KW  - ERT-based robotic skin
KW  - sparsely distributed electrodes
KW  - DNN-based signal processing
KW  - electrical resistance tomography
KW  - large-scale tactile sensor
KW  - conductivity distribution
KW  - physical model
KW  - curved surface
KW  - electrode configuration
KW  - edge region
KW  - sensor performance
KW  - carbon nanotube-dispersed solution
KW  - conductive sensing domain
KW  - Robot sensing systems
KW  - Electrodes
KW  - Conductivity
KW  - Image reconstruction
KW  - Inverse problems
DO  - 10.1109/ICRA40945.2020.9197361
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Electrical resistance tomography (ERT) has previously been utilized to develop a large-scale tactile sensor because this approach enables the estimation of the conductivity distribution among the electrodes based on a known physical model. Such a sensor made with a stretchable material can conform to a curved surface. However, this sensor cannot fully cover a cylindrical surface because in such a configuration, the edges of the sensor must meet each other. The electrode configuration becomes irregular in this edge region, which may degrade the sensor performance. In this paper, we introduce an ERT-based robotic skin with evenly and sparsely distributed electrodes. For implementation, we sprayed a carbon nanotube (CNT)-dispersed solution to form a conductive sensing domain on a cylindrical surface. The electrodes were firmly embedded in the surface so that the wires were not exposed to the outside. The sensor output images were estimated using a deep neural network (DNN), which was trained with noisy simulation data. An indentation experiment revealed that the localization error of the sensor was 5.2 ± 3.3 mm, which is remarkable performance with only 30 electrodes. A frame rate of up to 120 Hz could be achieved with a sensing domain area of 90 cm2. The proposed approach simplifies the fabrication of 3D-shaped sensors, allowing them to be easily applied to existing robot arms in a seamless and robust manner.
ER  - 

TY  - CONF
TI  - FBG-Based Triaxial Force Sensor Integrated with an Eccentrically Configured Imaging Probe for Endoluminal Optical Biopsy
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1625
EP  - 1631
AU  - Z. Wu
AU  - A. Gao
AU  - N. Liu
AU  - Z. Jin
AU  - G. -Z. Yang
PY  - 2020
KW  - biological tissues
KW  - biomedical optical imaging
KW  - Bragg gratings
KW  - fibre optic sensors
KW  - force measurement
KW  - force sensors
KW  - medical image processing
KW  - support vector machines
KW  - temperature sensors
KW  - FBG-based triaxial force sensor integrated
KW  - eccentrically configured imaging probe
KW  - endoluminal optical biopsy
KW  - endoluminal intervention
KW  - lesion
KW  - robotic bronchoscopy
KW  - FBG sensors
KW  - conical substrate
KW  - eccentric inner lumen
KW  - flexible imaging probe
KW  - laser-profiled continuum robot
KW  - temperature sensors
KW  - sensor substrate
KW  - developed triaxial force sensor
KW  - Robot sensing systems
KW  - Temperature sensors
KW  - Force
KW  - Force sensors
KW  - Optical fibers
DO  - 10.1109/ICRA40945.2020.9197128
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Accurate force sensing is important for endoluminal intervention in terms of both safety and lesion targeting. This paper develops an FBG-based force sensor for robotic bronchoscopy by configuring three FBG sensors at the lateral side of a conical substrate. It allows a large and eccentric inner lumen for the interventional instrument, enabling a flexible imaging probe inside to perform optical biopsy. The force sensor is embodied with a laser-profiled continuum robot and thermo drift is fully compensated by three temperature sensors integrated on the circumference surface of the sensor substrate. Different decoupling approaches are investigated, and nonlinear decoupling is adopted based on the cross-validation SVM and a Gaussian kernel function, achieving an accuracy of 10.58 mN, 14.57 mN and 26.32 mN along X, Y and Z axis, respectively. The tissue test is also investigated to further demonstrate the feasibility of the developed triaxial force sensor.
ER  - 

TY  - CONF
TI  - Calibrating a Soft ERT-Based Tactile Sensor with a Multiphysics Model and Sim-to-real Transfer Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1632
EP  - 1638
AU  - H. Lee
AU  - H. Park
AU  - G. Serhat
AU  - H. Sun
AU  - K. J. Kuchenbecker
PY  - 2020
KW  - calibration
KW  - inverse problems
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - robots
KW  - tactile sensors
KW  - tomography
KW  - soft ERT-based tactile sensor
KW  - sim-to-real transfer learning
KW  - electrical resistance tomography
KW  - finite element multiphysics model
KW  - contact pressure distributions
KW  - voltage measurements
KW  - model parameters
KW  - single-point dataset
KW  - contact force
KW  - calibration method
KW  - ERT-based tactile sensors
KW  - Fabrics
KW  - Computational modeling
KW  - Tactile sensors
KW  - Mathematical model
KW  - Electrodes
KW  - Force
KW  - Conductivity
DO  - 10.1109/ICRA40945.2020.9196732
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Tactile sensors based on electrical resistance tomography (ERT) have shown many advantages for implementing a soft and scalable whole-body robotic skin; however, calibration is challenging because pressure reconstruction is an ill-posed inverse problem. This paper introduces a method for calibrating soft ERT-based tactile sensors using sim-to-real transfer learning with a finite element multiphysics model. The model is composed of three simple models that together map contact pressure distributions to voltage measurements. We optimized the model parameters to reduce the gap between the simulation and reality. As a preliminary study, we discretized the sensing points into a 6 by 6 grid and synthesized single- and two-point contact datasets from the multiphysics model. We obtained another single-point dataset using the real sensor with the same contact location and force used in the simulation. Our new deep neural network architecture uses a de-noising network to capture the simulation-to-real gap and a reconstruction network to estimate contact force from voltage measurements. The proposed approach showed 82% hit rate for localization and 0.51 N of force estimation error performance in singlecontact tests and 78.5% hit rate for localization and 5.0 N of force estimation error in two-point contact tests. We believe this new calibration method has the possibility to improve the sensing performance of ERT-based tactile sensors.
ER  - 

TY  - CONF
TI  - Sim-to-Real Transfer for Optical Tactile Sensing
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1639
EP  - 1645
AU  - Z. Ding
AU  - N. F. Lepora
AU  - E. Johns
PY  - 2020
KW  - cameras
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - neural nets
KW  - tactile sensors
KW  - sim-to-real transfer methods
KW  - TacTip optical tactile sensor
KW  - deformable tip
KW  - soft body simulation
KW  - Unity physics engine
KW  - domain randomisation techniques
KW  - real-world data
KW  - optical tactile sensing
KW  - deep learning
KW  - reinforcement learning methods
KW  - flexible robot controllers
KW  - complex robot controllers
KW  - training data
KW  - data collection
KW  - size 1.0 mm
KW  - Robot sensing systems
KW  - Pins
KW  - Force
KW  - Strain
KW  - Data models
DO  - 10.1109/ICRA40945.2020.9197512
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Deep learning and reinforcement learning methods have been shown to enable learning of flexible and complex robot controllers. However, the reliance on large amounts of training data often requires data collection to be carried out in simulation, with a number of sim-to-real transfer methods being developed in recent years. In this paper, we study these techniques for tactile sensing using the TacTip optical tactile sensor, which consists of a deformable tip with a camera observing the positions of pins inside this tip. We designed a model for soft body simulation which was implemented using the Unity physics engine, and trained a neural network to predict the locations and angles of edges when in contact with the sensor. Using domain randomisation techniques for sim-to-real transfer, we show how this framework can be used to accurately predict edges with less than 1 mm prediction error in real-world testing, without any real-world data at all.
ER  - 

TY  - CONF
TI  - Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1646
EP  - 1652
AU  - Y. Zhu
AU  - K. Lu
AU  - K. Hauser
PY  - 2020
KW  - elastic deformation
KW  - image representation
KW  - robot vision
KW  - elastically deformable objects
KW  - data-driven models
KW  - point-based surface representation
KW  - inhomogeneous force response model
KW  - nonlinear force response model
KW  - robotic arm
KW  - arbitrary rigid object
KW  - Hertzian contact model
KW  - heterogeneous elastic objects
KW  - semiempirical method
KW  - point stiffness models
KW  - Force
KW  - Probes
KW  - Deformable models
KW  - Data models
KW  - Robot sensing systems
KW  - Strain
DO  - 10.1109/ICRA40945.2020.9197077
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a semi-empirical method for simulating contact with elastically deformable objects whose force response is learned using entirely data-driven models. A point-based surface representation and an inhomogeneous, nonlinear force response model are learned from a robotic arm acquiring force-displacement curves from a small number of poking interactions. The simulator then estimates displacement and force response when the deformable object is in contact with an arbitrary rigid object. It does so by estimating displacements by solving a Hertzian contact model, and sums the expected forces at individual surface points through querying the learned point stiffness models as a function of their expected displacements. Experiments on a variety of challenging objects show that our approach learns force response with sufficient accuracy to generate plausible contact response for novel rigid objects.
ER  - 

TY  - CONF
TI  - Low-Cost Fiducial-based 6-Axis Force-Torque Sensor
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1653
EP  - 1659
AU  - R. Ouyang
AU  - R. Howe
PY  - 2020
KW  - force sensors
KW  - strain gauges
KW  - 6-axis force-torque sensor
KW  - six-axis force-torque sensors
KW  - hard-touse
KW  - fiducial-based design
KW  - inexpensive webcam
KW  - consumer-grade 3D printer
KW  - open-source software
KW  - applied force-torque
KW  - browser-based interface
KW  - open source design files
KW  - human-computer interfaces
KW  - six-axis force-torque sensing
KW  - traditional strain-gauge based sensors
KW  - open-source sensor design
KW  - Robot sensing systems
KW  - Force
KW  - Three-dimensional displays
KW  - Webcams
KW  - Sensitivity
KW  - Prototypes
DO  - 10.1109/ICRA40945.2020.9196925
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Commercial six-axis force-torque sensors suffer from being some combination of expensive, fragile, and hard-touse. We propose a new fiducial-based design which addresses all three points. The sensor uses an inexpensive webcam and can be fabricated using a consumer-grade 3D printer. Open-source software is used to estimate the 3D pose of the fiducials on the sensor, which is then used to calculate the applied force-torque. A browser-based (installation free) interface demonstrates ease-of-use. The sensor is very light and can be dropped or thrown with little concern. We characterize our prototype in dynamic conditions under compound loading, finding a mean R2 of over 0.99 for the Fx, Fy, Mx, and My axes, and over 0.87 and 0.90 for the Fz and Mz axes respectively. The open source design files allow the sensor to be adapted for diverse applications ranging from robot fingers to human-computer interfaces, while the sdesign principle allows for quick changes with minimal technical expertise. This approach promises to bring six-axis force-torque sensing to new applications where the precision, cost, and fragility of traditional strain-gauge based sensors are not appropriate. The open-source sensor design can be viewed at http://sites.google.com/view/fiducialforcesensor.
ER  - 

TY  - CONF
TI  - Reliable frame-to-frame motion estimation for vehicle-mounted surround-view camera systems
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1660
EP  - 1666
AU  - Y. Wang
AU  - K. Huang
AU  - X. Peng
AU  - H. Li
AU  - L. Kneip
PY  - 2020
KW  - cameras
KW  - distance measurement
KW  - mobile robots
KW  - motion estimation
KW  - optimisation
KW  - path planning
KW  - robot vision
KW  - frame-to-frame visual odometry
KW  - vehicle-mounted surround-view camera system
KW  - reliable frame-to-frame motion estimation
KW  - vehicle-mounted surround-view camera systems
KW  - surround-view multicamera system
KW  - autonomous driving
KW  - 3D point related optimization variables
KW  - two-view optimization scheme
KW  - nonholonomic characteristics
KW  - relative displacements
KW  - nonholonomic vehicle motion
KW  - overly simplified assumptions
KW  - single camera
KW  - existing camera
KW  - relative vehicle displacement
KW  - Conferences
KW  - Automation
KW  - Reliability
KW  - Motion estimation
KW  - Cameras
KW  - Robot vision systems
DO  - 10.1109/ICRA40945.2020.9197176
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Modern vehicles are often equipped with a surround-view multi-camera system. The current interest in autonomous driving invites the investigation of how to use such systems for a reliable estimation of relative vehicle displacement. Existing camera pose algorithms either work for a single camera, make overly simplified assumptions, are computationally expensive, or simply become degenerate under non-holonomic vehicle motion. In this paper, we introduce a new, reliable solution able to handle all kinds of relative displacements in the plane despite the possibly non-holonomic characteristics. We furthermore introduce a novel two-view optimization scheme which minimizes a geometrically relevant error without relying on 3D point related optimization variables. Our method leads to highly reliable and accurate frame-to-frame visual odometry with a full-size, vehicle-mounted surround-view camera system.
ER  - 

TY  - CONF
TI  - Enabling Topological Planning with Monocular Vision
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1667
EP  - 1673
AU  - G. J. Stein
AU  - C. Bradley
AU  - V. Preston
AU  - N. Roy
PY  - 2020
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - multi-agent systems
KW  - path planning
KW  - robot vision
KW  - sensors
KW  - SLAM (robots)
KW  - heuristic priors
KW  - intelligent planning
KW  - monocular SLAM
KW  - low texture
KW  - highly cluttered environments
KW  - robust sparse map representation
KW  - monocular vision
KW  - learned sensor
KW  - high-level structure
KW  - sparse vertices
KW  - known free space
KW  - mapping technique
KW  - subgoal planning applications
KW  - enabling topological planning
KW  - topological strategies
KW  - navigation
KW  - possible actions
KW  - Planning
KW  - Image edge detection
KW  - Navigation
KW  - Robot sensing systems
KW  - Buildings
KW  - Robustness
DO  - 10.1109/ICRA40945.2020.9197484
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Topological strategies for navigation meaningfully reduce the space of possible actions available to a robot, allowing use of heuristic priors or learning to enable computationally efficient, intelligent planning. The challenges in estimating structure with monocular SLAM in low texture or highly cluttered environments have precluded its use for topological planning in the past. We propose a robust sparse map representation that can be built with monocular vision and overcomes these shortcomings. Using a learned sensor, we estimate high-level structure of an environment from streaming images by detecting sparse "vertices" (e.g., boundaries of walls) and reasoning about the structure between them. We also estimate the known free space in our map, a necessary feature for planning through previously unknown environments. We show that our mapping technique can be used on real data and is sufficient for planning and exploration in simulated multi-agent search and learned subgoal planning applications.
ER  - 

TY  - CONF
TI  - DeepMEL: Compiling Visual Multi-Experience Localization into a Deep Neural Network
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1674
EP  - 1681
AU  - M. Gridseth
AU  - T. D. Barfoot
PY  - 2020
KW  - distance measurement
KW  - image colour analysis
KW  - mobile robots
KW  - neurocontrollers
KW  - path planning
KW  - pose estimation
KW  - robot vision
KW  - robust control
KW  - stereo image processing
KW  - outdoor driving
KW  - deep neural network
KW  - visual odometry
KW  - vision-based path following
KW  - unstructured outdoor environments
KW  - visual multiexperience localization
KW  - colour-constant imaging
KW  - multiexperience VT&R
KW  - DeepMEL
KW  - stereo visual teach and repeat
KW  - robust long-range path following
KW  - environmental conditions
KW  - pose estimates
KW  - in-the-loop path following
KW  - Pose estimation
KW  - Image edge detection
KW  - Neural networks
KW  - Robots
KW  - Lighting
KW  - Snow
DO  - 10.1109/ICRA40945.2020.9197362
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Vision-based path following allows robots to autonomously repeat manually taught paths. Stereo Visual Teach and Repeat (VT&R) [1] accomplishes accurate and robust long-range path following in unstructured outdoor environments across changing lighting, weather, and seasons by relying on colour-constant imaging [2] and multi-experience localization [3]. We leverage multi-experience VT&R together with two datasets of outdoor driving on two separate paths spanning different times of day, weather, and seasons to teach a deep neural network to predict relative pose for visual odometry (VO) and for localization with respect to a path. In this paper we run experiments exclusively on datasets to study how the network generalizes across environmental conditions. Based on the results we believe that our system achieves relative pose estimates sufficiently accurate for in-the-loop path following and that it is able to localize radically different conditions against each other directly (i.e. winter to spring and day to night), a capability that our hand-engineered system does not have.
ER  - 

TY  - CONF
TI  - SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1682
EP  - 1688
AU  - L. Xie
AU  - A. Markham
AU  - N. Trigoni
PY  - 2020
KW  - collision avoidance
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neurocontrollers
KW  - robot vision
KW  - robust control
KW  - SnapNav
KW  - mapless visual navigation
KW  - sparse directional guidance
KW  - visual reference
KW  - robotics
KW  - deep neural network
KW  - visual navigation system
KW  - two-level hierarchy
KW  - directional commands
KW  - real-time control
KW  - obstacle avoidance
KW  - autonomous navigation
KW  - learning-based visual navigation
KW  - robust control
KW  - Robots
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Training
KW  - Collision avoidance
KW  - Turning
DO  - 10.1109/ICRA40945.2020.9197523
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.
ER  - 

TY  - CONF
TI  - Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1689
EP  - 1696
AU  - A. Rosinol
AU  - M. Abate
AU  - Y. Chang
AU  - L. Carlone
PY  - 2020
KW  - C++ language
KW  - control engineering computing
KW  - graph theory
KW  - image reconstruction
KW  - image segmentation
KW  - learning (artificial intelligence)
KW  - public domain software
KW  - robot vision
KW  - SLAM (robots)
KW  - open-source C++ library
KW  - visual-inertial SLAM libraries
KW  - ORB-SLAM
KW  - VINS-Mono
KW  - semantic labeling
KW  - visual-inertial odometry module
KW  - state estimation
KW  - robust pose graph optimizer
KW  - global trajectory estimation
KW  - lightweight 3D mesher module
KW  - fast mesh reconstruction
KW  - 3D metric-semantic reconstruction module
KW  - semantically labeled images
KW  - metric-semantic SLAM
KW  - real-time metric-semantic localization and mapping
KW  - Kimera
KW  - deep learning
KW  - Three-dimensional displays
KW  - Simultaneous localization and mapping
KW  - Robustness
KW  - Semantics
KW  - Libraries
KW  - Visualization
KW  - Real-time systems
DO  - 10.1109/ICRA40945.2020.9196885
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.
ER  - 

TY  - CONF
TI  - CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1697
EP  - 1704
AU  - M. Chancán
AU  - M. Milford
PY  - 2020
KW  - decision making
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - navigation
KW  - neural nets
KW  - robot vision
KW  - high-dimensional data
KW  - decision-making problems
KW  - deep reinforcement learning
KW  - place recognition feedback
KW  - visual navigation tasks
KW  - sample-efficient navigation policy learning
KW  - CityLearn environments
KW  - visual place recognition
KW  - extreme visual appearance changes
KW  - realistic environments
KW  - navigation algorithms
KW  - bimodal image representations
KW  - compact image representations
KW  - goal destination feedback
KW  - deep learning techniques
KW  - sample complexity
KW  - Navigation
KW  - Visualization
KW  - Task analysis
KW  - Robot sensing systems
KW  - Machine learning
KW  - Training
DO  - 10.1109/ICRA40945.2020.9197336
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.
ER  - 

TY  - CONF
TI  - High Resolution Soft Tactile Interface for Physical Human-Robot Interaction
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1705
EP  - 1711
AU  - I. Huang
AU  - R. Bajcsy
PY  - 2020
KW  - cameras
KW  - control engineering computing
KW  - haptic interfaces
KW  - human-robot interaction
KW  - image processing
KW  - tactile sensors
KW  - touch (physiological)
KW  - high resolution soft tactile interface
KW  - physical human-robot interaction
KW  - tactile interactions
KW  - intuitive communication tool
KW  - fundamental method
KW  - tactile abilities
KW  - mechanical safety
KW  - sensory intelligence
KW  - human-sized geometries
KW  - soft tactile interfaces
KW  - intrinsically safe mechanical properties
KW  - nonlinear characteristics
KW  - robotic system
KW  - completely soft interface
KW  - human upper limbs
KW  - high resolution tactile sensory readings
KW  - human finger
KW  - tactile input
KW  - human forearm
KW  - safe tactile interface
KW  - Robot sensing systems
KW  - Cameras
KW  - Safety
KW  - Task analysis
KW  - Visualization
DO  - 10.1109/ICRA40945.2020.9197365
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - If robots and humans are to coexist and cooperate in society, it would be useful for robots to be able to engage in tactile interactions. Touch is an intuitive communication tool as well as a fundamental method by which we assist each other physically. Tactile abilities are challenging to engineer in robots, since both mechanical safety and sensory intelligence are imperative. Existing work reveals a trade-off between these principles- tactile interfaces that are high in resolution are not easily adapted to human-sized geometries, nor are they generally compliant enough to guarantee safety. On the other hand, soft tactile interfaces deliver intrinsically safe mechanical properties, but their non-linear characteristics render them difficult for use in timely sensing and control. We propose a robotic system that is equipped with a completely soft and therefore safe tactile interface that is large enough to interact with human upper limbs, while producing high resolution tactile sensory readings via depth camera imaging of the soft interface. We present and validate a data-driven model that maps point cloud data to contact forces, and verify its efficacy by demonstrating two real-world applications. In particular, the robot is able to react to a human finger's pokes and change its pose based on the tactile input. In addition, we also demonstrate that the robot can act as an assistive device that dynamically supports and follows a human forearm from underneath.
ER  - 

TY  - CONF
TI  - Design and Validation of a Soft Robotic Ankle-Foot Orthosis (SR-AFO) Exosuit for Inversion and Eversion Ankle Support
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1735
EP  - 1741
AU  - C. M. Thalman
AU  - H. Lee
PY  - 2020
KW  - actuators
KW  - biomedical equipment
KW  - biomedical measurement
KW  - finite element analysis
KW  - gait analysis
KW  - mechanoception
KW  - medical robotics
KW  - muscle
KW  - orthotics
KW  - patient rehabilitation
KW  - frontal plane
KW  - sagittal plane
KW  - SR-AFO exosuit
KW  - wearable ankle robot
KW  - ankle stiffness
KW  - soft robotic ankle-foot orthosis exosuit
KW  - eversion ankle support
KW  - pressure 50.0 kPa
KW  - pressure 30.0 kPa
KW  - Actuators
KW  - Fabrics
KW  - Solids
KW  - Structural beams
KW  - Load modeling
KW  - Soft robotics
KW  - Soft Robotics
KW  - Wearable Robots
KW  - Assistive Robots
KW  - Rehabilitation
DO  - 10.1109/ICRA40945.2020.9197531
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a soft robotic ankle-foot orthosis (SR-AFO) exosuit designed to provide support to the human ankle in the frontal plane without restricting natural motion in the sagittal plane. The SR-AFO exosuit incorporates inflatable fabric-based actuators with a hollow cylinder design which requires less volume than the commonly used solid cylinder design for the same deflection. The actuators were modeled and characterized using finite element analysis techniques and experimentally validated. The SR-AFO exosuit was evaluated on healthy participants in both a sitting position using a wearable ankle robot and a standing position using a dual-axis robotic platform to characterize the effect of the exosuit on the change of 2D ankle stiffness in the sagittal and frontal planes. For both sitting and standing test protocols, a trend of increasing ankle stiffness in the frontal plane was observed up to 50 kPa while stiffness in the sagittal plane remained relatively constant over pressure levels. During quiet standing, the exosuit could effectively change eversion stiffness at the ankle joint from about 20 to 70 Nm/rad at relatively low- pressure levels (<; 30 kPa). Eversion stiffness was 84.9 Nm/rad at 50 kPa, an increase of 387.5% from the original free foot stiffness.
ER  - 

TY  - CONF
TI  - Velocity Field based Active-Assistive Control for Upper Limb Rehabilitation Exoskeleton Robot
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1742
EP  - 1748
AU  - E. -Y. Chia
AU  - Y. -L. Chen
AU  - T. -C. Chien
AU  - M. -L. Chiang
AU  - L. -C. Fu
AU  - J. -S. Lai
AU  - L. Lu
PY  - 2020
KW  - biomechanics
KW  - Kalman filters
KW  - medical robotics
KW  - motion control
KW  - observers
KW  - path planning
KW  - patient rehabilitation
KW  - torque control
KW  - wearable robots
KW  - upper limb rehabilitation exoskeleton robot
KW  - time-dependent trajectories
KW  - task-based rehabilitation exercise
KW  - multijoint motion
KW  - assistive mechanism
KW  - active-assistive control system
KW  - joint-position-dependent velocity field
KW  - task motion pattern
KW  - time-independent assistance
KW  - active motions
KW  - assistive motions
KW  - rehabilitation task
KW  - single joint tasks
KW  - Kalman filter based interactive torque observer
KW  - subject active motion intention
KW  - subject torque exertion
KW  - Task analysis
KW  - Torque
KW  - Robots
KW  - Kalman filters
KW  - Observers
KW  - Control systems
KW  - Sensors
DO  - 10.1109/ICRA40945.2020.9196766
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - There are limitations of conventional active-assistive control for upper limb rehabilitation exoskeleton robot, such as 1). prior time-dependent trajectories are generally required, 2). task-based rehabilitation exercise involving multi-joint motion is hard to implement, and 3). assistive mechanism normally is so inflexible that the resulting exercise performed by the subjects becomes inefficient. In this paper, we propose a novel velocity field based active-assistive control system to address these issues. First, we design a Kalman filter based interactive torque observer to obtain subjects' active intention of motion. Next, a joint-position-dependent velocity field which can be automatically generated via the task motion pattern is proposed to provide the time-independent assistance to the subjects. We further propose a novel integration method that combines the active and assistive motions based on the performance and the involvement of subjects to guide them to perform the task more voluntarily and precisely. The experiment results show that both the execution time and the subjects' torque exertion are reduced while performing both given single joint tasks and task-oriented multi-joint tasks as compared with the related work in the literature. To sum up, the proposed system not only can efficiently retain subjects' active intention but also can assist them to accomplish the rehabilitation task more precisely.
ER  - 

TY  - CONF
TI  - Design, Development, and Control of a Tendon-actuated Exoskeleton for Wrist Rehabilitation and Training
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1749
EP  - 1754
AU  - M. Dragusanu
AU  - T. L. Baldi
AU  - Z. Iqbal
AU  - D. Prattichizzo
AU  - M. Malvezzi
PY  - 2020
KW  - actuators
KW  - biomechanics
KW  - diseases
KW  - medical robotics
KW  - neurophysiology
KW  - patient rehabilitation
KW  - patient treatment
KW  - robot-mediated therapies
KW  - robotic technologies
KW  - social contexts
KW  - motion training
KW  - device design
KW  - wrist mobility
KW  - tendon-actuated exoskeleton
KW  - wrist rehabilitation
KW  - robot rehabilitation
KW  - emerging promising topic
KW  - neuroscience
KW  - neurological diseases
KW  - rehabilitation process
KW  - Exoskeletons
KW  - Wrist
KW  - Robots
KW  - Sensors
KW  - Tracking
KW  - Magnetometers
KW  - Medical treatment
DO  - 10.1109/ICRA40945.2020.9197013
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Robot rehabilitation is an emerging and promising topic that incorporates robotics with neuroscience and rehabilitation to define new methods for supporting patients with neurological diseases. As a consequence, the rehabilitation process could increase the efficacy exploiting the potentialities of robot-mediated therapies. Nevertheless, nowadays clinical effectiveness is not enough to widely introduce robotic technologies in such social contexts. In this paper we propose a step further, presenting an innovative exoskeleton for wrist flexion/extension and adduction/abduction motion training. It is designed to be wearable and easy to control and manage. It can be used by the patient in collaboration with the therapist or autonomously. The paper introduces the main steps of device design and development and presents some tests conducted with an user with limited wrist mobility.
ER  - 

TY  - CONF
TI  - Impedance Control of a Transfemoral Prosthesis using Continuously Varying Ankle Impedances and Multiple Equilibria
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1755
EP  - 1761
AU  - N. A. Kumar
AU  - W. Hong
AU  - P. Hur
PY  - 2020
KW  - artificial limbs
KW  - gait analysis
KW  - least squares approximations
KW  - medical robotics
KW  - prosthetics
KW  - springs (mechanical)
KW  - vibration control
KW  - squares estimation
KW  - impedance controller
KW  - squares optimization method
KW  - knee impedance
KW  - impedance control
KW  - lower limb prostheses
KW  - human joint torque
KW  - perturbation studies
KW  - least squares estimates
KW  - ankle impedance parameters
KW  - powered transfemoral prosthesis
KW  - Impedance
KW  - Damping
KW  - Torque
KW  - Optimization
KW  - Knee
KW  - Prosthetics
KW  - Perturbation methods
DO  - 10.1109/ICRA40945.2020.9197565
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Impedance controllers are popularly used in the field of lower limb prostheses and exoskeleton development. Such controllers assume the joint to be a spring-damper system described by a discrete set of equilibria and impedance parameters. These parameters are estimated via a least squares optimization that minimizes the difference between the controller's output torque and human joint torque. Other researchers have used perturbation studies to determine empirical values for ankle impedance. The resulting values vary greatly from the prior least squares estimates. While perturbation studies are more credible, they require immense investment. This paper extended the least squares approach to reproduce the results of perturbation studies. The resulting ankle impedance parameters were successfully tested on a powered transfemoral prosthesis, AMPRO II. Further, the paper investigated the effect of multiple equilibria on the least squares estimation and the performance of the impedance controller. Finally, the paper uses the proposed least squares optimization method to estimate knee impedance.
ER  - 

TY  - CONF
TI  - Gait patterns generation based on basis functions interpolation for the TWIN lower-limb exoskeleton*
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1778
EP  - 1784
AU  - C. Vassallo
AU  - S. De Giuseppe
AU  - C. Piezzo
AU  - S. Maludrottu
AU  - G. Cerruti
AU  - M. L. D’Angelo
AU  - E. Gruppioni
AU  - C. Marchese
AU  - S. Castellano
AU  - E. Guanziroli
AU  - F. Molteni
AU  - M. Laffranchi
AU  - L. De Michieli
PY  - 2020
KW  - gait analysis
KW  - injuries
KW  - interpolation
KW  - legged locomotion
KW  - medical robotics
KW  - neurophysiology
KW  - orthotics
KW  - patient rehabilitation
KW  - gait trajectory pattern
KW  - TWIN exoskeleton
KW  - spinal-cord injury patient
KW  - spinal cord injuries
KW  - rehabilitation centers
KW  - basis function interpolation method
KW  - stable trajectory pattern
KW  - feasible trajectory pattern
KW  - exoskeletons
KW  - biomedical orthotic devices
KW  - TWIN lower-limb exoskeleton
KW  - basis function interpolation
KW  - gait pattern generation
KW  - Trajectory
KW  - Legged locomotion
KW  - Exoskeletons
KW  - Torso
KW  - Hip
KW  - Foot
KW  - Knee
DO  - 10.1109/ICRA40945.2020.9197250
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Since the uprising of new biomedical orthotic devices, exoskeletons have been put in the spotlight for their possible use in rehabilitation. Even if these products might share some commonalities among them in terms of overall structure, degrees of freedom and possible actions, they quite often differ in their approach on how to generate a feasible, stable and comfortable gait trajectory pattern. This paper introduces three proposed trajectories that were generated by using a basis function interpolation method and by working closely with two major rehabilitation centers in Italy. The whole procedure has been focused on the concepts of a configurable walk for patients that suffer from spinal cord injuries. We tested the solutions on a group of healthy volunteers and on a spinal-cord injury patient with the use of the new TWIN exoskeleton developed at the Rehab Technologies Lab at the Italian Institute of Technology.
ER  - 

TY  - CONF
TI  - Human-Centric Active Perception for Autonomous Observation
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1785
EP  - 1791
AU  - D. Kent
AU  - S. Chernova
PY  - 2020
KW  - aerospace robotics
KW  - Markov processes
KW  - mobile communication
KW  - mobile robots
KW  - optimisation
KW  - space vehicles
KW  - autonomous observation systems
KW  - human activity
KW  - multiobjective optimization
KW  - autonomous human observation problem
KW  - robot-centric costs
KW  - scalarization-based MultiObjective MDP methods
KW  - NASA Astrobee robot operating
KW  - human-centric active perception
KW  - robot autonomy
KW  - SemiMDP formulation
KW  - constrained MDP method
KW  - NASA Astrobee robot
KW  - Task analysis
KW  - Cameras
KW  - Robot vision systems
KW  - Collision avoidance
KW  - Cost function
DO  - 10.1109/ICRA40945.2020.9197201
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - As robot autonomy improves, robots are increasingly being considered in the role of autonomous observation systems - free-flying cameras capable of actively tracking human activity within some predefined area of interest. In this work, we formulate the autonomous observation problem through multi-objective optimization, presenting a novel Semi-MDP formulation of the autonomous human observation problem that maximizes observation rewards while accounting for both human- and robot-centric costs. We demonstrate that the problem can be solved with both scalarization-based Multi-Objective MDP methods and Constrained MDP methods, and discuss the relative benefits of each approach. We validate our work on activity tracking using a NASA Astrobee robot operating within a simulated International Space Station environment.
ER  - 

TY  - CONF
TI  - Prediction of Human Full-Body Movements with Motion Optimization and Recurrent Neural Networks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1792
EP  - 1798
AU  - P. Kratzer
AU  - M. Toussaint
AU  - J. Mainprice
PY  - 2020
KW  - gradient methods
KW  - human-robot interaction
KW  - image motion analysis
KW  - mobile robots
KW  - optimisation
KW  - path planning
KW  - recurrent neural nets
KW  - robot vision
KW  - robot trajectory planning
KW  - gradient-based trajectory optimization
KW  - human full-body movement prediction
KW  - motion prediction
KW  - trajectory optimization
KW  - environmental constraints
KW  - short-term dynamics
KW  - long-term prediction
KW  - internal body dynamics
KW  - short-term prediction
KW  - recurrent neural network
KW  - motion optimization
KW  - Optimization
KW  - Trajectory
KW  - Predictive models
KW  - Recurrent neural networks
KW  - Collision avoidance
KW  - Robot kinematics
DO  - 10.1109/ICRA40945.2020.9197290
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Human movement prediction is difficult as humans naturally exhibit complex behaviors that can change drastically from one environment to the next. In order to alleviate this issue, we propose a prediction framework that decouples short-term prediction, linked to internal body dynamics, and long-term prediction, linked to the environment and task constraints. In this work we investigate encoding short-term dynamics in a recurrent neural network, while we account for environmental constraints, such as obstacle avoidance, using gradient-based trajectory optimization. Experiments on real motion data demonstrate that our framework improves the prediction with respect to state-of-the-art motion prediction methods, as it accounts to beforehand unseen environmental structures. Moreover we demonstrate on an example, how this framework can be used to plan robot trajectories that are optimized to coordinate with a human partner.
ER  - 

TY  - CONF
TI  - Predicting and Optimizing Ergonomics in Physical Human-Robot Cooperation Tasks
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1799
EP  - 1805
AU  - L. v. der Spaa
AU  - M. Gienger
AU  - T. Bates
AU  - J. Kober
PY  - 2020
KW  - biomechanics
KW  - ergonomics
KW  - graph theory
KW  - human-robot interaction
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - telerobotics
KW  - 32 DoF bimanual mobile robot
KW  - ergonomic-enhanced planner
KW  - reduced ergonomic cost
KW  - physical human-robot cooperation tasks
KW  - action sequences
KW  - continuous physical interaction
KW  - computational model
KW  - ergonomics assessment
KW  - human motion capture data
KW  - prediction model
KW  - informed graph search algorithm
KW  - ergonomic assessment
KW  - bimanual human-robot cooperation tasks
KW  - Ergonomics
KW  - Robots
KW  - Task analysis
KW  - Optimization
KW  - Predictive models
KW  - Computational modeling
KW  - Force
DO  - 10.1109/ICRA40945.2020.9197296
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - This paper presents a method to incorporate ergonomics into the optimization of action sequences for bi-manual human-robot cooperation tasks with continuous physical interaction. Our first contribution is a novel computational model of the human that allows prediction of an ergonomics assessment corresponding to each step in a task. The model is learned from human motion capture data in order to predict the human pose as realistically as possible. The second contribution is a combination of this prediction model with an informed graph search algorithm, which allows computation of human-robot cooperative plans with improved ergonomics according to the incorporated method for ergonomic assessment. The concepts have been evaluated in simulation and in a small user study in which the subjects manipulate a large object with a 32 DoF bimanual mobile robot as partner. For all subjects, the ergonomic-enhanced planner shows their reduced ergonomic cost compared to a baseline planner.
ER  - 

TY  - CONF
TI  - Active Reward Learning for Co-Robotic Vision Based Exploration in Bandwidth Limited Environments
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1806
EP  - 1812
AU  - S. Jamieson
AU  - J. P. How
AU  - Y. Girdhar
PY  - 2020
KW  - decision theory
KW  - learning (artificial intelligence)
KW  - Markov processes
KW  - mobile robots
KW  - query processing
KW  - robot vision
KW  - making queries
KW  - regret-based criterion
KW  - active reward learning strategy
KW  - co-robotic vision based exploration
KW  - robotic explorer
KW  - bandwidth-limited environments
KW  - autonomous visual exploration
KW  - high-dimensional observation space
KW  - communication strategy
KW  - reward model
KW  - observation model
KW  - human operator
KW  - scientifically relevant images
KW  - POMDP problem formulation
KW  - Robot sensing systems
KW  - Semantics
KW  - Bandwidth
KW  - Computational modeling
KW  - Visualization
KW  - Trajectory
DO  - 10.1109/ICRA40945.2020.9196922
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - We present a novel POMDP problem formulation for a robot that must autonomously decide where to go to collect new and scientifically relevant images given a limited ability to communicate with its human operator. From this formulation we derive constraints and design principles for the observation model, reward model, and communication strategy of such a robot, exploring techniques to deal with the very high-dimensional observation space and scarcity of relevant training data. We introduce a novel active reward learning strategy based on making queries to help the robot minimize path "regret" online, and evaluate it for suitability in autonomous visual exploration through simulations. We demonstrate that, in some bandwidth-limited environments, this novel regret-based criterion enables the robotic explorer to collect up to 17% more reward per mission than the next-best criterion.
ER  - 

TY  - CONF
TI  - VariPath: A Database for Modelling the Variance of Human Pathways in Manual and HRC Processes with Heavy-Duty Robots
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1821
EP  - 1826
AU  - M. Bdiwi
AU  - A. -K. Harsch
AU  - P. Reindel
AU  - M. Putz
PY  - 2020
KW  - human-robot interaction
KW  - industrial robots
KW  - occupational safety
KW  - path planning
KW  - heavy-duty robots
KW  - human pathway variations
KW  - human-robot collaboration
KW  - HRC process
KW  - women walking pathways
KW  - varipath database
KW  - planning process
KW  - safety
KW  - Legged locomotion
KW  - Service robots
KW  - Task analysis
KW  - Planning
KW  - Atmospheric measurements
KW  - Particle measurements
DO  - 10.1109/ICRA40945.2020.9196699
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - Unlike robots, humans do not have constant movements. Their pathways are individually changeable and influenced by circumstances. This paper presents a method to investigate human pathway variations in a real study. In systematically selected tasks, human pathways are examined for 100 participants in manual and human-robot collaboration (HRC) scenarios. As a result, the variations of pathways are presented depending on various features: e.g. in nearly all cases the variance of women's walking pathways is smaller than that of men. VariPath database can be used in any planning process of manual or HRC scenarios to ensure safety and efficiency.
ER  - 

TY  - CONF
TI  - A Compact and Low-cost Robotic Manipulator Driven by Supercoiled Polymer Actuators
T2  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
SP  - 1827
EP  - 18533
AU  - Y. Yang
AU  - Z. Liu
AU  - Y. Wang
AU  - S. Liu
AU  - M. Y. Wang
PY  - 2020
KW  - coils
KW  - design engineering
KW  - dexterous manipulators
KW  - electroactive polymer actuators
KW  - grippers
KW  - mobile robots
KW  - pneumatic actuators
KW  - fragile fruit
KW  - pick and place demonstration
KW  - Fin Ray Effect inspired soft gripper
KW  - ball-and-socket joints
KW  - Joule heating
KW  - electrical activation
KW  - bio-inspired robotic manipulator design
KW  - bio-inspired arm
KW  - manipulator prototype
KW  - SCP actuators
KW  - robotic arm
KW  - muscle-like form
KW  - twisting coiling polymer fibers
KW  - artificial muscle
KW  - supercoiled polymer actuator
KW  - low-cost robotic manipulator
KW  - Actuators
KW  - Manipulators
KW  - Muscles
KW  - Grippers
KW  - Polymers
DO  - 10.1109/ICRA40945.2020.9197390
JO  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
IS  - 
SN  - 2577-087X
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Robotics and Automation (ICRA)
Y1  - 31 May-31 Aug. 2020
AB  - The supercoiled polymer (SCP) actuator is a novel artificial muscle, which is manufactured by twisting and coiling polymer fibers. This new artificial muscle is soft, low-cost and shows good linearity. Being utilized as an actuator, the artificial muscle could generate significant mechanical power in a muscle-like form upon electrical activation by Joule heating. In this study, we adopt this new artificial muscle to actuate a novel designed robotic manipulator, which is composed of two parts. The first part is a robotic arm based on the inspiration of the musculoskeletal system. The arm is fabricated with two ball-and-socket joints as skeleton and SCP actuators as driven muscles. The second part is a Fin Ray Effect inspired soft gripper that can perform grasping tasks on fragile objects. The manipulator prototype is fabricated and experimental tests are conducted including both simple but effective control of the bio-inspired arm as well as characterization of the gripper. Lastly, a pick and place demonstration of a fragile fruit is performed utilizing the proposed manipulator. We envision that the bio-inspired robotic manipulator design driven by SCP actuators could potentially be used in other robotic applications.
ER  - 


