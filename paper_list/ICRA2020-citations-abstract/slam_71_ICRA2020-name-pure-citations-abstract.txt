total paper: 71
Title: Metrically-Scaled Monocular SLAM using Learned Scale Factors
Key Words: cameras  graph theory  mobile robots  neural nets  robot vision  SLAM (robots)  geometric SLAM factor graph  SLAM systems  relative geometry  learned depth estimation approaches  learned depth predictions  image space  network architecture  coarse images  GPU acceleration  learned metric data  unary scale factors  hardware accelerators  observable epipolar geometry  monocular SLAM  learned scale factors  monocular simultaneous localization and mapping  hardware acceleration  neural network  Simultaneous localization and mapping  Feature extraction  Cameras  Loss measurement  Neural networks  Estimation 
Abstract: We propose an efficient method for monocular simultaneous localization and mapping (SLAM) that is capable of estimating metrically-scaled motion without additional sensors or hardware acceleration by integrating metric depth predictions from a neural network into a geometric SLAM factor graph. Unlike learned end-to-end SLAM systems, ours does not ignore the relative geometry directly observable in the images. Unlike existing learned depth estimation approaches, ours leverages the insight that when used to estimate scale, learned depth predictions need only be coarse in image space. This allows us to shrink our network to the point that performing inference on a standard CPU becomes computationally tractable.We make several improvements to our network architecture and training procedure to address the lack of depth observability when using coarse images, which allows us to estimate spatially coarse, but depth-accurate predictions in only 30 ms per frame without GPU acceleration. At runtime we incorporate the learned metric data as unary scale factors in a Sim(3) pose graph. Our method is able to generate accurate, scaled poses without additional sensors, hardware accelerators, or special maneuvers and does not ignore or corrupt the observable epipolar geometry. We show compelling results on the KITTI benchmark dataset in addition to real-world experiments with a handheld camera.


Title: Inertial-Only Optimization for Visual-Inertial Initialization
Key Words: feature extraction  least squares approximations  maximum likelihood estimation  optimisation  SLAM (robots)  EuRoC dataset show  time visual-inertial initialization  optimal estimation problem  maximum-a-posteriori estimation  algebraic equations  ad-hoc cost functions  ORB-SLAM visual-inertial boosting  inertial-only optimization  IMU measurement uncertainty  MAP estimation  least squares  Estimation  Trajectory  Simultaneous localization and mapping  Gravity  Visualization  Optimization  Accelerometers 
Abstract: We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.


Title: Hierarchical Quadtree Feature Optical Flow Tracking Based Sparse Pose-Graph Visual-Inertial SLAM
Key Words: computational complexity  graph theory  image sequences  optimisation  pose estimation  quadtrees  pose-graph optimization time cost  localization accuracy  sparse pose-graph visual-inertial SLAM algorithms  hierarchical quadtree feature optical flow tracking algorithm  SPVIS  high-precision pose estimation  computational complexity  VIO-VI-SLAM system  GPU  Optical flow  Optimization  Simultaneous localization and mapping  Robustness  Tracking  Feature extraction  Visualization 
Abstract: Accurate, robust and real-time localization under constrained-resources is a critical problem to be solved. In this paper, we present a new sparse pose-graph visual-inertial SLAM (SPVIS). Unlike the existing methods that are costly to deal with a large number of redundant features and 3D map points, which are inefficient for improving positioning accuracy, we focus on the concise visual cues for high-precision pose estimating. We propose a novel hierarchical quadtree based optical flow tracking algorithm, it achieves high accuracy and robustness within very few concise features, which is only about one fifth features of the state-of-the-art visual-inertial SLAM algorithms. Benefiting from the efficient optical flow tracking, our sparse pose-graph optimization time cost achieves bounded complexity. By selecting and optimizing the informative features in sliding window and local VIO, the computational complexity is bounded, it achieves low time cost in long-term operation. We compare with the state-of-the-art VIO/VI-SLAM systems on the challenging public datasets by the embedded platform without GPUs, the results effectively verify that the proposed method has better real-time performance and localization accuracy.


Title: Keypoint Description by Descriptor Fusion Using Autoencoders
Key Words: convolutional neural nets  image fusion  image matching  learning (artificial intelligence)  robot vision  SLAM (robots)  keypoint description  keypoint matching  computer vision  visual simultaneous localization and mapping  SLAM  matching operation  descriptor fusion model  robust keypoint descriptor  CNN-based descriptors  DFM architecture  CNN models  mean mAP  HardNet  DenseNet169  convolutional neural networks  Fuses  Lighting  Robustness  Computer vision  Simultaneous localization and mapping  Image coding 
Abstract: Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.


Title: Towards Noise Resilient SLAM
Key Words: cameras  image colour analysis  image sensors  optimisation  photometry  pose estimation  SLAM (robots)  stereo image processing  RGB-D input  TUM datasets  EuRoC datasets  stereo image pairs  adaptive algorithm  error vector  outlier rejection  computational efficiency  map-point consensus  adaptive virtual camera  noise resilient SLAM  ORB-SLAM2  sparse-indirect SLAM systems  virtual camera location  axial depth error  pose optimization  consensus information  axial noise  lateral noise  depth noise components  axial components  lateral components  noise sources  scale information  SLAM frameworks  depth sensors  photometric invariance properties  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Measurement  Feature extraction  Optimization 
Abstract: Sparse-indirect SLAM systems have been dominantly popular due to their computational efficiency and photometric invariance properties. Depth sensors are critical to SLAM frameworks for providing scale information to the 3D world, yet known to be plagued by a wide variety of noise sources, possessing lateral and axial components. In this work, we demonstrate the detrimental impact of these depth noise components on the performance of the state-of-the-art sparse-indirect SLAM system (ORB-SLAM2). We propose (i) Map-Point Consensus based Outlier Rejection (MC-OR) to counter lateral noise, and (ii) Adaptive Virtual Camera (AVC) to combat axial noise accurately. MC-OR utilizes consensus information between multiple sightings of the same landmark to disambiguate noisy depth and filter it out before pose optimization. In AVC, we introduce an error vector as an accurate representation of the axial depth error. We additionally propose an adaptive algorithm to find the virtual camera location for projecting the error used in the objective function of the pose optimization. Our techniques work equally well for stereo image pairs and RGB-D input directly used by sparse-indirect SLAM systems. Our methods were tested on the TUM (RGB-D) and EuRoC (stereo) datasets and we show that they outperform existing state-of-the-art ORB-SLAM2 by 2-3x, especially in sequences critically affected by depth noise.


Title: LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments
Key Words: distance measurement  geophysical image processing  mobile robots  multi-robot systems  optical radar  robot vision  SLAM (robots)  stereo image processing  terrain mapping  tunnels  long corridors  salient features  spurious loop closures  repetitive appearance  stark contrast  highly-accurate 3D maps  underground extraterrestrial worlds  lidar-based multirobot SLAM system  DARPA subterranean challenge  subterranean operation  accurate lidar-based front-end  perceptually-degraded subterranean environments  complex subterranean environments  off-nominal conditions  uneven terrains  slippery terrains  large-scale autonomous mapping-positioning  simultaneous localization and mapping  unknown subterranean environment  large-scale subterranean environment  complex subterranean environment  inaccurate wheel odometry  disaster response  flexible back-end  robust back-end  tunnel circuit  Simultaneous localization and mapping  Laser radar  Three-dimensional displays  Base stations  Trajectory 
Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, unknown, and complex subterranean environments is a challenging problem. Sensors must operate in off-nominal conditions; uneven and slippery terrains make wheel odometry inaccurate, while long corridors without salient features make exteroceptive sensing ambiguous and prone to drift; finally, spurious loop closures that are frequent in environments with repetitive appearance, such as tunnels and mines, could result in a significant distortion of the entire map. These challenges are in stark contrast with the need to build highly-accurate 3D maps to support a wide variety of applications, ranging from disaster response to the exploration of underground extraterrestrial worlds. This paper reports on the implementation and testing of a lidar-based multi-robot SLAM system developed in the context of the DARPA Subterranean Challenge. We present a system architecture to enhance subterranean operation, including an accurate lidar-based front-end, and a flexible and robust back-end that automatically rejects outlying loop closures. We present an extensive evaluation in large-scale, challenging subterranean environments, including the results obtained in the Tunnel Circuit of the DARPA Subterranean Challenge. Finally, we discuss potential improvements, limitations of the state of the art, and future research directions.


Title: Keyfilter-Aware Real-Time UAV Object Tracking
Key Words: autonomous aerial vehicles  image filtering  image motion analysis  learning (artificial intelligence)  mobile robots  object detection  object tracking  robot vision  SLAM (robots)  keyframe-based simultaneous localization and mapping  keyfilter restriction  visual tracking  background distraction  filter corruption  boundary effect  unmanned aerial vehicle  correlation filter-based tracking  keyfilter-aware real-time UAV object tracking  Unmanned aerial vehicles  Correlation  Visualization  Object tracking  Frequency-domain analysis  Real-time systems 
Abstract: Correlation filter-based tracking has been widely applied in unmanned aerial vehicle (UAV) with high efficiency. However, it has two imperfections, i.e., boundary effect and filter corruption. Several methods enlarging the search area can mitigate boundary effect, yet introducing undesired background distraction. Existing frame-by-frame context learning strategies for repressing background distraction nevertheless lower the tracking speed. Inspired by keyframe-based simultaneous localization and mapping, keyfilter is proposed in visual tracking for the first time, in order to handle the above issues efficiently and effectively. Keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters, so that 1) context awareness can be transmitted to all the filters via keyfilter restriction, and 2) filter corruption can be repressed. Compared to the state-of-the-art results, our tracker performs better on two challenging benchmarks, with enough speed for UAV real-time applications.


Title: Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching
Key Words: cameras  distance measurement  gradient methods  image matching  image registration  image sensors  pose estimation  robot vision  SLAM (robots)  stereo image processing  visual SLAM systems  photometric consistency  gradient-based dissimilarity  camera pose estimation  map building  central ingredients  autonomous robots  photometric error  gradient orientation  magnitude-dependent scaling term  stereo estimation  visual odometry systems  direct image registration tasks  robust estimates  scene depth  camera trajectory  mapping capabilities  mobile robots  sensor data registration  Measurement  Robustness  Cameras  Estimation  Visual odometry  Simultaneous localization and mapping  Robot vision systems 
Abstract: Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicate that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.


Title: ICS: Incremental Constrained Smoothing for State Estimation
Key Words: matrix decomposition  mobile robots  optimisation  path planning  robot vision  SLAM (robots)  state estimation  ICS  primal-dual method  matrix factorizations  primal-dual methods  incremental factorization  matrix structure  incremental unconstrained optimization  robot state estimate  smoothing-based estimation methods  state estimation  incremental constrained smoothing  Optimization  Smoothing methods  Time measurement  Integrated circuits  Simultaneous localization and mapping 
Abstract: A robot operating in the world constantly receives information about its environment in the form of new measurements at every time step. Smoothing-based estimation methods seek to optimize for the most likely robot state estimate using all measurements up till the current time step. Existing methods solve for this smoothing objective efficiently by framing the problem as that of incremental unconstrained optimization. However, in many cases observed measurements and knowledge of the environment is better modeled as hard constraints derived from real-world physics or dynamics. A key challenge is that the new optimality conditions introduced by the hard constraints break the matrix structure needed for incremental factorization in these incremental optimization methods. Our key insight is that if we leverage primal-dual methods, we can recover a matrix structure amenable to incremental factorization. We propose a framework ICS that combines a primal-dual method like the Augmented Lagrangian with an incremental Gauss Newton approach that reuses previously computed matrix factorizations. We evaluate ICS on a set of simulated and real-world problems involving equality constraints like object contact and inequality constraints like collision avoidance.


Title: OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems
Key Words: cameras  computerised instrumentation  distance measurement  image matching  image reconstruction  image sequences  neural nets  stereo image processing  omnidirectional localization  wide-baseline multicamera systems  dense mapping system  wide-baseline multiview stereo setup  ultrawide field-of-view fisheye cameras  stereo observations  light-weighted deep neural networks  loop closing module  efficient feature matching process  omnidirectional depth maps  truncated signed distance function volume  rig estimation  omnidirectional depth map estimation  VO  FOV  TSDF  OmniSLAM  Cameras  Three-dimensional displays  Estimation  Feature extraction  Visual odometry  Sensors  Trajectory 
Abstract: In this paper, we present an omnidirectional localization and dense mapping system for a wide-baseline multiview stereo setup with ultra-wide field-of-view (FOV) fisheye cameras, which has a 360° coverage of stereo observations of the environment. For more practical and accurate reconstruction, we first introduce improved and light-weighted deep neural networks for the omnidirectional depth estimation, which are faster and more accurate than the existing networks. Second, we integrate our omnidirectional depth estimates into the visual odometry (VO) and add a loop closing module for global consistency. Using the estimated depth map, we reproject keypoints onto each other view, which leads to a better and more efficient feature matching process. Finally, we fuse the omnidirectional depth maps and the estimated rig poses into the truncated signed distance function (TSDF) volume to acquire a 3D map. We evaluate our method on synthetic datasets with ground-truth and real-world sequences of challenging environments, and the extensive experiments show that the proposed system generates excellent reconstruction results in both synthetic and real-world environments.


Title: Bidirectional Resonant Propulsion and Localization for AUVs
Key Words: autonomous underwater vehicles  diaphragms  electromagnetic actuators  marine control  mobile robots  motion control  robot vision  SLAM (robots)  electromagnetic voice coil motor  bidirectional resonant propulsion  AUV localization  thrust vectors  diaphragm pump mechanism  resonant motion  actuator design  bidirectional resonant pump  autonomous underwater vehicles  Propulsion  Resonant frequency  Strain  Standards  Damping  Reliability engineering 
Abstract: Battery life, reliability, and localization are prominent challenges in the design of autonomous underwater vehicles (AUVs). This work aims to address facets of these challenges using a single system. We describe the design of a bidirectional resonant pump that uses a single electromagnetic voice coil motor (VCM) capable of rotation around a central two degree-of-freedom flexure stage axis. This actuator design produces highly efficient resonant motion that drives two orthogonally oriented diaphragms simultaneously. The operation of this diaphragm pump mechanism produces both adjustable thrust vectors at the aft surface of the AUV and a monotonic relationship between thrust vectors and operating frequency. We propose using the unique frequency to thrust relationship to enhance AUV localization capabilities. We construct a prototype and use it to experimentally demonstrate the feasibility of the directionally-tunable resonance concept.


Title: Test Your SLAM! The SubT-Tunnel dataset and metric for mapping
Key Words: mobile robots  public domain software  robot vision  SLAM (robots)  SLAM  open source tools  robotic mapping algorithms  DARPA Subterranean challenge  SubT-Tunnel dataset  subterranean mine rescue dataset  Simultaneous localization and mapping  Cameras  Measurement  Laser radar  Robot vision systems 
Abstract: This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.


Title: Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships
Key Words: graph theory  image matching  mobile robots  robot vision  SLAM (robots)  robotics applications  simultaneously localization and mapping  spatial relationship similarities  spatial cues  visual cues  old landmarks  long-term environment changes  landmark information  integrate landmark appearances  worst-case graph matching  place recognition performance  long-term place recognition  worst appearance similarity  similar appearances  worst-case scenario  graph matching problem  visual appearances  angular spatial relationships  graph representation  Visualization  Simultaneous localization and mapping  Robustness  Strain  Image recognition  Tensile stress 
Abstract: Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.


Title: Linear RGB-D SLAM for Atlanta World
Key Words: cameras  image colour analysis  Kalman filters  mobile robots  object detection  object tracking  pose estimation  SLAM (robots)  Manhattan world assumption  orthogonal directions  Atlanta world  vertical direction  horizontal directions  SLAM techniques  Atlanta representation  Atlanta frame-aware linear SLAM framework  Atlanta structure  linear Kalman filter  linear RGB-D SLAM  simultaneous localization and mapping  tracking-by-detection scheme  scene structure  camera motion  planar map  synthetic datasets  real datasets  Simultaneous localization and mapping  Cameras  Three-dimensional displays  Tracking  Kalman filters  Visualization  Robustness 
Abstract: We present a new linear method for RGB-D based simultaneous localization and mapping (SLAM). Compared to existing techniques relying on the Manhattan world assumption defined by three orthogonal directions, our approach is designed for the more general scenario of the Atlanta world. It consists of a vertical direction and a set of horizontal directions orthogonal to the vertical direction and thus can represent a wider range of scenes. Our approach leverages the structural regularity of the Atlanta world to decouple the non-linearity of camera pose estimations. This allows us separately to estimate the camera rotation and then the translation, which bypasses the inherent non-linearity of traditional SLAM techniques. To this end, we introduce a novel tracking-by-detection scheme to estimate the underlying scene structure by Atlanta representation. Thereby, we propose an Atlanta frame-aware linear SLAM framework which jointly estimates the camera motion and a planar map supporting the Atlanta structure through a linear Kalman filter. Evaluations on both synthetic and real datasets demonstrate that our approach provides favorable performance compared to existing state-of-the-art methods while extending their working range to the Atlanta world.


Title: Probabilistic Data Association via Mixture Models for Robust Semantic SLAM
Key Words: Gaussian processes  image sensors  mobile robots  object detection  probability  robot vision  SLAM (robots)  target tracking  probabilistic data association  mixture models  robust semantic SLAM  robotic systems  cameras  lidar  visual models  reliable navigation  semantic uncertainty inherent  geometric uncertainty inherent  object detection methods  data association ambiguity  nonlinear Gaussian formulation  data association variables  max-marginalization  standard Gaussian posterior assumptions  max-mixture-type model  multiple data association hypotheses  indoor navigation tasks  outdoor semantic navigation tasks  semantic SLAM approaches  simultaneous localization and mapping  noisy odometry  Semantics  Simultaneous localization and mapping  Robustness  Optimization  Object detection  Uncertainty 
Abstract: Modern robotic systems sense the environment geometrically, through sensors like cameras, lidar, and sonar, as well as semantically, often through visual models learned from data, such as object detectors. We aim to develop robots that can use all of these sources of information for reliable navigation, but each is corrupted by noise. Rather than assume that object detection will eventually achieve near perfect performance across the lifetime of a robot, in this work we represent and cope with the semantic and geometric uncertainty inherent in object detection methods. Specifically, we model data association ambiguity, which is typically non-Gaussian, in a way that is amenable to solution within the common nonlinear Gaussian formulation of simultaneous localization and mapping (SLAM). We do so by eliminating data association variables from the inference process through max-marginalization, preserving standard Gaussian posterior assumptions. The result is a max-mixture-type model that accounts for multiple data association hypotheses. We provide experimental results on indoor and outdoor semantic navigation tasks with noisy odometry and object detection and find that the ability of the proposed approach to represent multiple hypotheses, including the "null" hypothesis, gives substantial robustness advantages in comparison to alternative semantic SLAM approaches.


Title: Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy
Key Words: closed loop systems  Global Positioning System  mobile robots  navigation  robot vision  SLAM (robots)  stereo image processing  tracking  representative state-of-the-art visual-inertial SLAM systems  visual estimation module  stereo visual-inertial SLAM systems  open-loop analysis  closed-loop navigation tasks  accurate trajectory tracking  visualinertial SLAM systems  closed-loop benchmarking simulation  visual-inertial estimation  trajectory tracking performance  Visualization  Navigation  Simultaneous localization and mapping  Benchmark testing  Estimation 
Abstract: Visual-inertial SLAM is essential for robot navigation in GPS-denied environments, e.g. indoor, underground. Conventionally, the performance of visual-inertial SLAM is evaluated with open-loop analysis, with a focus on the drift level of SLAM systems. In this paper, we raise the question on the importance of visual estimation latency in closed-loop navigation tasks, such as accurate trajectory tracking. To understand the impact of both drift and latency on visualinertial SLAM systems, a closed-loop benchmarking simulation is conducted, where a robot is commanded to follow a desired trajectory using the feedback from visual-inertial estimation. By extensively evaluating the trajectory tracking performance of representative state-of-the-art visual-inertial SLAM systems, we reveal the importance of latency reduction in visual estimation module of these systems. The findings suggest directions of future improvements for visual-inertial SLAM.


Title: Learning error models for graph SLAM
Key Words: autonomous aerial vehicles  graph theory  mobile robots  path planning  robot vision  SLAM (robots)  resistance distance  covisibility graph  simulated UAV coverage path  uncertainty models  monocular graph SLAM  topological features  error model learning  UAV coverage path planning trajectories  Simultaneous localization and mapping  Resistance  Uncertainty  Computational modeling  Computer architecture  Predictive models  Cameras 
Abstract: Following recent developments, this paper investigates the possibility to predict uncertainty models for monocular graph SLAM using topological features of the problem. An architecture to learn relative (i.e. inter-keyframe) uncertainty models using the resistance distance in the covisibility graph is presented. The proposed architecture is applied to simulated UAV coverage path planning trajectories and an analysis of the approaches strengths and shortcomings is provided.


Title: Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction
Key Words: autonomous aerial vehicles  computational geometry  image reconstruction  path planning  robot vision  SLAM (robots)  unmanned aerial vehicles  large-scale scene mapping  autonomous urban scene reconstruction  point cloud reconstruction  reconstruction quality  large-scale scene reconstruction  real-time UAV path planning  SLAM  Buildings  Image reconstruction  Three-dimensional displays  Path planning  Drones  Cameras  Layout 
Abstract: Unmanned aerial vehicles (UAVs) are frequently used for large-scale scene mapping and reconstruction. However, in most cases, drones are operated manually, which should be more effective and intelligent. In this article, we present a method of real-time UAV path planning for autonomous urban scene reconstruction. Considering the obstacles and time costs, we utilize the top view to generate the initial path. Then we estimate the building heights and take close-up pictures that reveal building details through a SLAM framework. To predict the coverage of the scene, we propose a novel method which combines information on reconstructed point clouds and possible coverage areas. The experimental results reveal that the reconstruction quality of our method is good enough. Our method is also more time-saving than the state-of-the-arts.


Title: Enabling Topological Planning with Monocular Vision
Key Words: learning (artificial intelligence)  mobile robots  multi-agent systems  path planning  robot vision  sensors  SLAM (robots)  heuristic priors  intelligent planning  monocular SLAM  low texture  highly cluttered environments  robust sparse map representation  monocular vision  learned sensor  high-level structure  sparse vertices  known free space  mapping technique  subgoal planning applications  enabling topological planning  topological strategies  navigation  possible actions  Planning  Image edge detection  Navigation  Robot sensing systems  Buildings  Robustness 
Abstract: Topological strategies for navigation meaningfully reduce the space of possible actions available to a robot, allowing use of heuristic priors or learning to enable computationally efficient, intelligent planning. The challenges in estimating structure with monocular SLAM in low texture or highly cluttered environments have precluded its use for topological planning in the past. We propose a robust sparse map representation that can be built with monocular vision and overcomes these shortcomings. Using a learned sensor, we estimate high-level structure of an environment from streaming images by detecting sparse "vertices" (e.g., boundaries of walls) and reasoning about the structure between them. We also estimate the known free space in our map, a necessary feature for planning through previously unknown environments. We show that our mapping technique can be used on real data and is sufficient for planning and exploration in simulated multi-agent search and learned subgoal planning applications.


Title: Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
Key Words: C++ language  control engineering computing  graph theory  image reconstruction  image segmentation  learning (artificial intelligence)  public domain software  robot vision  SLAM (robots)  open-source C++ library  visual-inertial SLAM libraries  ORB-SLAM  VINS-Mono  semantic labeling  visual-inertial odometry module  state estimation  robust pose graph optimizer  global trajectory estimation  lightweight 3D mesher module  fast mesh reconstruction  3D metric-semantic reconstruction module  semantically labeled images  metric-semantic SLAM  real-time metric-semantic localization and mapping  Kimera  deep learning  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Semantics  Libraries  Visualization  Real-time systems 
Abstract: We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.


Title: Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection
Key Words: computational geometry  feature extraction  image matching  mobile robots  optical radar  robot vision  SLAM (robots)  stereo image processing  intensity scan context  light detection and ranging sensor  discard intensity reading  geometric relation  intensity structure re-identification  coding intensity  simultaneous localization and mapping  LiDAR sensor  3D loop closure detection  geometrical only descriptor matching  place recognition  robot navigation  fast point feature histogram  Geometry  Laser radar  Three-dimensional displays  Histograms  Simultaneous localization and mapping  Rough surfaces  Surface roughness 
Abstract: Loop closure detection is an essential and challenging problem in simultaneous localization and mapping (SLAM). It is often tackled with light detection and ranging (LiDAR) sensor due to its view-point and illumination invariant properties. Existing works on 3D loop closure detection often leverage on matching of local or global geometrical-only descriptors which discard intensity reading. In this paper we explore the intensity property from LiDAR scan and show that it can be effective for place recognition. We propose a novel global descriptor, intensity scan context (ISC), that explores both geometry and intensity characteristics. To improve the efficiency for loop closure detection, an efficient two-stage hierarchical re-identification process is proposed, including binary-operation based fast geometric relation retrieval and intensity structure re-identification. Thorough experiments including both local experiment and public datasets test have been conducted to evaluate the performance of the proposed method. Our method achieves better recall rate and recall precision than existing geometric-only methods.


Title: TextSLAM: Visual SLAM with Planar Text Features
Key Words: augmented reality  data visualisation  navigation  robot vision  SLAM (robots)  stereo image processing  text analysis  text detection  text object integration  augmented reality  navigation  scene understanding  illumination-invariant photometric error  TextSLAM  text detection  text-based visual SLAM  3D text maps  visual SLAM pipeline  planar text features  visual SLAM system  planar feature  Simultaneous localization and mapping  Three-dimensional displays  Visualization  Feature extraction  Navigation  Cameras  Robustness 
Abstract: We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.


Title: Redesigning SLAM for Arbitrary Multi-Camera Systems
Key Words: cameras  distance measurement  robot vision  SLAM (robots)  stereo image processing  sensor-specific modifications  SLAM systems  robustness  camera configurations  adaptive SLAM system  multicamera setup  visual SLAM  adaptive initialization  scalable voxel-based map  sensor-agnostic information-theoretic keyframe selection algorithm  visual front-end design  visual-inertial odometry  Cameras  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Visualization 
Abstract: Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information- theoretic keyframe selection algorithm, and a scalable voxel- based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual- inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning.


Title: Dynamic SLAM: The Need For Speed
Key Words: feature extraction  image motion analysis  image segmentation  mobile robots  path planning  robot vision  SLAM (robots)  rigid moving objects  static structure  dynamic structure  rigid objects  object-aware dynamic SLAM algorithm  model-free  significant motion constraints  3D models  SLAM based approaches  unstructured dynamic environments  autonomous systems  increased deployment  simultaneous localisation  static world assumption  Simultaneous localization and mapping  Heuristic algorithms  Dynamics  Three-dimensional displays  Solid modeling  Tracking 
Abstract: The static world assumption is standard in most simultaneous localisation and mapping (SLAM) algorithms. Increased deployment of autonomous systems to unstructured dynamic environments is driving a need to identify moving objects and estimate their velocity in real-time. Most existing SLAM based approaches rely on a database of 3D models of objects or impose significant motion constraints. In this paper, we propose a new feature-based, model-free, object-aware dynamic SLAM algorithm that exploits semantic segmentation to allow estimation of motion of rigid objects in a scene without the need to estimate the object poses or have any prior knowledge of their 3D models. The algorithm generates a map of dynamic and static structure and has the ability to extract velocities of rigid moving objects in the scene. Its performance is demonstrated on simulated, synthetic and real-world datasets.


Title: ∇SLAM: Dense SLAM meets Automatic Differentiation
Key Words: gradient methods  graph theory  learning (artificial intelligence)  robot vision  SLAM (robots)  automatic differentiation  dense simultaneous localization  learning-based approaches  representation learning approaches  classical SLAM systems  differentiable function  optimize task performance  typical dense SLAM system  ∇SLAM  posing SLAM systems  differentiable computational graphs  differentiable trust-region optimizers  task-based error signals  Simultaneous localization and mapping  Optimization  Three-dimensional displays  Damping  Task analysis  Neural networks 
Abstract: The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Learning-based approaches have the potential to leverage data or task performance to directly inform the representation. However, blending representation learning approaches with "classical" SLAM systems has remained an open question, because of their highly modular and complex nature. A SLAM system transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals over the outputs of this function to learn representations that optimize task performance. However, this is infeasible as several components of a typical dense SLAM system are non-differentiable. In this work, we propose ∇SLAM (gradSLAM), a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM1.


Title: Accurate position tracking with a single UWB anchor
Key Words: inertial systems  Kalman filters  mobile robots  nonlinear filters  object tracking  observability  position measurement  SLAM (robots)  ultra wideband technology  velocity measurement  ultrawideband technology  UWB anchor  UWB range  moving robot tracking  position tracking  robotic applications  localization systems  optical tracking  9 DoF inertial measurement unit  UWB ranging source  UWB technology  robot speed estimation  orientation estimation  IMU sensor  observability  extended Kalman filter  EKF  robot pose estimation  Robot sensing systems  Estimation  Observability  Velocity measurement  Distance measurement  Mobile robots 
Abstract: Accurate localization and tracking are a fundamental requirement for robotic applications. Localization systems like GPS, optical tracking, simultaneous localization and mapping (SLAM) are used for daily life activities, research, and commercial applications. Ultra-wideband (UWB) technology provides another venue to accurately locate devices both indoors and outdoors. In this paper, we study a localization solution with a single UWB anchor, instead of the traditional multi-anchor setup. Besides the challenge of a single UWB ranging source, the only other sensor we require is a low-cost 9 DoF inertial measurement unit (IMU). Under such a configuration, we propose continuous monitoring of UWB range changes to estimate the robot speed when moving on a line. Combining speed estimation with orientation estimation from the IMU sensor, the system becomes temporally observable. We use an Extended Kalman Filter (EKF) to estimate the pose of a robot. With our solution, we can effectively correct the accumulated error and maintain accurate tracking of a moving robot.


Title: DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans
Key Words: Bayes methods  control engineering computing  convolutional neural nets  filtering theory  image segmentation  image sequences  learning (artificial intelligence)  mobile robots  neural net architecture  object detection  optical radar  path planning  recursive estimation  robot vision  SLAM (robots)  DeepTemporalSeg  temporally consistent semantic segmentation  3d LiDAR scans  semantic characteristics  autonomous robot operation  deep convolutional neural network  DCNN  LiDAR scan  pedestrian  bicyclist  dense blocks  depth separable convolutions  current semantic state  recursive estimation  isolated erroneous predictions  neural network architectures  Bayes filter approach  KITTI tracking benchmark  Semantics  Laser radar  Image segmentation  Task analysis  Three-dimensional displays  Neural networks  Automobiles 
Abstract: Understanding the semantic characteristics of the environment is a key enabler for autonomous robot operation. In this paper, we propose a deep convolutional neural network (DCNN) for semantic segmentation of a LiDAR scan into the classes car, pedestrian and bicyclist. This architecture is based on dense blocks and efficiently utilizes depth separable convolutions to limit the number of parameters while still maintaining the state-of-the-art performance. To make the predictions from the DCNN temporally consistent, we propose a Bayes filter based method. This method uses the predictions from the neural network to recursively estimate the current semantic state of a point in a scan. This recursive estimation uses the knowledge gained from previous scans, thereby making the predictions temporally consistent and robust towards isolated erroneous predictions. We compare the performance of our proposed architecture with other state-of-the-art neural network architectures and report substantial improvement. For the proposed Bayes filter approach, we shows results on various sequences in the KITTI tracking benchmark.


Title: Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors
Key Words: groupware  image fusion  mobile robots  multi-robot systems  path planning  sensor fusion  SLAM (robots)  dynamic collaborative mapping  multimodal environmental perception  heterogeneous sensor fusion model  local 3D maps  night rainforest  3D map fusion missions  multimodal sensors  long-term operation  collaborative robots  dynamic environment  dynamic objects  Collaboration  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Robot vision systems 
Abstract: Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.


Title: A GNC Architecture for Planetary Rovers with Autonomous Navigation
Key Words: aerospace navigation  aerospace robotics  distance measurement  Mars  mobile robots  optimal control  path planning  planetary rovers  robot vision  SLAM (robots)  trajectory control  GNC architecture  planetary rovers  autonomous navigation  Mars exploration missions  sample fetching rover  autonomous capabilities  two-level architecture  terrain  local path replanning  trajectory control  global localization  planetary exploration  planetary analog field test campaigns  guidance navigation and control architecture  hazard detection  visual odometry  adaptive SLAM algorithm  optimal path planning  Hazards  Navigation  Space vehicles  Autonomous robots  Computer architecture  Cameras  Trajectory 
Abstract: This paper proposes a Guidance, Navigation, and Control (GNC) architecture for planetary rovers targeting the conditions of upcoming Mars exploration missions such as Mars 2020 and the Sample Fetching Rover (SFR). The navigation requirements of these missions demand a control architecture featuring autonomous capabilities to achieve a fast and long traverse. The proposed solution presents a two-level architecture where the efficient navigation (lower) level is always active and the full navigation (upper) level is enabled according to the difficulty of the terrain. The first level is an efficient implementation of the basic functionalities for autonomous navigation based on hazard detection, local path replanning, and trajectory control with visual odometry. The second level implements an adaptive SLAM algorithm that improves the relative localization, evaluates the traversability of the terrain ahead for a more optimal path planning, and performs global (absolute) localization that corrects the pose drift during longer traverses. The architecture provides a solution for long-range, low supervision, and fast planetary exploration. Both navigation levels have been validated on planetary analog field test campaigns.


Title: Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms
Key Words: graph theory  least squares approximations  mobile robots  normal distribution  robot vision  SLAM (robots)  occupancy grid map  graph-based SLAM  occupancy normal distribution transforms  normal distributions transforms  simultaneous localization and mapping  mobile robotics  least squares problem  nonlinear optimizers  global NDT scan matcher  Simultaneous localization and mapping  Cost function  Google  Jacobian matrices  Gaussian distribution 
Abstract: Simultaneous Localization and Mapping (SLAM) is one of the basic problems in mobile robotics. While most approaches are based on occupancy grid maps, Normal Distributions Transforms (NDT) and mixtures like Occupancy Normal Distribution Transforms (ONDT) have been shown to represent sensor measurements more accurately. In this work, we slightly re-formulate the (O)NDT matching function such that it becomes a least squares problem that can be solved with various robust numerical and analytical non-linear optimizers. Further, we propose a novel global (O)NDT scan matcher for loop closure. In our evaluation, our NDT and ONDT methods are able to outperform the occupancy grid map based ones we adopted from Google's Cartographer implementation.


Title: Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV
Key Words: distance measurement  mobile robots  optical radar  path planning  robot vision  SLAM (robots)  FoV  autonomous vehicles  autonomous navigation  path planning  LOAM algorithm  LiDAR odometry and mapping  robot pose localization  Laser radar  Feature extraction  Three-dimensional displays  Measurement by laser beam  Laser noise  Real-time systems  Spinning 
Abstract: LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot's pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.


Title: Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration
Key Words: graph theory  mobile robots  navigation  path planning  robot vision  SLAM (robots)  underwater volumetric exploration  active SLAM framework  3D underwater environments  multibeam sonar  integrated SLAM  volumetric free-space information  informative loop closures  navigation policy  3D visual dictionary  submap saliency  sensor information  pose-graph SLAM formulation  global occupancy grid map  uncertainty-agnostic framework  Simultaneous localization and mapping  Three-dimensional displays  Uncertainty  Conferences  Automation  Sonar  Planning 
Abstract: In this paper, we present an active SLAM framework for volumetric exploration of 3D underwater environments with multibeam sonar. Recent work in integrated SLAM and planning performs localization while maintaining volumetric free-space information. However, an absence of informative loop closures can lead to imperfect maps, and therefore unsafe behavior. To solve this, we propose a navigation policy that reduces vehicle pose uncertainty by balancing between volumetric exploration and revisitation. To identify locations to revisit, we build a 3D visual dictionary from real-world sonar data and compute a metric of submap saliency. Revisit actions are chosen based on propagated pose uncertainty and sensor information gain. Loop closures are integrated as constraints in our pose-graph SLAM formulation and these deform the global occupancy grid map. We evaluate our performance in simulation and real-world experiments, and highlight the advantages over an uncertainty-agnostic framework.


Title: Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM
Key Words: mobile robots  path planning  pose estimation  robot vision  service robots  SLAM (robots)  simultaneous localization and mapping  data sequences  robotic autonomy  service robots  real-world indoor scenes  OpenLORIS-Scene datasets  SLAM problems  pose estimation  Simultaneous localization and mapping  Robot kinematics  Cameras  Synchronization  Trajectory 
Abstract: Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.


Title: Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks
Key Words: cameras  Global Positioning System  infrared detectors  mobile robots  optical radar  SLAM (robots)  WUXGA cameras  3D LiDAR  inertial measurement unit  infrared camera  differential RTK GNSS receiver  centimetre accuracy  public dataset  submillisecond precision  autonomous driving  Brno Urban dataset  self-driving agents  mapping tasks  Brno-Czech Republic  https://github.com/RoboticsBUT/Brno-Urban-Dataset  Cameras  Sensors  Global Positioning System  Global navigation satellite system  Receivers  Laser radar  Synchronization 
Abstract: Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https://github.com/RoboticsBUT/Brno-Urban-Dataset.


Title: To Learn or Not to Learn: Visual Localization from Essential Matrices
Key Words: cameras  feature extraction  image representation  learning (artificial intelligence)  neural nets  pose estimation  robot vision  SLAM (robots)  visual localization  scene-specific representations  relative pose estimation  feature-based approach  deep learning  camera  autonomous robots  Cameras  Visualization  Three-dimensional displays  Pipelines  Pose estimation  Image retrieval 
Abstract: Visual localization is the problem of estimating a camera within a scene and a key technology for autonomous robots. State-of-the-art approaches for accurate visual localization use scene-specific representations, resulting in the overhead of constructing these models when applying the techniques to new scenes. Recently, learned approaches based on relative pose estimation have been proposed, carrying the promise of easily adapting to new scenes. However, they are currently significantly less accurate than state-of-the-art approaches. In this paper, we are interested in analyzing this behavior. To this end, we propose a novel framework for visual localization from relative poses. Using a classical feature-based approach within this framework, we show state-of-the-art performance. Replacing the classical approach with learned alternatives at various levels, we then identify the reasons for why deep learned approaches do not perform well. Based on our analysis, we make recommendations for future work.


Title: Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter
Key Words: aerospace robotics  microrobots  mobile robots  multi-robot systems  pose estimation  probability  robot vision  SLAM (robots)  target tracking  multiMAV system  robot team  vision based detection  distance measurements  coupled probabilistic data association filter  nonlinear measurements  visual based robot to robot detection  vision based multiMAV localization  robot localization  robot pose estimation  multiple microaerial vehicles  Robot kinematics  Robot sensing systems  Noise measurement  Probabilistic logic  Task analysis  Cameras 
Abstract: We address the localization of robots in a multi-MAV system where external infrastructure like GPS or motion capture systems may not be available. Our approach lends itself to implementation on platforms with several constraints on size, weight, and power (SWaP). Particularly, our framework fuses the onboard VIO with the anonymous, visual-based robot-to-robot detection to estimate all robot poses in one common frame, addressing three main challenges: 1) the initial configuration of the robot team is unknown, 2) the data association between each vision-based detection and robot targets is unknown, and 3) the vision-based detection yields false negatives, false positives, inaccurate, and provides noisy bearing, distance measurements of other robots. Our approach extends the Coupled Probabilistic Data Association Filter [1] to cope with nonlinear measurements. We demonstrate the superior performance of our approach over a simple VIO-based method in a simulation with the measurement models statistically modeled using the real experimental data. We also show how onboard sensing, estimation, and control can be used for formation flight.


Title: Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features
Key Words: convolutional neural nets  image coding  image matching  image resolution  visual databases  image resolution  VGG16 CNN architecture  matching CNN features  query image  VGG16 Convolutional Neural Network architecture  Spatial Matching Visual Place Recognition  SSM-VPR  optimal image resolutions  Visualization  Robustness  Semantics  Task analysis  Histograms  Correlation  Simultaneous localization and mapping  Visual Place Recognition  Convolutional Neural Networks  SLAM  Loop Closure  Life-long Navigation 
Abstract: We revise, improve and extend the system previously introduced by us and named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition), largely boosting its performance above the current state of the art. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. It consists of two stages: given a query image of a place, (1) a list of candidates is selected from a database of places and (2) the candidates are geometrically compared with the query. The comparison is made by matching CNN features and, equally important, their spatial locations, selecting the best candidate as the recognized place. The performance of the system is maximized by finding optimal image resolutions during the second stage and by exploiting temporal correlation between consecutive frames in the employed datasets.


Title: Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure
Key Words: feature extraction  graph theory  image matching  image registration  image segmentation  learning (artificial intelligence)  legged locomotion  neural nets  optical radar  optimisation  pose estimation  robot vision  SLAM (robots)  stereo image processing  online LiDAR-SLAM  legged robot  robust registration  deep-learned loop closure  3D factor-graph LiDAR-SLAM system  industrial environments  point clouds  inertial-kinematic state estimator  ICP registration  loop proposal mechanism  deep learning method  odometry  loop closure factors  pose graph optimization  SLAM map  risk alignment prediction method  deeply learned feature-based loop closure detector  Laser radar  Legged locomotion  Three-dimensional displays  Simultaneous localization and mapping  Iterative closest point algorithm 
Abstract: In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.


Title: Voxel Map for Visual SLAM
Key Words: computer vision  feature extraction  image representation  image retrieval  SLAM (robots)  visual SLAM systems  camera field-of-view  voxel map representation  keyframe map  map points retrieval  simultaneous localization and mapping  Simultaneous localization and mapping  Three-dimensional displays  Cameras  Visualization  Cognition  Feature extraction  Task analysis 
Abstract: In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multi-camera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable.


Title: Visual Odometry Revisited: What Should Be Learnt?
Key Words: convolutional neural nets  distance measurement  geometry  learning (artificial intelligence)  pose estimation  SLAM (robots)  monocular visual odometry algorithm  geometry-based methods  monocular systems  scale-drift issue  deep learning  epipolar geometry  perspective-n-point method  frame-to-frame VO algorithm  DF-VO  scale consistent single-view depth CNN  Cameras  Adaptive optics  Geometry  Optical imaging  Machine learning  Estimation  Two dimensional displays 
Abstract: In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue. Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system. Code is available at here: DF-VO.


Title: Global visual localization in LiDAR-maps through shared 2D-3D embedding space
Key Words: image recognition  learning (artificial intelligence)  mobile robots  neural nets  optical radar  robot vision  SLAM (robots)  global visual localization  LiDAR-maps  place recognition  autonomous driving field  vision-based approaches  image database  high definition 3D maps  deep neural network  shared embedding space  3D-LiDAR place recognition  3D DNN  2D-3D embedding space  Oxford Robotcar Dataset  image w.r.t.  Three-dimensional displays  Task analysis  Laser radar  Feature extraction  Visualization  Robots  Two dimensional displays 
Abstract: Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.


Title: Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments
Key Words: Gaussian processes  learning (artificial intelligence)  mobile robots  Monte Carlo methods  neural nets  optical radar  path planning  recursive estimation  robot vision  SLAM (robots)  precise lidar-based robot localisation  large-scale environments  global localisation  mobile robots  Monte Carlo Localisation  MCL  fast localisation system  deep-probabilistic model  Gaussian process regression  deep kernel  precise recursive estimator  Gaussian method  deep probabilistic localisation  large-scale localisation  largescale environment  time 0.8 s  size 0.75 m  Robots  Neural networks  Three-dimensional displays  Gaussian processes  Laser radar  Monte Carlo methods  Kernel 
Abstract: This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deeplearned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and nonGaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a largescale environment of approximately 0.5 km2.


Title: Set-membership state estimation by solving data association
Key Words: mobile robots  position control  robot vision  sensor fusion  SLAM (robots)  state estimation  deterministic approach  data association  underwater robot  sonar data  membership state estimation  localization problem  indistinguishable landmarks  diving phase  unknown initial position  Sonar  State estimation  Rocks  Trajectory  Robot sensing systems  Reliability 
Abstract: This paper deals with the localization problem of a robot in an environment made of indistinguishable landmarks, and assuming the initial position of the vehicle is unknown. This scenario is typically encountered in underwater applications for which landmarks such as rocks all look alike. Furthermore, the position of the robot may be lost during a diving phase, which obliges us to consider unknown initial position. We propose a deterministic approach to solve simultaneously the problems of data association and state estimation, without combinatorial explosion. The efficiency of the method is shown on an actual experiment involving an underwater robot and sonar data.


Title: OpenVINS: A Research Platform for Visual-Inertial Estimation
Key Words: calibration  cameras  estimation theory  image filtering  Kalman filters  robot vision  SLAM (robots)  research platform  visual-inertial estimation research  open sourced codebase  visual-inertial systems  visual-inertial estimation features  on-manifold sliding window Kalman filter  consistent First-Estimates Jacobian treatments  modular type system  extendable visual-inertial system simulator  competing estimation performance  OpenVINS  online camera intrinsic calibration  open sourced algorithms  online camera extrinsic calibration  inertial sensor time offset calibration  SLAM landmarks  state management  Cameras  Current measurement  Jacobian matrices  Calibration  Documentation  Estimation  Robot sensing systems 
Abstract: In this paper, we present an open platform, termed OpenVINS, for visual-inertial estimation research for both the academic community and practitioners from industry. The open sourced codebase provides a foundation for researchers and engineers to quickly start developing new capabilities for their visual-inertial systems. This codebase has out of the box support for commonly desired visual-inertial estimation features, which include: (i) on-manifold sliding window Kalman filter, (ii) online camera intrinsic and extrinsic calibration, (iii) camera to inertial sensor time offset calibration, (iv) SLAM landmarks with different representations and consistent First-Estimates Jacobian (FEJ) treatments, (v) modular type system for state management, (vi) extendable visual-inertial system simulator, and (vii) extensive toolbox for algorithm evaluation. Moreover, we have also focused on detailed documentation and theoretical derivations to support rapid development and research, which are greatly lacked in the current open sourced algorithms. Finally, we perform comprehensive validation of the proposed OpenVINS against state-of-the-art open sourced algorithms, showing its competing estimation performance.


Title: Analytic Combined IMU Integration (ACI2) For Visual Inertial Navigation
Key Words: calibration  inertial navigation  maximum likelihood estimation  Monte Carlo methods  optimisation  robot vision  sensor fusion  SLAM (robots)  analytic combined IMU integration  visual inertial navigation  batch optimization  visual sensor fusion  robotic tasks  maximum likelihood estimation  partial-fixed estimates  ACI2  inertial measurement unit  Monte-Carlo simulations  Optimization  Jacobian matrices  Maximum likelihood estimation  Time measurement  Cameras  Visualization  Calibration 
Abstract: Batch optimization based inertial measurement unit (IMU) and visual sensor fusion enables high rate localization for many robotic tasks. However, it remains a challenge to ensure that the batch optimization is computationally efficient while being consistent for high rate IMU measurements without marginalization. In this paper, we derive inspiration from maximum likelihood estimation with partial-fixed estimates to provide a unified approach for handing both IMU preintegration and time-offset calibration. We present a modularized analytic combined IMU integrator (ACI2) with elegant derivations for IMU integrations, bias Jabcobians and related covariances. To simplify our derivation, we also prove that the right Jacobians for Hamilton quaterions and SO(3) are equivalent. Finally, we present a time offset calibrator that operates by fixing the linearization point for a given time offset. This reduces re-integration of the IMU measurements and thus improve efficiency. The proposed ACI2 and time-offset calibration is verified by intensive Monte-Carlo simulations generated from real world datasets. A proof-of-concept real world experiment is also conducted to verify the proposed ACI2 estimator.


Title: Fast Local Planning and Mapping in Unknown Off-Road Terrain
Key Words: collision avoidance  graph theory  mobile robots  motion control  remotely operated vehicles  SLAM (robots)  trajectory control  off-road terrain  on-line mapping  planning solution  obstacle detection  terrain gradient map  simple cost map  adaptable cost map  optimal paths  control input space  kinematic forward simulation  generated feasible trajectories  optimal trajectory  time operation  frequency 10.0 Hz  frequency 30.0 Hz  Trajectory  Robots  Planning  Aerospace electronics  Microsoft Windows  Three-dimensional displays  Real-time systems 
Abstract: In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.


Title: AC/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM
Key Words: calibration  cameras  sensitivity analysis  SLAM (robots)  calibration parameters  calibration sensitivity analysis  joint angle noise  joint angle values  calibration code  dynamic camera clusters  visual SLAM  time-varying set  extrinsic calibration transformations  DCC calibration accuracy  configuration space  pixel re-projection error  fiducial target  dynamic camera cluster  pose-loop error optimization  Cameras  Calibration  Robot vision systems  Simultaneous localization and mapping  Vehicle dynamics  Optimization  Measurement uncertainty 
Abstract: In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https://github.com/TRAILab/AC-DCC.


Title: Error estimation and correction in a spiking neural network for map formation in neuromorphic hardware
Key Words: error correction  mobile robots  neural chips  neural nets  path planning  pose estimation  SLAM (robots)  error correction  SNN mechanism  neuromorphic device  form-factor neuromorphic chip  spiking neural network  map formation  neuromorphic hardware  neural networks  robot control  error estimation  simultaneous localization and mapping  robot pose estimation  SNN-based SLAM  path integration speed  Neurons  Robots  Sociology  Statistics  Light emitting diodes  Neuromorphics  Synapses 
Abstract: Neuromorphic hardware offers computing platforms for the efficient implementation of spiking neural networks (SNNs) that can be used for robot control. Here, we present such an SNN on a neuromorphic chip that solves a number of tasks related to simultaneous localization and mapping (SLAM): forming a map of an unknown environment and, at the same time, estimating the robot's pose. In particular, we present an SNN mechanism to detect and estimate errors when the robot revisits a known landmark and updates both the map and the path integration speed to reduce the error. The whole system is fully realized in a neuromorphic device, showing the feasibility of a purely SNN-based SLAM, which could be efficiently implemented in a small form-factor neuromorphic chip.


Title: GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization
Key Words: cameras  optimisation  pose estimation  robot vision  SLAM (robots)  GPO  global plane optimization  homography estimation  homography decomposition  monocular SLAM initialization  monocular simultaneous localization and mapping problem  camera poses  chessboard dataset  Cameras  Simultaneous localization and mapping  Optimization  Matrix decomposition  Transmission line matrix methods  Estimation  Three-dimensional displays 
Abstract: Initialization is essential to monocular Simultaneous Localization and Mapping (SLAM) problems. This paper focuses on a novel initialization method for monocular SLAM based on planar features. The algorithm starts by homography estimation in a sliding window. It then proceeds to a global plane optimization (GPO) to obtain camera poses and the plane normal. 3D points can be recovered using planar constraints without triangulation. The proposed method fully exploits the plane information from multiple frames and avoids the ambiguities in homography decomposition. We validate our algorithm on the collected chessboard dataset against baseline implementations and present extensive analysis. Experimental results show that our method outperforms the ne-tuned baselines in both accuracy and real-time.


Title: Topological Mapping for Manhattan-like Repetitive Environments
Key Words: convolutional neural nets  graph theory  image representation  optimisation  SLAM (robots)  topology  Manhattan properties  topological graph  unoptimized Pose Graph  topological Manhattan relations  ground-truth Pose Graph  real-world indoor warehouse scenes  Manhattan-like repetitive environments  topological mapping framework  neighbouring nodes  indoor warehouse setting  warehouse topological construct  deep convolutional network  Siamese-style neural network  backend pose graph optimization framework  Manhattan graph aided loop closure relations  Topology  Network topology  Simultaneous localization and mapping  Neural networks  Optimization 
Abstract: We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework.


Title: Robust RGB-D Camera Tracking using Optimal Key-frame Selection
Key Words: cameras  image colour analysis  image motion analysis  image reconstruction  image sequences  integer programming  interpolation  iterative methods  motion estimation  SLAM (robots)  optimal key-frame selection  integer programming  VO method  camera motion  elastic-fusion  discontinuous camera motions  robust RGB-D camera tracking  adaptive visual odometry  TUM benchmark sequences  camera trajectory errors  iterative closed point  Cameras  Robustness  Optimization  Tracking  Iterative closest point algorithm  Robot vision systems  Three-dimensional displays 
Abstract: We propose a novel RGB-D camera tracking system that robustly reconstructs hand-held RGB-D camera sequences. The robustness of our system is achieved by two independent features of our method: adaptive visual odometry (VO) and integer programming-based key-frame selection. Our VO method adaptively interpolates the camera motion results of the direct VO (DVO) and the iterative closed point (ICP) to yield more optimal results than existing methods such as Elastic-Fusion. Moreover, our key-frame selection method locates globally optimum key-frames using a comprehensive objective function in a deterministic manner rather than heuristic or experience-based rules that prior methods mostly rely on. As a result, our method can complete reconstruction even if the camera fails to be tracked due to discontinuous camera motions, such as kidnap events, when conventional systems need to backtrack the scene. We validated our tracking system on 25 TUM benchmark sequences against state-of-the-art works, such as ORBSLAM2, Elastic-Fusion, and DVO SLAM, and experimentally showed that our method has smaller and more robust camera trajectory errors than these systems.


Title: Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM
Key Words: data visualisation  image representation  learning (artificial intelligence)  mobile robots  SLAM (robots)  view-based labelling  spatial AI systems  real-time height map fusion  map-based labelling  generated scene model  input view-wise data  estimate labels  clear groups  labelling scenes  semantic labels  geometric models  persistent scene representations  real-time SLAM  map-based semantic labelling  Labeling  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Cameras  Image reconstruction  Real-time systems 
Abstract: Generally capable Spatial AI systems must build persistent scene representations where geometric models are combined with meaningful semantic labels. The many approaches to labelling scenes can be divided into two clear groups: view-based which estimate labels from the input view-wise data and then incrementally fuse them into the scene model as it is built; and map-based which label the generated scene model. However, there has so far been no attempt to quantitatively compare view-based and map-based labelling. Here, we present an experimental framework and comparison which uses real-time height map fusion as an accessible platform for a fair comparison, opening up the route to further systematic research in this area.


Title: Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model
Key Words: control engineering computing  laser ranging  mobile robots  SLAM (robots)  solid modelling  Gazebo world construction  grayscale image  3D solid model  robot simulators  simulated physical environment  2D image  2D laser range finder data  Gazebo simulator  3D Collada  simultaneous localization and mapping  real-time factor  SLAM missions  RTF  Tools  Solid modeling  Three-dimensional displays  Robot sensing systems  Gray-scale  Collision avoidance 
Abstract: Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.


Title: Practical Persistence Reasoning in Visual SLAM
Key Words: mobile robots  robot vision  SLAM (robots)  static environments  dynamic environments  persistence filters  ORB-SLAM  visual SLAM algorithm  persistence filtering  persistence reasoning  semistatic environments  Simultaneous localization and mapping  Visualization  Estimation  Cognition  Probability  Feature extraction 
Abstract: Many existing SLAM approaches rely on the assumption of static environments for accurate performance. However, several robot applications require them to traverse repeatedly in semi-static or dynamic environments. There has been some recent research interest in designing persistence filters to reason about persistence in such scenarios. Our goal in this work is to incorporate such persistence reasoning in visual SLAM. To this end, we incorporate persistence filters [1] into ORB-SLAM, a well-known visual SLAM algorithm. We observe that the simple integration of their proposal results in inefficient persistence reasoning. Through a series of modifications and using two locally collected datasets, we demonstrate the utility of such persistence filtering as well as our customizations in ORB-SLAM. Overall, incorporating persistence filtering could result in a significant reduction in map size (about 30% in the best case) and a corresponding reduction in run-time while retaining similar accuracy to methods that use much larger maps.


Title: FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow
Key Words: cameras  image colour analysis  image motion analysis  image reconstruction  image segmentation  image sequences  mobile robots  motion estimation  robot vision  SLAM (robots)  dynamic environments  visual SLAM  moving objects  static environment features  lead  wrong camera motion estimation  dense RGB-D SLAM solution  camera ego-motion estimation  static background reconstructions  optical flow residuals  dynamic semantics  RGB-D point clouds  camera tracking  background reconstruction  dense reconstruction results  dynamic scenes  static environments  dynamic dense RGB-D SLAM  Cameras  Dynamics  Optical imaging  Simultaneous localization and mapping  Three-dimensional displays  Two dimensional displays  Robustness 
Abstract: Dynamic environments are challenging for visual SLAM since the moving objects occlude the static environment features and lead to wrong camera motion estimation. In this paper, we present a novel dense RGB-D SLAM solution that simultaneously accomplishes the dynamic/static segmentation and camera ego-motion estimation as well as the static background reconstructions. Our novelty is using optical flow residuals to highlight the dynamic semantics in the RGB-D point clouds and provide more accurate and efficient dynamic/static segmentation for camera tracking and background reconstruction. The dense reconstruction results on public datasets and real dynamic scenes indicate that the proposed approach achieved accurate and efficient performances in both dynamic and static environments compared to state-of-the-art approaches.


Title: The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles
Key Words: cameras  collision avoidance  image sensors  mobile robots  navigation  robot vision  SLAM (robots)  space vehicles  autonomous microaerial vehicle  autonomous Tiercel robots  collision detector design  fisheye camera  reflective obstacles  transparent obstacles  collision-resilient robot  Tiercel MAV  autonomous navigation  autonomous flight  Collision avoidance  Cameras  Robot vision systems  Planning 
Abstract: Autonomous flight through unknown environments in the presence of obstacles is a challenging problem for micro aerial vehicles (MAVs). A majority of the current state-of-art research assumes obstacles as opaque objects that can be easily sensed by optical sensors such as cameras or LiDARs. However in indoor environments with glass walls and windows, or scenarios with smoke and dust, robots (even birds) have a difficult time navigating through the unknown space.In this paper, we present the design of a new class of micro aerial vehicles that achieves autonomous navigation and are robust to collisions. In particular, we present the Tiercel MAV: a small, agile, light weight and collision-resilient robot powered by a cellphone grade CPU. Our design exploits contact to infer the presence of transparent or reflective obstacles like glass walls, integrating touch with visual perception for SLAM. The Tiercel is able to localize using visual-inertial odometry (VIO) running on board the robot with a single downward facing fisheye camera and an IMU. We show how our collision detector design and experimental set up enable us to characterize the impact of collisions on VIO. We further develop a planning strategy to enable the Tiercel to fly autonomously in an unknown space, sustaining collisions and creating a 2D map of the environment. Finally we demonstrate a swarm of three autonomous Tiercel robots safely navigating and colliding through an obstacle field to reach their objectives.


Title: Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance
Key Words: autonomous aerial vehicles  collision avoidance  control engineering computing  image motion analysis  image sequences  learning (artificial intelligence)  mobile robots  neural nets  object detection  robot vision  SLAM (robots)  visual perception  deep reinforcement learning  UAV obstacle avoidance  learning-based reaction local planner  microUAVs  image moment  illuminance variation  mapless navigation  moment-based LGMD  bioinspired monocular vision perception method  Navigation  Collision avoidance  Robustness  Lighting  Robots  Optical imaging  Machine learning 
Abstract: In this paper, a bio-inspired monocular vision perception method combined with a learning-based reaction local planner for obstacle avoidance of micro UAVs is presented. The system is more computationally efficient than other vision-based perception and navigation methods such as SLAM and optical flow because it does not need to calculate accurate distances. To improve the robustness of perception against illuminance change, the input image is remapped using image moment which is independent of illuminance variation. After perception, a local planner is trained using deep reinforcement learning for mapless navigation. The proposed perception and navigation methods are evaluated in some realistic simulation environments. The result shows that this light-weight monocular perception and navigation system works well in different complex environments without accurate depth information.


Title: View-Invariant Loop Closure with Oriented Semantic Landmarks
Key Words: geometry  pose estimation  robot vision  SLAM (robots)  view-invariant loop closure  oriented semantic landmarks  simultaneous localization and mapping  monocular semantic SLAM system  object identity  inter-object geometry  view-invariant loop detection  ORB-SLAM  local appearance-based features  indoor scenes  object orientation estimation  geometrical detailed semantic maps  object translation  object scale  Cameras  Simultaneous localization and mapping  Semantics  Trajectory  Layout  Robustness  Estimation 
Abstract: Recent work on semantic simultaneous localization and mapping (SLAM) have shown the utility of natural objects as landmarks for improving localization accuracy and robustness. In this paper we present a monocular semantic SLAM system that uses object identity and inter-object geometry for view-invariant loop detection and drift correction. Our system's ability to recognize an area of the scene even under large changes in viewing direction allows it to surpass the mapping accuracy of ORB-SLAM, which uses only local appearance-based features that are not robust to large viewpoint changes. Experiments on real indoor scenes show that our method achieves mean drift reduction of 70% when compared directly to ORB-SLAM. Additionally, we propose a method for object orientation estimation, where we leverage the tracked pose of a moving camera under the SLAM setting to overcome ambiguities caused by object symmetry. This allows our SLAM system to produce geometrically detailed semantic maps with object orientation, translation, and scale.


Title: A Flexible Method for Performance Evaluation of Robot Localization
Key Words: image motion analysis  mobile robots  path planning  pose estimation  robot vision  SLAM (robots)  robot localization  research issue  mobile robotics  performance assessment  robot SLAM algorithms  localization accuracy  SLAM algorithm  benchmark datasets  motion capture  environment-specific  spatial coverage  SLAM performance evaluation  distinctive markers  robot navigation environment  generative latent optimization problem  local robot-to-marker  global robot  Simultaneous localization and mapping  Navigation  Cameras  Performance evaluation  Robot localization 
Abstract: An important research issue in mobile robotics is performance assessment of robot SLAM algorithms in terms of their localization accuracy. Typically, SLAM algorithms are evaluated with the help of benchmark datasets or expensive equipment such as motion capture. Benchmark datasets however, are environment-specific, and use of motion capture constrains spatial coverage and affordability. In this paper, we present a novel method for SLAM performance evaluation, which only uses distinctive markers (such as AR tags), randomly placed in the robot navigation environment at arbitrary locations, and observes these markers with a camera onboard of the robot. Formulated as a generative latent optimization (GLO) problem, our method uses the local robot-to-marker poses to evaluate the global robot pose estimates by a SLAM algorithm and therefore its performance. Through extensive experiments on two robots, three localization/SLAM algorithms and both LiDAR and RGB-D sensors, we demonstrate the feasibility and accuracy of our proposed method.


Title: Flydar: Magnetometer-based High Angular Rate Estimation during Gyro Saturation for SLAM
Key Words: gyroscopes  Kalman filters  magnetometers  mobile robots  nonlinear filters  optical radar  SLAM (robots)  Flydar  magnetometer-based high angular rate estimation  SLAM  simultaneous localisation and mapping  Flying Li-DAR  EKF-based algorithm  sinusoidal magnetometer measurement  continuously rotating airframe  IMU sensors  gyro measurement  gyro bias  gyro saturation condition  rotating locomotion  robot hovering angular velocity  Robots  Magnetometers  Sensors  Estimation  Frequency measurement  Saturation magnetization  Angular velocity 
Abstract: In this paper, the high angular rate estimation for simultaneous localisation and mapping (SLAM) of a Flying Li-DAR (Flydar) is presented. The proposed EKF-based algorithm exploits the sinusoidal magnetometer measurement generated by the continuously rotating airframe for estimation of the robot hovering angular velocity. Significantly, the proposed method does not rely on additional sensors other than existing IMU sensors already being used for flight stabilization. The gyro measurement and the gyro bias are incorporated as a control input and a filter state respectively to enable estimation even under gyro saturation condition. Additionally, this work proposes leveraging on the inherently rotating locomotion to generate a planar lidar scan using only a single-point laser for possible lightweight autonomy. The proposed estimation method was experimentally evaluated on a ground rotating rig up to twice the gyro saturation limit with an effective rms error of 0.0045Hz; and on the proposed aerial platform - Flydar - hovering beyond the saturation limit with a rms error of 0.0056Hz. Lastly, the proposed method for SLAM using the rotating dynamics of Flydar was demonstrated with a localisation accuracy of 0.11m.


Title: An Efficient and Continuous Approach to Information-Theoretic Exploration
Key Words: computational complexity  information theory  mobile robots  path planning  robot vision  SLAM (robots)  information-theoretic exploration  continuous occupancy map framework  |Θ| measurement beams  recursive structure  robotics applications  autonomous navigation task  Robot sensing systems  Mutual information  Distortion measurement  Gain measurement  Time measurement 
Abstract: Exploration of unknown environments is embedded and essential in many robotics applications. Traditional algorithms, that decide where to explore by computing the expected information gain of an incomplete map from future sensor measurements, are limited to very powerful computational platforms. In this paper, we describe a novel approach for computing this expected information gain efficiently, as principally derived via mutual information. The key idea behind the proposed approach is a continuous occupancy map framework and the recursive structure it reveals. This structure makes it possible to compute the expected information gain of sensor measurements across an entire map much faster than computing each measurements' expected gain independently. Specifically, for an occupancy map composed of |M| cells and a range sensor that emits |Θ| measurement beams, the algorithm (titled FCMI) computes the information gain corresponding to measurements made at each cell in O(|Θ||M|) steps. To the best of our knowledge, this complexity bound is better than all existing methods for computing information gain. In our experiments, we observe that this novel, continuous approach is two orders of magnitude faster than the state-of-the-art FSMI algorithm.


Title: CAPRICORN: Communication Aware Place Recognition using Interpretable Constellations of Objects in Robot Networks
Key Words: feature extraction  image colour analysis  image matching  image representation  mobile robots  multi-robot systems  object detection  robot vision  SLAM (robots)  particular communication bandwidth  limited communication bandwidth  relative object positions  2step decentralized loop closure verification  compact semantic descriptors  bandwidth requirements  communication aware place recognition  interpretable constellations  robot networks  multiple robots  mapping environments  CAPRICORN  exploring environments  3D points  compact spatial descriptors  matching robots  geometric information  global image descriptors  TUM RGB-D SLAM sequence  Semantics  Three-dimensional displays  Simultaneous localization and mapping  Robustness  Visualization  Bandwidth 
Abstract: Using multiple robots for exploring and mapping environments can provide improved robustness and performance, but it can be difficult to implement. In particular, limited communication bandwidth is a considerable constraint when a robot needs to determine if it has visited a location that was previously explored by another robot, as it requires for robots to share descriptions of places they have visited. One way to compress this description is to use constellations, groups of 3D points that correspond to the estimate of a set of relative object positions. Constellations maintain the same pattern from different viewpoints and can be robust to illumination changes or dynamic elements. We present a method to extract from these constellations compact spatial and semantic descriptors of the objects in a scene. We use this representation in a 2step decentralized loop closure verification: first, we distribute the compact semantic descriptors to determine which other robots might have seen scenes with similar objects; then we query matching robots with the full constellation to validate the match using geometric information. The proposed method requires less memory, is more interpretable than global image descriptors, and could be useful for other tasks and interactions with the environment. We validate our system's performance on a TUM RGB-D SLAM sequence and show its benefits in terms of bandwidth requirements.


Title: Efficient two step optimization for large embedded deformation graph based SLAM
Key Words: computational complexity  embedded systems  graph theory  Hessian matrices  robot vision  SLAM (robots)  stereo image processing  parameter estimation  computation complexity  two step optimization  deformable geometry  stereo camera  SLAM applications  large scale embedded deformation graph  Hessian matrix  Simultaneous localization and mapping  Strain  Jacobian matrices  Optimization  Cameras  Deformable models  Geometry 
Abstract: Embedded deformation graph is a widely used technique in deformable geometry and graphical problems. Although the technique has been transmitted to stereo (or RGB-D) camera based SLAM applications, it remains challenging to compromise the computational cost as the model grows. In practice, the processing time grows rapidly in accordance with the expansion of maps. In this paper, we propose an approach to decouple the nodes of deformation graph in large scale dense deformable SLAM and keep the estimation time to be constant. We observe that only partial deformable nodes in the graph are connected to visible points. Based on this fact, the sparsity of the original Hessian matrix is utilized to split the parameter estimation into two independent steps. With this new technique, we achieve faster parameter estimation with amortized computation complexity reduced from O(n2) to almost O(1). As a result, the computational cost barely increases as the map keeps growing. Based on our strategy, the computational bottleneck in large scale embedded deformation graph based applications will be greatly mitigated. The effectiveness is validated by experiments, featuring large scale deformation scenarios.


Title: Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar
Key Words: distance measurement  feature extraction  image colour analysis  image sensors  image sequences  mobile robots  motion estimation  object recognition  object tracking  radar computing  robot vision  SLAM (robots)  supervised learning  predict robust keypoints  odometry estimation  metric localisation  self-supervised framework  differentiable point-based motion estimator  localisation error  Oxford Radar RobotCar Dataset  point-based radar odometry  Radar  Measurement  Task analysis  Estimation  Robot sensing systems  Computer architecture 
Abstract: This paper presents a self-supervised framework for learning to detect robust keypoints for odometry estimation and metric localisation in radar. By embedding a differentiable point-based motion estimator inside our architecture, we learn keypoint locations, scores and descriptors from localisation error alone. This approach avoids imposing any assumption on what makes a robust keypoint and crucially allows them to be optimised for our application. Furthermore the architecture is sensor agnostic and can be applied to most modalities. We run experiments on 280km of real world driving from the Oxford Radar RobotCar Dataset and improve on the state-of-the-art in point-based radar odometry, reducing errors by up to 45% whilst running an order of magnitude faster, simultaneously solving metric loop closures. Combining these outputs, we provide a framework capable of full mapping and localisation with radar in urban environments.


Title: Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas
Key Words: image registration  iterative methods  mobile robots  navigation  robot vision  SLAM (robots)  link-points  multiple indoor areas  outdoor areas  map quality  single map approaches  semantic map management approach  multiple maps  modular map structure  utilized SLAM method  laser scan data  appropriate SLAM configuration  single independent maps  appearance-based method  iterative closest point registration  point clouds  simultaneous localization and mapping configurations  Simultaneous localization and mapping  Lasers  Navigation  Feature extraction  Three-dimensional displays 
Abstract: This work presents a semantic map management approach for various environments by triggering multiple maps with different simultaneous localization and mapping (SLAM) configurations. A modular map structure allows to add, modify or delete maps without influencing other maps of different areas. The hierarchy level of our algorithm is above the utilized SLAM method. Evaluating laser scan data (e.g. the detection of passing a doorway) triggers a new map, automatically choosing the appropriate SLAM configuration from a manually predefined list. Single independent maps are connected by link-points, which are located in an overlapping zone of both maps, enabling global navigation over several maps. Loop- closures between maps are detected by an appearance-based method, using feature matching and iterative closest point (ICP) registration between point clouds. Based on the arrangement of maps and link-points, a topological graph is extracted for navigation purpose and tracking the global robot's position over several maps. Our approach is evaluated by mapping a university campus with multiple indoor and outdoor areas and abstracting a metrical-topological graph. It is compared to a single map running with different SLAM configurations. Our approach enhances the overall map quality compared to the single map approaches by automatically choosing predefined SLAM configurations for different environmental setups.


Title: Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments
Key Words: image representation  indoor navigation  mobile robots  path planning  robot vision  SLAM (robots)  stereo image processing  topology  indoor environments  topological global representations  3D dense submaps  hybrid global map  autonomous exploration  autonomous navigation  path planning  dense 3D maps  3D dense representations  3D dense mapping systems  hybrid topological mapping  metric 3D maps  standard CPU  Three-dimensional displays  Measurement  Robots  Semantics  Two dimensional displays  Indoor environments  Path planning 
Abstract: Robots require a detailed understanding of the 3D structure of the environment for autonomous navigation and path planning. A popular approach is to represent the environment using metric, dense 3D maps such as 3D occupancy grids. However, in large environments the computational power required for most state-of-the-art 3D dense mapping systems is compromising precision and real-time capability. In this work, we propose a novel mapping method that is able to build and maintain 3D dense representations for large indoor environments using standard CPUs. Topological global representations and 3D dense submaps are maintained as hybrid global map. Submaps are generated for every new visited place. A place (room) is identified as an isolated part of the environment connected to other parts through transit areas (doors). This semantic partitioning of the environment allows for a more efficient mapping and path-planning. We also propose a method for autonomous exploration that directly builds the hybrid representation in real time.We validate the real-time performance of our hybrid system on simulated and real environments regarding mapping and path-planning. The improvement in execution time and memory requirements upholds the contribution of the proposed work.


Title: Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints*
Key Words: computer vision  object detection  optimisation  path planning  pose estimation  robot vision  SLAM (robots)  lifted algorithm  combinatorial complexity  heuristic criterion  planar pose estimation  marker-based mapping  highly ambiguous inputs  PPE ambiguities  possible marker orientation solutions  rotation averaging formulation  marker corners  computer vision  planar markers  clique constraints  robust rotation averaging  marker pose ambiguity  Cameras  Pipelines  Machine-to-machine communications  Image edge detection  Pose estimation  Simultaneous localization and mapping  Histograms 
Abstract: Planar markers are useful in robotics and computer vision for mapping and localisation. Given a detected marker in an image, a frequent task is to estimate the 6DOF pose of the marker relative to the camera, which is an instance of planar pose estimation (PPE). Although there are mature techniques, PPE suffers from a fundamental ambiguity problem, in that there can be more than one plausible pose solutions for a PPE instance. Especially when localisation of the marker corners is noisy, it is often difficult to disambiguate the pose solutions based on reprojection error alone. Previous methods choose between the possible solutions using a heuristic criterion, or simply ignore ambiguous markers.We propose to resolve the ambiguities by examining the consistencies of a set of markers across multiple views. Our specific contributions include a novel rotation averaging formulation that incorporates long-range dependencies between possible marker orientation solutions that arise from PPE ambiguities. We analyse the combinatorial complexity of the problem, and develop a novel lifted algorithm to effectively resolve marker pose ambiguities, without discarding any marker observations. Results on real and synthetic data show that our method is able to handle highly ambiguous inputs, and provides more accurate and/or complete marker-based mapping and localisation.


Title: Pose-Estimate-Based Target Tracking for Human-Guided Remote Sensor Mounting with a UAV
Key Words: autonomous aerial vehicles  image sequences  pose estimation  SLAM (robots)  target tracking  pose-estimate-based target tracking  human-guided remote sensor mounting  autonomous aerial manipulation  unstructured environments  UAV localization  PBTT method  target point  fully on-board computation  RGB-D camera  downward-facing optical flow camera  horizontal localization  autonomous flight tests  interacting-boomcopter UAV platform  UAV position estimator  Target tracking  Cameras  Unmanned aerial vehicles  Visualization  Task analysis  Surface cleaning  Three-dimensional displays 
Abstract: In this paper, we present a method for pose-estimate-based target tracking (PBTT) that enables the performance of autonomous aerial manipulation operations in unstructured environments using fully on-board computation for both UAV localization and target tracking. The PBTT method does not depend on extracting traditional visual features (e.g. using SIFT, SURF, ORB, etc.) on or near the target. Instead, the algorithm combines input from an RGB-D camera and the UAV's position estimator (which utilizes a downward-facing optical flow camera for horizontal localization) to track a target point selected by a human operator. The effectiveness of the PBTT method is evaluated through several autonomous flight tests performed with the Interacting-Boomcopter (I-BC) UAV platform in unstructured environments and in the presence of light wind disturbances.


Title: Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps
Key Words: cameras  graph theory  image colour analysis  image sensors  mobile robots  normal distribution  robot vision  SLAM (robots)  camera origin  pose graph  NDT-OM  keyframe-based dense mapping  keyframe-based mapping system  RGB-D sensor  2D view-dependent structures  uncertainty model  RGB-D cameras  view-dependent local maps  normal distribution transform maps  global map  loop closure detection  autonomous robots  SLAM  Ellipsoids  Three-dimensional displays  Robot sensing systems  Cameras  Two dimensional displays  Uncertainty 
Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.


Title: Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping with Streaming Data
Key Words: Gaussian processes  mobile robots  path planning  regression analysis  robot vision  SLAM (robots)  sparse Gaussian process experts  implicit surface mapping  streaming data  creating maps  robotics  navigation  compact surface map  continuous implicit surface map  range data  approximate Gaussian process experts  GP models  model complexity  prediction error  real-world data sets  compact surface models  accurate implicit surface models  exact GP regression  subsampled data  Surface treatment  Data models  Predictive models  Covariance matrices  Gaussian processes  Computational modeling  Measurement uncertainty 
Abstract: Creating maps is an essential task in robotics and provides the basis for effective planning and navigation. In this paper, we learn a compact and continuous implicit surface map of an environment from a stream of range data with known poses. For this, we create and incrementally adjust an ensemble of approximate Gaussian process (GP) experts which are each responsible for a different part of the map. Instead of inserting all arriving data into the GP models, we greedily trade-off between model complexity and prediction error. Our algorithm therefore uses less resources on areas with few geometric features and more where the environment is rich in variety. We evaluate our approach on synthetic and real-world data sets and analyze sensitivity to parameters and measurement noise. The results show that we can learn compact and accurate implicit surface models under different conditions, with a performance comparable to or better than that of exact GP regression with subsampled data.


Title: DirectShape: Direct Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation
Key Words: image reconstruction  image segmentation  neural nets  object detection  pose estimation  shape recognition  stereo image processing  state-of-the-art deep learning based 3D object detectors  previous geometric approach  adaptive sparse point selection scheme  silhouette alignment term  dense stereo reconstruction  stereo image pair  3D rigid-body poses  3D bounding boxes  instance segmentations  simple bounding boxes  object level  autonomous driving  scene understanding  shape estimation  visual vehicle pose  shape priors  direct photometric alignment  Shape  Three-dimensional displays  Two dimensional displays  Automobiles  Current measurement  Solid modeling  Image reconstruction 
Abstract: Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.


